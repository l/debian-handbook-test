<?xml version='1.0' encoding='utf-8' ?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [
]>
<chapter id="advanced-administration" lang="ru-RU">
	<chapterinfo>
		 <keywordset>
			<keyword>RAID</keyword>
			 <keyword>LVM</keyword>
			 <keyword>FAI</keyword>
			 <keyword>Пресидинг</keyword>
			 <keyword>Мониторинг</keyword>
			 <keyword>Виртуализация</keyword>
			 <keyword>Xen</keyword>
			 <keyword>LXC</keyword>

		</keywordset>

	</chapterinfo>
	 <title>Углублённое администрирование</title>
	 <highlights> <para>
		Эта глава возвращается к некоторым аспектам, уже описанным ранее, но в другом ракурсе: вместо установки на одном компьютере мы изучим массовое разворачивание систем; вместо создания томов RAID или LVM во время установки мы научимся делать это вручную, чтобы иметь возможность пересмотреть наш изначальный выбор. Наконец, мы обсудим инструменты мониторинга и технологии виртуализации. Таким образом, эта глава предназначена главным образом для профессиональных администраторов и в несколько меньшей мере — для отдельных лиц, ответственных за свою домашнюю сеть.
	</para>
	 </highlights> <section id="sect.raid-and-lvm">
		<title>RAID и LVM</title>
		 <para>
			В <link linkend="installation">главе, посвящённой установке</link>, эти технологии были рассмотрены с точки зрения установщика и того, как они встроены в него, чтобы сделать начальное разворачивание максимально простым. После начальной установки администратор должен иметь возможность управляться с меняющимися потребностями в дисковом пространстве без необходимости прибегать к затратной переустановке. Поэтому ему необходимо освоить инструменты для настройки томов RAID и LVM.
		</para>
		 <para>
			И RAID, и LVM являются технологиями абстрагирования монтируемых томов от их физических эквивалентов (жёстких дисков или разделов на них); первая обеспечивает надёжность хранения данных, добавляя избыточность, а вторая делает управление данными более гибким и независимым от реального размера физических дисков. В обоих случаях система получает новые блочные устройства, которые могут использоваться для создания файловых систем или пространства подкачки без обязательного размещения их на одном физическом диске. RAID и LVM возникли из разных нужд, но их функциональность может в чём-то перекрываться, поэтому их часто и упоминают вместе.
		</para>
		 <sidebar> <title><emphasis>ПЕРСПЕКТИВА</emphasis> Btrfs сочетает LVM и RAID</title>
		 <para>
			В то время как LVM и RAID являются двумя отдельными подсистемами ядра, встраивающимися между дисковыми блочными устройствами и файловыми системами на них, <emphasis>btrfs</emphasis> — это новая файловая система, изначально разработанная в Oracle, целью которой является объединить функциональность LVM и RAID и дополнить её. Она в целом работоспособна, хотя до сих пор помечена как «экспериментальная», поскольку её разработка не завершена (некоторые возможности ещё не реализованы), она уже используется кое-где на производстве. <ulink type="block" url="http://btrfs.wiki.kernel.org/" />
		</para>
		 <para>
			Среди примечательных особенностей — возможность создания снимков дерева файловой системы в любой момент времени. Этот снимок изначально не занимает места на диске, данные копируются только при изменении одной из копий. Файловая система также обеспечивает прозрачное сжатие файлов, а контрольные суммы гарантируют сохранность всех записанных данных.
		</para>
		 </sidebar> <para>
			В случае и RAID, и LVM ядро предоставляет файл блочного устройства, сходный с соответствующими жёсткому диску или разделу. Когда приложению или другой части ядра требуется доступ к блоку такого устройства, надлежащая подсистема передаёт блок соответствующему физическому слою. В зависимости от конфигурации этот блок может быть сохранён на одном или нескольких физических дисках, и его физическое расположение может не прямо соотноситься с расположением блока в логическом устройстве.
		</para>
		 <section id="sect.raid-soft">
			<title>Программный RAID</title>
			 <indexterm>
				<primary>RAID</primary>
			</indexterm>
			 <para>
				RAID расшифровывается как <foreignphrase>Redundant Array of Independent Disks</foreignphrase> — избыточный массив независимых дисков. Цель этой системы — предотвратить потерю данных в случае сбоя жёсткого диска. Основной принцип прост: данные хранятся на нескольких физических дисках вместо одного, с настраиваемым уровнем избыточности, и даже в случае неожиданного выхода диска из строя данные могут быть без потерь восстановлены с остальных дисков.
			</para>
			 <sidebar> <title><emphasis>КУЛЬТУРА</emphasis> <foreignphrase>Independent</foreignphrase> или <foreignphrase>inexpensive</foreignphrase>?</title>
			 <para>
				I в аббревиатуре RAID изначально обозначала <emphasis>inexpensive</emphasis> — «недорогой», поскольку RAID позволял резко увеличить сохранность данных без необходимости инвестиций в дорогостоящие диски класса high-end. Возможно из соображений поддержания имиджа, однако, она сейчас чаще расшифровывается как <emphasis>independent</emphasis> — «независимый», что не имеет неприятного привкуса дешевизны.
			</para>
			 </sidebar> <para>
				RAID может быть реализован как в виде специального оборудования (модули RAID, встроенные в карты контроллеров SCSI или SATA), так и в виде программной абстракции (ядро). Как аппаратный, так и программный RAID с достаточной избыточностью может прозрачно продолжать работу, когда диск выходит из строя; верхние уровни стека (приложения) могут даже продолжать доступ к данным несмотря на сбой. Разумеется, такой «деградированный режим» может повлиять на производительность, а избыточность уменьшается, так что отказ следующего диска может привести к потере данных. На деле, однако, работать в этом деградированном режиме придётся лишь столько времени, сколько потребуется для замены отказавшего диска. Как только новый диск будет на месте, система RAID сможет восстановить необходимые данные для возврата в безопасный режим. Приложения не заметят ничего, кроме возможно снизившейся скорости доступа в то время, когда массив пребывает в деградированном состоянии, или на этапе восстановления.
			</para>
			 <para>
				Когда RAID реализован аппаратно, его настройка в общем случае производится с помощью инструмента настройки BIOS, и ядро принимает RAID-массив за отдельный диск, который будет работать как обычный физический диск, хотя его имя может быть другим (в зависимости от драйвера).
			</para>
			 <para>
				В этой книге мы сосредоточимся исключительно на программном RAID.
			</para>
			 <section id="sect.raid-levels">
				<title>Разные уровни RAID</title>
				 <para>
					RAID представляет собой не единую систему, а набор систем, различаемых по их уровням; уровни отличаются по схеме размещения данных и по степени избыточности. Более избыточный является более отказоустойчивым, поскольку система сможет продолжить работу с бо́льшим числом вышедших из строя дисков. С другой стороны, доступное пространство для того же набора дисков уменьшается; другими словами, для хранения того же объёма данных потребуется больше дисков.
				</para>
				 <variablelist>
					<varlistentry>
						<term>Linear RAID</term>
						 <listitem>
							<para>
								Хотя RAID-подсистема ядра позволяет создавать так называемый «linear RAID», собственно RAID он не является, поскольку не подразумевает какой-либо избыточности. Ядро просто объединяет несколько дисков «встык» и представляет получившийся том как один виртуальный диск (одно блочное устройство). Это единственное его назначение. Такая настройка редко используется сама по себе (об исключениях см. ниже), главным образом потому что отсутствие избыточности означает, что сбой одного диска делает всё объединение и, соответственно, все данные, недоступными.
							</para>

						</listitem>

					</varlistentry>
					 <varlistentry>
						<term>RAID-0</term>
						 <listitem>
							<para>
								Этот уровень также не обеспечивает избыточности, но диски не просто соединяются один за другим : они разделяются на <emphasis>полосы</emphasis>, и блоки виртуального устройства сохраняются на полосах физических дисков поочерёдно. В двухдисковом RAID-0, например, чётные блоки виртуального устройства будут сохраняться на первом физическом диске, а нечётные разместятся на втором физическом диске.
							</para>
							 <para>
								Целью такой системы является не повышение надёжности, поскольку (как и в случае с linear) доступность всех данных оказывается под угрозой, как только один из дисков отказывает, а увеличение производительности: при последовательном доступе к большому объёму непрерывных данных ядро сможет читать с обоих дисков (или производить запись на них) параллельно, что увеличит скорость передачи данных. Однако RAID-0 используется всё реже: его нишу занимает LVM (см. ниже).
							</para>

						</listitem>

					</varlistentry>
					 <varlistentry>
						<term>RAID-1</term>
						 <listitem>
							<para>
								Этот уровень, также известный как «зеркальный RAID», является одновременно и самым простым, и самым широко используемым. В своём стандартном виде он использует два физических диска одного размера и предоставляет логический том опять-таки того же размера. Данные хранятся одинаково на обоих дисках, отсюда и название «зеркало». Когда один диск выходит из строя, данные по-прежнему доступны с другого. Для действительно ценных данных RAID-1, конечно, может быть настроен на более чем двух дисках, с пропорциональным увеличением отношения цены оборудования к доступному пространству.
							</para>
							 <sidebar> <title><emphasis>ПРИМЕЧАНИЕ</emphasis> Размеры дисков и кластера</title>
							 <para>
								Если два диска разного размера настроены зеркалом, больший из них будет использоваться не полностью, поскольку он будет содержать те же данные, что и меньший, и ничего сверх этого. Таким образом доступное полезное пространство, предоставляемое томом RAID-1, соответствует размеру меньшего диска в массиве. Это справедливо и для томов RAID более высокого уровня, хотя избыточность в них реализована другим образом.
							</para>
							 <para>
								По этой причине при настройке RAID-массивов (за исключением RAID-0 и «linear RAID») важно использовать диски идентичных или очень близких размеров, чтобы избежать пустой траты ресурсов.
							</para>
							 </sidebar> <sidebar> <title><emphasis>ПРИМЕЧАНИЕ</emphasis> Резервные диски</title>
							 <para>
								Уровни RAID, включающие избыточность, позволяют добавлять больше дисков, чем требуется для массива. Дополнительные диски используются в качестве резервных, когда один из основных дисков выходит из строя. К примеру, в зеркале из двух дисков с одним резервным при отказе одного из первых двух дисков ядро автоматически (и немедленно) восстанавливает зеркало с использованием резервного диска, так что избыточность остаётся на гарантированном уровне по истечении времени на восстановление. Это может быть использовано как ещё одна мера предосторожности для ценных данных.
							</para>
							 <para>
								Естественно может возникнуть вопрос, чем это лучше простого зеркалирования сразу на три диска. Преимущество конфигурации с резервным диском заключается в том, что резервный диск может быть общим для нескольких RAID-томов. Например, можно иметь три зеркальных тома с гарантированной избыточностью даже в случае сбоя одного диска, при наличии всего семи дисков (три пары плюс один общий резерв) вместо девяти, которые потребовались бы для трёх триплетов.
							</para>
							 </sidebar> <para>
								Данный уровень RAID хотя и дорог (поскольку в лучшем случае используется только половина физического хранилища), но широко применяется на практике. Он прост для понимания и позволяет легко делать резервные копии: поскольку оба диска хранят одинаковое содержимое, один из них может быть временно извлечён без влияния на работающую систему. Скорость чтения часто возрастает, поскольку ядро может считывать половину данных с каждого диска одновременно, в то время как скорость записи существенно не уменьшается. В случае массива RAID-1 из N дисков данные остаются доступными даже при отказе N-1 диска.
							</para>

						</listitem>

					</varlistentry>
					 <varlistentry>
						<term>RAID-4</term>
						 <listitem>
							<para>
								Этот довольно редко применяемый уровень RAID, использует N дисков для хранения полезных данных и дополнительный диск для хранения избыточной информации. Если этот диск выходит из строя, система восстанавливает его содержимое с оставшихся N дисков. Если один из N дисков с данными отказывает, оставшиеся N-1 в сочетании с диском контроля чётности содержат достаточно информации для восстановления необходимых данных.
							</para>
							 <para>
								RAID-4 не так дорог, поскольку приводит к увеличению цены только на один из N и не оказывает существенного влияния на скорость чтения, но запись замедляется. Кроме того, поскольку запись на любой из N дисков влечёт за собой запись на диск контроля чётности, на последний запись производится значительно чаще, и как следствие его время жизни существенно сокращается. Данные на массиве RAID-4 сохранны при отказе только одного диска (из N+1).
							</para>

						</listitem>

					</varlistentry>
					 <varlistentry>
						<term>RAID-5</term>
						 <listitem>
							<para>
								RAID-5 нацелен на исправление асимметрии RAID-4: блоки контроля чётности распределяются по всем N+1 дискам, без выделения специального диска.
							</para>
							 <para>
								Скорость чтения и записи идентичны RAID-4. Опять-таки, система остаётся работоспособной только с одним отказавшим диском (из N+1), не более.
							</para>

						</listitem>

					</varlistentry>
					 <varlistentry>
						<term>RAID-6</term>
						 <listitem>
							<para>
								RAID-6 можно считать расширением RAID-5, где каждая последовательность из N блоков предполагает два избыточных блока, и каждая последовательность из N+2 блоков распределяется по N+2 дискам.
							</para>
							 <para>
								Этот уровень RAID несколько более дорогостоящ, чем предыдущие два, но он добавляет надёжности, поскольку до двух дисков (из N+2) могут выйти из строя без ущерба для доступа к данным. С другой стороны, операции записи теперь предполагают запись одного блока данных и двух избыточных блоков, что делает их ещё более медленными.
							</para>

						</listitem>

					</varlistentry>
					 <varlistentry>
						<term>RAID-1+0</term>
						 <listitem>
							<para>
								Строго говоря, это не уровень RAID, а наложение двух группировок RAID. Начиная с 2×N дисков, первая собирает их попарно в тома RAID-1; эти N томов затем собираются в один при посредстве «linear RAID» или (всё чаще) LVM. Этот последний случай не является RAID в чистом виде, но это не создаёт проблем.
							</para>
							 <para>
								RAID-1+0 может пережить выход из строя нескольких дисков: до N в массиве из 2×N, описанном выше, в случае если хотя бы один диск остаётся работоспособным в каждой паре RAID-1.
							</para>
							 <sidebar id="sidebar.raid-10"> <title><emphasis>УГЛУБЛЯЕМСЯ</emphasis> RAID-10</title>
							 <para>
								RAID-10 в общем случае считается синонимом RAID-1+0, но из-за специфики LINUX это на самом деле является обобщением. Эта установка позволяет создать систему, где каждый блок хранится на двух разных дисках, даже при нечётном числе дисков, и копии распределяются на основании изменяемой модели.
							</para>
							 <para>
								Производительность будет изменяться в зависимости от выбранной модели распределения и степени избыточности, а также нагрузки на логический том.
							</para>
							 </sidebar>
						</listitem>

					</varlistentry>

				</variablelist>
				 <para>
					Безусловно, уровень RAID следует выбирать в соответствии с ограничениями и потребностями конкретного приложения. Учтите, что в одном компьютере может быть несколько отдельных RAID-массивов разных конфигураций.
				</para>

			</section>
			 <section id="sect.raid-setup">
				<title>Настройка RAID</title>
				 <indexterm>
					<primary><emphasis role="pkg">mdadm</emphasis></primary>
				</indexterm>
				 <para>
					Настройка томов RAID требует пакета <emphasis role="pkg">mdadm</emphasis>; он предоставляет команду <command>mdadm</command>, с помощью которой можно создавать RAID-массивы и манипулировать ими, а также сценарии и инструменты для интеграции с остальными компонентами системы, в том числе с системами мониторинга.
				</para>
				 <para>
					Для примера рассмотрим сервер с несколькими дисками, некоторые из которых уже используются, а другие доступны для создания RAID. Изначально у нас есть такие диски и разделы:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							диск <filename>sdb</filename>, 4 ГБ, полностью доступен;
						</para>

					</listitem>
					 <listitem>
						<para>
							диск <filename>sdc</filename>, 4 ГБ, также полностью доступен;
						</para>

					</listitem>
					 <listitem>
						<para>
							на диске <filename>sdd</filename> доступен только раздел <filename>sdd2</filename> (около 4 ГБ);
						</para>

					</listitem>
					 <listitem>
						<para>
							наконец, диск <filename>sde</filename>, также 4 ГБ, полностью доступен.
						</para>

					</listitem>

				</itemizedlist>
				 <sidebar> <title><emphasis>ЗАМЕТКА</emphasis> Идентификация существующих томов RAID</title>
				 <para>
					В файл <filename>/proc/mdstat</filename> перечислены существующие тома и их состояние. При создании нового тома RAID следует быть осторожным, чтобы не дать ему имя, совпадающее с именем уже существующего тома.
				</para>
				 </sidebar> <para>
					Мы собираемся использовать эти физические носители для сборки двух томов, одного RAID-0 и одного зеркала (RAID-1). Начнём с тома RAID-0:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</userinput>
<computeroutput>mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
# </computeroutput><userinput>mdadm --query /dev/md0</userinput>
<computeroutput>/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.
# </computeroutput><userinput>mdadm --detail /dev/md0</userinput>
<computeroutput>/dev/md0:
        Version : 1.2
  Creation Time : Wed May  6 09:24:34 2015
     Raid Level : raid0
     Array Size : 8387584 (8.00 GiB 8.59 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:24:34 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

     Chunk Size : 512K

           Name : mirwiz:0  (local to host mirwiz)
           UUID : bb085b35:28e821bd:20d697c9:650152bb
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       1       8       32        1      active sync   /dev/sdc
# </computeroutput><userinput>mkfs.ext4 /dev/md0</userinput>
<computeroutput>mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 2095104 4k blocks and 524288 inodes
Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </computeroutput><userinput>mkdir /srv/raid-0</userinput>
<computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0</userinput>
<computeroutput># </computeroutput><userinput>df -h /srv/raid-0</userinput>
<computeroutput>Filesystem      Size  Used Avail Use% Mounted on
/dev/md0        7.9G   18M  7.4G   1% /srv/raid-0
</computeroutput></screen>
				 <para>
					Команда <command>mdadm --create</command> требует нескольких параметров: имени создаваемого тома (<filename>/dev/md*</filename>, где MD расшифровывается как <foreignphrase>Multiple Device</foreignphrase>), уровня RAID, количества дисков (это обязательный параметр, хотя он и имеет значение только для RAID-1 и выше), и физические устройства для использования. Когда устройство создано, мы можем использовать его, как если бы это был обычный раздел: создавать файловую систему на нём, монтировать эту файловую систему и т. п. Обратите внимание, что создание тома RAID-0 под именем <filename>md0</filename> — не более чем совпадение, и нумерация массивов не обязана соответствовать выбранному уровню избыточности. Также можно создать именованные RAID-массивы, передавая <command>mdadm</command> такие параметры как <filename>/dev/md/linear</filename> вместо <filename>/dev/md0</filename>.
				</para>
				 <para>
					RAID-1 создаётся сходным образом, различия заметны только после создания:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</userinput>
<computeroutput>mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%
Continue creating array? </computeroutput><userinput>y</userinput>
<computeroutput>mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md1 started.
# </computeroutput><userinput>mdadm --query /dev/md1</userinput>
<computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
        Version : 1.2
  Creation Time : Wed May  6 09:30:19 2015
     Raid Level : raid1
     Array Size : 4192192 (4.00 GiB 4.29 GB)
  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:30:40 2015
          State : clean, resyncing (PENDING) 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       1       8       64        1      active sync   /dev/sde
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
          State : clean
[...]
</computeroutput></screen>
				 <sidebar> <title><emphasis>ПОДСКАЗКА</emphasis> RAID, диски и разделы</title>
				 <para>
					Как показано в нашем примере, устройства RAID могут быть собраны из дисковых разделов, а не обязательно из целых дисков.
				</para>
				 </sidebar> <para>
					Здесь уместны несколько замечаний. Во-первых, <command>mdadm</command> предупреждает, что физические элементы имеют разные размеры; поскольку это подразумевает, что часть пространства на большем элементе будет потеряна, здесь требуется подтверждение.
				</para>
				 <para>
					Что более важно, обратите внимание на состояние зеркала. Нормальное состояние зеркала RAID — когда содержимое двух дисков полностью идентично. Однако ничто не гарантирует этого, когда том только что создан. Поэтому подсистема RAID берёт эту гарантию на себя, и как только устройство RAID будет создано, начнётся этап синхронизации. Некоторое время спустя (точное его количество будет зависеть от размера дисков…) массив RAID переходит в состояние «active». Заметьте что на этом этапе восстановления зеркало находится в деградированном состоянии, и избыточность не гарантируется. Сбой диска в этот рискованный промежуток времени может привести к потере всех данных. Большие объёмы важных данных, однако, редко сохраняются на только что созданном RAID до конца начальной синхронизации. Отметьте, что даже в деградированном состоянии <filename>/dev/md1</filename> может использоваться, на нём можно создать файловую систему и скопировать в неё какие-то данные.
				</para>
				 <sidebar> <title><emphasis>СОВЕТ</emphasis> Запуск зеркала в деградированном состоянии</title>
				 <para>
					Иногда два диска недоступны сразу, когда появляется желание создать зеркало RAID-1, например потому что один из дисков, которые планируется включить в зеркало, уже используется для хранения данных, которые необходимо перенести на массив. В таких случаях можно специально создать деградированный массив RAID-1, передав <filename>missing</filename> вместо файла устройства как один из аргументов <command>mdadm</command>. После того, как данные будут скопированы на «зеркало», старый диск можно добавить в массив. После этого начнётся синхронизация, которая и обеспечит нам избыточность, которой мы хотели добиться.
				</para>
				 </sidebar> <sidebar> <title><emphasis>СОВЕТ</emphasis> Настройка зеркала без синхронизации</title>
				 <para>
					Тома RAID-1 часто создаются для использования в качестве нового диска, зачастую считающегося пустым. Начальное содержимое диска поэтому не особо важно, ведь необходимо обеспечить доступность только данных, записанных после создания тома, а именно файловой системы.
				</para>
				 <para>
					По этой причине можно усомниться в смысле синхронизации обоих дисков во время создания. Зачем беспокоиться об этом, если идентично содержимое тех областей тома, которые будут читаться только после того, как мы записали на них что-то?
				</para>
				 <para>
					К счастью, этот этап синхронизации можно пропустить, передав опцию <literal>--assume-clean</literal> команде <command>mdadm</command>. Однако эта опция может повлечь неприятные сюрпризы в случаях, когда начальные данные будут читаться (например если на физических дисках уже присутствовала файловая система), поэтому она не включена по умолчанию.
				</para>
				 </sidebar> <para>
					Теперь посмотрим, что происходит, когда один из элементов массива RAID-1 выходит из строя. <command>mdadm</command>, а точнее её опция <literal>--fail</literal>, позволяет симулировать такой отказ диска:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde</userinput>
<computeroutput>mdadm: set /dev/sde faulty in /dev/md1
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
    Update Time : Wed May  6 09:39:39 2015
          State : clean, degraded 
 Active Devices : 1
Working Devices : 1
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 19

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       0        0        2      removed

       1       8       64        -      faulty   /dev/sde</computeroutput></screen>
				 <para>
					Содержимое тома по-прежнему доступно (и, если он смонтирован, приложения ничего не заметят), но сохранность данных больше не застрахована: если диск <filename>sdd</filename> в свою очередь выйдет из строя, данные будут потеряны. Мы хотим избежать такого риска, поэтому мы заменим отказавший диск новым, <filename>sdf</filename>:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>
<computeroutput>mdadm: added /dev/sdf
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
   Raid Devices : 2
  Total Devices : 3
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:48:49 2015
          State : clean, degraded, recovering 
 Active Devices : 1
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 1

 Rebuild Status : 28% complete

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 26

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      spare rebuilding   /dev/sdf

       1       8       64        -      faulty   /dev/sde
# </computeroutput><userinput>[...]</userinput>
<computeroutput>[...]
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
    Update Time : Wed May  6 09:49:08 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 41

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf

       1       8       64        -      faulty   /dev/sde</computeroutput></screen>
				 <para>
					Опять-таки, ядро автоматически запускает этап восстановления, на протяжении которого том, хотя и по-прежнему доступный, находится в деградированном состоянии. Когда восстановление завершается, массив RAID возвращается в нормальное состояние. Можно сказать системе, что диск <filename>sde</filename> следует удалить из массива, в результате чего получится классическое зеркало RAID на двух дисках:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>
<computeroutput>mdadm: hot removed /dev/sde from /dev/md1
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf</computeroutput></screen>
				 <para>
					После этого диск может быть физически извлечён из сервера при следующем отключении, или даже из работающего сервера, если аппаратная конфигурация позволяет горячую замену. Такие конфигурации включают некоторые контроллеры SCSI, большинство SATA-дисков и внешние накопители, работающие через USB или Firewire.
				</para>

			</section>
			 <section id="sect.backup-raid-config">
				<title>Создание резервной копии настроек</title>
				 <para>
					Большая часть метаданных, касающихся томов RAID, сохраняется непосредственно на дисках, входящих в эти массивы, так что ядро может определить массивы и их компоненты и собрать их автоматически при запуске системы. И всё же резервное копирование конфигурации крайне желательно, поскольку такое определение не защищено от ошибок, и следует ожидать, что оно наверняка даст сбой в самый неподходящий момент. В нашем примере, если бы отказ диска <filename>sde</filename> был настоящим (а не симулированным), и система перезагрузилась бы без удаления этого диска, он мог бы начать работать опять, поскольку был бы обнаружен при перезагрузке. Ядро получило бы три физических элемента, каждый из которых заявлял бы, что содержит половину одного и того же тома RAID. Другой источник путаницы может возникнуть, когда тома RAID с двух серверов переносятся на один и тот же сервер. Если эти массивы работали нормально до того, как диски были перемещены, ядро смогло бы обнаружить и пересобрать пары корректно; но если перемещённые диски были объединены в <filename>md1</filename> на прежнем сервере, а на новом сервере уже был бы <filename>md1</filename>, одно из зеркал было бы переименовано.
				</para>
				 <para>
					Поэтому резервное копирование важно хотя бы для справки. Стандартный путь для этого — редактирование файла <filename>/etc/mdadm/mdadm.conf</filename>, пример которого приводится здесь:
				</para>
				 <example id="example.mdadm-conf">
					<title>Конфигурационный файл <command>mdadm</command></title>
					 
<programlisting># mdadm.conf
#
# Please refer to mdadm.conf(5) for information about this file.
#

# by default (built-in), scan all partitions (/proc/partitions) and all
# containers for MD superblocks. alternatively, specify devices to scan, using
# wildcards if desired.
DEVICE /dev/sd*

# auto-create devices with Debian standard permissions
CREATE owner=root group=disk mode=0660 auto=yes

# automatically tag new arrays as belonging to the local system
HOMEHOST &lt;system&gt;

# instruct the monitoring daemon where to send mail alerts
MAILADDR root

# definitions of existing MD arrays
ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464

# This configuration was auto-generated on Thu, 17 Jan 2013 16:21:01 +0100
# by mkconf 3.2.5-3</programlisting>

				</example>
				 <para>
					Один из наиболее важных элементов здесь — опция <literal>DEVICE</literal>, в которой перечисляются устройства, на которых система будет автоматически искать компоненты томов RAID во время запуска. В нашем примере мы заменили значение по умолчанию, <literal>partitions containers</literal>, на явный список файлов устройств, поскольку мы выбрали использование целых дисков, а не только разделов, для некоторых томов.
				</para>
				 <para>
					Последние две строки в нашем примере позволяют ядру безопасно выбирать, какой номер тома какому массиву следует назначить. Метаданных, хранящихся на самих дисках, достаточно для пересборки томов, но не для определения номера тома (и соответствующего имени устройства <filename>/dev/md*</filename>).
				</para>
				 <para>
					К счастью, эти строки могут быть сгенерированы автоматически:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>
<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</computeroutput></screen>
				 <para>
					Содержимое этих последних двух строк не зависит от списка дисков, входящих в том. Поэтому нет необходимости перегенерировать эти строки при замене вышедшего из строя диска новым. С другой стороны, следует аккуратно обновлять этот файл при создании или удалении массива RAID.
				</para>

			</section>

		</section>
		 <section id="sect.lvm">
			<title>LVM</title>
			 <indexterm>
				<primary>LVM</primary>
			</indexterm>
			 <indexterm>
				<primary>Logical Volume Manager</primary>
			</indexterm>
			 <para>
				LVM, или <emphasis>менеджер логических томов</emphasis> (<foreignphrase>Logical Volume Manager</foreignphrase>), — другой подход к абстрагированию логических томов от их физических носителей, который фокусируется на увеличении гибкости, а не надёжности. LVM позволяет изменять логический том прозрачно для приложений; к примеру, можно добавить новые диски, перенести на них данные и удалить старые диски без отмонтирования тома.
			</para>
			 <section id="sect.lvm-concepts">
				<title>Принципы работы LVM</title>
				 <para>
					Такая гибкость достигается за счёт уровня абстракции, включающего три понятия.
				</para>
				 <para>
					Первое, PV (<emphasis>физический том</emphasis> — <foreignphrase>Physical Volume</foreignphrase>), ближе всего к аппаратной стороне: это могут быть разделы на диске, целый диск или иное блочное устройство (в том числе и RAID-массив). Обратите внимание, что когда физический элемент настроен на использование в роли PV для LVM, доступ к нему должен осуществляться только через LVM, иначе система будет сбита с толку.
				</para>
				 <para>
					Несколько PV могут быть объединены в VG (<emphasis>группу томов</emphasis> — <foreignphrase>Volume Group</foreignphrase>), которую можно сравнить с виртуальными расширяемыми дисками. VG абстрактны и не имеют представления в виде файла в структуре иерархии <filename>/dev</filename>, так что риска использовать их напрямую нет.
				</para>
				 <para>
					Третий тип объектов — LV (<emphasis>логический том</emphasis> — <foreignphrase>Logical Volume</foreignphrase>), который является частью VG; если продолжить аналогию VG с диском, то LV соответствует разделу. LV представляется как блочное устройство в <filename>/dev</filename> и может использоваться точно так же, как и любой физический раздел (как правило — для размещения файловой системы или пространства подкачки).
				</para>
				 <para>
					Важно, что разбиение VG на LV совершенно независимо от его физических компонент (PV). VG с единственным физическим компонентом (например диском) может быть разбита на десяток логических томов; точно так же VG может использовать несколько физических дисков и представляться в виде единственного большого логического тома. Единственным ограничением является то, что, само собой, общий размер, выделенный LV, не может быть больше, чем общая ёмкость всех PV в группе томов.
				</para>
				 <para>
					Часто, однако, имеет смысл использовать однородные физические компоненты в составе VG. К примеру, если доступны быстрые диски и более медленные, быстрые можно объединить в одну VG, а более медленные — в другую; порции первой можно выдавать приложениям, требующим быстрого доступа к данным, а вторую оставить для менее требовательных задач.
				</para>
				 <para>
					В любом случае помните, что LV не закреплены за конкретным PV. Можно повлиять на то, где физически хранятся данные с LV, но эта возможность не требуется для повседневного использования. С другой стороны, когда набор физических компонентов VG меняется, физические места хранения, соответствующие конкретному LV, можно переносить между дисками (в пределах PV, закреплённых за VG, разумеется).
				</para>

			</section>
			 <section id="sect.lvm-setup">
				<title>Настройка LVM</title>
				 <para>
					Давайте пройдём шаг за шагом процесс настройки LVM для типичного случая: мы хотим упростить чрезмерно усложнённую ситуацию с хранилищами. Такое обычно получается в результате долгой и витиеватой истории накопления временных мер. Для иллюстрации возьмём сервер, на котором со временем возникала потребность в изменении хранилища, что в конечном итоге привело к путанице из доступных разделов, распределённых по нескольким частично используемым дискам. Если более конкретно, доступны следующие разделы:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							на диске <filename>sdb</filename> — раздел <filename>sdb2</filename>, 4 ГБ;
						</para>

					</listitem>
					 <listitem>
						<para>
							на диске <filename>sdс</filename> — раздел <filename>sdс3</filename>, 3 ГБ;
						</para>

					</listitem>
					 <listitem>
						<para>
							диск <filename>sdd</filename>, 4 ГБ, доступен полностью;
						</para>

					</listitem>
					 <listitem>
						<para>
							на диске <filename>sdf</filename> — раздел <filename>sdf1</filename>, 4 ГБ, и раздел <filename>sdf2</filename>, 5 ГБ.
						</para>

					</listitem>

				</itemizedlist>
				 <para>
					Кроме того, давайте считать, что диски <filename>sdb</filename> и <filename>sdf</filename> быстрее двух других.
				</para>
				 <para>
					Наша цель — настроить три логических тома для трёх разных приложений: файлового сервера, требующего 5 ГБ дискового пространства, базы данных (1 ГБ), и некоторое пространство для резервных копий (12 ГБ). Первым двум требуется хорошая производительность, а резервные копии менее критичны к скорости доступа. Все эти ограничения не позволяют разделы сами по себе; используя LVM, можно абстрагироваться от физического размера устройств, так что единственным ограничением является общее доступное пространство.
				</para>
				 <para>
					Необходимые инструменты находятся в пакете <emphasis role="pkg">lvm2</emphasis> и его зависимостях. После их установки настройка LVM проходит в три шага, соответствующих трём уровням организации.
				</para>
				 <para>
					Первым делом мы подготавливаем физические тома с помощью <command>pvcreate</command>:
				</para>
				 
<screen id="screen.pvcreate"><computeroutput># </computeroutput><userinput>pvdisplay</userinput>
<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2</userinput>
<computeroutput>  Physical volume "/dev/sdb2" successfully created
# </computeroutput><userinput>pvdisplay</userinput>
<computeroutput>  "/dev/sdb2" is a new physical volume of "4.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/sdb2
  VG Name               
  PV Size               4.00 GiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I

# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</userinput>
<computeroutput>  Physical volume "/dev/sdc3" successfully created
  Physical volume "/dev/sdd" successfully created
  Physical volume "/dev/sdf1" successfully created
  Physical volume "/dev/sdf2" successfully created
# </computeroutput><userinput>pvdisplay -C</userinput>
<computeroutput>  PV         VG   Fmt  Attr PSize PFree
  /dev/sdb2       lvm2 ---  4.00g 4.00g
  /dev/sdc3       lvm2 ---  3.09g 3.09g
  /dev/sdd        lvm2 ---  4.00g 4.00g
  /dev/sdf1       lvm2 ---  4.10g 4.10g
  /dev/sdf2       lvm2 ---  5.22g 5.22g
</computeroutput></screen>
				 <para>
					Пока всё идёт неплохо; отметим, что PV может быть размещён как на целом диске, так и на отдельном его разделе. Как показано выше, команда <command>pvdisplay</command> выводит список существующих PV, с двумя возможными форматами вывода.
				</para>
				 <para>
					Теперь давайте соберём эти физические элементы в VG с помощью <command>vgcreate</command>. Мы соберём PV с быстрых дисков в VG под названием <filename>vg_critical</filename>; другая VG, <filename>vg_normal</filename>, будет также включать более медленные элементы.
				</para>
				 
<screen id="screen.vgcreate"><computeroutput># </computeroutput><userinput>vgdisplay</userinput>
<computeroutput>  No volume groups found
# </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1</userinput>
<computeroutput>  Volume group "vg_critical" successfully created
# </computeroutput><userinput>vgdisplay</userinput>
<computeroutput>  --- Volume group ---
  VG Name               vg_critical
  System ID             
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               8.09 GiB
  PE Size               4.00 MiB
  Total PE              2071
  Alloc PE / Size       0 / 0   
  Free  PE / Size       2071 / 8.09 GiB
  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp

# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</userinput>
<computeroutput>  Volume group "vg_normal" successfully created
# </computeroutput><userinput>vgdisplay -C</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize  VFree 
  vg_critical   2   0   0 wz--n-  8.09g  8.09g
  vg_normal     3   0   0 wz--n- 12.30g 12.30g
</computeroutput></screen>
				 <para>
					И снова команды довольно просты (и <command>vgdisplay</command> предоставляет два формата вывода). Заметьте, что можно использовать два раздела одного физического диска в двух разных VG. Мы использовали приставку <filename>vg_</filename> в именах наших VG, но это не более чем соглашение.
				</para>
				 <para>
					Теперь у нас есть два «виртуальных диска» размером около 8 ГБ и 12 ГБ соответственно. Давайте разделим их на «виртуальные разделы» (LV). Для этого потребуется команда <command>lvcreate</command> и несколько более сложный синтаксис:
				</para>
				 
<screen id="screen.lvcreate"><computeroutput># </computeroutput><userinput>lvdisplay</userinput>
<computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical</userinput>
<computeroutput>  Logical volume "lv_files" created
# </computeroutput><userinput>lvdisplay</userinput>
<computeroutput>  --- Logical volume ---
  LV Path                /dev/vg_critical/lv_files
  LV Name                lv_files
  VG Name                vg_critical
  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT
  LV Write Access        read/write
  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400
  LV Status              available
  # open                 0
  LV Size                5.00 GiB
  Current LE             1280
  Segments               2
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0

# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical</userinput>
<computeroutput>  Logical volume "lv_base" created
# </computeroutput><userinput>lvcreate -n lv_backups -L 12G vg_normal</userinput>
<computeroutput>  Logical volume "lv_backups" created
# </computeroutput><userinput>lvdisplay -C</userinput>
<computeroutput>  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_base    vg_critical -wi-a---  1.00g                                           
  lv_files   vg_critical -wi-a---  5.00g                                           
  lv_backups vg_normal   -wi-a--- 12.00g</computeroutput></screen>
				 <para>
					При создании логических томов обязательны два параметра; они должны быть переданы <command>lvcreate</command> как опции. Имя создаваемого LV указывается с опцией <literal>-n</literal>, а его размер обычно указывается с опцией <literal>-L</literal>. Конечно, нужно ещё указать имя VG, который следует использовать, отсюда последний параметр командной строки.
				</para>
				 <sidebar> <title><emphasis>УГЛУБЛЯЕМСЯ</emphasis> Опции <command>lvcreate</command></title>
				 <para>
					У команды <command>lvcreate</command> есть ряд опций для тонкой настройки создания LV.
				</para>
				 <para>
					Сначала опишем опцию <literal>-l</literal>, с которой размер LV может быть указан в виде числа блоков (в противоположность «человеческим» единицам, которые мы использовали выше). Эти блоки (называемые PE — <emphasis>физическими экстентами</emphasis>, <foreignphrase>Physical Extents</foreignphrase> — в терминологии LVM) являются непрерывными единицами хранения на PV, и они не могут быть распределены между LV. При необходимости указать пространство для LV с некоторой точностью, например для использования всего доступного пространства, опция <literal>-l</literal> может оказаться полезнее, чем <literal>-L</literal>.
				</para>
				 <para>
					Также можно указать физическое размещение LV, чтобы его экстенты физически размещались на конкретном PV (разумеется, из числа выделенных для VG). Поскольку мы знаем, что <filename>sdb</filename> быстрее <filename>sdf</filename>, мы можем предпочесть записать <filename>lv_base</filename> туда, если хотим дать преимущество серверу баз данных по сравнению с файловым сервером. Командная строка будет выглядеть так: <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command>. Обратите внимание, что эта команда может завершиться с ошибкой, если на PV недостаточно свободных экстентов. В нашем примере имеет смысл создать <filename>lv_base</filename> раньше <filename>lv_files</filename> чтобы избежать такой ситуации — или освободить немного места на <filename>sdb2</filename> с помощью команды <command>pvmove</command>.
				</para>
				 </sidebar> <para>
					Созданные логические тома появляются как блочные устройства в <filename>/dev/mapper/</filename>:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>ls -l /dev/mapper</userinput>
<computeroutput>total 0
crw------- 1 root root 10, 236 Jun 10 16:52 control
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2
# </computeroutput><userinput>ls -l /dev/dm-*</userinput>
<computeroutput>brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0
brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1
brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2
</computeroutput></screen>
				 <sidebar> <title><emphasis>ЗАМЕТКА</emphasis> Автоматическое определение томов LVM</title>
				 <para>
					Когда компьютер загружается, сервис systemd <filename>lvm2-activation</filename> запускает команду <command>vgchange -aay</command> чтобы «активировать» группы томов: она сканирует доступные устройства; те, которые были инициализированы как физические тома LVM, регистрируются в подсистеме LVM, принадлежащие к группам томов собираются, и соответствующие логические тома запускаются и делаются доступными. Поэтому нет необходимости редактировать конфигурационные файлы при создании или изменении томов LVM.
				</para>
				 <para>
					Обратите внимание, однако, что резервная копия конфигурации элементов LVM (физических и логических томов и групп томов) сохраняется в <filename>/etc/lvm/backup</filename>, что может пригодиться при возникновении проблем (или просто чтобы мельком взглянуть под капот).
				</para>
				 </sidebar> <para>
					Для облегчения жизни также создаются символические ссылки в каталогах, соответствующих VG:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical</userinput>
<computeroutput>total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0
# </computeroutput><userinput>ls -l /dev/vg_normal</userinput>
<computeroutput>total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</computeroutput></screen>
				 <para>
					LV можно использовать в точности как обычные разделы:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups</userinput>
<computeroutput>mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 3145728 4k blocks and 786432 inodes
Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d
[...]
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </computeroutput><userinput>mkdir /srv/backups</userinput>
<computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups</userinput>
<computeroutput># </computeroutput><userinput>df -h /srv/backups</userinput>
<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_normal-lv_backups   12G   30M   12G   1% /srv/backups
# </computeroutput><userinput>[...]</userinput>
<computeroutput>[...]
# </computeroutput><userinput>cat /etc/fstab</userinput>
<computeroutput>[...]
/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2
/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2
/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</computeroutput></screen>
				 <para>
					С точки зрения приложений, множество маленьких разделов теперь представлены в виде одного 12-гигабайтного тома с удобным именем.
				</para>

			</section>
			 <section id="sect.lvm-over-time">
				<title>Эволюция LVM</title>
				 <para>
					Хотя возможность объединять разделы или физические диски и удобна, не она является главным преимуществом LVM. Её гибкость особенно заметна с течением времени, когда возникают потребности в изменениях. Допустим, что в нашем примере возникла потребность в сохранении новых больших файлов, и что LV, выделенный файловому серверу, слишком мал для них. Поскольку мы использовали не всё пространство, доступное на <filename>vg_critical</filename>, мы можем увеличить <filename>lv_files</filename>. Для этого мы используем команду <command>lvresize</command>, затем <command>resize2fs</command> чтобы соответствующим образом подогнать файловую систему:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>
<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files
# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>
<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 5.00g
# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   2   2   0 wz--n- 8.09g 2.09g
# </computeroutput><userinput>lvresize -L 7G vg_critical/lv_files</userinput>
<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).
  Logical volume lv_files successfully resized
# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>
<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 7.00g
# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>
<computeroutput>resize2fs 1.42.12 (29-Aug-2014)
Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.

# </computeroutput><userinput>df -h /srv/files/</userinput>
<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</computeroutput></screen>
				 <sidebar> <title><emphasis>ОСТОРОЖНО</emphasis> Изменение размера файловых систем</title>
				 <para>
					Размеры не всех файловых систем можно изменять во время работы; поэтому изменение размера тома может потребовать отмонтирования файловой системы в начале и обратного монтирования её в конце. Разумеется, при желании уменьшить пространство, выделенное под LV, файловая система должна быть уменьшена первой; при изменении размера в другом направлении порядок обратный: логический том должен быть увеличен прежде, чем файловая система на нём. Это вполне очевидно, ведь файловая система никогда не должна быть больше блочного устройства, на котором она размещается (будь это устройство физическим разделом или логическим томом).
				</para>
				 <para>
					Файловые системы ext3, ext4 и xfs могут быть увеличены онлайн, без размонтирования; уменьшение требует размонтирования. Файловая система reiserfs позволяет изменение размера онлайн в обоих направлениях. Преклонная ext2 не позволяет ни того, ни другого, и всегда должна быть отмонтирована.
				</para>
				 </sidebar> <para>
					Мы могли бы, действуя тем же образом, расширить том, на котором размещается база данных, только мы достигли предела доступного места на VG:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>
<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_base 1008M  854M  104M  90% /srv/base
# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree 
  vg_critical   2   2   0 wz--n- 8.09g 92.00m</computeroutput></screen>
				 <para>
					Это не имеет значения, поскольку LVM позволяет добавлять физические тома в существующие группы томов. Например, мы заметили, что на разделе <filename>sdb1</filename>, использовавшемся вне LVM, размещались только архивы, которые можно переместить на <filename>lv_backups</filename>. Теперь можно утилизировать его и ввести в группу томов, тем самым восстановив доступное пространство. Для этой цели существует команда <command>vgextend</command>. Само собой, раздел должен быть предварительно подготовлен как физический раздел. Когда VG расширена, мы можем использовать такие же команды, как и раньше, для увеличения логического тома, а затем файловой системы:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>
<computeroutput>  Physical volume "/dev/sdb1" successfully created
# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>
<computeroutput>  Volume group "vg_critical" successfully extended
# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   3   2   0 wz--n- 9.09g 1.09g
# </computeroutput><userinput>[...]</userinput>
<computeroutput>[...]
# </computeroutput><userinput>df -h /srv/base/</userinput>
<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</computeroutput></screen>
				 <sidebar> <title><emphasis>УГЛУБЛЯЕМСЯ</emphasis> Более подробно о LVM</title>
				 <para>
					LVM угодит и более опытным пользователям, позволяя задавать вручную множество параметров. Например, администратор может настроить размер блоков, составляющих физические и логические тома, как и их физическое размещение. Также можно перемещать блоки между PV, к примеру для тонкой настройки производительности или, в более прозаичном случае, чтобы освободить PV, когда необходимо извлечь соответствующий физический диск из VG (чтобы присоединить его к другой VG или вовсе удалить из LVM). Страницы руководства, описывающие команды, в целом ясны и подробны. Для начала хорошо подойдёт страница <citerefentry><refentrytitle>lvm</refentrytitle>
					 <manvolnum>8</manvolnum></citerefentry>.
				</para>
				 </sidebar>
			</section>

		</section>
		 <section id="sect.raid-or-lvm">
			<title>RAID или LVM?</title>
			 <para>
				Как RAID, так и LVM предоставляют бесспорные преимущества как только мы выходим за рамки простейшего случая настольного компьютера с одним жёстким диском, где схема использования не меняется с течением времени.
			</para>
			 <para>
				Есть несколько простых примеров, где вопрос выбора не встаёт. Если требуется защитить данные от аппаратных сбоев, безусловно следует создать RAID на избыточном дисковом массиве, ведь LVM просто не предназначен для решения этой проблемы. Если, с другой стороны, требуется гибкая система хранения, где тома не зависят от реальных физических дисков, RAID мало чем поможет, и естественно выбрать LVM.
			</para>
			 <sidebar> <title><emphasis>ЗАМЕТКА</emphasis> Если производительность имеет значение…</title>
			 <para>
				В случаях, когда важна скорость ввода-вывода, особенно время доступа, использование LVM и/или RAID в какой-либо из возможных комбинаций может повлиять на производительность, и это может оказаться важным фактором при выборе одной из них. Однако эти различия в производительности крайне малы, и заметны в очень немногих случаях. Если важна производительность, лучшим выбором будет использование накопителей без вращающихся частей (<indexterm><primary>SSD</primary></indexterm><emphasis>твердотельных накопителей</emphasis>, или SSD); их удельная стоимость за мегабайт выше, чем у обычных жёстких дисков, и их вместимость обычно меньше, но они обеспечивают превосходную скорость случайного доступа. Если характер использования предполагает много операций ввода-выводы, распределённых по всей файловой системе, например в случае баз данных, где часто выполняются сложные запросы, преимущество использования SSD значительно перевесит то, что можно выжать, выбирая между LVM поверх RAID и обратным вариантом. В таких ситуациях выбор должен определяться иными соображениями, нежели скорость, поскольку вопрос производительности легче всего решается использованием SSD.
			</para>
			 </sidebar> <para>
				Третий характерный случай — когда хочется просто объединить два диска в один том из соображений производительности или чтобы иметь единую файловую систему, которая больше любого из доступных дисков. В этом случае подходят как RAID-0 (или даже linear-RAID), так и том LVM. В такой ситуации, если нет дополнительных ограничений (вроде унификации с другими компьютерами, на которых используется только RAID), более предпочтительным часто является выбор LVM. Начальная настройка несколько более сложна, но это небольшое увеличение сложности более чем покрывается дополнительной гибкостью, которую привнесёт LVM, если потребности изменятся, или если понадобится добавить новые диски.
			</para>
			 <para>
				Ну и конечно, есть ещё по-настоящему интересный случай, когда систему хранения нужно сделать одновременно устойчивой к аппаратным сбоям и гибкой, когда дело доходит до выделения томов. Ни RAID, ни LVM не могут удовлетворить обоим требованиям сами по себе; не страшно, в этом случае мы используем их одновременно — точнее, одно поверх другого. Схема, включающая всё и ставшая стандартом с тех пор, как RAID и LVM достигли стабильности, заключается в обеспечении сначала избыточности группировкой дисков в небольшое число RAID-массивов и использовании этих массивов в качестве физических томов LVM; логические разделы будут потом выделяться из этих LV для файловых систем. Преимущество такой настройки заключается в том, что при отказе диска потребуется пересобрать только небольшое число RAID-массивов, тем самым экономя время, которое потребуется администратору на восстановление.
			</para>
			 <para>
				Возьмём конкретный пример: отделу связей с общественностью Falcot Corp требуется рабочая станция для редактирования видео, но бюджет отдела не позволяет приобрести полный комплект оборудования класса high-end. Решено отдать предпочтение оборудованию, специфичному для работы с графикой (монитору и видеокарте), а для хранения использовать оборудование общего назначения. Однако, как общеизвестно, цифровое видео предъявляет определённые требования к хранилищу: объём данных велик, а скорость чтения и записи важна для производительности системы в целом (больше чем типичное время доступа, к примеру). Эти требования должны быть удовлетворены с помощью обычного оборудования, в данном случае двух жёстких дисков SATA объёмом по 300 ГБ; также необходимо сделать системные данные устойчивыми к аппаратным сбоям, в то время как обрабатываемое видео менее важно, поскольку оно ещё записано на видеокассеты.
			</para>
			 <para>
				Чтобы удовлетворить этим требованиям, совмещены RAID-1 и LVM. Диски подключены к двум разным SATA-контроллерам для оптимизации параллельного доступа и снижения риска одновременного отказа, поэтому они представлены как <filename>sda</filename> и <filename>sdc</filename>. Они размечены одинаково по следующей схеме:
			</para>
			 
<screen><computeroutput># </computeroutput><userinput>fdisk -l /dev/sda</userinput>
<computeroutput>
Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x00039a9f

Device    Boot     Start       End   Sectors Size Id Type
/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect
/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris
/dev/sda3        4000185 586099395 582099210 298G 5  Extended
/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect
/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect
/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</computeroutput></screen>
			 <itemizedlist>
				<listitem>
					<para>
						Первые разделы обоих дисков (около 1 ГБ) собраны в том RAID-1, <filename>md0</filename>. Это зеркало напрямую используется для корневой файловой системы.
					</para>

				</listitem>
				 <listitem>
					<para>
						Разделы <filename>sda2</filename> и <filename>sdc2</filename> используются как разделы подкачки, предоставляющие 2 ГБ пространства подкачки. С 1 ГБ ОЗУ рабочая станция имеет достаточный объём доступной памяти.
					</para>

				</listitem>
				 <listitem>
					<para>
						Разделы <filename>sda5</filename> и <filename>sdc5</filename>, как и <filename>sda6</filename> с <filename>sdc6</filename>, собраны в два новых тома RAID-1, примерно по 100 ГБ каждый, <filename>md1</filename> и <filename>md2</filename>. Оба эти зеркала инициализированы как физические тома LVM, и добавлены в группу томов <filename>vg_raid</filename>. Таким образом эта VG содержит около 200 ГБ надёжного пространства.
					</para>

				</listitem>
				 <listitem>
					<para>
						Остальные разделы, <filename>sda7</filename> и <filename>sdc7</filename>, напрямую используются как физические тома, и добавлены в другую VG под названием <filename>vg_bulk</filename>, которая поэтому содержит приблизительно 200 ГБ пространства.
					</para>

				</listitem>

			</itemizedlist>
			 <para>
				После создания VG можно разбить их весьма гибким образом. Следует помнить, что LV, созданные на <filename>vg_raid</filename> будут сохранны даже если один из дисков выйдет из строя, чего нельзя сказать о LV, созданных на <filename>vg_bulk</filename>; с другой стороны, последние будут размещаться параллельно на обоих дисках, что обеспечит более высокие скорости чтения и записи больших файлов.
			</para>
			 <para>
				По этой причине мы создадим LV <filename>lv_usr</filename>, <filename>lv_var</filename> и <filename>lv_home</filename> на <filename>vg_raid</filename> для размещения соответствующих файловых систем; другой большой LV, <filename>lv_movies</filename>, будет использоваться для размещения окончательных версий роликов после редактирования. Другая VG будет разбита на большой <filename>lv_rushes</filename> для данных, захваченных с видеокамер, и <filename>lv_tmp</filename> для временных файлов. Размещение рабочей области — не такой простой выбор: в то время как для этого тома нужна хорошая производительность, стоит ли она риска потери работы, если диск выйдет из строя во время сессии? В зависимости от ответа на этот вопрос соответствующий LV следует создать на одной VG или на другой.
			</para>
			 <para>
				Теперь у нас есть некоторая избыточность для важных данных и большая гибкость в распределении доступного пространства между приложениями. Если в дальнейшем будет устанавливаться новое программное обеспечение (для редактирования аудиозаписей, например), LV, на котором размещается <filename>/usr/</filename>, может быть безболезненно увеличен.
			</para>
			 <sidebar> <title><emphasis>ПРИМЕЧАНИЕ</emphasis> Почему три тома RAID-1?</title>
			 <para>
				Мы могли ограничиться одним томом RAID-1 для размещения физического тома под <filename>vg_raid</filename>. Зачем же создавать три?
			</para>
			 <para>
				Смысл первого разделения (<filename>md0</filename> от остальных) в обеспечении сохранности данных: данные, записанные на оба элемента зеркала RAID-1 в точности совпадают, поэтому можно обойти RAID и смонтировать один из дисков напрямую. В случае ошибки в ядре, например, или если метаданные LVM окажутся повреждены, всё равно можно загрузить минимальную систему для доступа к важным данным, таким как выделение дисков под RAID и LVM тома; метаданные можно восстановить и получить доступ к файлам снова, так что система может быть возвращена в рабочее состояние.
			</para>
			 <para>
				Смысл второго разделения (<filename>md1</filename> от <filename>md2</filename>) менее очевиден и базируется на тезисе о непредсказуемости будущего. При первичной сборке рабочей станции точные требования к хранилищу не обязательно известны; они могут и изменяться со временем. В нашем случае мы не можем заведомо знать, сколько потребуется места для рабочего видеоматериала и для готовых видеороликов. Если отдельный ролик потребует большого количества рабочего материала, а VG, выделенный для данных с избыточностью, заполнен менее чем наполовину, мы можем использовать часть невостребованного пространства на нём. Мы можем удалить один из физических томов, скажем <filename>md2</filename>, из <filename>vg_raid</filename> и либо подключить его к <filename>vg_bulk</filename> напрямую (если ожидаемая продолжительность операции достаточно коротка, чтобы мы могли пережить временное падение производительности), либо разобрать RAID на <filename>md2</filename> и интегрировать его компоненты <filename>sda6</filename> и <filename>sdc6</filename> в <filename>vg_bulk</filename> (который увеличится в таком случае на 200 ГБ, а не на 100); логический том <filename>lv_rushes</filename> тогда можно будет увеличить в соответствии с требованиями.
			</para>
			 </sidebar>
		</section>

	</section>
	 <section id="sect.virtualization">
		<title>Виртуализация</title>
		 <indexterm>
			<primary>виртуализация</primary>
		</indexterm>
		 <para>
			Виртуализация — это одно из крупнейших достижений вычислительной техники последних лет. Этот термин включает в себя различные абстракции и технологии имитации виртуальных компьютеров с разной степенью независимости от реального оборудования. На одном физическом сервере могут размещаться несколько систем, работающих одновременно и изолированных друг от друга. Приложений много, и зачастую они были бы невозможны без такой изоляции: к примеру, тестовые окружения с различными конфигурациями или разделение сервисов по разным виртуальным машинам для безопасности.
		</para>
		 <para>
			Существует множество решений для виртуализации, каждое со своими достоинствами и недостатками. Эта книга сфокусируется на Xen, LXC и KVM, но есть и другие реализации, достойные упоминания:
		</para>
		 <indexterm>
			<primary><emphasis>VMWare</emphasis></primary>
		</indexterm>
		 <indexterm>
			<primary><emphasis>Bochs</emphasis></primary>
		</indexterm>
		 <indexterm>
			<primary><emphasis>QEMU</emphasis></primary>
		</indexterm>
		 <indexterm>
			<primary><emphasis>VirtualBox</emphasis></primary>
		</indexterm>
		 <indexterm>
			<primary><emphasis>KVM</emphasis></primary>
		</indexterm>
		 <indexterm>
			<primary><emphasis>LXC</emphasis></primary>
		</indexterm>
		 <itemizedlist>
			<listitem>
				<para>
					QEMU — это программный эмулятор полноценного компьютера; производительность далека от скоростей, которых можно было бы достичь, запуская программы нативно, но это позволяет запуск немодифицированных или экспериментальных операционных систем на эмулируемом оборудовании. Он также позволяет эмулировать разные аппаратные архитектуры, например на системе <emphasis>amd64</emphasis> можно сэмулировать <emphasis>arm</emphasis>-компьютер. QEMU является свободным ПО. <ulink type="block" url="http://www.qemu.org/" />
				</para>

			</listitem>
			 <listitem>
				<para>
					Bochs — другая свободная виртуальная машина, но она эмулирует только архитектуры x86 (i386 и amd64).
				</para>

			</listitem>
			 <listitem>
				<para>
					VMWare — это собственническая виртуальная машина; будучи одной из самых старых, она является и одной из самых известных. Она работает на принципах, сходных с QEMU. VMWare предлагает расширенный функционал, такой как создание снимков работающей виртуальной машины. <ulink type="block" url="http://www.vmware.com/" />
				</para>

			</listitem>
			 <listitem>
				<para>
					VirtualBox is a virtual machine that is mostly free software (some extra components are available under a proprietary license). Unfortunately it is in Debian's “contrib” section because it includes some precompiled files that cannot be rebuilt without a proprietary compiler and it currently only resides in Debian Unstable as Oracle's policies make it impossible to keep it secure in a Debian stable release (see <ulink url="https://bugs.debian.org/794466">#794466</ulink>). While younger than VMWare and restricted to the i386 and amd64 architectures, it still includes some snapshotting and other interesting features. <ulink type="block" url="http://www.virtualbox.org/" />
				</para>

			</listitem>

		</itemizedlist>
		 <section id="sect.xen">
			<title>Xen</title>
			 <para>
				Xen <indexterm><primary>Xen</primary></indexterm> — это решение для «паравиртуализации». Оно вводит тонкий слой абстракции, называемый «гипервизором», между оборудованием и вышележащими системами; он играет роль арбитра, контролирующего доступ к оборудованию из виртуальных машин. Однако он обрабатывает лишь немногие инструкции, остальные напрямую выполняются оборудованием от имени систем. Главное преимущество заключается в том, что производительность не страдает, и системы работают со скоростью, близкой к нативной; минусом является то, что ядра операционных систем, которые нужно запускать на гипервизоре Xen, должны быть адаптированы для этого.
			</para>
			 <para>
				Уделим немного времени терминологии. Гипервизор является нижним слоем, выполняющимся непосредственно на оборудовании, даже ниже ядра. Гипервизор может разделять остальное программное обеспечение по нескольким <emphasis>доменам</emphasis>, которые могут выглядеть как множество виртуальных машин. Один из этих доменов (первый, который запускается) известен как <emphasis>dom0</emphasis> и имеет особую роль, поскольку только этот домен может управлять гипервизором и исполнением других доменов. Эти другие домены известны как <emphasis>domU</emphasis>. Другими словами, с точки зрения пользователя <emphasis>dom0</emphasis> соответствует «хосту» в других системах виртуализации, а <emphasis>domU</emphasis> — «гостю».
			</para>
			 <sidebar> <title><emphasis>КУЛЬТУРА</emphasis> Xen и разные версии Linux</title>
			 <para>
				Xen изначально разрабатывался как набор заплат, живший вне официального дерева и не интегрированный в ядро Linux. В то же время некоторые развивающиеся системы виртуализации (включая KVM) требовали некоторых общих функций, связанных с виртуализацией, для облегчения их интеграции, и ядро Linux получило такой набор функций (известный как интерфейс <emphasis>paravirt_ops</emphasis> или <emphasis>pv_ops</emphasis>). Поскольку заплаты Xen дублировали часть функционала этого интерфейса, они не могли быть приняты официально.
			</para>
			 <para>
				Xensource, компания, стоящая за Xen, по этой причине должна была перенести Xen на этот новый каркас, чтобы заплаты Xen могли быть влиты в официальное ядро Linux. Это означало переписывание большого объёма кода, и хотя Xensource вскоре получила работающую версию, основанную на интерфейсе paravirt_ops, заплаты были лишь постепенно влиты в официальное ядро. Процесс закончился в Linux 3.0. <ulink type="block" url="http://wiki.xenproject.org/wiki/XenParavirtOps" />
			</para>
			 <para>
				Поскольку <emphasis role="distribution">Jessie</emphasis> основан на версии 3.16 ядра Linux, стандартные пакеты <emphasis role="pkg">linux-image-686-pae</emphasis> и <emphasis role="pkg">linux-image-amd64</emphasis> включают необходимый код, и в наложении заплат, которые требовались для <emphasis role="distribution">Squeeze</emphasis> и более ранних версий Debian, более нет нужды. <ulink type="block" url="http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix" />
			</para>
			 </sidebar> <sidebar> <title><emphasis>ЗАМЕТКА</emphasis> Архитектуры, совместимые с Xen</title>
			 <para>
				Xen в настоящее время доступен только для архитектур i386, amd64, arm64 и armhf.
			</para>
			 </sidebar> <sidebar> <title><emphasis>КУЛЬТУРА</emphasis> Xen и ядра, отличные от Linux</title>
			 <para>
				Xen требует изменений во всех операционных системах, которые хочется на нём запустить; не все ядра достигли полной функциональности в этом отношении. Многие полнофункциональны как dom0 и domU: Linux 3.0 и выше, NetBSD 4.0 и выше и OpenSolaris. Другие работают только как domU. Статус каждой операционной системы можно проверить в вики Xen: <ulink type="block" url="http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen" /> <ulink type="block" url="http://wiki.xenproject.org/wiki/DomU_Support_for_Xen" />
			</para>
			 <para>
				Однако если Xen может положиться на аппаратные функции виртуализации (которые наличествуют только в недавно выпущенных процессорах), даже немодифицированные операционные системы могут запускаться как domU (включая Windows).
			</para>
			 </sidebar> <para>
				Чтобы использовать Xen в Debian, нужны три компонента:
			</para>
			 <itemizedlist>
				<listitem>
					<para>
						Сам гипервизор. В соответствии с доступным оборудованием пакет будет называться <emphasis role="pkg">xen-hypervisor-4.4-amd64</emphasis>, <emphasis role="pkg">xen-hypervisor-4.4-armhf</emphasis> или <emphasis role="pkg">xen-hypervisor-4.4-arm64</emphasis>.
					</para>

				</listitem>
				 <listitem>
					<para>
						Ядро, работающее на этом гипервизоре. Любое ядро, новее 3.0, включая версию 3.16 из состава <emphasis role="distribution">Jessie</emphasis>.
					</para>

				</listitem>
				 <listitem>
					<para>
						Для архитектуры i386 также требуется стандартная библиотека с заплатами, использующими Xen; она находится в пакете <emphasis role="pkg">libc6-xen</emphasis>.
					</para>

				</listitem>

			</itemizedlist>
			 <para>
				Чтобы избежать мороки с выбором этих компонентов вручную, для удобства создано несколько пакетов (таких как <emphasis role="pkg">xen-linux-system-amd64</emphasis>); они тянут за собой заведомо работоспособный набор соответствующих пакетов гипервизора и ядра. С гипервизором также поставляется пакет <emphasis role="pkg">xen-utils-4.4</emphasis>, содержащий инструменты для управления гипервизором из dom0. Он в свою очередь зависит от соответствующей стандартной библиотеки. Во время установки всего этого конфигурационные сценарии также создают новую запись в меню загрузчика Grub, чтобы запустить выбранное ядро в Xen dom0. Заметьте однако, что эта запись обычно устанавливается не первой в списке, и поэтому не выбирается по умолчанию. Если это не то поведение, которого вы хотели, следующие команды изменят его:
			</para>
			 
<screen><computeroutput># </computeroutput><userinput>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen
</userinput><computeroutput># </computeroutput><userinput>update-grub
</userinput></screen>
			 <para>
				Когда всё необходимое установлено, следующим шагом будет тестирование поведения самого dom0; оно включает перезагрузку в гипервизор и ядро Xen. Система должна загрузиться обычным образом, с несколькими дополнительными сообщениями в консоли на ранних стадиях инициализации.
			</para>
			 <para>
				Теперь время собственно установить подходящие системы в domU с помощью инструментов из <emphasis role="pkg">xen-tools</emphasis>. Этот пакет предоставляет команду <command>xen-create-image</command>, которая в значительной мере автоматизирует задачу. Единственный обязательный параметр — <literal>--hostname</literal>, передающий имя domU; другие опции важны, но они могут быть сохранены в конфигурационном файле <filename>/etc/xen-tools/xen-tools.conf</filename>, и их отсутствие в командной строке не вызовет ошибки. Поэтому следует проверить содержимое этого файла перед созданием образов, или же использовать дополнительные параметры в вызове <command>xen-create-image</command>. Отметим следующие важные параметры:
			</para>
			 <itemizedlist>
				<listitem>
					<para>
						<literal>--memory</literal> для указания количества ОЗУ, выделенного вновь создаваемой системе;
					</para>

				</listitem>
				 <listitem>
					<para>
						<literal>--size</literal> и <literal>--swap</literal>, чтобы задать размер «виртуальных дисков», доступных для domU;
					</para>

				</listitem>
				 <listitem>
					<para>
						<literal>--debootstrap</literal>, чтобы новая система устанавливалась с помощью <command>debootstrap</command>; в этом случае также чаще всего используется опция <literal>--dist</literal> (с указанием имени дистрибутива, например <emphasis role="distribution">jessie</emphasis>).
					</para>
					 <sidebar> <title><emphasis>УГЛУБЛЯЕМСЯ</emphasis> Установка систем, отличных от Debian, в domU</title>
					 <para>
						В случае системы, основанной не на Linux, следует быть аккуратным при указании ядра, которое должно использоваться domU, с помощью опции <literal>--kernel</literal>.
					</para>
					 </sidebar>
				</listitem>
				 <listitem>
					<para>
						<literal>--dhcp</literal> объявляет, что конфигурация сети domU должна быть получена по DHCP, в то время как <literal>--ip</literal> позволяет задать статический IP-адрес.
					</para>

				</listitem>
				 <listitem>
					<para>
						Наконец, следует выбрать метод хранения для создаваемых образов (тех, которые будут видны как жёсткие диски из domU). Самый простой метод, соответствующий опции <literal>--dir</literal>, заключается в создании одного файла на dom0 для каждого устройства, которое будет передано domU. Для систем, использующих LVM, альтернативой является использование опции <literal>--lvm</literal>, за которой указывается имя группы томов; в таком случае <command>xen-create-image</command> создаст новый логический том в этой группе, и этот логический том станет доступным для domU как жёсткий диск.
					</para>
					 <sidebar> <title><emphasis>ЗАМЕТКА</emphasis> Хранилище в domU</title>
					 <para>
						Целые жёсткие диски также могут быть экспортированы в domU, равно как разделы, RAID-массивы или ранее созданные логические тома LVM. Эти операции не автоматизированы <command>xen-create-image</command>, однако, поэтому требуется редактирование конфигурационного файла образа Xen после его создания с помощью <command>xen-create-image</command>.
					</para>
					 </sidebar>
				</listitem>

			</itemizedlist>
			 <para>
				Когда выборы сделаны, мы можем создать образ для нашего будущего Xen domU:
			</para>
			 
<screen><computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</userinput>
<computeroutput>
[...]
General Information
--------------------
Hostname       :  testxen
Distribution   :  jessie
Mirror         :  http://ftp.debian.org/debian/
Partitions     :  swap            128Mb (swap)
                  /               2G    (ext3)
Image type     :  sparse
Memory size    :  128Mb
Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64
Initrd path    :  /boot/initrd.img-3.16.0-4-amd64
[....]
Logfile produced at:
         /var/log/xen-tools/testxen.log

Installation Summary
---------------------
Hostname        :  testxen
Distribution    :  jessie
MAC Address     :  00:16:3E:8E:67:5C
IP-Address(es)  :  dynamic
RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b
Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ
</computeroutput></screen>
			 <para>
				Теперь у нас есть виртуальная машина, но она ещё не запущена (и поэтому только занимает место на жёстком диске dom0). Разумеется, мы можем создать больше образов, возможно с разными параметрами.
			</para>
			 <para>
				До включения этих виртуальных машин нам нужно определить, как будет получаться доступ к ним. Разумеется, они могут быть назначены изолированными машинами, доступными только через системную консоль, но это редко соответствует сценарию работы. Большую часть времени domU будет считаться удалённым сервером, и доступ к нему будет осуществляться только через сеть. Однако было бы весьма неудобным добавлять сетевую карту для каждого domU; по этой причине Xen позволяет создавать виртуальные интерфейсы, которые каждый домен может видеть и использовать обычным образом. Заметьте, что эти карты, хоть они и виртуальные, будут полезными только когда они подключены к сети, хотя бы виртуальной. У Xen есть несколько сетевых моделей для этого:
			</para>
			 <itemizedlist>
				<listitem>
					<para>
						Простейшей является модель <emphasis>моста</emphasis>; все сетевые карты eth0 (как в dom0, так и в domU-системах) ведут себя, как если бы они были напрямую подключены к Ethernet-коммутатору.
					</para>

				</listitem>
				 <listitem>
					<para>
						Следующая модель — <emphasis>маршрутизируемая</emphasis>, когда dom0 ведёт себя как маршрутизатор, находящийся между domU-системами и (физической) внешней сетью.
					</para>

				</listitem>
				 <listitem>
					<para>
						Наконец, в модели <emphasis>NAT</emphasis> dom0 опять находится между domU-системами и остальной сетью, но domU-системы не доступны извне напрямую, и трафик проходит через преобразование адресов на dom0.
					</para>

				</listitem>

			</itemizedlist>
			 <para>
				Эти три сетевых режима включают различные интерфейсы с необычными именами, такими как <filename>vif*</filename>, <filename>veth*</filename>, <filename>peth*</filename> и <filename>xenbr0</filename>. Гипервизор Xen комбинирует их в соответствии с заданной схемой под контролем инструментов пространства пользователя. Поскольку NAT и маршрутизируемая модель приспособлены лишь для отдельных случаев, мы рассмотрим только модель моста.
			</para>
			 <para>
				Стандартная конфигурация пакетов Xen не меняет общесистемных сетевых настроек. Однако демон <command>xend</command> настроен на подключение виртуальных сетевых интерфейсов к любому уже существующему сетевому мосту (при наличии нескольких таких мостов предпочтение отдаётся <filename>xenbr0</filename>). Поэтому нам надо настроить мост в <filename>/etc/network/interfaces</filename> (для этого требуется установить пакет <emphasis role="pkg">bridge-utils</emphasis>, поэтому он рекомендуется пакетом <emphasis role="pkg">xen-utils-4.4</emphasis>), заменив существующую запись eth0:
			</para>
			 
<programlisting>auto xenbr0
iface xenbr0 inet dhcp
    bridge_ports eth0
    bridge_maxwait 0
</programlisting>
			 <para>
				Перезагрузившись для проверки, что мост создаётся автоматически, мы можем запустить domU с помощью инструментов управления Xen, а именно команды <command>xl</command>. Эта команда позволяет производить различные манипуляции с доменами, в частности выводить их список, запускать их и останавливать.
			</para>
			 
<screen><computeroutput># </computeroutput><userinput>xl list</userinput>
<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   463     1     r-----      9.8
# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>
<computeroutput>Parsing config from /etc/xen/testxen.cfg
# </computeroutput><userinput>xl list</userinput>
<computeroutput>Name                                       ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   366     1     r-----     11.4
testxen                                      1   128     1     -b----      1.1</computeroutput></screen>
			 <sidebar> <title><emphasis>ИНСТРУМЕНТ</emphasis> Выбор набора инструментов для управления Xen</title>
			 <indexterm>
				<primary><command>xm</command></primary>
			</indexterm>
			 <indexterm>
				<primary><command>xe</command></primary>
			</indexterm>
			 <para>
				В Debian 7 и более старых версиях эталонной командой для управления виртуальными машинами Xen была <command>xm</command>. Теперь её заменила <command>xl</command>, которая по большей части сохраняет обратную совместимость. Но это не единственные доступные инструменты: альтернативами являются <command>virsh</command> из libvirt и <command>xe</command> из XenServer XAPI (коммерческий продукт на базе Xen).
			</para>
			 </sidebar> <sidebar> <title><emphasis>ОСТОРОЖНО</emphasis> Только один domU на образ!</title>
			 <para>
				Хотя, безусловно, возможно запускать несколько domU-систем параллельно, каждая из них должна иметь свой собственный образ, ведь каждый domU создан, как если бы он работал на своём собственном оборудовании (за исключением маленькой части ядра, общающейся с гипервизором). В частности, две запущенных одновременно domU-системы не могут использовать общее хранилище. Если системы не запускаются одновременно, всё же возможно использовать для них один раздел подкачки или раздел, на котором размещается файловая система <filename>home</filename>.
			</para>
			 </sidebar> <para>
				Заметьте, что domU <filename>testxen</filename> использует реальную память, взятую из ОЗУ, которая иначе была бы доступна dom0, а не виртуальную. Поэтому при сборке сервера для размещения машин Xen следует побеспокоиться об обеспечении достаточного объёма физического ОЗУ.
			</para>
			 <para>
				Voilà! Наша виртуальная машина запускается. Мы можем получить доступ к ней в одном из двух режимов. Обычный путь — подключаться к ней «удалённо» через сеть, как мы подключались бы к реальной машине; для этого обычно требуется настройка либо DHCP-сервера, либо DNS. Другой путь, который может стать единственно возможным в случае неправильной настройки сети, — использование консоли <filename>hvc0</filename> с помощью команды <command>xl console</command>:
			</para>
			 
<screen><computeroutput># </computeroutput><userinput>xl console testxen</userinput>
<computeroutput>[...]

Debian GNU/Linux 8 testxen hvc0

testxen login: </computeroutput></screen>
			 <para>
				После этого можно начать сессию, как если бы вы сидели за клавиатурой виртуальной машины. Для отключения от этой консоли служит сочетание клавиш <keycombo action="simul"><keycap>Control</keycap> <keycap>]</keycap></keycombo>.
			</para>
			 <sidebar> <title><emphasis>СОВЕТ</emphasis> Получение консоли сразу</title>
			 <para>
				Иногда хочется запустить domU-систему и сразу же подключиться к её консоли; для этого команда <command>xl create</command> может принимать флаг <literal>-c</literal>. Запуск domU с этим флагом приведёт к отображению всех сообщений во время загрузки системы.
			</para>
			 </sidebar> <sidebar> <title><emphasis>ИНСТРУМЕНТ</emphasis> OpenXenManager</title>
			 <para>
				OpenXenManager (в пакете <emphasis role="pkg">openxenmanager</emphasis>) — это графический интерфейс, позволяющий удалённо управлять доменами Xen через API Xen. Он предоставляет большую часть возможностей команды <command>xl</command>.
			</para>
			 </sidebar> <para>
				Когда domU запущен, он может использоваться как любой другой сервер (ведь это, помимо прочего, система GNU/Linux). Однако благодаря тому, что это виртуальная машина, доступны и некоторые дополнительные возможности. К примеру, domU может быть временно приостановлен, а затем вновь запущен с помощью команд <command>xl pause</command> и <command>xl unpause</command>. Заметьте, что хотя приостановленный domU не использует ресурсы процессора, выделенная ему память по-прежнему занята. Может иметь смысл использовать команды <command>xl save</command> и <command>xl restore</command>: сохранение domU освобождает ресурсы, которые ранее использовались этим domU, в том числе и ОЗУ. После восстановления (или снятия с паузы) domU не замечает ничего кроме того, что прошло некоторое время. Если domU был запущен, когда dom0 выключается, сценарии из пакетов автоматически сохраняют domU и восстанавливают его при следующей загрузке. Отсюда, конечно, проистекает обычное неудобство, проявляющееся, например, при переводе ноутбука в спящий режим; в частности, если domU приостановлен слишком надолго, сетевые подключения могут завершиться. Заметьте также, что Xen на данный момент несовместим с большей частью системы управления питанием ACPI, что мешает приостановке dom0-системы.
			</para>
			 <sidebar> <title><emphasis>ДОКУМЕНТАЦИЯ</emphasis> Опции <command>xl</command></title>
			 <para>
				Большая часть подкоманд <command>xl</command> требуют одного или более аргументов, часто — имени domU. Эти аргументы подробно описаны в странице руководства <citerefentry><refentrytitle>xl</refentrytitle>
				 <manvolnum>1</manvolnum></citerefentry>.
			</para>
			 </sidebar> <para>
				Выключение или перезагрузка domU могут быть выполнены как изнутри domU (с помощью команды <command>shutdown</command>), так и из dom0, с помощью <command>xl shutdown</command> или <command>xl reboot</command>.
			</para>
			 <sidebar> <title><emphasis>УГЛУБЛЯЕМСЯ</emphasis> Xen углублённо</title>
			 <para>
				У Xen есть гораздо больше возможностей, чем мы могли описать в этих нескольких абзацах. В частности, система очень динамична, и многие параметры домена (такие как объём выделенной памяти, видимые жёсткие диски, поведение планировщика задач и так далее) могут быть изменены даже когда домен запущен. domU может быть даже перенесён с одного сервера на другой без отключения, и даже без потери сетевых подключений! Главным источником информации обо всех этих углублённых аспектах является официальная документация Xen. <ulink type="block" url="http://www.xen.org/support/documentation.html" />
			</para>
			 </sidebar>
		</section>
		 <section id="sect.lxc">
			<title>LXC</title>
			 <indexterm>
				<primary>LXC</primary>
			</indexterm>
			 <para>
				Хотя она и используется для создания «виртуальных машин», LXC является, строго говоря, не системой виртуализации, а системой для изоляции групп процессов друг от друга, даже если они все выполняются на одном узле. Она использует набор недавних изменений в ядре Linux, известных под общим названием <emphasis>control groups</emphasis>, благодаря которому разные наборы процессов, называемые «группами», имеют разные представления о некоторых аспектах системы. Наиболее примечательные из этих аспектов — идентификаторы процессов, конфигурация сети и точки монтирования. Такая группа изолированных процессов не будет иметь доступа к другим процессам в системе, и её доступ к файловой системе может быть ограничен определённым подмножеством. У неё также могут быть свои собственные сетевой интерфейс и таблица маршрутизации, и она может быть настроена так, чтобы видеть только подмножество устройств, присутствующих в системе.
			</para>
			 <para>
				С помощью комбинации этих возможностей можно изолировать целое семейство процессов начиная с процесса <command>init</command>, и получившийся набор будет выглядеть чрезвычайно похоже на виртуальную машину. Официальное название для такой схемы «контейнер» (отсюда и неофициальное название LXC: <emphasis>LinuX Containers</emphasis>), но весьма значительным отличием от «настоящих» виртуальных машин, таких как предоставляемые Xen или KVM, заключается в отсутствии второго ядра; контейнер использует то же самое ядро, что и хост-система. У этого есть как преимущества, так и недостатки: к преимуществам относится великолепная производительность благодаря полному отсутствию накладных расходов, а также тот факт, что ядро видит все процессы в системе, поэтому планировщик может работать более эффективно, чем если бы два независимых ядра занимались планированием выполнения разных наборов задач. Основное из неудобств — невозможность запустить другое ядро в контейнере (как другую версию Linux, так и другую операционную систему).
			</para>
			 <sidebar> <title><emphasis>ЗАМЕТКА</emphasis> Ограничения изоляции LXC</title>
			 <para>
				Контейнеры LXC не предоставляют такого уровня изоляции, который достижим с помощью более серьёзных эмуляторов или виртуальных машин. В частности:
			</para>
			 <itemizedlist>
				<listitem>
					<para>
						поскольку ядро разделяется между хост-системой и контейнерами, процессы, заключённые в контейнеры, всё же могут получать доступ к сообщениям ядра, что может привести к утечкам информации, если сообщения исходят из контейнера;
					</para>

				</listitem>
				 <listitem>
					<para>
						по той же причине, если контейнер скомпрометирован и была эксплуатирована уязвимость ядра, другие контейнеры также могут быть затронуты;
					</para>

				</listitem>
				 <listitem>
					<para>
						ядро проверяет права доступа файловых систем в соответствии с числовыми идентификаторами пользователей и групп; эти идентификаторы могут обозначать разных пользователей и группы в зависимости от контейнера, что следует помнить, если доступные для записи части файловой системы разделяются между контейнерами.
					</para>

				</listitem>

			</itemizedlist>
			 </sidebar> <para>
				Поскольку мы имеем дело с изоляцией, а не обычной виртуализацией, настройка контейнеров LXC более сложна, чем простой запуск debian-installer на виртуальной машине. Мы опишем некоторые предварительные требования, затем перейдём к конфигурации сети; после этого мы сможем собственно создать систему для запуска в контейнере.
			</para>
			 <section>
				<title>Предварительные шаги</title>
				 <para>
					Пакет <emphasis role="pkg">lxc</emphasis> содержит инструменты, необходимые для запуска LXC, поэтому его необходимо установить.
				</para>
				 <para>
					LXC также требует систему конфигурации <emphasis>control groups</emphasis>, представляющую собой виртуальную файловую систему, которая должна быть смонтирована в <filename>/sys/fs/cgroup</filename>. Так как Debian 8 перешел на systemd, который также зависит от control groups, это делается автоматически во время загрузки без дополнительной настройки.
				</para>

			</section>
			 <section id="sect.lxc.network">
				<title>Сетевые настройки</title>
				 <para>
					Цель установки LXC — в запуске виртуальных машин; хотя мы, разумеется, можем держать их изолированными от сети и взаимодействовать с ними только через файловую систему, для большинства задач требуется хотя бы минимальный сетевой доступ к контейнерам. В типичном случае каждый контейнер получит виртуальный сетевой интерфейс присоединённый к реальной сети через мост. Этот виртуальный интерфейс может быть подключён либо напрямую к физическому сетевому интерфейсу хост-системы (в таком случае контейнер непосредственно в сети), либо к другому виртуальному интерфейсу, определённому в хост-системе (тогда хост сможет фильтровать или маршрутизировать трафик). В обоих случаях потребуется пакет <emphasis role="pkg">bridge-utils</emphasis>.
				</para>
				 <para>
					В простейшем случае это всего лишь вопрос правки <filename>/etc/network/interfaces</filename>, переноса конфигурации физического интерфейса (например <literal>eth0</literal>) на интерфейс моста (обычно <literal>br0</literal>) и настройки связи между ними. Например, если конфигурационный файл сетевых интерфейсов изначально содержит записи вроде таких:
				</para>
				 
<programlisting>auto eth0
iface eth0 inet dhcp</programlisting>
				 <para>
					Их следует отключить и заменить на следующие:
				</para>
				 
<programlisting>#auto eth0
#iface eth0 inet dhcp

auto br0
iface br0 inet dhcp
  bridge-ports eth0</programlisting>
				 <para>
					Результат такой настройки будет похож на тот, какой мы получили бы, если бы контейнеры были машинами, подключёнными к той же физической сети, что и хост-машина. Конфигурация «мост» управляет прохождением кадров Ethernet между всеми связанными интерфейсами, включая и физический <literal>eth0</literal>, и интерфейсы, заданные для контейнеров.
				</para>
				 <para>
					В случаях, когда такую конфигурацию использовать невозможно (например если контейнерам нельзя выделить публичные IP-адреса), будет создан и подключён к мосту виртуальный <emphasis>tap</emphasis>-интерфейс. Это будет эквивалентно сетевой топологии, при которой вторая сетевая карта подключена к отдельному коммутатору, и к нему же подключены контейнеры. Хост тогда должен выступать как шлюз для контейнеров, если им требуется соединяться с остальным миром.
				</para>
				 <para>
					В дополнение к <emphasis role="pkg">bridge-utils</emphasis> для «продвинутой» конфигурации потребуется пакет <emphasis role="pkg">vde2</emphasis>; файл <filename>/etc/network/interfaces</filename> тогда примет следующий вид:
				</para>
				 
<programlisting># Интерфейс eth0 без изменений
auto eth0
iface eth0 inet dhcp

# Виртуальный интерфейс
auto tap0
iface tap0 inet manual
  vde2-switch -t tap0

# Мост для контейнеров
auto br0
iface br0 inet static
  bridge-ports tap0
  address 10.0.0.1
  netmask 255.255.255.0</programlisting>
				 <para>
					Сеть может быть настроена как статически в контейнерах, так и динамически и помощью DHCP-сервера, запущенного на хост-системе. Такой DHCP-сервер должен быть сконфигурирован для ответа на запросы на интерфейсе <literal>br0</literal>.
				</para>

			</section>
			 <section>
				<title>Установка системы</title>
				 <para>
					Давайте теперь настроим файловую систему для использования контейнером. Поскольку эта «виртуальная машина» не будет запускаться непосредственно на оборудовании, потребуются некоторые дополнительные манипуляции по сравнению с обычной файловой системой, особенно когда дело касается ядра, устройств и консолей. К счастью, пакет <emphasis role="pkg">lxc</emphasis> включает сценарии, которые в значительной степени автоматизируют эту настройку. В частности, следующие команды (для которых требуются пакеты <emphasis role="pkg">debootstrap</emphasis> и <emphasis role="pkg">rsync</emphasis>) установят контейнер с Debian:
				</para>
				 
<screen><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-create -n testlxc -t debian
</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap
Проверяю кэш, скаченный в /var/cache/lxc/debian/rootfs-jessie-amd64 ... 
Загружаю debian minimal ...
I: Перезапрашиваю Release 
I: Перезапрашиваю Release.gpg 
[...]
Скачен комплект.
Копирую rootfs в /var/lib/lxc/testlxc/rootfs...
[....]
Пароль администратора есть 'sSiKhMzI', пожалуйста измени !
root@mirwiz:~# </computeroutput>
</screen>
				 <para>
					Заметьте, что файловая система изначально создана в <filename>/var/cache/lxc</filename>, а затем перемещена в каталог назначения. Это позволяет создавать идентичные контейнеры намного быстрее, поскольку требуется лишь скопировать их.
				</para>
				 <para>
					Заметьте, что сценарий создания шаблона debian принимает опцию <option>--arch</option> с указанием архитектуры системы для установки и опцию <option>--release</option>, если вы вы хотите установить что-то отличное от текущего стабильного релиза Debian. Вы можете также установить переменную окружения <literal>MIRROR</literal>, чтобы указать на локальное зеркало Debian.
				</para>
				 <para>
					Только что созданная файловая система теперь содержит минимальную систему Debian, и по умолчанию у контейнера нет сетевого интерфейса (за исключением loopback). Поскольку это не то, чего мы хотели, мы отредактируем конфигурационный файл контейнера (<filename>/var/lib/lxc/testlxc/config</filename>) и добавим несколько записей <literal>lxc.network.*</literal>:
				</para>
				 
<programlisting>lxc.network.type = veth
lxc.network.flags = up
lxc.network.link = br0
lxc.network.hwaddr = 4a:49:43:49:79:20</programlisting>
				 <para>
					Эти записи означают, соответственно, что в контейнере будет создан виртуальный интерфейс, что он будет автоматически подниматься при запуске этого контейнера, что он будет автоматически соединяться с мостом <literal>br0</literal> на хост-системе и что его MAC-адрес будет соответствовать указанному. Если бы эта последняя запись отсутствовала или была отключена, генерировался бы случайный MAC-адрес.
				</para>
				 <para>
					Другая полезная запись в этом файле — имя узла:
				</para>
				 
<programlisting>lxc.utsname = testlxc</programlisting>

			</section>
			 <section>
				<title>Запуск контейнера</title>
				 <para>
					Теперь, когда наша виртуальная машина готова, давайте запустим контейнер:
				</para>
				 
<screen role="scale" width="94"><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-start --daemon --name=testlxc
</userinput><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-console -n testlxc
</userinput><computeroutput>Debian GNU/Linux 8 testlxc tty1

testlxc login: </computeroutput><userinput>root</userinput><computeroutput>
Password: 
Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
root@testlxc:~# </computeroutput><userinput>ps auxwf</userinput>
<computeroutput>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init
root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald
root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D
root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux
root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux
root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux
root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     
root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \_ -bash
root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \_ ps auxfw
root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102
root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e
root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux
root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux
root@testlxc:~# </computeroutput></screen>
				 <para>
					Теперь мы в контейнере; наш доступ к процессам ограничен только теми, которые запущены изнутри самого контейнера, и наш доступ к файловой системе также ограничен до выделенного подмножества полной файловой системы (<filename>/var/lib/lxc/testlxc/rootfs</filename>). Мы можем выйти из консоли с помощью <keycombo action="simul"><keycap>Control</keycap> <keycap>a</keycap></keycombo> <keycombo><keycap>q</keycap></keycombo>.
				</para>
				 <para>
					Заметьте, что мы запустили контейнер как фоновый процесс благодаря опции <option>--daemon</option> команды <command>lxc-start</command>. Контейнер можно прервать впоследствии с помощью такой команды как <command>lxc-stop --name=testlxc</command>.
				</para>
				 <para>
					Пакет <emphasis role="pkg">lxc</emphasis>содержит сценарий инициализации, который может автоматически запускать один или несколько контейнеров при загрузке хост-системы (он использует <command>lxc-autostart</command>, запускающую контейнеры, параметр <literal>lxc.start.auto</literal> которых установлен в значение 1). Более тонкий контроль порядка запуска возможен с помощью <literal>lxc.start.order</literal> и <literal>lxc.group</literal>: по умолчанию сценарий инициализации сначала запускает контейнеры, входящие в группу <literal>onboot</literal>, а затем — контейнеры, не входящие ни в какие группы. В обоих случаях порядок внутри группы определяется параметром <literal>lxc.start.order</literal>.
				</para>
				 <sidebar> <title><emphasis>УГЛУБЛЯЕМСЯ</emphasis> Массовая виртуализация</title>
				 <para>
					Поскольку LXC — очень легковесная система изоляции, её в частности можно приспособить для массового размещения виртуальных серверов. Сетевая конфигурация будет, возможно, несколько более сложной, чем мы описали выше, но «продвинутой» конфигурации с использованием интерфейсов <literal>tap</literal> и <literal>veth</literal> должно быть достаточно во многих случаях.
				</para>
				 <para>
					Может также иметь смысл сделать общей часть файловой системы, такую как ветки <filename>/usr</filename> и <filename>/lib</filename>, чтобы избежать дупликации программного обеспечения, которое может быть общим для нескольких контейнеров. Это обычно достигается с помощью записей <literal>lxc.mount.entry</literal> в конфигурационных файлах контейнеров. Интересным побочным эффектом является то, что процессы станут потреблять меньше физической памяти, поскольку ядро способно определить, что программы используются совместно. Минимальные затраты на один дополнительный контейнер могут быть снижены до дискового пространства, выделенного под его специфические данные, и нескольких дополнительных процессов, которыми должно управлять ядро.
				</para>
				 <para>
					Разумеется, мы не описали всех доступных опций; более исчерпывающая информация может быть получена из страниц руководства <citerefentry> <refentrytitle>lxc</refentrytitle>
					 <manvolnum>7</manvolnum> </citerefentry> и <citerefentry> <refentrytitle>lxc.container.conf</refentrytitle>
					 <manvolnum>5</manvolnum></citerefentry> и тех, на которые они ссылаются.
				</para>
				 </sidebar>
			</section>

		</section>
		 <section>
			<title>Виртуализация с помощью KVM</title>
			 <indexterm>
				<primary>KVM</primary>
			</indexterm>
			 <para>
				KVM, что расшифровывается как <emphasis>Kernel-based Virtual Machine</emphasis>, является первым и главным модулем ядра, предоставляющим большую часть инфраструктуры, которая может использоваться виртуализатором, но не является самим виртуализатором. Собственно контроль за виртуализацией осуществляется приложением, основанным на QEMU. Не переживайте, если в этом разделе будут упоминаться команды <command>qemu-*</command>: речь всё равно о KVM.
			</para>
			 <para>
				В отличие от других систем виртуализации, KVM был влит в ядро Linux с самого начала. Его разработчики выбрали использование наборов инструкций процессора, выделенных для виртуализации (Intel-VT и AMD-V), благодаря чему KVM получился легковесным, элегантным и не прожорливым до ресурсов. Обратной стороной медали является, естественно, то, что KVM работает не на любом компьютере, а только на таком, в котором установлен подобающий процессор. Для x86-машин можно убедиться, такой ли у вас процессор, проверив наличие флага «vmx» или «svm» в файле <filename>/proc/cpuinfo</filename>.
			</para>
			 <para>
				Поскольку его разработка активно поддерживается Red Hat, KVM стал в той или иной степени эталоном виртуализации в Linux.
			</para>
			 <section>
				<title>Предварительные шаги</title>
				 <indexterm>
					<primary><command>virt-install</command></primary>
				</indexterm>
				 <para>
					В отличие от таких инструментов, как VirtualBox, сам по себе KVM не включает никакого пользовательского интерфейса для создания виртуальных машин и управления ими. Пакет <emphasis role="pkg">qemu-kvm</emphasis> предоставляет лишь исполняемый файл, способный запустить виртуальную машину, а также инициализационный скрипт, загружающий соответствующие модули ядра.
				</para>
				 <indexterm>
					<primary>libvirt</primary>
				</indexterm>
				 <indexterm>
					<primary><emphasis role="pkg">virt-manager</emphasis></primary>
				</indexterm>
				 <para>
					К счастью, Red Hat также предоставляет набор инструментов для решения этой проблемы, разрабатывая библиотеку <emphasis>libvirt</emphasis> и связанные с ней инструменты <emphasis>менеджера виртуальных машин</emphasis>. libvirt позволяет управлять виртуальными машинами унифицированным образом, независимо от стоящей за ней системой виртуализации (на данный момент она поддерживает QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare и UML). <command>virtual-manager</command> — это графический интерфейс, который использует libvirt для создания виртуальных машин и управления ими.
				</para>
				 <indexterm>
					<primary><emphasis role="pkg">virtinst</emphasis></primary>
				</indexterm>
				 <para>
					Первым делом мы установим необходимые пакеты с помощью команды <command>apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</command>. <emphasis role="pkg">libvirt-bin</emphasis> предоставляет демон <command>libvirtd</command>, позволяющий (возможно удалённо) управлять виртуальными машинами, запущенными на хосте, и запускает необходимые виртуальные машины при загрузке хоста. Кроме того, этот пакет предоставляет утилиту <command>virsh</command> с интерфейсом командной строки, которая позволяет контролировать виртуальные машины, управляемые <command>libvirt</command>.
				</para>
				 <para>
					Пакет <emphasis role="pkg">virtinst</emphasis> предоставляет <command>virt-install</command>, которая позволяет создавать виртуальные машины из командной строки. Наконец, <emphasis role="pkg">virt-viewer</emphasis> позволяет получать доступ к графической консоли виртуальной машины.
				</para>

			</section>
			 <section>
				<title>Сетевые настройки</title>
				 <para>
					Как и в случаях Xen и LXC, наиболее распространённая сетевая конфигурация включает мост, группирующий сетевые интерфейсы виртуальных машин (см. <xref linkend="sect.lxc.network" />).
				</para>
				 <para>
					В качестве альтернативы, в конфигурации KVM по умолчанию, виртуальной машине выдаётся адрес из частного диапазона (192.168.122.0/24), и NAT настраивается таким образом, чтобы виртуальная машина могла получить доступ во внешнюю сеть.
				</para>
				 <para>
					Ниже в этом разделе считается, что на хост-системе имеются физический интерфейс <literal>eth0</literal> и мост <literal>br0</literal>, и что первый присоединён к последнему.
				</para>

			</section>
			 <section>
				<title>Установка с помощью <command>virt-install</command></title>
				 <indexterm>
					<primary><command>virt-install</command></primary>
				</indexterm>
				 <para>
					Создание виртуальной машины очень похоже на установку обычной системы с той разницей, что характеристики виртуальной машины описываются в командной строке, кажущейся бесконечной.
				</para>
				 <para>
					С практической точки зрения это значит, что мы будем использовать установщик Debian, загружая виртуальную машину с виртуального привода DVD-ROM, соответствующего образу DVD Debian, хранящемуся на хост-системе. Виртуальная машина экспортирует свой графический интерфейс по протоколу VNC (см. подробности в <xref linkend="sect.remote-desktops" />), что позволит нам контролировать процесс установки.
				</para>
				 <para>
					Для начала потребуется сказать libvirtd, где хранить образы дисков, если только нас не устраивает расположение по умолчанию (<filename>/var/lib/libvirt/images/</filename>).
				</para>
				 
<screen><computeroutput>root@mirwiz:~# </computeroutput><userinput>mkdir /srv/kvm</userinput>
<computeroutput>root@mirwiz:~# </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>
<computeroutput>Pool srv-kvm created

root@mirwiz:~# </computeroutput></screen>
				 <sidebar> <title><emphasis>СОВЕТ</emphasis> Добавьте пользователя в группу libvirt</title>
				 <para>
					Все примеры в этом разделе подразумевают выполнение команд от имени root. Фактически для управления локальным демоном libvirt надо или быть root, или входить в группу <literal>libvirt</literal> (по умолчанию пользователи в неё не добавляются). Так что при желании избежать слишком частого использования привилегий root можно добавить себя в группу <literal>libvirt</literal> и запускать команды от собственного имени.
				</para>
				 </sidebar> <para>
					Давайте запустим процесс установки на виртуальной машине и поближе взглянем на наиболее важные опции <command>virt-install</command>. Эта команда регистрирует виртуальную машину и её параметры в libvirtd, а затем запускает её, чтобы приступить к установке.
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id="virtinst.connect"></co>
               --virt-type kvm           <co id="virtinst.type"></co>
               --name testkvm            <co id="virtinst.name"></co>
               --ram 1024                <co id="virtinst.ram"></co>
               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <co id="virtinst.disk"></co>
               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <co id="virtinst.cdrom"></co>
               --network bridge=br0      <co id="virtinst.network"></co>
               --vnc                     <co id="virtinst.vnc"></co>
               --os-type linux           <co id="virtinst.os"></co>
               --os-variant debianwheezy
</userinput><computeroutput>
Starting install...
Allocating 'testkvm.qcow'             |  10 GB     00:00
Creating domain...                    |    0 B     00:00
Guest installation complete... restarting guest.
</computeroutput></screen>
				 <calloutlist>
					<callout arearefs="virtinst.connect">
						<para>
							Опция <literal>--connect</literal> указывает, какой «гипервизор» использовать. Он указывается в виде URL, содержащего систему виртуализации(<literal>xen://</literal>, <literal>qemu://</literal>, <literal>lxc://</literal>, <literal>openvz://</literal>, <literal>vbox://</literal> и т. п.) и машину, на которой должны размещаться виртуальные машины (это поле можно оставить пустым в случае локального узла). В дополнение к этому, в случае QEMU/KVM каждый пользователь может управлять виртуальными машинами, работающими с ограниченными правами, и путь URL позволяет дифференцировать «системные» машины (<literal>/system</literal>) от остальных (<literal>/session</literal>).
						</para>

					</callout>
					 <callout arearefs="virtinst.type">
						<para>
							Так как KVM управляется тем же образом, что и QEMU, в <literal>--virt-type kvm</literal> можно указать использование KVM, хотя URL и выглядит так же, как для QEMU.
						</para>

					</callout>
					 <callout arearefs="virtinst.name">
						<para>
							Опция <literal>--name</literal> задаёт (уникальное) имя виртуальной машины.
						</para>

					</callout>
					 <callout arearefs="virtinst.ram">
						<para>
							Опция <literal>--ram</literal> позволяет указать объём ОЗУ (в МБ), который будет выделен виртуальной машине.
						</para>

					</callout>
					 <callout arearefs="virtinst.disk">
						<para>
							<literal>--disk</literal> служит для указания местоположения файла образа, который будет представляться жёстким диском виртуальной машины; этот файл создаётся, если только ещё не существует, а его размер (в ГБ) указывается параметром <literal>size</literal>. Параметр <literal>format</literal> позволяет выбрать из нескольких способов хранения образа файла. Формат по умолчанию (<literal>raw</literal>) — это отдельный файл, в точности соответствующий диску по размеру и содержимому. Мы выбрали здесь более передовой формат, специфичный для QEMU и позволяющий начать с небольшого файла, увеличивающегося только по мере того, как виртуальная машина использует пространство.
						</para>

					</callout>
					 <callout arearefs="virtinst.cdrom">
						<para>
							Опция <literal>--cdrom</literal> используется, чтобы указать, где искать оптический диск для установки. Путь может быть либо локальным путём к ISO-файлу, либо URL, по которому можно получить файл, либо файлом устройства физического привода CD-ROM (то есть <filename>/dev/cdrom</filename>).
						</para>

					</callout>
					 <callout arearefs="virtinst.network">
						<para>
							С помощью опции <literal>--network</literal> указывается, каким образом виртуальная сетевая карта интегрируется в сетевую конфигурацию хоста. Поведением по умолчанию (которое мы задали явно в этом примере) является интеграция в любой существующий сетевой мост. Если ни одного моста нет, виртуальная машина сможет получить доступ к физической сети только через NAT, поэтому она получает адрес в подсети из частного диапазона (192.168.122.0/24).
						</para>

					</callout>
					 <callout arearefs="virtinst.vnc">
						<para>
							<literal>--vnc</literal> означает, что подключение к графической консоли нужно сделать доступным через VNC. По умолчанию соответствующий VNC-сервер слушает только на локальном интерфейсе; если VNC-клиент должен запускаться на другой системе, для подключения потребуется использовать SSH-туннель (см. <xref linkend="sect.ssh-port-forwarding" />). Как вариант, можно использовать опцию <literal>--vnclisten=0.0.0.0</literal>, чтобы VNC-сервер стал доступен на всех интерфейсах; заметьте, что если вы сделаете так, вам серьёзно стоит заняться настройкой межсетевого экрана.
						</para>

					</callout>
					 <callout arearefs="virtinst.os">
						<para>
							Опции <literal>--os-type</literal> и <literal>--os-variant</literal> позволяют оптимизировать некоторые параметры виртуальной машины, исходя из известных особенностей указанной операционной системы.
						</para>

					</callout>

				</calloutlist>
				 <para>
					Сейчас виртуальная машина запущена, и нам надо подключиться к графической консоли, чтобы произвести установку. Если предыдущий шаг выполнялся в графическом окружении, это подключение установится автоматически. В противном случае, или же при удалённой работе, чтобы открыть графическую консоль, можно запустить <command>virt-viewer</command> в любом графическом окружении (пароль root на удалённой машине запрашивается дважды, поскольку для работы требуется два SSH-соединения):
				</para>
				 
<screen><computeroutput>$ </computeroutput><userinput>virt-viewer --connect qemu+ssh://root@<replaceable>server</replaceable>/system testkvm
</userinput><computeroutput>root@server's password: 
root@server's password: </computeroutput></screen>
				 <para>
					Когда процесс установки завершится, виртуальная машина перезагрузится и будет готова к работе.
				</para>

			</section>
			 <section>
				<title>Управление машинами с помощью <command>virsh</command></title>
				 <indexterm>
					<primary><command>virsh</command></primary>
				</indexterm>
				 <para>
					Теперь, когда установка выполнена, давайте посмотрим, как обращаться с имеющимися виртуальными машинами. Первым делом попробуем попросить у <command>libvirtd</command> список управляемых им виртуальных машин:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all
 Id Name                 State
----------------------------------
  - testkvm              shut off
</userinput></screen>
				 <para>
					Давайте запустим нашу тестовую виртуальную машину:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>virsh -c qemu:///system start testkvm
</userinput><computeroutput>Domain testkvm стартует </computeroutput></screen>
				 <para>
					Теперь можно получить инструкции для подключения к графической консоли (возвращённый VNC-дисплей можно передать в качестве параметра команде <command>vncviewer</command>):
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm
</userinput><computeroutput>:0</computeroutput></screen>
				 <para>
					В число прочих подкоманд <command>virsh</command> входят:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							<literal>reboot</literal> для перезапуска виртуальной машины;
						</para>

					</listitem>
					 <listitem>
						<para>
							<literal>shutdown</literal> для корректного завершения работы;
						</para>

					</listitem>
					 <listitem>
						<para>
							<literal>destroy</literal> для грубого прерывания работы;
						</para>

					</listitem>
					 <listitem>
						<para>
							<literal>suspend</literal> для временной приостановки;
						</para>

					</listitem>
					 <listitem>
						<para>
							<literal>resume</literal> для продолжения работы после приостановки;
						</para>

					</listitem>
					 <listitem>
						<para>
							<literal>autostart</literal> для включения (или для выключения, с опцией <literal>--disable</literal>) автоматического запуска виртуальной машины при запуске хост-системы;
						</para>

					</listitem>
					 <listitem>
						<para>
							<literal>undefine</literal> для удаления всех следов виртуальной машины из <command>libvirtd</command>.
						</para>

					</listitem>

				</itemizedlist>
				 <para>
					Все эти подкоманды принимают идентификатор виртуальной машины в качестве параметра.
				</para>

			</section>
			 <section>
				<title>Установка RPM-системы в Debian с помощью yum</title>
				 <para>
					Если виртуальная машина предназначается для запуска Debian (или одного из производных дистрибутивов), систему можно инициализировать с помощью <command>debootstrap</command>, как описано выше. Но если на виртуальную машину надо установить систему, основанную на RPM (такую как Fedora, CentOS или Scientific Linux), установку следует производить с помощью утилиты <command>yum</command> (которая доступна из одноимённого пакета).
				</para>
				 <para>
					Эта процедура требует использования <command>rpm</command> для распаковки начального набора файлов, включая, в частности, конфигурационные файлы <command>yum</command>, а затем вызов <command>yum</command> для распаковки оставшихся пакетов. Но поскольку <command>yum</command> вызывается извне chroot, потребуется внести некоторые временные изменения. В примере ниже целевой chroot — <filename>/srv/centos</filename>.
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>rootdir="/srv/centos"
</userinput><computeroutput># </computeroutput><userinput>mkdir -p "$rootdir" /etc/rpm
</userinput><computeroutput># </computeroutput><userinput>echo "%_dbpath /var/lib/rpm" &gt; /etc/rpm/macros.dbpath
</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root "$rootdir" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!
rpm: However assuming you know what you are doing...
warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY
# </computeroutput><userinput>sed -i -e "s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g" $rootdir/etc/yum.repos.d/*.repo
</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core
</userinput><computeroutput>[...]
# </computeroutput><userinput>sed -i -e "s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g" $rootdir/etc/yum.repos.d/*.repo
</userinput></screen>

			</section>

		</section>

	</section>
	 <section id="sect.automated-installation">
		<title>Автоматизированная установка</title>
		 <indexterm>
			<primary>развёртывание</primary>
		</indexterm>
		 <indexterm>
			<primary>установка</primary>
			<secondary>автоматизированная установка</secondary>
		</indexterm>
		 <para>
			Администраторам Falcot Corp, как и многим администраторам больших IT-инфраструктур, необходимы инструменты для быстрой установки (или переустановки), причём по возможности автоматической, на новых машинах.
		</para>
		 <para>
			Эти потребности можно удовлетворить с помощью широкого диапазона решений. С одной стороны, универсальные инструменты вроде SystemImager делают это, создавая образ, основанный на шаблонной машине, после чего развёртывают этот образ на целевых системах; с другой стороны, стандартный установщик Debian может быть преднастроен с помощью конфигурационного файла, содержащего ответы на задаваемые в процессе установки вопросы. Промежуточным вариантом являются такие гибридные инструменты как FAI (<emphasis>Fully Automatic Installer</emphasis>), которые производят установку с помощью пакетной системы, но также используют свою собственную инфраструктуру для задач, специфичных для массового развёртывания (таких как запуск, разметка, конфигурирование и т. п.).
		</para>
		 <para>
			У каждого из этих решений есть свои преимущества и недостатки: SystemImager работает независимо от какой бы то ни было системы управления пакетами, что позволяет управлять большими наборами машин с несколькими различными дистрибутивами Linux. Он также включает систему обновления, не требующую переустановки, но эта система обновлений подходит только для тех случаев, когда на отдельных машинах не вносится независимых изменений; другими словами, пользователь не должен самостоятельно обновлять никакое программное обеспечение или устанавливать новое. Аналогично, обновления безопасности не должны быть автоматизированы, потому что им следует проити через централизованный эталонный образ, поддерживаемый SystemImager. Кроме того, парк машин должен быть гомогенным, иначе придётся хранить и поддерживать много разных образов (образ i386 не подойдёт для powerpc-машины и т. п.).
		</para>
		 <para>
			С другой стороны, автоматизированная установка с помощью debian-installer может приспособиться к специфике каждой машины: установщик выберет подходящее ядро и пакеты программного обеспечения из соответствующих репозиториев, определит доступное оборудование, разметит весь жёсткий диск, чтобы максимально использовать доступное пространство, установит систему Debian и настроит загрузчик. Однако стандартный установщик будет устанавливать только стандартные версии Debian с базовой системой и набором предварительно выбранных «задач»; это не позволяет установить специфическую систему с приложениями не из пакетов. Для удовлетворения такой специфической потребности требуется модификация установщика… К счастью, установщик имеет модульную архитектуру, и существуют инструменты для автоматизации большей части работы, необходимой для такой модификации, в первую очередь simple-CDD (CDD — это аббревиатура от <emphasis>Custom Debian Derivative</emphasis>). Однако даже решение с simple-CDD решает только вопрос установки; обычно это не проблема, поскольку инструменты APT справляются с эффективным развёртыванием обновлений в дальнейшем.
		</para>
		 <para>
			Мы предоставим только краткий обзор FAI и совсем пропустим SystemImager (который больше не входит в состав Debian), чтобы более внимательно сосредоточиться на debian-installer и simple-CDD, которые более интересны в контексте Debian.
		</para>
		 <section id="sect.fai">
			<title>Fully Automatic Installer (FAI)</title>
			 <indexterm>
				<primary>Fully Automatic Installer (FAI)</primary>
			</indexterm>
			 <para>
				<foreignphrase>Fully Automatic Installer</foreignphrase> — это, возможно, самая старая система автоматизированного развёртывания Debian, чем объясняется её статус эталонной; но её очень гибкая натура едва компенсирует привносимую ей сложность.
			</para>
			 <para>
				FAI требуется серверная система для хранения информации для развёртывания и обеспечения загрузки целевых машин по сети. Для этого сервера нужен пакет <emphasis role="pkg">fai-server</emphasis> (или <emphasis role="pkg">fai-quickstart</emphasis>, в который также входят необходимые элементы для стандартной конфигурации).
			</para>
			 <para>
				В FAI используется специфический подход к определению разных профилей установки. FAI не просто дублирует эталонную установку, он является полноценным установщиком, полностью настраиваемым через набор файлов и сценариев, хранящихся на сервере; расположение их по умолчанию <filename>/srv/fai/config/</filename> не создаётся автоматически, так что администратору нужно создать его вместе с соответствующими файлами. В большинстве случаев эти файлы будут модифицированными файлами примеров, взятых из документации пакета <emphasis role="pkg">fai-doc</emphasis>, а точнее из каталога <filename>/usr/share/doc/fai-doc/examples/simple/</filename>.
			</para>
			 <para>
				Когда профили определены, с помощью команды <command>fai-setup</command> генерируются элементы, необходимые для запуска FAI-установки; под этим подразумевается главным образом подготовка или обновление минимальной системы (NFS-root), используемой в процессе установки. Альтернативой является генерация специального загрузочного CD с помощью <command>fai-cd</command>.
			</para>
			 <para>
				Для создания всех этих конфигурационных файлов нужно иметь представление о том, как работает FAI. Типичный процесс установки включает следующие шаги:
			</para>
			 <itemizedlist>
				<listitem>
					<para>
						получение ядра по сети и загрузка его;
					</para>

				</listitem>
				 <listitem>
					<para>
						монтирование корневой файловой системы по NFS;
					</para>

				</listitem>
				 <listitem>
					<para>
						запуск <command>/usr/sbin/fai</command>, который контролирует оставшуюся часть процесса (последующие шаги, таким образом, запускаются этим сценарием);
					</para>

				</listitem>
				 <listitem>
					<para>
						копирование конфигурации с сервера в <filename>/fai/</filename>;
					</para>

				</listitem>
				 <listitem>
					<para>
						запуск <command>fai-class</command>. Сценарии <filename>/fai/class/[0-9][0-9]*</filename> последовательно выполняются и возвращают имена «классов», которые применяются к устанавливаемой машине; эта информация послужит основой для дальнейших шагов. Это придаёт некоторую гибкость в определении сервисов, которые следует установить и настроить.
					</para>

				</listitem>
				 <listitem>
					<para>
						получение набора переменных конфигурации, в зависимости от соответствующих классов;
					</para>

				</listitem>
				 <listitem>
					<para>
						разметка дисков и форматирование разделов на основании информации, предоставленной классом <filename>/fai/disk_config/<replaceable>class</replaceable></filename>;
					</para>

				</listitem>
				 <listitem>
					<para>
						монтирование указанных раделов;
					</para>

				</listitem>
				 <listitem>
					<para>
						установка базовой системы;
					</para>

				</listitem>
				 <listitem>
					<para>
						предварительная подготовка базы данных Debconf с помощью <command>fai-debconf</command>;
					</para>

				</listitem>
				 <listitem>
					<para>
						получение списка доступных пакетов для APT;
					</para>

				</listitem>
				 <listitem>
					<para>
						установка пакетов, перечисленных в <filename>/fai/package_config/<replaceable>class</replaceable></filename>;
					</para>

				</listitem>
				 <listitem>
					<para>
						выполнение постконфигурационных сценариев, <filename>/fai/scripts/<replaceable>class</replaceable>/[0-9][0-9]*</filename>;
					</para>

				</listitem>
				 <listitem>
					<para>
						запись журналов установки, отмонтирование разделов и перезагрузка.
					</para>

				</listitem>

			</itemizedlist>

		</section>
		 <section id="sect.d-i-preseeding">
			<title>Пресидинг Debian-Installer</title>
			 <indexterm>
				<primary>preseed</primary>
			</indexterm>
			 <indexterm>
				<primary>преднастройка</primary>
			</indexterm>
			 <para>
				В конце концов, если рассуждать логически, лучшим инструментом для установки Debian должен быть официальный установщик Debian. По этой причине debian-installer с самого начала разрабатывался для автоматизированной установки, используя возможности, предоставляемые <emphasis role="pkg">debconf</emphasis>. Последняя позволяет, с одной стороны, уменьшить число задаваемых вопросов (для скрытых вопросов будет использоваться ответ, заданный по умолчанию), а с другой стороны, устанавливать ответы по умолчанию отдельно, так что установка может быть неинтерактивной. Последняя возможность известна как <emphasis>пресидинг</emphasis> (<foreignphrase>preseeding</foreignphrase>).
			</para>
			 <sidebar> <title><emphasis>УГЛУБЛЯЕМСЯ</emphasis> Debconf с централизованной базой данных</title>
			 <indexterm>
				<primary><command>debconf</command></primary>
			</indexterm>
			 <para>
				Пресидинг позволяет предоставить набор ответов на вопросы, задаваемые Debconf во время установки, но эти ответы являются статичными и не меняются с течением времени. Поскольку уже установленные машины могут нуждаться в обновлении, и могут потребоваться новые ответы, конфигурационный файл <filename>/etc/debconf.conf</filename> можно настроить таким образом, чтобы Debconf использовал внешние источники данных (таки как сервер каталогов LDAP или удалённый файл, доступный через NFS или Samba). Можно задать несколько разных источников данных, которые будут дополнять друг друга. Локальная база данных по-прежнему будет использоваться (для доступа на чтение и запись), в то время как удалённые базы обычно ограничиваются только чтением. На странице руководства <citerefentry><refentrytitle>debconf.conf</refentrytitle>
				 <manvolnum>5</manvolnum></citerefentry> подробно описаны возможные варианты (понадобится пакет <emphasis role="pkg">debconf-doc</emphasis>).
			</para>
			 </sidebar> <section>
				<title>Использование preseed-файла</title>
				 <para>
					Есть несколько мест, откуда установщик может получить файл пресидинга:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							в initrd, используемом для запуска машины; в этом случае пресидинг происходит на самом раннем этапе установки, и можно избежать каких бы то ни было вопросов. Нужно лишь назвать файл <filename>preseed.cfg</filename> и сохранить его в корне initrd.
						</para>

					</listitem>
					 <listitem>
						<para>
							на загрузочном носителе (CD или USB-брелоке); пресидинг в таком случае происходит, как только носитель смонтирован, то есть сразу после вопросов о языке и раскладке клавиатуры. Для указания расположения файла пресидинга можно использовать параметр загрузки <literal>preseed/file</literal> (например, <filename>/cdrom/preseed.cfg</filename> при установке с CD-ROM, или <filename>/hd-media/preseed.cfg</filename> в случае USB-брелока).
						</para>

					</listitem>
					 <listitem>
						<para>
							из сети; в таком случае пресидинг происходит после (автоматическиой) настройки сети; соответствующий загрузочный параметр в таком случае — <literal>preseed/url=http://<replaceable>server</replaceable>/preseed.cfg</literal>.
						</para>

					</listitem>

				</itemizedlist>
				 <para>
					На первый взгляд, включение файла пресидинга в initrd выглядит наиболее интересным решением; однако оно редко используется на практике, потому что генерация initrd установщика довольно сложна. Другие два решения гораздо более общеприняты, тем более сто параметры загрузки предоставляют другой путь пресидинг ответов на первые вопросы процесса установки. Обычный путь избежания возни с вписыванием параметров загрузки вручную при каждой установки — сохранить их в конфигурации <command>isolinux</command> (в случае CD-ROM) или <command>syslinux</command> (USB-брелок).
				</para>

			</section>
			 <section>
				<title>Создание preseed-файла</title>
				 <para>
					Preseed-файл — это простой текстовый файл, в котором каждая строка содержит ответ на один вопрос Debconf. Строка разбита на четыре поля, разделённых между собой пробельными символами (пробелами или символами табуляции), например <literal>d-i mirror/suite string stable</literal>:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							первое поле — это «владелец» вопроса; «d-i» используется для вопросов, относящихся к установщику, но это также может быть имя пакета для вопросов, относящихся к пакетам Debian;
						</para>

					</listitem>
					 <listitem>
						<para>
							второе поле — это идентификатор вопроса;
						</para>

					</listitem>
					 <listitem>
						<para>
							третье — тип вопроса;
						</para>

					</listitem>
					 <listitem>
						<para>
							четвёртое и последнее поле содержит значение ответа. Заметьте, что оно должно быть отделено от третьего поля одним пробелом; если пробелов больше одного, последующие пробелы будут считаться частью значения.
						</para>

					</listitem>

				</itemizedlist>
				 <para>
					Простейший путь написать preseed-файл — установить систему вручную. После этого <command>debconf-get-selections --installer</command> предоставит ответы, относящиеся к установщику. Ответы о других пакетах могут быть получены с помощью <command>debconf-get-selections</command>. Однако более правильным решением будет написать preseed-файл вручную, руководствуясь примером и справочной документацией: при таком подходе пресидингу подвергнутся только вопросы, для которых следует изменить значение ответа по умолчанию; используя параметр загрузки <literal>priority=critical</literal>, можно указать Debconf, что следует задавать только критические вопросы, и использовать ответ по умолчанию для остальных.
				</para>
				 <sidebar> <title><emphasis>ДОКУМЕНТАЦИЯ</emphasis> Приложение к руководству по установке</title>
				 <para>
					Руководство по установке, доступное онлайн, включает подробную документацию по использованию preseed-файла в приложении. В него также входит пример такого файла с подробными комментариями, который может служить основой для подстройки под свои нужды. <ulink type="block" url="https://www.debian.org/releases/jessie/amd64/apb.html" /> <ulink type="block" url="https://www.debian.org/releases/jessie/example-preseed.txt" />
				</para>
				 </sidebar>
			</section>
			 <section>
				<title>Создание модифицированного загрузочного носителя</title>
				 <para>
					Знать, где разместить preseed-файл, конечно, уже хорошо, но одного этого знания недостаточно: нужно, так или иначе, внести изменения в загрузочные параметры носителя, с которого осуществляется установка, и добавить preseed-файл.
				</para>
				 <section>
					<title>Сетевая загрузка</title>
					 <para>
						Когда компьютер загружается по сети, сервер, отправляющий элементы для инициализации, также определяет параметры загрузки. Таким образом, изменения надо вносить в конфигурацию PXE на сервере загрузки; точнее, в его конфигурационный файл <filename>/tftpboot/pxelinux.cfg/default</filename>. Предварительно нужно настроить сетевую загрузку; подробности смотрите в инструкции по установке. <ulink type="block" url="http://www.debian.org/releases/jessie/amd64/ch04s05.html" />
					</para>

				</section>
				 <section>
					<title>Подготовка загрузочного USB-брелока</title>
					 <para>
						Когда загрузочный брелок подготовлен (см. <xref linkend="sect.install-usb" />), нужно выполнить несколько дополнительных операций. Если содержимое брелока доступно в каталоге <filename>/media/usbdisk/</filename>:
					</para>
					 <itemizedlist>
						<listitem>
							<para>
								скопируйте файл ответов в <filename>/media/usbdisk/preseed.cfg</filename>
							</para>

						</listitem>
						 <listitem>
							<para>
								отредактируйте <filename>/media/usbdisk/syslinux.cfg</filename> и добавьте необходимые параметры загрузки (см. пример ниже).
							</para>

						</listitem>

					</itemizedlist>
					 <example>
						<title>файл syslinux.cfg и параметры файла ответов</title>
						 
<programlisting>default vmlinuz
append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --</programlisting>

					</example>

				</section>
				 <section>
					<title>Создание образа CD-ROM</title>
					 <indexterm>
						<primary>debian-cd</primary>
					</indexterm>
					 <para>
						USB-брелок является перезаписываемым носителем, поэтому нам было легко добавить туда файл и изменить несколько параметров. В случае CD-ROM эта процедура усложняется, поскольку требуетс перегенерировать весь ISO-образ. Для этой задачи служит <emphasis role="pkg">debian-cd</emphasis>, но этот инструмент несколько неудобен в использовании: ему требуется локальное зеркало, и для работы с ним необходимо понимать все опции <filename>/usr/share/debian-cd/CONF.sh</filename>; даже при соблюдении этих условий нужно несколько раз запускать <command>make</command>. По этой причине крайне рекомендуется ознакомиться с файлом <filename>/usr/share/debian-cd/README</filename>.
					</para>
					 <para>
						С другой стороны, debian-cd всегда работает сходным образом: создаётся каталог «образа» с содержимым CD-ROM, а затем он преобразуется в ISO-образ с помощью такого инструмента как <command>genisoimage</command>, <command>mkisofs</command> или <command>xorriso</command>. Создание каталога образа завершается после выполнения шага <command>make image-trees</command>. На этом этапе мы добавляем preseed-файл в соответствующий каталог (обычно <filename>$TDIR/$CODENAME/CD1/</filename>, где $TDIR и $CODENAME являются параметрами, определёнными в конфигурационном файле <filename>CONF.sh</filename>). На CD-ROM в качестве загрузчика используется <command>isolinux</command>, и необходимо изменить его конфигурацию, сгенерированную debian-cd, чтобы добавить нужные параметры загрузки (в файле <filename>$TDIR/$CODENAME/boot1/isolinux/isolinux.cfg</filename>). После этого можно продолжить «нормальную» процедуру и сгенерировать ISO-образ с помощью <command>make image CD=1</command> (или <command>make images</command>, если генерируются несколько образов).
					</para>

				</section>

			</section>

		</section>
		 <section id="sect.simple-cdd">
			<title>Simple-CDD: решение «всё-в-одном»</title>
			 <indexterm>
				<primary>simple-cdd</primary>
			</indexterm>
			 <para>
				Простого использования preseed-файла недостаточно, чтобы удовлетворить всем требованиям, которые могут предъявляться при массовом развёртывании. Несмотря на наличие возможности выполнить некоторые сценарии в конце обычного процесса установки, выбор набора пакетов для установки всё же недостаточно гибок (собственно, можно выбрать для установки только «задачи»); что более важно, возможна установка только официальных пакетов Debian, но не локально собранных.
			</para>
			 <para>
				С другой стороны, debian-cd способен включать сторонние пакеты, а debian-installer может быть расширен путём включения новых шагов в процесс установки. Совмещение этих возможностей позволило бы создать модифицированный установщик, удовлетворяющий нашим запросам; он даже мог бы быть способен сконфигурировать некоторые сервисы после распаковки необходимых пакетов. К счастью, это не пустое предположение, поскольку это в точности то, что делает Simple-CDD (в пакете <emphasis role="pkg">simple-cdd</emphasis>).
			</para>
			 <para>
				Назначение Simple-CDD — дать возможность каждому легко создавать дистрибутив, производный от Debian, выбрав набор пакетов из числа доступных, предварительно настроив их с помощью Debconf, добавив специальное программное обеспечение и добавив сценарии, которые будут выполнены в конце установки. Это соответствует принцину «универсальной операционной системы», поскольку каждый может адаптировать её под свои собственные нужды.
			</para>
			 <section>
				<title>Создание профилей</title>
				 <para>
					Simple-CDD определяет «профили», сходные с «классами» FAI, причём у машины может быть несколько профилей (назначенных во время установки). Профиль определяется набором файлов <filename>profiles/<replaceable>profile</replaceable>.*</filename>:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							файл <filename>.description</filename> содержит одну строку с описанием профиля;
						</para>

					</listitem>
					 <listitem>
						<para>
							файл <filename>.packages</filename> содержит список пакетов, которые будут автоматически установлены при выборе профиля;
						</para>

					</listitem>
					 <listitem>
						<para>
							файл <filename>.downloads</filename> содержит список пакетов, которые будут записаны на установочный носитель, но не обязательно установлены;
						</para>

					</listitem>
					 <listitem>
						<para>
							файл <filename>.preseed</filename> содержит информацию для пресидинга вопросов Debconf (для установщика и/или пакетов);
						</para>

					</listitem>
					 <listitem>
						<para>
							файл <filename>.postinst</filename> содержит сценарий, который будет запущен по завершении процесса установки;
						</para>

					</listitem>
					 <listitem>
						<para>
							наконец, файл <filename>.conf</filename> позволяет менять некоторые параметры Simple-CDD на основании профилей, включённых в образ.
						</para>

					</listitem>

				</itemizedlist>
				 <para>
					Профиль <literal>default</literal> играет особую роль, поскольку он всегда выбран; это минимальный профиль, необходимый для работы Simple-CDD. Единственное, что обычно настраивается в этом профиле, — параметр пресидинга <literal>simple-cdd/profiles</literal>: это позволяет избежать вопроса, добавленного Simple-CDD, о том, какие профили необходимо установить.
				</para>
				 <para>
					Заметьте также, что команды потребуется вызывать из родительского каталога по отношению к каталогу <filename>profiles</filename>.
				</para>

			</section>
			 <section>
				<title>Настройка и использование <command>build-simple-cdd</command></title>
				 <indexterm>
					<primary><command>build-simple-cdd</command></primary>
				</indexterm>
				 <sidebar> <title><emphasis>КРАТКИЙ ЭКСКУРС</emphasis> Конфигурационный файл в подробностях</title>
				 <para>
					Пример конфигурационого файлв Simple-CDD со всеми возможными параметрами входит в состав пакета (<filename>/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz</filename>). Его можно использовать как отправную точку при создании своего конфигурационного файла.
				</para>
				 </sidebar> <para>
					Simple-CDD для полноценной работы требуется передать множество параметров. Чаще всего они указываются в конфигурационном файле, который передаётся <command>build-simple-cdd</command> с помощью опции <literal>--conf</literal>, но они также могут быть указаны в виде отдельных параметров <command>build-simple-cdd</command>. Вот беглый обзор того, как эта соманда себя ведёт, и как можно использовать эти параметры:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							параметр <literal>profiles</literal> служит для перечисления профилей, которые будут включены на генерируемый образ CD-ROM;
						</para>

					</listitem>
					 <listitem>
						<para>
							на основании списка необходимых пакетов Simple-CDD загружает соответствующие файлы с сервера, указанного в параметре <literal>server</literal>, и собирает их них частичное зеркало (которое будет затем передано debian-cd);
						</para>

					</listitem>
					 <listitem>
						<para>
							пользовательские пакеты, указанные в параметре <literal>local_packages</literal>, также включаются в это локальное зеркало;
						</para>

					</listitem>
					 <listitem>
						<para>
							затем запускается debian-cd (в каталоге по умолчанию, который можно задать с помощью переменной <literal>debian_cd_dir</literal>) со списком пакетов для включения;
						</para>

					</listitem>
					 <listitem>
						<para>
							когда debian-cd подготовит свой каталог, Simple-CDD вносит в него некоторые изменения;
						</para>
						 <itemizedlist>
							<listitem>
								<para>
									файлы с профилями добавляются в подкаталог <filename>simple-cdd</filename> (который будет записан на CD-ROM);
								</para>

							</listitem>
							 <listitem>
								<para>
									также добавляются другие файлы, перечисленные в параметре <literal>all_extras</literal>;
								</para>

							</listitem>
							 <listitem>
								<para>
									параметры загрузки изменяются, чобы включить пресидинг. Вопросов о языке и стране можно избежать, если сохранить необходимую информацию в переменных <literal>language</literal> и <literal>country</literal>.
								</para>

							</listitem>

						</itemizedlist>

					</listitem>
					 <listitem>
						<para>
							После этого debian-cd генерирует окончательный ISO-образ.
						</para>

					</listitem>

				</itemizedlist>

			</section>
			 <section>
				<title>Генерация ISO-образа</title>
				 <para>
					Когда мы написали конфигурационный файл и определили наши профили, осталось только запустить <command>build-simple-cdd --conf simple-cdd.conf</command>. Через несколько минут мы получим требуемый образ <filename>images/debian-8.0-amd64-CD-1.iso</filename>.
				</para>

			</section>

		</section>

	</section>
	 <section id="sect.monitoring">
		<title>Мониторинг</title>
		 <para>
			Мониторинг — это общее понятие, и разные его аспекты преследуют разные цели: с одной стороны, отслеживание использования машинных ресурсов позволяет предсказать их исчерпание и необходимость их увеличения; с другой стороны, уведомление администратора, когда сервис стал недоступен или работает некорректно, означает, что возникающие проблемы будут устраняться скорее.
		</para>
		 <para>
			<emphasis>Munin</emphasis> покрывает первую область, отображая графики истории ряда параметров (используемой ОЗУ, занятого дискового пространства, загрузки процессора, сетевого трафика, нагрузки Apache/MySQL, и т. п.). <emphasis>Nagios</emphasis> покрывает вторую область, регулярно проверяя, что сервисы работают и доступны, и посылая аварийные уведомления по соответствующим каналам (e-mail, текстовые сообщения и т. п.). Оба построены модульно, что позволяет легко создавать новые плагины для мониторинга специфических параметров и сервисов.
		</para>
		 <sidebar> <title><emphasis>АЛЬТЕРНАТИВА</emphasis> Zabbix, комплексный инструмент мониторинга</title>
		 <indexterm>
			<primary>Zabbix</primary>
		</indexterm>
		 <para>
			Хотя Munin и Nagios используются очень широко, они — не единственные игроки на поле мониторинга, и каждый из них выполняет только половину задачи (отображение графиков с одной стороны, аварийные оповещения с другой). Zabbix же объединяет обе части мониторинга; у него также есть веб-интерфейс для настройки наиболее общих аспектов. Он развивался скачкообразно на протяжении последних нескольких лет и теперь может считаться достойным конкурентом. На сервере следует установить <emphasis role="pkg">zabbix-server-pgsql</emphasis> (или <emphasis role="pkg">zabbix-server-mysql</emphasis>), возможно вместе с <emphasis role="pkg">zabbix-frontend-php</emphasis>, чтобы получить веб-интерфейс. На узлах, которые надлежит мониторить, следует установить <emphasis role="pkg">zabbix-agent</emphasis>, отправляющий данные на сервер. <ulink type="block" url="http://www.zabbix.org/" />
		</para>
		 </sidebar> <sidebar> <title><emphasis>АЛЬТЕРНАТИВА</emphasis> Icinga, ответвление Nagios</title>
		 <indexterm>
			<primary>Icinga</primary>
		</indexterm>
		 <para>
			Из-за расхождения во взглядах на модель разработки Nagios (который контролируется компанией) часть разработчиков создала ответвление Nagios под названием Icinga. Icinga остаётся совместимым — по крайней мере пока — с настройками и плагинами Nagios, но добавляет дополнительный функционал. <ulink type="block" url="http://www.icinga.org/" />
		</para>
		 </sidebar> <section id="sect.munin">
			<title>Настройка Munin</title>
			 <indexterm>
				<primary>Munin</primary>
			</indexterm>
			 <para>
				Назначение Munin — наблюдать за множеством машин, и вполне естественно, что он имеет клиент-серверную архитектуру. Центральный узел — построитель графиков — собирает данные со всех наблюдаемых узлов и создаёт графики истории.
			</para>
			 <section>
				<title>Настройка узлов для мониторинга</title>
				 <para>
					Первый шаг — установка пакета <emphasis role="pkg">munin-node</emphasis>. Демон, устанавливаемый этим пакетом, слушает порт 4949 и отправляет данные, собранные всеми активными плагинами. Каждый плагин представляет собой простую программу, возвращающую описание собранных данных и последнее измеренное значение. Плагины хранятся в <filename>/usr/share/munin/plugins/</filename>, но на деле используются только те из них, символьные ссылки на которые присутствуют в <filename>/etc/munin/plugins/</filename>.
				</para>
				 <para>
					После установки пакета набор активных плагинов определяется в зависимости от доступного программного обеспечение и текущей настройки узла. Однако такая автоматическая настройка должна поддерживаться каждым плагином, и, как правило, бывает неплохо проверить результаты и исправить их вручную. Может оказаться небезынтересным просмотреть <ulink url="http://gallery.munin-monitoring.org">Галерею плагинов</ulink>, хотя не все плагины сопровождаются исчерпывающей документацией. К счастью, все плагины являются сценариями, и большинство их довольно просты и содержат подробные комментарии. Так что просмотр содержимого <filename>/etc/munin/plugins/</filename> — хороший способ понять, для чего нужен каждый плагин, и определиться, какие из них следует удалить. Аналогичным образом можно включить интересный плагин, найденный в <filename>/usr/share/munin/plugins/</filename>, просто создав символьную ссылку на него с помощью <command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command>. Заметьте, что когда имя плагина заканчивается символом подчёркивания «_», плагину требуется параметр. Этот параметр должен храниться в имени символьной ссылки; например, плагин «if_» следует включить, создав символьную ссылку <filename>if_eth0</filename>, тогда он будет отслеживать сетевой трафик на интерфейсе eth0.
				</para>
				 <para>
					Когда плагины настроены, следует отредактировать конфигурацию демона, описав правила контроля доступа к собранным данным. Для этого служат директивы <literal>allow</literal> в файле <filename>/etc/munin/munin-node.conf</filename>. Настройка по умолчанию — <literal>allow ^127\.0\.0\.1$</literal>, она разрешает доступ только с локального узла. Обычно администратору требуется добавить аналогичную строку, содержащую IP-адрес узла построения графиков, а затем перезапустить демон с помощью <command>service munin-node restart</command>.
				</para>
				 <sidebar> <title><emphasis>УГЛУБЛЯЕМСЯ</emphasis> Создание локальных плагинов</title>
				 <para>
					Munin содержит подробную документацию о том, как плагины должны себя вести, и как разрабатывать новые плагины. <ulink type="block" url="http://munin-monitoring.org/wiki/plugins" />
				</para>
				 <para>
					Лучше всего тестировать плагин, запуская его в тех же самых условиях, в которых он будет вызываться munin-node; их можно имитировать, запустив <command>munin-run <replaceable>plugin</replaceable></command> от имени суперпользователя. Возможный второй параметр этой команды (например <literal>config</literal>) передаётся как параметр плагину.
				</para>
				 <para>
					Когда плагин вызывается с параметром <literal>config</literal>, он должен описать себя, вернув набор полей:
				</para>
				 
<screen><computeroutput>$ </computeroutput><userinput>sudo munin-run load config
</userinput><computeroutput>graph_title Load average
graph_args --base 1000 -l 0
graph_vlabel load
graph_scale no
graph_category system
load.label load
graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run "immediately").
load.info 5 minute load average
</computeroutput></screen>
				 <para>
					Разные доступные поля описаны в «Справочнике по плагинам», доступному как часть «Руководства Munin». <ulink type="block" url="http://munin.readthedocs.org/en/latest/reference/plugin.html" />
				</para>
				 <para>
					Будучи вызванным без параметра, плагин просто возвращает последние измеренные значения; к примеру, вызов <command>sudo munin-run load</command> может вернуть <literal>load.value 0.12</literal>.
				</para>
				 <para>
					Наконец, когда плагин вызывается с параметром <literal>autoconf</literal>, он должен вернуть «yes» (и статус выхода 0) или «no» (и статус выхода 1) в зависимости от того, следует ли включать плагин на этом узле.
				</para>
				 </sidebar>
			</section>
			 <section>
				<title>Настройка построителя графиков</title>
				 <para>
					«Построитель графиков» — это просто компьютер, собирающий данные и создающий на их основании графики. Необходимое для него программное обеспечение находится в пакете <emphasis role="pkg">munin</emphasis>. Стандартная конфигурация запускает <command>munin-cron</command> (раз в 5 минут), который собирает данные со всех узлов, перечисленных в <filename>/etc/munin/munin.conf</filename> (по умолчанию там указан только локальный узел), сохраняет данные в файлах RRD (<emphasis>Round Robin Database</emphasis> — формат файлов, разработанный для хранения данных, меняющихся со временем), хранящихся в <filename>/var/lib/munin/</filename>, и генерирующий HTML-страницу с графиками в <filename>/var/cache/munin/www/</filename>.
				</para>
				 <para>
					Итак, все наблюдаемые машины должны быть перечислены в конфигурационном файле <filename>/etc/munin/munin.conf</filename>. Каждая машина указывается как целая секция с именем, соответствующим машине, и как минимум записью <literal>address</literal>, содержащей её IP-адрес.
				</para>
				 
<programlisting>[ftp.falcot.com]
    address 192.168.0.12
    use_node_name yes</programlisting>
				 <para>
					Секции могут быть более сложными и описывать дополнительные графики, которые могут быть созданы путём сочетания данных с разных машин. Примеры, приведённые в конфигурационном файле, будут неплохой начальной точкой для настройки.
				</para>
				 <para>
					Последний шаг — публикация сгенерированных страниц; для этого требуется настроить веб-сервер таким образом, чтобы содержимое <filename>/var/cache/munin/www/</filename> было доступно на сайте. Доступ к этому сайту зачастую будет ограничен с помощью или механизма аутентификации, или правил контроля доступа по IP-адресам. Подробности см. в <xref linkend="sect.http-web-server" />.
				</para>

			</section>

		</section>
		 <section id="sect.nagios">
			<title>Настройка Nagios</title>
			 <indexterm>
				<primary>Nagios</primary>
			</indexterm>
			 <para>
				В отличие от Munin, Nagios не требует обязательной установки чего бы то ни было на наблюдаемых узлах; чаще всего Nagios используется для проверки доступности сетевых сервисов. Например, Nagios может подключиться к веб-серверу и проверить, что конкретная веб-страница может быть получена за заданное время.
			</para>
			 <section>
				<title>Установка</title>
				 <para>
					Первый шаг установки Nagios заключается в установке пакетов <emphasis role="pkg">nagios3</emphasis>, <emphasis role="pkg">nagios-plugins</emphasis> и <emphasis role="pkg">nagios3-doc</emphasis>. В процессе установки настраивается веб-интерфейс и создаётся первый пользователь <literal>nagiosadmin</literal> (для которого запрашивается пароль). Других пользователей можно добавить в файл <filename>/etc/nagios3/htpasswd.users</filename> с помощью команды <command>htpasswd</command> из состава Apache. Если во время установки не отображался диалог Debconf, задать пароль пользователя <literal>nagiosadmin</literal> можно с помощью <command>dpkg-reconfigure nagios3-cgi</command>.
				</para>
				 <para>
					Открыв в обозревателе <literal>http://<replaceable>server</replaceable>/nagios3/</literal>, можно попасть в веб-интерфейс; заметьте, что Nagios отслеживает некоторые параметры машины, на которой он запущен. Однако некоторые интерактивные функции, такие как добавление комментариев к узлу, не работают. Они выключены в конфигурации Nagios по умолчанию, которая сильно ограничена в целях безопасности.
				</para>
				 <para>
					Как описано в <filename>/usr/share/doc/nagios3/README.Debian</filename>, для включения некоторых функций требуется отредактировать <filename>/etc/nagios3/nagios.cfg</filename> и установить в нём значение параметра <literal>check_external_commands</literal> в «1». Нам также потребуется дать права на запись в каталог, используемый Nagios, с помощью таких команд:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>service nagios3 stop</userinput>
<computeroutput>[...]
# </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios3/rw
</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios3
</userinput><computeroutput># </computeroutput><userinput>service nagios3 start</userinput>
<computeroutput>[...]</computeroutput></screen>

			</section>
			 <section>
				<title>Настройка</title>
				 <para>
					Веб-интерфейс Nagios довольно симпатичный, но не позволяет менять настройки или добавлять наблюдаемые узлы и сервисы. Вся конфигурация управляется через файлы, указанные в центральном конфигурационном файле <filename>/etc/nagios3/nagios.cfg</filename>.
				</para>
				 <para>
					В эти файлы не стоит погружаться, не вникнув в некоторые базовые принципы Nagios. В конфигурации перечисляются объекты следующих типов:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							<emphasis>host</emphasis> (узел) — машина, которую необходимо наблюдать;
						</para>

					</listitem>
					 <listitem>
						<para>
							<emphasis>hostgroup</emphasis> (группа узлов) — набор узлов, которые следует сгруппировать вместе при отображении или для учёта некоторых общих элементов конфигурации;
						</para>

					</listitem>
					 <listitem>
						<para>
							<emphasis>service</emphasis> (сервис) — тестируемый элемент, относящийся к узлу или группе узлов. Это, как правило, проверка сетевого сервиса, хотя сюда может входить и проверка, держатся ли некоторые параметры на приемлемом уровне (например свободное дисковое пространство или загрузка процессора);
						</para>

					</listitem>
					 <listitem>
						<para>
							<emphasis>servicegroup</emphasis> (группа сервисов) — набор сервисов, которые следует сгруппировать вместе при отображении;
						</para>

					</listitem>
					 <listitem>
						<para>
							<emphasis>contact</emphasis> (контакт) — лицо, которому следует направлять аварийные предупреждения;
						</para>

					</listitem>
					 <listitem>
						<para>
							<emphasis>contactgroup</emphasis> (группа контактов) — набор таких контактов;
						</para>

					</listitem>
					 <listitem>
						<para>
							<emphasis>timeperiod</emphasis> (временной интервал) — промежуток времени, в течение которого должны быть проверены некоторые сервисы;
						</para>

					</listitem>
					 <listitem>
						<para>
							<emphasis>command</emphasis> (команда) — командная строка, выполняемая для проверки данного сервиса.
						</para>

					</listitem>

				</itemizedlist>
				 <para>
					У каждого объекта, в соответствии с его типом, есть набор свойств, которые можно менять. Полный список слишком длинен, чтобы приводить его здесь, поэтому отметим только самые важные свойства и отношения между объектами.
				</para>
				 <para>
					Сервис использует команду для проверки состояния некой функциональности на узле (или группе узлов) на протяжении временного интервала. В случае проблемы Nagios отправляет предупреждение всем членам группы контактов, привязанной к сервису. Предупреждение отправляется каждому члену в соответствии с каналом, описанным в соответствующем объекте контакта.
				</para>
				 <para>
					Система наследования позволяет легко разделять наборы свойств между объектами без дублирования информации. Более того, начальная конфигурация уже включает набор стандартных объектов; зачастую для определения новых узлов, сервисов и контактов достаточно сделать их потомками стандартных объектов. Файлы в <filename>/etc/nagios3/conf.d/</filename> — хороший источник информации о том, как это работает.
				</para>
				 <para>
					Администраторы Falcot Corp используют следующую конфигурацию:
				</para>
				 <example>
					<title>Файл <filename>/etc/nagios3/conf.d/falcot.cfg</filename></title>
					 
<programlisting>define contact{
    name                            generic-contact
    service_notification_period     24x7
    host_notification_period        24x7
    service_notification_options    w,u,c,r
    host_notification_options       d,u,r
    service_notification_commands   notify-service-by-email
    host_notification_commands      notify-host-by-email
    register                        0 ; Template only
}
define contact{
    use             generic-contact
    contact_name    rhertzog
    alias           Raphael Hertzog
    email           hertzog@debian.org
}
define contact{
    use             generic-contact
    contact_name    rmas
    alias           Roland Mas
    email           lolando@debian.org
}

define contactgroup{
    contactgroup_name     falcot-admins
    alias                 Falcot Administrators
    members               rhertzog,rmas
}

define host{
    use                   generic-host ; Name of host template to use
    host_name             www-host
    alias                 www.falcot.com
    address               192.168.0.5
    contact_groups        falcot-admins
    hostgroups            debian-servers,ssh-servers
}
define host{
    use                   generic-host ; Name of host template to use
    host_name             ftp-host
    alias                 ftp.falcot.com
    address               192.168.0.6
    contact_groups        falcot-admins
    hostgroups            debian-servers,ssh-servers
}

# команда 'check_ftp' с пользовательскими параметрами
define command{
    command_name          check_ftp2
    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35
}

# Стандартный сервис Falcot
define service{
    name                  falcot-service
    use                   generic-service
    contact_groups        falcot-admins
    register              0
}

# Сервисы, проверяемые на www-host
define service{
    use                   falcot-service
    host_name             www-host
    service_description   HTTP
    check_command         check_http
}
define service{
    use                   falcot-service
    host_name             www-host
    service_description   HTTPS
    check_command         check_https
}
define service{
    use                   falcot-service
    host_name             www-host
    service_description   SMTP
    check_command         check_smtp
}

# Сервисы, проверяемые на ftp-host
define service{
    use                   falcot-service
    host_name             ftp-host
    service_description   FTP
    check_command         check_ftp2
}</programlisting>

				</example>
				 <para>
					В этом конфигурационном файле описаны два наблюдаемых узла. Первый — веб-сервер, на нём проверяются порты HTTP (80) и HTTPS (443). Nagios также проверяет, что на порту 25 запущен SMTP-сервер. Второй узел — FTP-сервер, для него проверяется среди прочего, что он отвечает в течение 20 секунд. При превышении этого порога отправляется <emphasis>предупреждение</emphasis>; при задержке более 30 секунд ситуация считается критической. Веб-интерфейс Nagios также показывает, что наблюдается сервис SSH: это общая настройка всех узлов группы <literal>ssh-servers</literal>. Соответствующий стандартный сервис определён в <filename>/etc/nagios3/conf.d/services_nagios2.cfg</filename>.
				</para>
				 <para>
					Обратите внимание на использование наследования: то, что объект наследует другому объекту, указывается с помощью «use <replaceable>имя-родителя</replaceable>». Родительский объект должен быть идентифицируемым, для чего ему должно быть установлено свойство «name <replaceable>идентификатор</replaceable>». Если родительский объект не является реальным объектом, а служит только для создания потомков, следует установить есу свойство «register 0»; оно укажет Nagios, что объект не надо учитывать, и тогда нехватка некоторых параметров, которые в ином случае были бы обязательными, будет проигнорирована.
				</para>
				 <sidebar> <title><emphasis>ДОКУМЕНТАЦИЯ</emphasis> Список свойств объектов</title>
				 <para>
					Более глубокое понимание различных способов настройки Nagios можно получить с помощью документации, предоставляемой пакетом <emphasis role="pkg">nagios3-doc</emphasis>. Эта документация непосредственно доступна через веб-интерфейс, с помощью ссылки «Документация» в верхнем левом углу. Она включает список всех типов объектов со всеми свойствами, которыми они могут быть наделены. Там же описано, как создавать новые плагины.
				</para>
				 </sidebar> <sidebar> <title><emphasis>УГЛУБЛЯЕМСЯ</emphasis> Удалённые проверки с помощью NRPE</title>
				 <para>
					Многие плагины Nagios позволяют проверять ряд параметров локально на узле; если требуется производить такие проверки на многих машинах, в то время как центральная установка будет их собирать информацию, нужно установить плагин NRPE (<emphasis>Nagios Remote Plugin Executor</emphasis>). Пакет <emphasis role="pkg">nagios-nrpe-plugin</emphasis> должен быть установлен на сервере Nagios, а <emphasis role="pkg">nagios-nrpe-server</emphasis> — на узлах, где следует запускать локальные проверки. Последний читает конфигурацию из <filename>/etc/nagios/nrpe.cfg</filename>. Этот файл должен содержать список проверок, которые могут запускаться удалённо, и IP-адреса машин, которые могут их запускать. На стороне Nagios эти удалённые проверки включаются путём добавления соответствующих сервисов с использованием новой команды <emphasis>check_nrpe</emphasis>.
				</para>
				 </sidebar>
			</section>

		</section>

	</section>
</chapter>

