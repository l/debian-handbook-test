<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html
    xmlns="http://www.w3.org/1999/xhtml"><head><meta
        http-equiv="Content-Type"
        content="text/html; charset=UTF-8" /><title
        xmlns:d="http://docbook.org/ns/docbook">Chapitre 12. Administration avancée</title><link
        rel="stylesheet"
        type="text/css"
        href="Common_Content/css/default.css" /><link
        rel="stylesheet"
        media="print"
        href="Common_Content/css/print.css"
        type="text/css" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="generator"
        content="publican v4.3.2" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="package"
        content="Debian-debian-handbook-8-fr-FR-1.0-1" /><meta
        name="keywords"
        content="RAID, LVM, FAI, Présélection, Supervision, Virtualisation, Xen, LXC" /><link
        rel="home"
        href="index.html"
        title="Le cahier de l'administrateur Debian" /><link
        rel="up"
        href="index.html"
        title="Le cahier de l'administrateur Debian" /><link
        rel="prev"
        href="sect.rtc-services.html"
        title="11.8. Services de communication en temps réel" /><link
        rel="next"
        href="sect.virtualization.html"
        title="12.2. Virtualisation" /><link
        xmlns=""
        rel="canonical"
        href="https://debian-handbook.info/browse/fr-FR/stable/advanced-administration.html" /></head><body><div
        id="banner"><a
          href="http://debian-handbook.info/get/"><span
            class="text">Download the ebook</span></a></div><p
        id="title"><a
          class="left"
          href="http://www.debian.org"><img
            alt="Product Site"
            src="Common_Content/images//image_left.png" /></a><a
          class="right"
          href="index.html"><img
            alt="Documentation Site"
            src="Common_Content/images//image_right.png" /></a></p><ul
        class="docnav top"><li
          class="previous"><a
            accesskey="p"
            href="sect.rtc-services.html"><strong>Précédent</strong></a></li><li
          class="home">Le cahier de l'administrateur Debian</li><li
          class="next"><a
            accesskey="n"
            href="sect.virtualization.html"><strong>Suivant</strong></a></li></ul><div
        xml:lang="fr-FR"
        class="chapter"
        lang="fr-FR"><div
          class="titlepage"><div><div><h1
                class="title"><a
                  xmlns=""
                  id="advanced-administration"></a>Chapitre 12. Administration avancée</h1></div></div></div><div
          class="toc"><dl
            class="toc"><dt><span
                class="section"><a
                  href="advanced-administration.html#sect.raid-and-lvm">12.1. RAID et LVM</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.raid-soft">12.1.1. RAID logiciel</a></span></dt><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.lvm">12.1.2. LVM</a></span></dt><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.raid-or-lvm">12.1.3. RAID ou LVM ?</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.virtualization.html">12.2. Virtualisation</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.virtualization.html#sect.xen">12.2.1. Xen</a></span></dt><dt><span
                    class="section"><a
                      href="sect.virtualization.html#sect.lxc">12.2.2. LXC</a></span></dt><dt><span
                    class="section"><a
                      href="sect.virtualization.html#id-1.15.5.14">12.2.3. Virtualisation avec KVM</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.automated-installation.html">12.3. Installation automatisée</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.fai">12.3.1. Fully Automatic Installer (FAI)</a></span></dt><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.d-i-preseeding">12.3.2. Debian-installer avec préconfiguration</a></span></dt><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.simple-cdd">12.3.3. Simple-CDD : la solution tout en un</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.monitoring.html">12.4. Supervision</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.monitoring.html#sect.munin">12.4.1. Mise en œuvre de Munin</a></span></dt><dt><span
                    class="section"><a
                      href="sect.monitoring.html#sect.nagios">12.4.2. Mise en œuvre de Nagios</a></span></dt></dl></dd></dl></div><div
          class="highlights"><div
            class="para">
		Ce chapitre est l'occasion de revenir sur des aspects déjà abordés mais avec une nouvelle perspective : au lieu d'installer une machine, nous étudierons les solutions pour déployer un parc de machines ; au lieu de créer une partition RAID ou LVM avec les outils intégrés à l'installateur, nous apprendrons à le faire manuellement afin de pouvoir revenir sur les choix initiaux. Enfin, la découverte des outils de supervision et de virtualisation parachèvera ce chapitre avant tout destiné aux administrateurs professionnels plus qu'aux particuliers responsables de leur réseau familial.
	</div></div><div
          class="section"><div
            class="titlepage"><div><div><h2
                  class="title"><a
                    xmlns=""
                    id="sect.raid-and-lvm"></a>12.1. RAID et LVM</h2></div></div></div><div
            class="para">
			Le <a
              class="xref"
              href="installation.html">Chapitre 4, <em>Installation</em></a> a présenté ces technologies en montrant comment l'installateur facilitait leur déploiement. Au-delà de cette étape cruciale, un bon administrateur doit pouvoir gérer l'évolution de ses besoins en espace de stockage sans devoir recourir à une coûteuse réinstallation. Il convient donc de maîtriser les outils qui servent à manipuler des volumes RAID et LVM.
		</div><div
            class="para">
			Ces deux techniques permettent d'abstraire les volumes à monter de leurs contreparties physiques (disques durs ou partitions), la première pour sécuriser les données face aux pannes matérielles en dupliquant les informations, et la seconde pour gérer ses données à sa guise en faisant abstraction de la taille réelle de ses disques. Dans les deux cas, cela se traduit par de nouveaux périphériques de type bloc sur lesquels on pourra donc créer des systèmes de fichiers, ou des espaces de mémoire virtuelle, qui ne correspondent pas directement à un seul disque dur réel. Bien que ces deux systèmes aient des origines bien distinctes, leurs fonctionnalités se recoupent en partie ; c'est pourquoi ils sont souvent mentionnés ensemble.
		</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>PERSPECTIVE</em></span> Btrfs combine LVM et RAID</strong></p></div></div></div><div
              class="para">
			Alors que LVM et RAID sont deux sous-systèmes distincts du noyau qui viennent s'intercaler entre les périphériques blocs des disques et les systèmes de fichiers, <span
                class="emphasis"><em>btrfs</em></span> est un nouveau système de fichiers initié par Oracle qui combine les fonctionnalités de LVM et RAID, et plus encore. Bien que fonctionnel, et même s'il est encore marqué d'un sceau « expérimental » (il n'est pas terminé, certaines fonctionnalités ne sont pas encore implémentées), il a déjà fait l'objet de quelques déploiements en production. <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://btrfs.wiki.kernel.org/">http://btrfs.wiki.kernel.org/</a></div>
		</div><div
              class="para">
			Parmi les fonctionnalités les plus intéressantes, on peut noter la possibilité de capturer l'état d'une arborescence à un instant donné. Cette copie ne consomme initialement pas d'espace disque, chaque fichier n'étant réellement dupliqué que lorsque l'une des deux copies est modifiée. Il gère également la compression (transparente) des fichiers et des sommes de contrôle pour s'assurer de l'intégrité de toutes les données stockées.
		</div></div><div
            class="para">
			À la fois pour RAID et pour LVM, le noyau fournit un fichier de périphérique accessible en mode bloc (donc de la même manière qu'un disque dur ou une partition de celui-ci). Lorsqu'une application, ou une autre partie du noyau, a besoin d'accéder à un bloc de ce périphérique, le sous-système correspondant se charge d'effectuer le routage de ce bloc vers la couche physique appropriée, ce bloc pouvant être stocké sur un ou plusieurs disques, à un endroit non directement corrélé avec l'emplacement demandé dans le périphérique logique.
		</div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      xmlns=""
                      id="sect.raid-soft"></a>12.1.1. RAID logiciel</h3></div></div></div><a
              id="id-1.15.4.6.2"
              class="indexterm"></a><div
              class="para">
				RAID signifie <span
                class="emphasis"><em>Redundant Array of Independent Disks</em></span> (ensemble redondant de disques indépendants). Ce système a vu le jour pour fournir une protection contre les pannes de disques durs. Le principe général est simple : les données sont stockées sur un ensemble de plusieurs disques physiques au lieu d'être concentrées sur un seul, avec un certain degré (variable) de redondance. Selon ce niveau de redondance, en cas de panne subite d'un de ces disques, les données peuvent être reconstruites à l'identique à partir des disques qui restent opérationnels, ce qui évite de les perdre.
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>CULTURE</em></span> <span
                          class="foreignphrase"><em
                            class="foreignphrase">Independent</em></span> ou <span
                          class="foreignphrase"><em
                            class="foreignphrase">inexpensive</em></span> ?</strong></p></div></div></div><div
                class="para">
				Le « I » de RAID signifiait à l'origine <span
                  class="emphasis"><em>inexpensive</em></span> (« bon marché »), car le RAID permet d'obtenir une nette augmentation de la sécurité des données sans investir dans des disques hors de prix. Peut-être pour des considérations d'image, on a tendance à préférer <span
                  class="emphasis"><em>independent</em></span>, qui n'a pas la connotation de bricolage associée au bon marché.
			</div></div><div
              class="para">
				Le RAID peut être mis en œuvre par du matériel dédié (soit des modules RAID intégrés à des cartes pour contrôleur SCSI, soit directement sur la carte mère) ou par l'abstraction logicielle (le noyau). Qu'il soit matériel ou logiciel, un système RAID disposant de suffisamment de redondance peut, en cas de défaillance d'un disque, rester opérationnel en toute transparence, le niveau supérieur (les applications) pouvant même continuer à accéder aux données sans interruption malgré la panne. Évidemment, ce « mode dégradé » peut avoir des implications en termes de performances et il réduit la quantité de redondance du système ; une deuxième panne simultanée peut aboutir à la perte des données. Il est donc d'usage de ne rester en mode dégradé que le temps de se procurer un remplaçant pour le disque défaillant. Une fois qu'il est mis en place, le système RAID peut reconstruire les données qui doivent y être présentes, de manière à revenir à un état sécurisé. Le tout se fait bien entendu de manière invisible pour les applications, hormis les baisses éventuelles de performances pendant la durée du mode dégradé et la phase de reconstruction qui s'ensuit.
			</div><div
              class="para">
				Lorsque le RAID est implémenté par le matériel, c'est le setup du BIOS qui permet généralement de le configurer et le noyau Linux va considérer le volume RAID comme un seul disque, fonctionnant comme un disque standard, à ceci près que le nom du périphérique peut différer.
			</div><div
              class="para">
				Nous ne traiterons que du RAID logiciel dans ce livre.
			</div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.raid-levels"></a>12.1.1.1. Différents niveaux de RAID</h4></div></div></div><div
                class="para">
					On distingue plusieurs niveaux de RAID, différant dans l'agencement des données et le degré de redondance qu'ils proposent. Plus la redondance est élevée, plus la résistance aux pannes sera forte, puisque le système pourra continuer à fonctionner avec un plus grand nombre de disques en panne ; la contrepartie est que le volume utile de données devient plus restreint (ou, pour voir les choses différemment, qu'il sera nécessaire d'avoir plus de disques pour stocker la même quantité de données).
				</div><div
                class="variablelist"><dl
                  class="variablelist"><dt><span
                      class="term">RAID linéaire</span></dt><dd><div
                      class="para">
								Bien que le sous-système RAID du noyau permette la mise en œuvre de « RAID linéaire », il ne s'agit pas à proprement parler de RAID, puisqu'il n'y a aucune redondance. Le noyau se contente d'agréger plusieurs disques les uns à la suite des autres et de les proposer comme un seul disque virtuel (en fait, un seul périphérique bloc). C'est à peu près sa seule utilité et il n'est que rarement utilisé seul (voir plus loin), d'autant que l'absence de redondance signifie que la défaillance d'un seul disque rend la totalité des données inaccessibles.
							</div></dd><dt><span
                      class="term">RAID-0</span></dt><dd><div
                      class="para">
								Ici non plus, aucune redondance n'est proposée, mais les disques ne sont plus simplement mis bout à bout : ils sont en réalité découpés en <span
                        class="emphasis"><em>stripes</em></span> (bandes), ces bandes étant alors intercalées dans le disque logique. Ainsi, dans le cas de RAID-0 à deux disques, les blocs impairs du volume virtuel seront stockés sur le premier disque et les blocs pairs sur le second.
							</div><div
                      class="para">
								Le but de ce système n'est pas d'augmenter la fiabilité, puisqu'ici aussi un seul disque en panne rend inaccessible la totalité des données, mais d'améliorer les performances : lors d'un accès séquentiel à de grandes quantités de données contiguës, le noyau pourra lire (ou écrire) en parallèle depuis les deux (ou plus...) disques qui composent l'ensemble, ce qui augmente le débit. L'usage du RAID-0 a tendance à disparaître au profit de LVM, qui sera abordé par la suite.
							</div></dd><dt><span
                      class="term">RAID-1</span></dt><dd><div
                      class="para">
								Aussi connu sous le nom de « RAID miroir », c'est le système RAID le plus simple. Il utilise en général deux disques physiques de tailles identiques et fournit un volume logique de la même taille. Les données sont stockées à l'identique sur les deux disques, d'où l'appellation de « miroir ». En cas de panne d'un disque, les données restent accessibles sur l'autre. Pour les données vraiment critiques, on peut utiliser le RAID-1 sur plus de deux disques, au prix de devoir multiplier le rapport entre le coût des disques et la quantité de données utiles.
							</div><div
                      class="sidebar"><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>NOTE</em></span> Taille des disques, taille de l'ensemble</strong></p></div></div></div><div
                        class="para">
								Si deux disques de tailles différentes sont groupés dans un volume en RAID miroir, le plus gros disque ne sera pas intégralement utilisé, puisqu'il contiendra exactement les mêmes données que le plus petit (et rien de plus). La capacité utile du volume RAID-1 est donc la plus petite des tailles des disques qui le composent. Il en va de même pour les volumes RAID de niveau plus élevé, même si la redondance est répartie différemment.
							</div><div
                        class="para">
								On veillera donc à n'assembler dans un même volume RAID (sauf RAID-0 et linéaire) que des disques de même taille (ou de tailles très proches), afin d'éviter un gaspillage des ressources.
							</div></div><div
                      class="sidebar"><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>NOTE</em></span> Disques de secours</strong></p></div></div></div><div
                        class="para">
								Dans les niveaux qui offrent une redondance, on peut affecter à un volume RAID plus de disques que nécessaire, qui serviront de secours en cas de défaillance d'un disque principal. Ainsi, il est possible de mettre en place un miroir de deux disques plus un de secours ; si l'un des deux premiers disques tombe, le noyau va automatiquement en reconstruire le contenu sur le disque de secours, de sorte que la redondance restera assurée (après le temps de reconstruction). Ceci est utile pour des données vraiment critiques.
							</div><div
                        class="para">
								On peut s'interroger sur l'intérêt de cette possibilité, comparé par exemple à l'établissement d'un miroir sur trois disques. L'avantage de cette configuration est en fait que le disque de secours peut être partagé entre plusieurs volumes RAID. On peut ainsi avoir trois volumes miroir, avec l'assurance que les données seront toujours stockées en double même en cas de panne d'un disque, avec seulement 7 disques (trois paires, plus un disque de secours partagé) au lieu de 9 (trois triplets).
							</div></div><div
                      class="para">
								Ce niveau de RAID, bien qu'onéreux (puisque seule la moitié, au mieux, de l'espace disque physique se retrouve utilisable), reste assez utilisé en pratique. Il est en effet conceptuellement simple et il permet de réaliser très simplement des sauvegardes (puisque les deux disques sont identiques, on peut en extraire temporairement un sans perturber le fonctionnement du système). Les performances en lecture sont généralement améliorées par rapport à un simple disque (puisque le système peut théoriquement lire la moitié des données sur chaque disque, en parallèle sur les deux), sans trop de perte en vitesse d'écriture. Dans le cas de RAID-1 à N disques, les données restent disponibles même en cas de panne de N-1 disques.
							</div></dd><dt><span
                      class="term">RAID-4</span></dt><dd><div
                      class="para">
								Ce niveau de RAID, assez peu usité, utilise N disques pour stocker les données utiles, et un disque supplémentaire pour des informations de redondance. Si ce disque tombe en panne, le système peut le reconstruire à l'aide des N autres. Si c'est un des N disques de données qui tombe, les N-1 restants et le disque de parité contiennent suffisamment d'informations pour reconstruire les données.
							</div><div
                      class="para">
								Le RAID-4 est peu onéreux (puisqu'il n'entraîne qu'un surcoût d'un disque pour N), n'a pas d'impact notable sur les performances en lecture, mais ralentit les écritures. De plus, comme chaque écriture sur un des N disques s'accompagne d'une écriture sur le disque de parité, celui-ci se voit confier beaucoup plus d'écritures que ceux-là et peut voir sa durée de vie considérablement réduite en conséquence. Il résiste au maximum à la panne d'un disque parmi les N+1.
							</div></dd><dt><span
                      class="term">RAID-5</span></dt><dd><div
                      class="para">
								Le RAID-5 corrige ce dernier défaut du RAID-4, en répartissant les blocs de parité sur les N+1 disques, qui jouent à présent un rôle identique.
							</div><div
                      class="para">
								Les performances en lecture et en écriture sont inchangées par rapport au RAID-4. Là encore, le système reste fonctionnel en cas de défaillance d'un disque parmi les N+1, mais pas plus.
							</div></dd><dt><span
                      class="term">RAID-6</span></dt><dd><div
                      class="para">
								Le RAID-6 peut être considéré comme une extension du RAID-5, dans laquelle à chaque série de N blocs correspondent non plus un mais deux blocs de parité, qui sont ici aussi répartis sur les N+2 disques.
							</div><div
                      class="para">
								Ce niveau de RAID, légèrement plus coûteux que les deux précédents, apporte également une protection supplémentaire puisqu'il conserve l'intégralité des données même en cas de panne simultanée de deux disques (sur N+2). La contrepartie est que les opérations d'écriture impliquent dorénavant l'écriture d'un bloc de données et de deux blocs de contrôle, ce qui les ralentit d'autant plus.
							</div></dd><dt><span
                      class="term">RAID-1+0</span></dt><dd><div
                      class="para">
								Il ne s'agit pas à proprement parler d'un niveau de RAID, mais d'un RAID à deux niveaux. Si l'on dispose de 2×N disques, on commence par les apparier en N volumes de RAID-1. Ces N volumes sont alors agrégés en un seul, soit par le biais de RAID linéaire, soit par du RAID-0, voire (de plus en plus fréquemment) par LVM. On sort dans ce dernier cas du RAID pur, mais cela ne pose pas de problème.
							</div><div
                      class="para">
								Le RAID-1+0 tolère la panne de disques multiples, jusqu'à N dans le cas d'un groupe de 2×N, à condition qu'au moins un disque reste fonctionnel dans chaque paire associée en RAID-1.
							</div><div
                      class="sidebar"><a
                        xmlns=""
                        id="sidebar.raid-10"></a><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>POUR ALLER PLUS LOIN</em></span> RAID-10</strong></p></div></div></div><div
                        class="para">
								« RAID-10 » est généralement considéré comme un synonyme pour « RAID-1+0 », mais une spécificité de Linux en fait en réalité une généralisation. On peut dans ce mode de fonctionnement obtenir un système où chaque bloc existe en double sur deux disques différents, malgré un nombre impair de disques, les copies étant réparties selon différents modèles en fonction de la configuration.
							</div><div
                        class="para">
								Les performances pourront varier en fonction du modèle et du niveau de redondance choisis, ainsi que du type d'activité sur le volume logique.
							</div></div></dd></dl></div><div
                class="para">
					On choisira bien entendu le niveau de RAID en fonction des contraintes et des besoins spécifiques de chaque application. Notons qu'on peut constituer plusieurs volumes RAID distincts, avec des configurations différentes, sur le même ordinateur.
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.raid-setup"></a>12.1.1.2. Mise en place du RAID</h4></div></div></div><a
                id="id-1.15.4.6.9.2"
                class="indexterm"></a><div
                class="para">
					La mise en place de volumes RAID se fait grâce au paquet <span
                  class="pkg pkg">mdadm</span> ; ce dernier contient la commande du même nom, <code
                  class="command">mdadm</code>, qui permet de créer et manipuler des ensembles RAID, ainsi que les scripts et outils permettant l'intégration avec le système et la supervision.
				</div><div
                class="para">
					Prenons l'exemple d'un serveur sur lequel sont branchés un certain nombre de disques, dont certains sont occupés et d'autres peuvent être utilisés pour établir du RAID. On dispose initialement des disques et partitions suivants :
				</div><div
                xmlns:d="http://docbook.org/ns/docbook"
                class="itemizedlist"><ul><li
                    class="listitem"><div
                      class="para">
							le disque <code
                        class="filename">sdb</code>, de 4 Go, est entièrement disponible ;
						</div></li><li
                    class="listitem"><div
                      class="para">
							le disque <code
                        class="filename">sdc</code>, de 4 Go, est également entièrement disponible ;
						</div></li><li
                    class="listitem"><div
                      class="para">
							sur le disque <code
                        class="filename">sdd</code>, seule la partition <code
                        class="filename">sdd2</code> d'environ 4 Go est disponible ;
						</div></li><li
                    class="listitem"><div
                      class="para">
							enfin, un disque <code
                        class="filename">sde</code>, toujours de 4 Go, est entièrement disponible.
						</div></li></ul></div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>NOTE</em></span> Identifier les volumes RAID existants</strong></p></div></div></div><div
                  class="para">
					Le fichier <code
                    class="filename">/proc/mdstat</code> liste les volumes existants et leur état. On prendra soin lors de la création d'un nouveau volume RAID de ne pas essayer de le nommer avec le nom d'un volume existant.
				</div></div><div
                class="para">
					Nous allons construire sur ces éléments physiques deux volumes, l'un en RAID-0, l'autre en miroir. Commençons par le RAID-0 :
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</code></strong>
<code
                  class="computeroutput">mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
# </code><strong
                  class="userinput"><code>mdadm --query /dev/md0</code></strong>
<code
                  class="computeroutput">/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md0</code></strong>
<code
                  class="computeroutput">/dev/md0:
        Version : 1.2
  Creation Time : Wed May  6 09:24:34 2015
     Raid Level : raid0
     Array Size : 8387584 (8.00 GiB 8.59 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:24:34 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

     Chunk Size : 512K

           Name : mirwiz:0  (local to host mirwiz)
           UUID : bb085b35:28e821bd:20d697c9:650152bb
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       1       8       32        1      active sync   /dev/sdc
# </code><strong
                  class="userinput"><code>mkfs.ext4 /dev/md0</code></strong>
<code
                  class="computeroutput">mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 2095104 4k blocks and 524288 inodes
Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </code><strong
                  class="userinput"><code>mkdir /srv/raid-0</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mount /dev/md0 /srv/raid-0</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/raid-0</code></strong>
<code
                  class="computeroutput">Filesystem      Size  Used Avail Use% Mounted on
/dev/md0        7.9G   18M  7.4G   1% /srv/raid-0
</code></pre><div
                class="para">
					La commande <code
                  class="command">mdadm --create</code> requiert en arguments le nom du volume à créer (<code
                  class="filename">/dev/md*</code>, MD signifiant <span
                  class="foreignphrase"><em
                    class="foreignphrase">Multiple Device</em></span>), le niveau de RAID, le nombre de disques (qui prend tout son sens à partir du RAID-1 mais qui est systématiquement obligatoire) et les périphériques à utiliser. Une fois le périphérique créé, nous pouvons l'utiliser comme une partition normale, y créer un système de fichiers, le monter, etc. On notera que le fait que nous ayons créé un volume RAID-0 sur <code
                  class="filename">md0</code> est une pure coïncidence et que le numéro d'un ensemble n'a pas à être corrélé avec le modèle de redondance (ou non) choisi. Il est également possible de créer des volumes RAID nommés, en indiquant à <code
                  class="command">mdadm</code> des noms de volume comme <code
                  class="filename">/dev/md/lineaire</code> au lieu de <code
                  class="filename">/dev/md0</code>.
				</div><div
                class="para">
					La création d'un volume RAID-1 se fait de manière similaire, les différences n'apparaissant qu'après sa création :
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%
Continue creating array? </code><strong
                  class="userinput"><code>y</code></strong>
<code
                  class="computeroutput">mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md1 started.
# </code><strong
                  class="userinput"><code>mdadm --query /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
        Version : 1.2
  Creation Time : Wed May  6 09:30:19 2015
     Raid Level : raid1
     Array Size : 4192192 (4.00 GiB 4.29 GB)
  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:30:40 2015
          State : clean, resyncing (PENDING) 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       1       8       64        1      active sync   /dev/sde
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
          State : clean
[...]
</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>ASTUCE</em></span> RAID, disques et partitions</strong></p></div></div></div><div
                  class="para">
					Comme on peut le constater sur notre exemple, il n'est nullement nécessaire d'utiliser des disques entiers pour faire du RAID, on peut très bien n'en utiliser que certaines partitions.
				</div></div><div
                class="para">
					Plusieurs remarques ici. Premièrement, <code
                  class="command">mdadm</code> constate que les deux éléments physiques n'ont pas la même taille ; comme cela implique que de l'espace sera perdu sur le plus gros des deux éléments, une confirmation est nécessaire.
				</div><div
                class="para">
					La deuxième remarque, plus importante, concerne l'état du miroir. Les deux disques sont en effet censés avoir, en fonctionnement normal, un contenu rigoureusement identique. Comme rien ne garantit que ce soit le cas à la création du volume, le système RAID va s'en assurer. Il y a donc une phase de synchronisation, automatique, dès la création du périphérique RAID. Si l'on patiente quelques instants (qui varient selon la taille des disques...), on obtient finalement un état « actif » ou « propre ». Il est à noter que durant cette étape de reconstruction du miroir, l'ensemble RAID est en mode dégradé et que la redondance n'est pas assurée. Une panne de l'un des disques pendant cette fenêtre sensible peut donc aboutir à la perte de l'intégralité des données. Il est cependant rare que de grandes quantités de données critiques soient placées sur un volume RAID fraîchement créé avant que celui-ci ait eu le temps de se synchroniser. On notera également que même en mode dégradé, le périphérique <code
                  class="filename">/dev/md1</code> est utilisable (pour créer le système de fichiers et commencer à copier des données, éventuellement).
				</div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>ASTUCE</em></span> Démarrer un miroir en mode dégradé</strong></p></div></div></div><div
                  class="para">
					Lorsque l'on souhaite sécuriser des données présentes sur un disque non RAID et les migrer vers un volume RAID-1, il n'est pas toujours possible de disposer à la fois de l'ancien disque et des deux nouveaux en même temps (souvent parce que le disque existant va être intégré dans le miroir). On peut alors délibérément créer le volume RAID-1 en mode dégradé, en passant <code
                    class="filename">missing</code> en argument à <code
                    class="command">mdadm</code> à la place d'un des deux périphériques à utiliser. Une fois que les données auront été copiées vers le « miroir », le disque historique pourra être intégré au volume. Une synchronisation aura alors lieu, à la suite de quoi les données seront stockées de manière redondante.
				</div></div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>ASTUCE</em></span> Mise en place d'un miroir sans synchronisation</strong></p></div></div></div><div
                  class="para">
					Lorsque l'on crée un volume RAID-1, c'est généralement pour l'utiliser comme un disque neuf, donc a priori vierge. Le contenu initial de ce disque importe peu, puisque l'on n'a besoin que de savoir que les données écrites seront bien restituées (en particulier le système de fichiers).
				</div><div
                  class="para">
					Il peut donc sembler superflu de synchroniser les deux disques d'un miroir lors de sa création. À quoi sert d'avoir un contenu identique sur des zones du volume dont on sait qu'on ne se servira qu'après y avoir écrit ?
				</div><div
                  class="para">
					Il est heureusement possible de se passer de cette phase de synchronisation, grâce à l'option <code
                    class="literal">--assume-clean</code> de <code
                    class="command">mdadm</code>. Cette option pouvant amener à des résultats surprenants dans les cas d'usage où les données initiales seront utilisées (par exemple si un système de fichiers est déjà présent), elle n'est pas active par défaut.
				</div></div><div
                class="para">
					Voyons à présent ce qui se passe en cas de panne d'un élément de l'ensemble RAID-1. <code
                  class="command">mdadm</code> permet de simuler cette défaillance, grâce à son option <code
                  class="literal">--fail</code> :
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --fail /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: set /dev/sde faulty in /dev/md1
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Update Time : Wed May  6 09:39:39 2015
          State : clean, degraded 
 Active Devices : 1
Working Devices : 1
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 19

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       0        0        2      removed

       1       8       64        -      faulty   /dev/sde</code></pre><div
                class="para">
					Le contenu du volume reste accessible (et, s'il est monté, les applications ne s'aperçoivent de rien), mais la sécurité des données n'est plus assurée : si le disque <code
                  class="filename">sdd</code> venait à tomber en panne, les données seraient perdues. Pour éviter ce risque, nous allons remplacer ce disque par un disque neuf, <code
                  class="filename">sdf</code> :
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --add /dev/sdf</code></strong>
<code
                  class="computeroutput">mdadm: added /dev/sdf
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
   Raid Devices : 2
  Total Devices : 3
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:48:49 2015
          State : clean, degraded, recovering 
 Active Devices : 1
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 1

 Rebuild Status : 28% complete

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 26

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      spare rebuilding   /dev/sdf

       1       8       64        -      faulty   /dev/sde
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Update Time : Wed May  6 09:49:08 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 41

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf

       1       8       64        -      faulty   /dev/sde</code></pre><div
                class="para">
					Ici encore, nous avons une phase de reconstruction, déclenchée automatiquement, pendant laquelle le volume, bien qu'accessible, reste en mode dégradé. Une fois qu'elle est terminée, le RAID revient dans son état normal. On peut alors signaler au système que l'on va retirer le disque <code
                  class="filename">sde</code> de l'ensemble, pour se retrouver avec un miroir classique sur deux disques :
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --remove /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: hot removed /dev/sde from /dev/md1
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf</code></pre><div
                class="para">
					Le disque pourra alors être démonté physiquement lors d'une extinction de la machine. Dans certaines configurations matérielles, les disques peuvent même être remplacés à chaud, ce qui permet de se passer de cette extinction. Parmi ces configurations, on trouvera certains contrôleurs SCSI, la plupart des systèmes SATA et les disques externes sur bus USB ou Firewire.
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.backup-raid-config"></a>12.1.1.3. Sauvegarde de la configuration</h4></div></div></div><div
                class="para">
					La plupart des métadonnées concernant les volumes RAID sont sauvegardées directement sur les disques qui les composent, de sorte que le noyau peut détecter les différents ensembles avec leurs composants et les assembler automatiquement lors du démarrage du système. Cela dit, il convient de sauvegarder cette configuration, car cette détection n'est pas infaillible et aura tendance à faillir précisément en période sensible. Si dans notre exemple la panne du disque <code
                  class="filename">sde</code> était réelle, et si on redémarrait le système sans le retirer, ce disque pourrait, à la faveur du redémarrage, « retomber en marche ». Le noyau aurait alors trois éléments physiques, chacun prétendant représenter la moitié du même volume RAID. Une autre source de confusion peut subvenir si l'on consolide des volumes RAID de deux serveurs sur un seul. Si ces ensembles étaient en fonctionnement normal avant le déplacement des disques, le noyau saura reconstituer les paires correctement. Mais pour peu que les disques déplacés soient agrégés en un <code
                  class="filename">/dev/md1</code> et qu'il existe également un <code
                  class="filename">md1</code> sur le serveur consolidé, l'un des miroirs sera contraint de changer de nom.
				</div><div
                class="para">
					Il est donc important de sauvegarder la configuration, ne serait-ce qu'à des fins de référence. Pour cela, on éditera le fichier <code
                  class="filename">/etc/mdadm/mdadm.conf</code>, dont un exemple est donné ci-dessous.
				</div><div
                class="example"><a
                  xmlns=""
                  id="example.mdadm-conf"></a><p
                  class="title"><strong>Exemple 12.1. Fichier de configuration de mdadm</strong></p><div
                  class="example-contents"><pre
                    class="programlisting"># mdadm.conf
#
# Please refer to mdadm.conf(5) for information about this file.
#

# by default (built-in), scan all partitions (/proc/partitions) and all
# containers for MD superblocks. alternatively, specify devices to scan, using
# wildcards if desired.
DEVICE /dev/sd*

# auto-create devices with Debian standard permissions
CREATE owner=root group=disk mode=0660 auto=yes

# automatically tag new arrays as belonging to the local system
HOMEHOST &lt;system&gt;

# instruct the monitoring daemon where to send mail alerts
MAILADDR root

# definitions of existing MD arrays
ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464

# This configuration was auto-generated on Thu, 17 Jan 2013 16:21:01 +0100
# by mkconf 3.2.5-3
</pre></div></div><div
                class="para">
					Une des informations les plus souvent utiles est l'option <code
                  class="literal">DEVICE</code>, qui spécifie l'ensemble des périphériques sur lesquels le système va chercher automatiquement des composants de volumes RAID au démarrage. Nous avons ici remplacé la valeur implicite, <code
                  class="literal">partitions containers</code>, par une liste explicite de fichiers de périphérique ; nous avons en effet choisi d'utiliser des disques entiers, et non simplement des partitions, pour certains volumes.
				</div><div
                class="para">
					Les deux dernières lignes de notre exemple sont celles qui permettent au noyau de choisir en toute sécurité quel numéro de volume associer à quel ensemble. Les méta-informations stockées sur les disques sont en effet suffisantes pour reconstituer les volumes, mais pas pour en déterminer le numéro (donc le périphérique <code
                  class="filename">/dev/md*</code> correspondant).
				</div><div
                class="para">
					Fort heureusement, ces lignes peuvent être générées automatiquement :
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --misc --detail --brief /dev/md?</code></strong>
<code
                  class="computeroutput">ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</code></pre><div
                class="para">
					Le contenu de ces deux dernières lignes ne dépend pas de la liste des disques qui composent les volumes. On pourra donc se passer de les régénérer si l'on remplace un disque défectueux par un neuf. En revanche, il faudra prendre soin de les mettre à jour après chaque création ou suppression de volume.
				</div></div></div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      xmlns=""
                      id="sect.lvm"></a>12.1.2. LVM</h3></div></div></div><a
              id="id-1.15.4.7.2"
              class="indexterm"></a><a
              id="id-1.15.4.7.3"
              class="indexterm"></a><div
              class="para">
				LVM, ou <span
                class="foreignphrase"><em
                  class="foreignphrase">Logical Volume Manager</em></span>, est une autre approche servant à abstraire les volumes logiques des disques physiques. Le but principal n'était pas ici de gagner en fiabilité des données mais en souplesse d'utilisation. LVM permet en effet de modifier dynamiquement un volume logique, en toute transparence du point de vue des applications. Par exemple, on peut ainsi ajouter de nouveaux disques, migrer les données dessus et récupérer les anciens disques ainsi libérés, sans démonter le volume.
			</div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.lvm-concepts"></a>12.1.2.1. Concepts de LVM</h4></div></div></div><div
                class="para">
					LVM manipule trois types de volumes pour atteindre cette flexibilité.
				</div><div
                class="para">
					Premièrement, les PV ou <span
                  class="foreignphrase"><em
                    class="foreignphrase">physical volumes</em></span> sont les entités les plus proches du matériel : il peut s'agir de partitions sur des disques ou de disques entiers, voire de n'importe quel périphérique en mode bloc (y compris, par exemple, un volume RAID). Attention, lorsqu'un élément physique est initialisé en PV pour LVM, il ne faudra plus l'utiliser directement, sous peine d'embrouiller le système.
				</div><div
                class="para">
					Les PV sont alors regroupés en VG <span
                  class="foreignphrase"><em
                    class="foreignphrase">(volume groups)</em></span>, que l'on peut considérer comme des disques virtuels (et extensibles, comme on le verra). Les VG sont abstraits et ne disposent pas de fichier spécial dans <code
                  class="filename">/dev/</code>, aucun risque donc de les utiliser directement.
				</div><div
                class="para">
					Enfin, les LV <span
                  class="foreignphrase"><em
                    class="foreignphrase">(logical volumes)</em></span> sont des subdivisions des VG, que l'on peut comparer à des partitions sur les disques virtuels que les VG représentent. Ces LV deviennent des périphériques, que l'on peut utiliser comme toute partition physique (par exemple pour y établir des systèmes de fichiers).
				</div><div
                class="para">
					Il faut bien réaliser que la subdivision d'un groupe de volumes en LV est entièrement décorrélée de sa composition physique (les PV). On peut ainsi avoir un VG subdivisé en une douzaine de volumes logiques tout en ne comportant qu'un volume physique (un disque, par exemple), ou au contraire un seul gros volume logique réparti sur plusieurs disques physiques ou partitions. La seule contrainte, bien entendu, est que la somme des tailles des LV d'un groupe ne doive pas excéder la capacité totale des PV qui le composent.
				</div><div
                class="para">
					En revanche, il est souvent utile de grouper dans un même VG des éléments physiques présentant des caractéristiques similaires et de subdiviser ce VG en volumes logiques qui seront utilisés de manière comparable également. Par exemple, si l'on dispose de disques rapides et de disques lents, on pourra regrouper les rapides dans un VG et les plus lents dans un autre. Les subdivisions logiques du premier VG pourront alors être affectées à des tâches nécessitant de bonnes performances, celles du second étant réservées aux tâches qui peuvent se contenter de vitesses médiocres.
				</div><div
                class="para">
					Dans tous les cas, il faut également garder à l'esprit qu'un LV n'est pas accroché à un PV ou un autre. Même si on peut influencer l'emplacement physique où les données d'un LV sont écrites, en fonctionnement normal c'est une information qui n'a pas d'intérêt particulier. Au contraire : lors d'une modification des composants physiques d'un groupe, les LV peuvent être amenés à se déplacer (tout en restant, bien entendu, confinés aux PV qui composent ce groupe).
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.lvm-setup"></a>12.1.2.2. Mise en place de LVM</h4></div></div></div><div
                class="para">
					Nous allons suivre pas à pas une utilisation typique de LVM, pour simplifier une situation complexe. De telles situations sont souvent le résultat d'un historique chargé, où des solutions temporaires se sont accumulées au fil du temps. Considérons donc pour notre exemple un serveur dont les besoins en stockage ont varié et pour lequel on se retrouve avec une configuration complexe de partitions disponibles, morcelées sur différents disques hétéroclites et partiellement utilisés. Concrètement, on dispose des partitions suivantes :
				</div><div
                xmlns:d="http://docbook.org/ns/docbook"
                class="itemizedlist"><ul><li
                    class="listitem"><div
                      class="para">
							sur le disque <code
                        class="filename">sdb</code>, une partition <code
                        class="filename">sdb2</code> de 4 Go ;
						</div></li><li
                    class="listitem"><div
                      class="para">
							sur le disque <code
                        class="filename">sdc</code>, une partition <code
                        class="filename">sdc3</code> de 3 Go ;
						</div></li><li
                    class="listitem"><div
                      class="para">
							le disque <code
                        class="filename">sdd</code>, de 4 Go, est entièrement disponible ;
						</div></li><li
                    class="listitem"><div
                      class="para">
							sur le disque <code
                        class="filename">sdf</code>, une partition <code
                        class="filename">sdf1</code> de 4 Go et une <code
                        class="filename">sdf2</code> de 5 Go.
						</div></li></ul></div><div
                class="para">
					On notera de plus que les disques <code
                  class="filename">sdb</code> et <code
                  class="filename">sdf</code> ont de meilleures performances que les deux autres.
				</div><div
                class="para">
					Le but de la manœuvre est de mettre en place trois volumes logiques distincts, pour trois applications séparées : un serveur de fichiers (qui nécessite 5 Go), une base de données (1 Go) et un emplacement pour les sauvegardes (12 Go). Les deux premières ont de forts besoins de performance, mais pas la troisième, qui est moins critique. Ces contraintes empêchent l'utilisation des partitions isolément ; l'utilisation de LVM permet de s'affranchir des limites imposées par leurs tailles individuelles, pour n'être limité que par leur capacité totale.
				</div><div
                class="para">
					Le prérequis est le paquet <span
                  class="pkg pkg">lvm2</span> (et ses dépendances). Lorsque ce paquet est installé, la mise en place de LVM se fait en trois étapes, correspondant aux trois couches de LVM.
				</div><div
                class="para">
					Commençons par préparer les volumes physiques à l'aide de <code
                  class="command">pvcreate</code> :
				</div><a
                xmlns=""
                id="screen.pvcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvdisplay</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvcreate /dev/sdb2</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdb2" successfully created
# </code><strong
                  class="userinput"><code>pvdisplay</code></strong>
<code
                  class="computeroutput">  "/dev/sdb2" is a new physical volume of "4.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/sdb2
  VG Name               
  PV Size               4.00 GiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I

# </code><strong
                  class="userinput"><code>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdc3" successfully created
  Physical volume "/dev/sdd" successfully created
  Physical volume "/dev/sdf1" successfully created
  Physical volume "/dev/sdf2" successfully created
# </code><strong
                  class="userinput"><code>pvdisplay -C</code></strong>
<code
                  class="computeroutput">  PV         VG   Fmt  Attr PSize PFree
  /dev/sdb2       lvm2 ---  4.00g 4.00g
  /dev/sdc3       lvm2 ---  3.09g 3.09g
  /dev/sdd        lvm2 ---  4.00g 4.00g
  /dev/sdf1       lvm2 ---  4.10g 4.10g
  /dev/sdf2       lvm2 ---  5.22g 5.22g
</code></pre><div
                class="para">
					Rien de bien sorcier jusqu'à présent ; on remarquera que l'on peut établir un PV aussi bien sur un disque entier que sur des partitions. Comme on le constate, la commande <code
                  class="command">pvdisplay</code> est capable de lister les PV déjà établis, sous deux formes.
				</div><div
                class="para">
					Constituons à présent des groupes de volumes (VG) à partir de ces éléments physiques, à l'aide de la commande <code
                  class="command">vgcreate</code>. Nous allons placer dans le VG <code
                  class="filename">vg_critique</code> uniquement des PV appartenant à des disques rapides ; le deuxième VG, <code
                  class="filename">vg_normal</code>, contiendra des éléments physiques plus lents.
				</div><a
                xmlns=""
                id="screen.vgcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>vgdisplay</code></strong>
<code
                  class="computeroutput">  No volume groups found
# </code><strong
                  class="userinput"><code>vgcreate vg_critique /dev/sdb2 /dev/sdf1</code></strong>
<code
                  class="computeroutput">  Volume group "vg_critique" successfully created
# </code><strong
                  class="userinput"><code>vgdisplay</code></strong>
<code
                  class="computeroutput">  --- Volume group ---
  VG Name               vg_critique
  System ID
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               8,09 GiB
  PE Size               4,00 MiB
  Total PE              2071
  Alloc PE / Size       0 / 0
  Free  PE / Size       2071 / 8,09 GiB
  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp

# </code><strong
                  class="userinput"><code>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</code></strong>
<code
                  class="computeroutput">  Volume group "vg_normal" successfully created
# </code><strong
                  class="userinput"><code>vgdisplay -C</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize  VFree
  vg_critique   2   0   0 wz--n-  8,09g  8,09g
  vg_normal     3   0   0 wz--n- 12,30g 12,30g
</code></pre><div
                class="para">
					Ici encore, les commandes sont relativement simples (et <code
                  class="command">vgdisplay</code> présente deux formats de sortie). Notons que rien n'empêche de placer deux partitions d'un même disque physique dans deux VG différents ; le préfixe <code
                  class="filename">vg_</code> utilisé ici est une convention, mais n'est pas obligatoire.
				</div><div
                class="para">
					Nous disposons maintenant de deux « disques virtuels », respectivement d'environ 8 Go et 12 Go. Nous pouvons donc les subdiviser en « partitions virtuelles » (des LV). Cette opération passe par la commande <code
                  class="command">lvcreate</code>, dont la syntaxe est un peu plus complexe :
				</div><a
                xmlns=""
                id="screen.lvcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>lvdisplay</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>lvcreate -n lv_files -L 5G vg_critical</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_files" created
# </code><strong
                  class="userinput"><code>lvdisplay</code></strong>
<code
                  class="computeroutput">  --- Logical volume ---
  LV Path                /dev/vg_critical/lv_files
  LV Name                lv_files
  VG Name                vg_critical
  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT
  LV Write Access        read/write
  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400
  LV Status              available
  # open                 0
  LV Size                5.00 GiB
  Current LE             1280
  Segments               2
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0

# </code><strong
                  class="userinput"><code>lvcreate -n lv_base -L 1G vg_critical</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_base" created
# </code><strong
                  class="userinput"><code>lvcreate -n lv_backups -L 12G vg_normal</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_backups" created
# </code><strong
                  class="userinput"><code>lvdisplay -C</code></strong>
<code
                  class="computeroutput">  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_base    vg_critical -wi-a---  1.00g                                           
  lv_files   vg_critical -wi-a---  5.00g                                           
  lv_backups vg_normal   -wi-a--- 12.00g</code></pre><div
                class="para">
					Deux informations sont obligatoires lors de la création des volumes logiques et doivent être passées sous forme d'options à <code
                  class="command">lvcreate</code>. Le nom du LV à créer est spécifié par l'option <code
                  class="literal">-n</code> et sa taille est en général spécifiée par <code
                  class="literal">-L</code>. Évidemment, il faut également expliciter à l'intérieur de quel groupe de volumes on souhaite créer le LV, d'où le dernier paramètre de la ligne de commande.
				</div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>POUR ALLER PLUS LOIN</em></span> Options de <code
                            class="command">lvcreate</code></strong></p></div></div></div><div
                  class="para">
					La commande <code
                    class="command">lvcreate</code> propose plusieurs options influençant la manière dont le LV est créé.
				</div><div
                  class="para">
					Notons tout d'abord l'existence de l'option <code
                    class="literal">-l</code>, qui permet de spécifier la taille du LV à créer non pas en unités « humaines » comme nous l'avons fait dans nos exemples, mais en nombre de blocs. Ces blocs (appelés <acronym
                    class="acronym">PE</acronym>, pour <span
                    class="foreignphrase"><em
                      class="foreignphrase">physical extents</em></span>) sont des unités contiguës de stockage, qui font partie des PV et qui ne sont pas divisibles entre LV. Si l'on souhaite définir précisément l'espace alloué à un LV, par exemple pour utiliser totalement l'espace disponible sur un VG, on pourra préférer utiliser <code
                    class="literal">-l</code> plutôt que <code
                    class="literal">-L</code>.
				</div><div
                  class="para">
					Il est également possible de spécifier qu'un LV devra être physiquement stocké sur un PV plutôt qu'un autre (tout en restant dans les PV affectés au groupe, bien entendu). Ainsi, si l'on sait que le disque <code
                    class="filename">sdb</code> est plus rapide que <code
                    class="filename">sdf</code>, on pourra placer le LV <code
                    class="filename">lv_base</code> dessus (donc privilégier les performances de la base de données par rapport à celles du serveur de fichiers). La ligne de commande deviendra alors : <code
                    class="command">lvcreate -n lv_base -L 1G vg_critique /dev/sdb2</code>. Il est à noter que cette commande peut échouer si le PV spécifié n'a plus assez de blocs libres pour contenir le LV demandé. Dans notre exemple, il faudrait probablement créer <code
                    class="filename">lv_base</code> avant <code
                    class="filename">lv_fichiers</code> pour éviter cet inconvénient — ou libérer de l'espace sur <code
                    class="filename">sdb2</code> grâce à la commande <code
                    class="command">pvmove</code>.
				</div></div><div
                class="para">
					Les volumes logiques, une fois créés, sont représentés par des fichiers de périphériques situés dans <code
                  class="filename">/dev/mapper/</code> :
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>ls -l /dev/mapper</code></strong>
<code
                  class="computeroutput">total 0
crw------- 1 root root 10, 236 Jun 10 16:52 control
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2
# </code><strong
                  class="userinput"><code>ls -l /dev/dm-*</code></strong>
<code
                  class="computeroutput">brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0
brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1
brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2
</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>NOTE</em></span> Détection automatique des volumes LVM</strong></p></div></div></div><div
                  class="para">
					Lors du démarrage de l'ordinateur, le service systemd <code
                    class="filename">lvm2-activation</code> lance la commande <code
                    class="command">vgchange -aay</code> pour activer les groupes de volumes. Pour cela, il effectue un balayage des périphériques disponibles ; ceux qui ont été préparés comme des volumes physiques pour LVM sont alors répertoriés auprès du sous-système LVM, ceux qui font partie de groupes de volumes sont assemblés et les volumes logiques sont mis en fonctionnement. Il n'est donc pas nécessaire de modifier des fichiers de configuration lors de la création ou de l'altération de volumes LVM.
				</div><div
                  class="para">
					On notera cependant que la disposition des divers éléments impliqués dans LVM est stockée dans <code
                    class="filename">/etc/lvm/backup</code>, ce qui peut servir en cas de problème, ou bien pour aller voir ce qui se passe sous le capot.
				</div></div><div
                class="para">
					Pour faciliter les choses, des liens symboliques sont également créés automatiquement dans des répertoires correspondant aux VG :
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>ls -l /dev/vg_critical</code></strong>
<code
                  class="computeroutput">total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0
# </code><strong
                  class="userinput"><code>ls -l /dev/vg_normal</code></strong>
<code
                  class="computeroutput">total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</code></pre><div
                class="para">
					On peut dès lors utiliser les LV tout comme on utiliserait des partitions classiques :
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mkfs.ext4 /dev/vg_normal/lv_backups</code></strong>
<code
                  class="computeroutput">mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 3145728 4k blocks and 786432 inodes
Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d
[...]
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </code><strong
                  class="userinput"><code>mkdir /srv/backups</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mount /dev/vg_normal/lv_backups /srv/backups</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/backups</code></strong>
<code
                  class="computeroutput">Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_normal-lv_backups   12G   30M   12G   1% /srv/backups
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>cat /etc/fstab</code></strong>
<code
                  class="computeroutput">[...]
/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2
/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2
/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</code></pre><div
                class="para">
					On s'est ainsi abstrait, du point de vue applicatif, de la myriade de petites partitions, pour se retrouver avec une seule partition de 12 Go.
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.lvm-over-time"></a>12.1.2.3. LVM au fil du temps</h4></div></div></div><div
                class="para">
					Cette capacité à agréger des partitions ou des disques physiques n'est pas le principal attrait de LVM. La souplesse offerte se manifeste surtout au fil du temps, lorsque les besoins évoluent. Supposons par exemple que de nouveaux fichiers volumineux doivent être stockés et que le LV dévolu au serveur de fichiers ne suffise plus. Comme nous n'avons pas utilisé l'intégralité de l'espace disponible dans <code
                  class="filename">vg_critique</code>, nous pouvons étendre <code
                  class="filename">lv_fichiers</code>. Nous allons pour cela utiliser la commande <code
                  class="command">lvresize</code> pour étendre le LV, puis <code
                  class="command">resize2fs</code> pour ajuster le système de fichiers en conséquence :
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/files/</code></strong>
<code
                  class="computeroutput">Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files
# </code><strong
                  class="userinput"><code>lvdisplay -C vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 5.00g
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   2   2   0 wz--n- 8.09g 2.09g
# </code><strong
                  class="userinput"><code>lvresize -L 7G vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).
  Logical volume lv_files successfully resized
# </code><strong
                  class="userinput"><code>lvdisplay -C vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 7.00g
# </code><strong
                  class="userinput"><code>resize2fs /dev/vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">resize2fs 1.42.12 (29-Aug-2014)
Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.

# </code><strong
                  class="userinput"><code>df -h /srv/files/</code></strong>
<code
                  class="computeroutput">Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>ATTENTION</em></span> Retaillage de systèmes de fichiers</strong></p></div></div></div><div
                  class="para">
					En l'état actuel des choses, tous les systèmes de fichiers ne peuvent pas être retaillés en ligne et le retaillage d'un volume peut donc nécessiter de démonter le système de fichiers au préalable et le remonter par la suite. Bien entendu, si l'on souhaite réduire l'espace occupé par un LV, il faudra d'abord réduire le système de fichiers ; lors d'un élargissement, le volume logique devra être étendu avant le système de fichiers. C'est somme toute fort logique : la taille du système de fichiers ne doit à aucun moment dépasser celle du périphérique bloc sur laquelle il repose, qu'il s'agisse d'une partition physique ou d'un volume logique.
				</div><div
                  class="para">
					Dans le cas du système ext3 ou ext4, on peut procéder à l'extension (mais pas la réduction) du système sans le démonter. Le système xfs est dans le même cas. reiserfs permet le retaillage dans les deux sens. ext2 n'en permet aucun et nécessite toujours un démontage.
				</div></div><div
                class="para">
					On pourrait procéder de même pour étendre le volume qui héberge la base de données, mais on arrive à la limite de la capacité du VG :
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/base/</code></strong>
<code
                  class="computeroutput">Sys. de fichiers      Tail. Uti. Disp. Uti% Monté sur
/dev/mapper/vg_critique-lv_base
                     1008M  854M  104M  90% /srv/base
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critique</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree
  vg_critique   2   2   0 wz--n- 8,09g 92,00m</code></pre><div
                class="para">
					Qu'à cela ne tienne, LVM permet également d'ajouter des volumes physiques à des groupes de volumes existants. Par exemple, on a pu s'apercevoir que la partition <code
                  class="filename">sdb1</code>, qui jusqu'à présent était utilisée en dehors de LVM, contenait uniquement des archives qui ont pu être déplacées dans <code
                  class="filename">lv_sauvegardes</code>. On peut donc l'intégrer au groupe de volumes pour récupérer l'espace disponible. Ceci passe par la commande <code
                  class="command">vgextend</code>. Il faut bien entendu préparer la partition comme un volume physique au préalable. Une fois le VG étendu, on peut suivre le même cheminement que précédemment pour étendre le système de fichiers.
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvcreate /dev/sdb1</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdb1" successfully created
# </code><strong
                  class="userinput"><code>vgextend vg_critical /dev/sdb1</code></strong>
<code
                  class="computeroutput">  Volume group "vg_critical" successfully extended
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   3   2   0 wz--n- 9.09g 1.09g
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>df -h /srv/base/</code></strong>
<code
                  class="computeroutput">Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>POUR ALLER PLUS LOIN</em></span> LVM avancé</strong></p></div></div></div><div
                  class="para">
					Il existe des utilisations plus avancées de LVM, dans lesquelles de nombreux détails peuvent être spécifiés. On peut par exemple influer sur la taille des blocs composant les volumes logiques sur les volumes physiques et leur disposition. Il est également possible de déplacer ces blocs entre les PV. Ce peut être pour jouer sur la performance, ou plus prosaïquement pour vider un PV lorsqu'on souhaite sortir le disque correspondant du groupe de volumes (par exemple pour l'affecter à un autre VG, ou le sortir complètement de LVM). Les pages de manuel des différentes commandes, bien que non traduites en français, sont généralement claires. Un bon point d'entrée est la page <span
                    class="citerefentry"><span
                      class="refentrytitle">lvm</span>(8)</span>.
				</div></div></div></div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      xmlns=""
                      id="sect.raid-or-lvm"></a>12.1.3. RAID ou LVM ?</h3></div></div></div><div
              class="para">
				Les apports de RAID et LVM sont indéniables dès que l'on s'éloigne d'un poste bureautique simple, à un seul disque dur, dont l'utilisation ne change pas dans le temps. Cependant, RAID et LVM constituent deux directions différentes, chacune ayant sa finalité, et l'on peut se demander lequel de ces systèmes adopter. La réponse dépendra des besoins, présents et futurs.
			</div><div
              class="para">
				Dans certains cas simples, la question ne se pose pas vraiment. Si l'on souhaite immuniser des données contre des pannes de matériel, on ne pourra que choisir d'installer du RAID sur un ensemble redondant de disques, puisque LVM ne répond pas à cette problématique. Si au contraire il s'agit d'assouplir un schéma de stockage et de rendre des volumes indépendants de l'agencement des disques qui les composent, RAID ne pourra pas aider et l'on choisira donc LVM.
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>NOTE</em></span> Si les performances comptent…</strong></p></div></div></div><div
                class="para">
				Si les performances des entrées-sorties sont importantes, notamment en termes de temps d'accès, il faut savoir que l'usage de LVM et/ou de RAID peut avoir un impact qui influerea sur votre choix. Toutefois, ces différences de performance sont vraiment mineures et ne seront mesurables que dans quelques cas. Si les performances comptent, le meilleur gain que l'on puisse obtenir viendra de l'usage de disques non rotatifs (<a
                  id="id-1.15.4.8.4.2.1"
                  class="indexterm"></a><span
                  class="foreignphrase"><em
                    class="foreignphrase">Solid-State Drives</em></span> ou SSD) ; leur coût au mégaoctet est plus élevé que celui des disques durs standards et leur capacité généralement inférieure, mais ils fournissent d'excellentes performances pour des accès aléatoires aux données. Si le cas d'usage inclut de nombreuses lectures/écritures réparties sur tout le système de fichiers, comme pour des bases de données sur lesquelles des requêtes complexes sont fréquemment exécutées, alors le bénéfice apporté par le SSD dépasse largement ce qui peut être gagné en privilégiant LVM sur RAID ou l'inverse. Dans ces situations, le choix doit être déterminé par d'autres considérations que la vitesse pure, puisque la problématique des performances est plus facilement gérée par l'usage de SSD.
			</div></div><div
              class="para">
				Un troisième cas est celui où l'on souhaite juste agréger deux disques en un, que ce soit pour des raisons de performance ou pour disposer d'un seul système de fichiers plus gros que chacun des disques dont on dispose. Ce problème peut être résolu aussi bien par la mise en place d'un ensemble RAID-0 (voire linéaire) que par un volume LVM. Dans cette situation, à moins de contraintes spécifiques (par exemple pour rester homogène avec le reste du parc informatique qui n'utilise que RAID), on choisira le plus souvent LVM. La mise en place initiale est légèrement plus complexe, mais elle est un bien maigre investissement au regard de la souplesse offerte par la suite, si les besoins changent ou si l'on désire ajouter des disques.
			</div><div
              class="para">
				Vient enfin le cas le plus intéressant, celui où l'on souhaite concilier de la tolérance de pannes et de la souplesse dans l'allocation des volumes. Chacun des deux systèmes étant insuffisant pour répondre à ces besoins, on va devoir faire appel aux deux en même temps — ou plutôt, l'un après l'autre. L'usage qui se répand de plus en plus depuis la maturité des deux systèmes est d'assurer la sécurité des données d'abord, en groupant des disques redondants dans un petit nombre de volumes RAID de grande taille, et d'utiliser ces volumes RAID comme des éléments physiques pour LVM pour y découper des partitions qui seront utilisées pour des systèmes de fichiers. Ceci permet, en cas de défaillance d'un disque, de n'avoir qu'un petit nombre d'ensembles RAID à reconstruire, donc de limiter le temps d'administration.
			</div><div
              class="para">
				Prenons un exemple concret : le département des relations publiques de Falcot SA a besoin d'une station de travail adaptée au montage vidéo. En revanche, les contraintes de budget du département ne permettent pas d'investir dans du matériel neuf entièrement haut de gamme. Le choix est fait de privilégier le matériel spécifiquement graphique (écran et carte vidéo) et de rester dans le générique pour les éléments de stockage. Or la vidéo numérique, comme on le sait, a des besoins en stockage particuliers : les données à stocker sont volumineuses et la vitesse de lecture et d'écriture de ces données est importante pour les performances du système (plus que le temps d'accès moyen, par exemple). Il faut donc répondre à ces contraintes avec du matériel générique, en l'occurrence deux disques durs SATA de 300 Go, tout en garantissant la disponibilité du système et de certaines des données. Les films montés doivent en effet rester disponibles, mais les fichiers bruts non encore montés sont moins critiques, puisque les <span
                class="foreignphrase"><em
                  class="foreignphrase">rushes</em></span> restent sur les bandes.
			</div><div
              class="para">
				Le choix se porte donc sur une solution combinant RAID-1 et LVM. Les deux disques sont montés sur deux contrôleurs SATA différents (afin d'optimiser les accès parallèles et limiter les risques de panne simultanée) et apparaissent donc comme <code
                class="filename">sda</code> et <code
                class="filename">sdc</code>. Le schéma de partitionnement, identique sur les deux disques, sera le suivant :
			</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>fdisk -l /dev/sda</code></strong>
<code
                class="computeroutput">
Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x00039a9f

Device    Boot     Start       End   Sectors Size Id Type
/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect
/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris
/dev/sda3        4000185 586099395 582099210 298G 5  Extended
/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect
/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect
/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</code></pre><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
						La première partition de chaque disque (environ 1 Go) est utilisée pour assembler un volume RAID-1, <code
                      class="filename">md0</code>. Ce miroir est utilisé directement pour stocker le système de fichiers racine.
					</div></li><li
                  class="listitem"><div
                    class="para">
						Les partitions <code
                      class="filename">hda2</code> et <code
                      class="filename">hdc2</code> sont utilisées comme partitions d'échange, pour fournir un total de 2 Go de mémoire d'échange. Avec 1 Go de mémoire vive, la station de travail dispose ainsi d'une quantité confortable de mémoire.
					</div></li><li
                  class="listitem"><div
                    class="para">
						Les partitions <code
                      class="filename">hda5</code> et <code
                      class="filename">hdc5</code> d'une part, <code
                      class="filename">hda6</code> et <code
                      class="filename">hdc6</code> d'autre part sont utilisées pour monter deux nouveaux volumes RAID-1 de 100 Go chacun, <code
                      class="filename">md1</code> et <code
                      class="filename">md2</code>. Ces deux miroirs sont initialisés comme des volumes physiques LVM et affectés au groupe de volumes <code
                      class="filename">vg_raid</code>. Ce groupe de volumes dispose ainsi d'environ 200 Go d'espace sécurisé.
					</div></li><li
                  class="listitem"><div
                    class="para">
						Les partitions restantes, <code
                      class="filename">hda7</code> et <code
                      class="filename">hdc7</code>, sont directement initialisées comme des volumes physiques et affectées à un autre VG, <code
                      class="filename">vg_vrac</code>, qui pourra contenir presque 200 Go de données.
					</div></li></ul></div><div
              class="para">
				Une fois les VG établis, il devient possible de les découper de manière très flexible, en gardant à l'esprit que les LV qui seront créés dans <code
                class="filename">vg_raid</code> seront préservés même en cas de panne d'un des deux disques, à l'opposé des LV pris sur <code
                class="filename">vg_vrac</code> ; en revanche, ces derniers seront alloués en parallèle sur les deux disques, ce qui permettra des débits élevés lors de la lecture ou de l'écriture de gros fichiers.
			</div><div
              class="para">
				On créera donc des volumes logiques <code
                class="filename">lv_usr</code>, <code
                class="filename">lv_var</code>, <code
                class="filename">lv_home</code> sur <code
                class="filename">vg_raid</code>, pour y accueillir les systèmes de fichiers correspondants ; on y créera également un <code
                class="filename">lv_films</code>, d'une taille conséquente, pour accueillir les films déjà montés. L'autre VG sera quant à lui utilisé pour un gros <code
                class="filename">lv_rushes</code>, qui accueillera les données directement sorties des caméras numériques, et un <code
                class="filename">lv_tmp</code>, pour les fichiers temporaires. Pour l'espace de travail, la question peut se poser : certes, il est important qu'il offre de bonnes performances, mais doit-on pour autant courir le risque de perdre le travail effectué si un disque défaille alors qu'un montage n'est pas fini ? C'est un choix à faire ; en fonction de ce choix, on créera le LV dans l'un ou l'autre VG.
			</div><div
              class="para">
				On dispose ainsi à la fois d'une certaine redondance des données importantes et d'une grande flexibilité dans la répartition de l'espace disponible pour les différentes applications. Si de nouveaux logiciels doivent être installés par la suite (pour des montages sonores, par exemple), on pourra aisément agrandir l'espace utilisé par <code
                class="filename">/usr/</code>.
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>NOTE</em></span> Pourquoi trois volumes RAID-1 ?</strong></p></div></div></div><div
                class="para">
				On aurait fort bien pu se contenter d'un seul volume RAID-1, qui servirait de volume physique pour <code
                  class="filename">vg_raid</code>. Pourquoi donc en avoir créé trois ?
			</div><div
                class="para">
				La décision de séparer le premier miroir des deux autres est motivée par des considérations de sécurité des données. En effet, en RAID-1, les données écrites sur les disques sont strictement identiques ; il est donc possible de monter un seul disque directement, sans passer par la couche RAID. En cas de problème dans le noyau, par exemple, ou de corruption des métadonnées LVM, on peut ainsi démarrer un système minimal (sans les applications ou les données) et récupérer des données cruciales, par exemple l'agencement des disques dans les volumes RAID, et surtout dans les ensembles LVM ; on peut ainsi reconstruire les métadonnées et récupérer les fichiers, ce qui permettra de remettre le système dans un état opérationnel.
			</div><div
                class="para">
				Le pourquoi de la séparation entre <code
                  class="filename">md1</code> et <code
                  class="filename">md2</code> est plus subjectif et découle d'une certaine prudence. En effet, lors de l'assemblage de la station de travail, les besoins exacts ne sont pas forcément connus précisément ; ou ils peuvent changer au fil du temps. Dans notre cas, il est difficile de connaître à l'avance les besoins en stockage pour les films montés et les épreuves de tournage. Si un film à monter nécessite de très grandes quantités de <span
                  class="foreignphrase"><em
                    class="foreignphrase">rushes</em></span> et que le groupe de volumes dédié aux données sécurisées est occupé à moins de 50%, on pourra envisager d'en récupérer une partie. Pour cela, on pourra soit sortir l'un des composants de <code
                  class="filename">vg_raid</code> et le réaffecter directement à <code
                  class="filename">vg_vrac</code> (si l'opération est temporaire et si l'on peut vivre avec la perte de performances induite), soit abandonner le RAID-1 sur <code
                  class="filename">md2</code> et intégrer <code
                  class="filename">hda6</code> et <code
                  class="filename">hdc6</code> dans le VG non sécurisé (si l'on a besoin des performances — on récupère d'ailleurs dans ce cas 200 Go d'espace, au lieu des 100 Go du miroir) ; il suffira alors d'élargir le volume logique en fonction des besoins.
			</div></div></div></div></div><ul
        class="docnav"><li
          class="previous"><a
            accesskey="p"
            href="sect.rtc-services.html"><strong>Précédent</strong>11.8. Services de communication en temps réel</a></li><li
          class="up"><a
            accesskey="u"
            href="#"><strong>Niveau supérieur</strong></a></li><li
          class="home"><a
            accesskey="h"
            href="index.html"><strong>Sommaire</strong></a></li><li
          class="next"><a
            accesskey="n"
            href="sect.virtualization.html"><strong>Suivant</strong>12.2. Virtualisation</a></li></ul></body></html>
