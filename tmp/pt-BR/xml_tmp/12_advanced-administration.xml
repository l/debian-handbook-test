<?xml version='1.0' encoding='utf-8' ?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [
]>
<chapter id="advanced-administration">
  <chapterinfo>
    <mediaobject condition="pdf">
      <imageobject>
        <imagedata fileref="images/chap-advanced-administration.png" scalefit="1" />
      </imageobject>
    </mediaobject>
    <keywordset>
      <keyword>RAID</keyword>
      <keyword>LVM</keyword>
      <keyword>FAI</keyword>
      <keyword>Pré-configuração</keyword>
      <keyword>Monitoramento</keyword>
      <keyword>Virtualização</keyword>
      <keyword>Xen</keyword>
      <keyword>LXC</keyword>
    </keywordset>
  </chapterinfo>
  <title>Administração Avançada</title>
  <highlights>
    <para>Este capítulo retoma alguns aspectos já descritos, com uma perspectiva diferente: em vez de instalar em um único computador, vamos estudar a implantação de sistemas em massa; em vez de criar volumes RAID ou LVM no momento da instalação, vamos aprender a fazer tudo na mão para que mais tarde possamos rever nossas escolhas iniciais. Finalmente, vamos discutir as ferramentas de monitoramento e técnicas de virtualização. Como consequência, este capítulo é mais particularmente alvo de administradores profissionais e centra-se um pouco menos nos indivíduos responsáveis pela sua rede doméstica.</para>
  </highlights>
  <section id="sect.raid-and-lvm">
    <title>RAID e LVM</title>

    <para><xref linkend="installation" /> apresentou estas tecnologias do ponto de vista do instalador e como ele as integrou para fazer a sua implantação fácil desde o início. Após a instalação inicial, um administrador deve ser capaz de lidar com as necessidades de espaço de armazenamento em evolução, sem ter que recorrer a uma reinstalação cara. Devem, portanto, compreender as ferramentas necessárias para manipular volumes RAID e LVM.</para>

    <para>RAID e LVM são duas técnicas para abstrair os volumes montados a partir de seus equivalentes físicos (reais unidades de disco rígido ou partições do mesmo), o primeiro protege os dados de falhas no hardware através da introdução de redundância, o último torna o gerenciamento de volumes mais flexível e independente do tamanho real nos discos. Em ambos os casos, o sistema acaba com novos volumes (partições, blocos), que podem ser usados para criar sistemas de arquivos ou espaço de troca, sem necessariamente ter eles mapeados em um disco físico. LVM e RAID vêm de origens bem diferentes, mas sua funcionalidade pode sobrepor-se um pouco, é por isso que eles são muitas vezes mencionados juntos.</para>

    <sidebar>
      <title><emphasis>PERSPECTIVE</emphasis> Btrfs combina LVM e RAID</title>

      <para>Enquanto LVM e RAID são dois subsistemas do kernel distintos que estão entre os dispositivos de bloco do disco e seus sistemas de arquivos, <emphasis>Btrfs</emphasis> é um novo sistema de arquivos, desenvolvido inicialmente pela Oracle, que pretende combinar os conjuntos de recursos de LVM e RAID e muito mais. É sobretudo funcional, embora ainda seja definido como "experimental", pois seu desenvolvimento é incompleto (alguns recursos ainda não estão implementados). <ulink type="block" url="http://btrfs.wiki.kernel.org/" /></para>

      <para>Entre as características marcantes estão a capacidade de tirar um instantâneo de uma árvore de diretórios em qualquer ponto no tempo. Este instantâneo inicialmente não utiliza nenhum espaço em disco, os dados só serão duplicados quando um dos arquivos copiados for modificado. O sistema de arquivos também lida com a compressão transparente de arquivos e somas de verificação (checksums) garantem a integridade de todos os dados armazenados.</para>
    </sidebar>

    <para>Em ambos os casos RAID e LVM, o kernel fornece um arquivo de dispositivo de bloco semelhantes aos que correspondem a uma unidade de disco rígido ou partição. Quando um pedido ou uma outra parte do núcleo, requer o acesso a um bloco de um tal dispositivo, as rotas de subsistemas apropriadas do bloco são usadas para a camada física relevante. Dependendo da configuração, este bloco pode ser armazenado em um ou vários discos físicos e sua localização física pode não ser directamente relacionada com a localização do bloco no dispositivo lógico.</para>
    <section id="sect.raid-soft">
      <title>RAID Por Software</title>
      <indexterm><primary>RAID</primary></indexterm>

      <para>RAID significa <emphasis> Redundant Array of Independent Disks - conjunto redundante de discos independentes</emphasis>. O objetivo deste sistema é evitar perda de dados em caso de falha do disco rígido. O princípio geral é bastante simples: os dados são armazenados em vários discos físicos em vez de apenas um, com um nível configurável de redundância. Dependendo desta quantidade de redundância, e mesmo no caso de uma falha de disco inesperado, dados podem ser reconstruídos sem perdas dos restantes discos.</para>

      <sidebar>
        <title><emphasis>CULTURA</emphasis> <foreignphrase>Independent</foreignphrase> or <foreignphrase>inexpensive</foreignphrase>?</title>

	<para>O I da sigla RAID inicialmente significava <emphasis>inexpensive</emphasis> (barato), por que o RAID permitia um aumento drástico na segurança de dados sem precisar investir em discos de alta qualidade. Provavelmente, devido a questões de melhoria da imagem, o I é agora normalmente chamado de <emphasis>independent</emphasis>, para não ficar com este aspecto de economia.</para>
      </sidebar>

      <para>O RAID pode ser implementado tanto por hardware dedicado (módulos RAID integrados em placas controladoras SCSI ou SATA) ou por abstração de software (o núcleo). Seja por hardware ou software, um sistema RAID com redundância suficiente pode, de forma transparente, continuar operacional quando um disco falha; as camadas superiores da pilha (aplicações) podem até manter o acesso aos dados apesar da falha. Claro que, esse “modo degradado” pode ter impacto na performance, e a redundância é reduzida, então uma falha profunda do disco pode levar a perda de dados. Na prática, entretanto, um irá se esforçar para apenas ficar nesse modo degradado o tempo que for necessário para que se possa substituir o disco falho. Uma vez que o novo disco seja colocado, o sistema RAID pode reconstruir os dados necessários e então retornar ao modo seguro. As aplicações não notarão nada, fora a potencial redução da velocidade de acesso, enquanto a array estiver no modo degradado ou durante a fase de reconstrução.</para>

      <para>Quando o RAID é implementado por hardware, sua configuração geralmente acontece dentro da ferramenta de configuração da BIOS, e o núcleo irá considerar uma array RAID como um único disco, que irá funcionar como um disco físico padrão, embora o nome do dispositivo possa ser diferente (dependendo do driver).</para>

      <para>Nós apenas focamos em RAID de software neste livro.</para>

      <section id="sect.raid-levels">
        <title>Diferentes Níveis de RAID</title>

	<para>RAID não é na verdade um único sistema, mas vários sistemas identificados por seus níveis; os níveis diferem por sua disposição e quantidade de redundância que eles fornecem. Quanto mais redundante, mais à prova de falhas, uma vez que o sistema será capaz de continuar a trabalhar quando mais discos falharem. A contrapartida é que reduz o espaço utilizável para um dado conjunto de discos; visto de outra forma, mais discos serão necessários para armazenar a mesma quantidade de dados.</para>
        <variablelist>
          <varlistentry>
            <term>RAID Linear</term>
            <listitem>
	      <para>Mesmo o que o subsistema de RAID do núcleo permita a criação de um “RAID linear”, isso não é um RAID propriamente, já que essa configuração não envolve qualquer redundância. O núcleo apenas agrega vários discos fim-a-fim e provê o volume agregado resultante como um disco virtual (um dispositivo de bloco). Essa é sua única função. Essa configuração raramente é usada por ela própria (veja mais adiante sobre as exceções), especialmente porque a falta de redundância significa que a falha de um disco faz com que todo o agregado, e portanto todos os dados, fiquem indisponíveis.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>RAID-0</term>
            <listitem>
	      <para>Esse nível também não provê nenhuma redundância, mas os discos não são simplesmente ligados pelo final um após o outro: eles são divididos em <emphasis>listras (stripes)</emphasis>, e os blocos no dispositivo virtual são armazenados em listras (stripes) em discos físicos alternados. Em uma configuração de RAID-0 de dois discos, por exemplo, em blocos de número par do dispositivo virtual serão armazenados no primeiro disco físico, enquanto os blocos ímpares ficarão no segundo disco físico.</para>

	      <para>Esse sistema não tem por objetivo um aumento de credibilidade, já que (como em um caso linear) a disponibilidade de todos os dados é comprometida assim que um disco falhar, mas um aumento de desempenho: durante um acesso sequencial a grandes quantidades de dados contíguos, o núcleo será capaz de ler a partir dos dois discos (ou escrever neles) em paralelo, o que incrementa a taxa de transferência de dados. Contudo, o uso do RAID-0 está murchando, seu nicho está sendo preenchido pelo LVM (veja mais adiante).</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>RAID-1</term>
            <listitem>
	      <para>Esse nível, também conhecido como “espelhamento RAID”, é tanto o mais simples quanto a mais amplamente usada configuração. Em sua forma padrão, ele usa dois discos físicos de mesmo tamanho e fornece um volume lógico de mesmo tamanho mais uma vez. Os dados são armazenados identicamente nos dois discos, por isso o apelido “espelho”. Quando um disco falha, os dados ainda estão disponíveis no outro. Para dados realmente críticos, RAID-1 pode, é claro, ser configurado para mais de dois discos, com impacto direto na relação de custo de hardware versus espaço de carga disponível.</para>

              <sidebar>
                <title><emphasis>NOTA</emphasis> Discos e tamanhos de cluster</title>

		<para>Se dois discos de tamanhos diferentes são criados em um espelho, o maior não será totalmente usado, pois ele irá conter os mesmos dados como o menor e nada mais. O espaço útil disponível fornecido por um volume RAID-1, portanto, corresponde ao tamanho do disco menor na matriz. Isso ainda vale para volumes RAID com um maior nível RAID, apesar de redundância é armazenada de forma diferente.</para>

		<para>Por isso é importante, ao configurar arrays RAID (exceto RAID-0 "RAID linear"), só montar discos de tamanhos idênticos, ou muito perto, para evitar o desperdício de recursos.</para>
              </sidebar>

              <sidebar>
                <title><emphasis>NOTA</emphasis> Discos de reposição</title>

		<para>Níveis RAID que incluem redundância permitem atribuir mais discos do que o necessário para uma array. Os discos extras são usados como reservas quando um dos principais discos falha. Por exemplo, em um espelho de dois discos e mais um reserva, se um dos dois primeiros discos falhar, o núcleo irá automaticamente (e imediatamente) reconstruir o espelho usando o disco reserva, para que a redundância continue garantida após o momento de reconstrução. Isso pode ser usado como outro tipo de salva guarda para dados críticos.</para>

		<para>Alguém poderia ser perdoado por questionar como isso pode ser melhor do que simplesmente espelhar três discos como início. A vantagem da configuração de um “disco reserva” é que o disco reserva pode ser compartilhado entre vários volumes RAID. Por exemplo, é possível ter três volumes espelhados, com garantia de redundância mesmo no caso de falha de um disco, com apenas sete discos (três pares, mais um reserva compartilhado), ao invés dos nove discos que seriam necessários para três trigêmeos.</para>
              </sidebar>

	      <para>Esse nível de RAID, embora caro (já que apenas metade do espaço físico de armazenagem, na melhor das hipóteses, é útil), é amplamente usado na prática. Ele é simples de entender, e ele permite cópias de segurança (backups) bem simples: como os dois discos tem conteúdos idênticos, um deles pode ser temporariamente extraído, sem impacto no sistema em funcionamento. O desempenho de leitura geralmente é incrementado , já que o núcleo pode ler metade dos dados em cada disco, em paralelo, enquanto o desempenho de escrita não é muito severamente degradado. No caso de uma array RAID-1 de N discos, os dados continuam disponíveis mesmo com a falha do disco N-1.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>RAID-4</term>
            <listitem>
	      <para>Esse nível de RAID, não amplamente usado, usa N discos para armazenar dados úteis, e um disco extra para armazenar informação redundante. Se esse disco falhar, o sistema pode reconstruir seu conteúdo a partir do outro N. Se um dos N discos de dados falhar, O N-1 remanescente combinado com o disco “paridade” contém informação suficiente para reconstruir os dados requeridos.</para>

	      <para>RAID-4 não é muito caro já que ele apenas envolve um incremento de um-em-N nos custos e não se tem impacto perceptível no desempenho de leitura, mas a escrita é mais devagar. Além disso, como uma escrita em qualquer um dos N discos também envolve a escrita no disco de paridade, esse último tem muito mais escritas que o anterior, e sua vida útil pode ser dramaticamente diminuída como consequência. Os dados na array RAID-4 só está segura até um disco falhar (dos N+1).</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>RAID-5</term>
            <listitem>
	      <para>RAID-5 resolve o problema de assimetria do RAID-4: a paridade de blocos é distribuída por todos os N+1 discos, sendo que nenhum tem um papel particular.</para>

	      <para>A performance de leitura e escrita são idênticas ao RAID-4. Aqui novamente, o sistema continua funcional mesmo com a falha de um disco (do N+1), mas não mais.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>RAID-6</term>
            <listitem>
	      <para>RAID-6 pode ser considerado uma extensão do RAID-5, onde cada série de N blocos envolvem dois blocos redundantes, e cada série de N+2 blocos e distribuída sobre N+2 discos.</para>

	      <para>Esse nível de RAID é levemente mais caro que os dois anteriores, mas ele traz alguma segurança extra já que até dois drives (dos N+2) podem falhar sem comprometer a disponibilidade dos dados. A contraparte é que as operações de escrita agora envolvem escrever um bloco de dados e dois blocos de redundância, o que os torna ainda mais lento.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>RAID-1+0</term>
            <listitem>
	      <para>Isso não é, estritamente falando, um nível RAID, mas um empilhamento de dois agrupamentos RAID. A partir de 2×N discos, primeiro se configura eles por pares em volumes N RAID-1; esses volumes N são então agregados em um só, seja por “linear RAID” ou (cada vez mais) por LVM. Esse último caso vai além do puro RAID, mas não existe problema quanto a isso.</para>

	      <para>RAID-1+0 pode sobreviver com múltiplas falhas nos discos: até N na 2xN série descrita acima, provendo ao menos um disco funcional em cada par de RAID-1.</para>

              <sidebar id="sidebar.raid-10">
                <title><emphasis>Aprofundando</emphasis> RAID-10</title>

		<para>RAID-10 é ,geralmente , considerado um sinônimo de RAID-1+0, mas uma especifidade Linux faz com que ele seja realmente uma generalização. Essa configuração permite um sistema aonde cada bloco seja armazenado em dois diferentes discos, mesmo com um número impar de discos, sendo as cópias espalhadas ao longo de um modelo configurável.</para>

		<para>A performance variará dependendo da escolha do modelo de repartição e do nível de redundância, e da carga de trabalho do volume lógico.</para>
              </sidebar>
            </listitem>
          </varlistentry>
        </variablelist>

	<para>Obviamente, o nível de RAID será escolhido de acordo com as restrições e requerimentos de cada aplicação. Note que um computador sozinho pode ter diversos tipos de RAIDs distintos com diversas configurações.</para>
      </section>
      <section id="sect.raid-setup">
        <title>Configurando um RAID</title>
        <indexterm><primary><emphasis role="pkg">mdadm</emphasis></primary></indexterm> 

	<para>Configurar volumes RAID requer o pacote <emphasis role="pkg">mdadm</emphasis>; ele provê o comando <command>mdadm</command>, que permite a criação e maipulação de arrays RAID, assim como scripts e ferramentas para integração com o resto do sistema, incluindo o sistema de monitoração.</para>

	<para>Nossos exemplo será um servidor com um número de discos, sendo que alguns já estão em uso, e o resto está disponível para a configuração do RAID. Nós inicialmente temos os seguintes discos e partições:</para>
        <itemizedlist>
          <listitem>
	    <para>o disco <filename>sdb</filename>, 4 GB, está completamente disponível;</para>
          </listitem>
          <listitem>
	    <para>o disco <filename>sdc</filename>, 4 GB, também está completamente disponível;</para>
          </listitem>
          <listitem>
	    <para>no disco <filename>sdd</filename>, somente a partição <filename>sdd2</filename> (cerca de 4 GB) está disponível;</para>
          </listitem>
          <listitem>
	    <para>finalmente, um disco <filename>sde</filename>, ainda com 4 GB, disponível.</para>
          </listitem>
        </itemizedlist>

        <sidebar>
          <title><emphasis>NOTA</emphasis> Identificando os volumes RAID existentes</title>

	  <para>O arquivo <filename>/proc/mdstat</filename> lista os volumes existentes e seus estados. Quando criando um novo volume RAID, devemos tomar cuidado para não nomeá-lo da mesma maneira que um volume existente.</para>
        </sidebar>

	<para>Iremos usar estes elementos físicos para criar dois volumes, um RAID-0 e um espelho (RAID-1). Comecemos com o volume RAID-0:</para>

        <screen><computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</userinput>
<computeroutput>mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
# </computeroutput><userinput>mdadm --query /dev/md0</userinput>
<computeroutput>/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.
# </computeroutput><userinput>mdadm --detail /dev/md0</userinput>
<computeroutput>/dev/md0:
        Version : 1.2
  Creation Time : Wed May  6 09:24:34 2015
     Raid Level : raid0
     Array Size : 8387584 (8.00 GiB 8.59 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:24:34 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

     Chunk Size : 512K

           Name : mirwiz:0  (local to host mirwiz)
           UUID : bb085b35:28e821bd:20d697c9:650152bb
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       1       8       32        1      active sync   /dev/sdc
# </computeroutput><userinput>mkfs.ext4 /dev/md0</userinput>
<computeroutput>mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 2095104 4k blocks and 524288 inodes
Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </computeroutput><userinput>mkdir /srv/raid-0</userinput>
<computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0</userinput>
<computeroutput># </computeroutput><userinput>df -h /srv/raid-0</userinput>
<computeroutput>Filesystem      Size  Used Avail Use% Mounted on
/dev/md0        7.9G   18M  7.4G   1% /srv/raid-0
</computeroutput></screen>

	<para>O comando <command>mdadm --create</command> requer vários parâmetros: o nome do volume a ser criado (<filename>/dev/md*</filename>, com MD significando <foreignphrase>Multiple Device</foreignphrase>), o nível RAID, o número de discos (que é obrigatório, apesar de ser significante apenas com RAID-1 e acima), e os drives físicos a usar. Uma vez que o dispositivo seja criado, nós podemos usá-lo como usamos uma partição normal, criando um sistema de arquivos nela, montando esse sistema de arquivos, e assim por diante. Note que nossa criação de um volume RAID-0 em <filename>md0</filename> não passa de coincidência, e a numeração da array não precisa ser correlacionada com a quantidade escolhida de redundância. Também é possível criar arrays RAID nomeadas, dando ao <command>mdadm</command> parâmetros como <filename>/dev/md/linear</filename> ao invés de <filename>/dev/md0</filename>.</para>

	<para>A criação do RAID-1 segue estilo similar, as diferenças somente serão notadas após a criação:</para>

        <screen><computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</userinput>
<computeroutput>mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%
Continue creating array? </computeroutput><userinput>y</userinput>
<computeroutput>mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md1 started.
# </computeroutput><userinput>mdadm --query /dev/md1</userinput>
<computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
        Version : 1.2
  Creation Time : Wed May  6 09:30:19 2015
     Raid Level : raid1
     Array Size : 4192192 (4.00 GiB 4.29 GB)
  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:30:40 2015
          State : clean, resyncing (PENDING) 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       1       8       64        1      active sync   /dev/sde
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
          State : clean
[...]
</computeroutput></screen>

        <sidebar>
          <title><emphasis>DICA</emphasis> RAID, discos e partições</title>

	  <para>Como ilustrado pelo nosso exemplo, dispositivos RAID podem ser construídos à partir de partições de disco, e não necessitam discos inteiros.</para>
        </sidebar>

	<para>Algumas observações em ordem. Primeiro, <command>mdadm</command> nota que os elementos físicos possuem tamanhos diferentes; já que isso implica que algum espaço será perdido no maior elemento, uma confirmação é necessária.</para>

	<para>Ainda mais importante, note o estado do espelhamento.O estado normal de um espelho RAID é que os dois discos tenham exatamente o mesmo conteúdo. Contudo, nada garante que esse é o caso quando o volume é criado pela primeira vez. O subsistema RAID irá, por conseguinte, prover essa garantia por si mesmo, e acontecerá uma fase de sincronização assim que o dispositivo RAID for criado. Após algum tempo (a quantidade exata irá depender do real tamanho dos discos…), a array RAID alternará para o estado “ativo” ou "limpo". Note que durante essa fase de reconstrução, o espelho está em modo degradado, e a redundância não é garantida. Um disco falhando durante essa janela de risco poderia levar a perda de todos os dados. Grandes quantidades de dados críticos, contudo, raramente são armazenados em uma array RAID recentemente criada, antes de sua sincronização inicial. Note que mesmo em modo degradado, o <filename>/dev/md1</filename> é usável, e um sistema de arquivos pode ser criado nele, assim como alguns dados podem ser copiados para ele.</para>

        <sidebar>
          <title><emphasis>DICA</emphasis> Começando um espelho em modo reduzido</title>

	  <para>Às vezes, dois discos não estão imediatamente disponíveis quando se quer iniciar um espelho RAID-1, por exemplo, porque se pretende incluir um dos discos já usado para armazenar os dados que se quer passar para a array. Em tais circunstâncias, é possível criar deliberadamente uma degradada array RAID-1, passando <filename>missing</filename> em vez de um arquivo de dispositivo como um dos argumentos para <command>mdadm</command>. Uma vez que os dados tenham sido copiados para o "espelho", o disco antigo pode ser adicionado à array. A sincronização irá então acontecer, dando-nos a redundância que foi desejada, em primeiro lugar.</para>
        </sidebar>

        <sidebar>
          <title><emphasis>DICA</emphasis> Configurando um espelho sem sincronização</title>

	  <para>Volumes RAID-1 são geralmente criados para serem usados como disco novo, geralmente considerados vazios. O conteúdo inicial real do disco portanto não é muito relevante, já que alguém apenas precisa saber que os dados escritos após a criação do volume, em particular o sistema de arquivos, podem ser acessados mais tarde.</para>

	  <para>Pode-se portanto querer saber sobre o ponto de sincronização de ambos os discos no momento da criação. Por que se importar se os conteúdos são idênticos em zonas do volume que nós sabemos que apenas serão lidas após nós termos escrito nelas?</para>

	  <para>Felizmente, essa fase de sincronização pode ser evitada passando a opção <literal>--assume-clean</literal> para <command>mdadm</command>. Contudo, essa opção pode levar a surpresas no caso de os dados iniciais serem lidos (por exemplo se um sistema de arquivos já esteja presente nos discos físicos), que é o por que dela não ser habilitada por padrão.</para>
        </sidebar>

	<para>Agora vamos ver o que acontece quando um dos elementos da array RAID-1 falha. O <command>mdadm</command>, em particular sua opção <literal>--fail</literal>, permite simular uma falha de disco desse tipo:</para>

        <screen><computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde</userinput>
<computeroutput>mdadm: set /dev/sde faulty in /dev/md1
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
    Update Time : Wed May  6 09:39:39 2015
          State : clean, degraded 
 Active Devices : 1
Working Devices : 1
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 19

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       0        0        2      removed

       1       8       64        -      faulty   /dev/sde</computeroutput></screen>

	<para>O conteúdo do volume ainda está acessível (e, se montado, as aplicações não  notarão nada), mas a segurança dos dados não é mais garantida: se, por sua vez, o disco <filename>sdd</filename> falhar, os dados serão perdidos. Nós queremos evitar esse risco, então nós vamos substituir o disco falho por um novo, <filename>sdf</filename>:</para>

        <screen><computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>
<computeroutput>mdadm: added /dev/sdf
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
   Raid Devices : 2
  Total Devices : 3
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:48:49 2015
          State : clean, degraded, recovering 
 Active Devices : 1
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 1

 Rebuild Status : 28% complete

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 26

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      spare rebuilding   /dev/sdf

       1       8       64        -      faulty   /dev/sde
# </computeroutput><userinput>[...]</userinput>
<computeroutput>[...]
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
    Update Time : Wed May  6 09:49:08 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 41

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf

       1       8       64        -      faulty   /dev/sde</computeroutput></screen>

	<para>Aqui, mais uma vez, o núcleo automaticamente dispara uma fase de reconstrução, durante a qual o volume, embora ainda acessível, está em um modo degradado. Uma vez que a reconstrução esteja terminada, a array RAID está de volta ao estado normal. Pode-se então dizer ao sistema que o disco <filename>sde</filename> está para ser removido da array, para que se possa terminar com um espelhamento RAID clássico nos dois discos:</para>

        <screen><computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>
<computeroutput>mdadm: hot removed /dev/sde from /dev/md1
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf</computeroutput></screen>

	<para>A partir de então, a unidade pode ser fisicamente removida quando o servidor está para ser desligado, ou até mesmo removida com o sistema ligado (hot-removed) quando a configuração de hardware permite tal operação (hot-swap). Tais configurações incluem alguns controladores SCSI, a maioria dos discos SATA e unidades externas que operam com USB ou Firewire.</para>
      </section>
      <section id="sect.backup-raid-config">
        <title>Fazendo Backup da Configuração</title>

	<para>A maioria dos meta-dados referentes a volumes RAID são salvos diretamente nos discos que compoem essas arrays, para que o núcleo possa detectar as arrays e seus componentes e montá-los automaticamente quando o sistema for iniciado. Contudo, é encorajado o uso de cópia de segurança dessa configuração, porque essa detecção não é à prova de falhas, e só se tem a expectativa de falha dela precisamente em circunstâncias sensíveis. Em nosso exemplo, se a falha do disco <filename>sde</filename> tivesse sido real (ao invés de simulada) e o sistema tivesse sido reiniciado sem a remoção desse disco <filename>sde</filename>, esse disco poderia começar a trabalhar novamente por ter sido verificado durante a reinicialização. O núcleo iria ter então três elementos físicos, cada um clamando por conter metade do mesmo volume RAID. Outra fonte de confusão pode vir quando volumes RAID  de dois servidores são consolidados em apenas um servidor apenas. Se essas arrays estavam rodando normalmente antes dos discos serem removidos, o núcleo seria capaz de detectar e remontar os pares de maneira apropriada; mas se os discos movidos tiverem sido agregados em um <filename>md1</filename> no servidor antigo, e o novo servidor já tiver um <filename>md1</filename>, um dos espelhos seria renomeado.</para>

	<para>Fazer uma cópia de segurança da configuração é portanto importante, mesmo que apenas para referência. A maneira padrão de fazer isso é editando o arquivo <filename>/etc/mdadm/mdadm.conf</filename>, um exemplo do que é listado aqui:</para>

        <example id="example.mdadm-conf">
          <title><command>mdadm</command> arquivo de configuração</title>

          <programlisting># mdadm.conf
#
# Please refer to mdadm.conf(5) for information about this file.
#

# by default (built-in), scan all partitions (/proc/partitions) and all
# containers for MD superblocks. alternatively, specify devices to scan, using
# wildcards if desired.
DEVICE /dev/sd*

# auto-create devices with Debian standard permissions
CREATE owner=root group=disk mode=0660 auto=yes

# automatically tag new arrays as belonging to the local system
HOMEHOST &lt;system&gt;

# instruct the monitoring daemon where to send mail alerts
MAILADDR root

# definitions of existing MD arrays
ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464

# This configuration was auto-generated on Thu, 17 Jan 2013 16:21:01 +0100
# by mkconf 3.2.5-3</programlisting>
        </example>

	<para>Um dos detalhes mais úteis é a opção <literal>DEVICE</literal>, que lista os dispositivos aonde o sistema irá automaticamente procurar por componentes dos volumes RAID no momento da inicialização. No nosso exemplo, nós substituímos o valor padrão, <literal>partitions containers</literal>, por uma explícita lista de arquivos de dispositivos, já que nós escolhemos usar discos inteiros e não apenas partições, para alguns volumes.</para>

	<para>As duas últimas linhas em nosso exemplo são aquelas que permitem ao núcleo escolher, com segurança, qual número de volume atribuir a qual array. O metadado armazenado nos próprios discos são suficientes para remontar (re-assemble) os volumes, mas não para determinar o número do volume (e o nome de dispositivo que coincide com <filename>/dev/md*</filename>).</para>

	<para>Felizmente, estas linhas podem ser geradas automaticamente:</para>

        <screen><computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>
<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</computeroutput></screen>

	<para>O conteúdo dessas duas últimas linhas não depende da lista de discos incluídos no volume. Logo, não é necessário regenerar essas linhas quando se for substituir um disco falho por um novo. Por outro lado, tem que se tomar o cuidado de atualizar o arquivo ao se criar ou remover uma array RAID.</para>
      </section>
    </section>
    <section id="sect.lvm">
      <title>LVM</title>
      <indexterm><primary>LVM</primary></indexterm>
      <indexterm><primary>Logical Volume Manager - Gerenciador de Volume Lógico</primary></indexterm>

      <para>LVM, o <emphasis>Logical Volume Manager</emphasis>, é uma outra abordagem para abstrair volumes lógicos a partir de seus suportes físicos, que se concentra em aumentar a flexibilidade em vez de aumentar a confiabilidade. O LVM permite mudar um volume lógico de forma transparente, até aonde os aplicativos tem interesse ; por exemplo, é possível adicionar novos discos, migrar os dados para eles, e remover os discos velhos, sem desmontar o volume.</para>
      <section id="sect.lvm-concepts">
        <title>Conceitos sobre LVM</title>

	<para>Esta flexibilidade é atingida graças ao nível de abstração envolvendo três conceitos.</para>

	<para>Primeiro, o PV (<emphasis>Physical Volume</emphasis>) é a entidade mais próxima ao hardware: ele pode ser partições em um disco, ou um disco inteiro, ou até mesmo qualquer outro dispositivo de bloco (incluindo, por exemplo, uma array RAID). Note que quando um elemento é configurado para ser um PV para o LVM, ele deveria ser acessado via LVM apenas, de outra forma o sistema irá ficar confuso.</para>

	<para>Vários PVs podem ser agrupados em um VG (<emphasis>Volume Group</emphasis>), que pode ser comparado com discos tanto virtual quanto extensível. VGs são abstratos, e não aparecem em um arquivo de dispositivo na hierarquia <filename>/dev</filename>, então não a risco em usá-los diretamente.</para>

	<para>O terceiro tipo de objeto é o LV (<emphasis>Logical Volume</emphasis>), que é um pedaço de um VG; se nós mantermos a analogia VG-como-disco, o LV se compara a uma partição. O LV aparece como um dispositivo de bloco com uma entrada em <filename>/dev</filename>, e ele pode ser usado como qualquer outra partição física pode ser (mais comumente, para hospedar um sistema de arquivos ou espaço swap).</para>

	<para>A coisa importante é que a divisão de um VG em LVs é inteiramente independente de seus componentes físicos (os PVs). Um VG com apenas um componente físico (um disco por exemplo) pode ser dividido em uma dúzia de volumes lógicos; similarmente, um VG pode usar vários discos físicos e parecer como um único e grande volume lógico. A única restrição, obviamente, é que o tamanho total alocado aos LVs não podem ser maiores que a capacidade total dos PVs no grupo de volume.</para>

	<para>Geralmente faz sentido, contudo, ter algum tipo de homogeneidade entre os componentes físicos de um VG, e dividir o VG em volumes lógicos que irão ter padrões de uso similares. Por exemplo, se o hardware disponível inclui discos rápidos e discos lentos, os rápidos poderiam ser agrupados em um VG e os lentos em outro; pedaços do primeiro podem então se designados para aplicações que requerem rápido acesso a dados, enquanto o segundo seria mantido para tarefas de menor demanda.</para>

	<para>Em todo caso, tenha em mente que um LV não está particularmente anexado a nenhum PV. É possível influenciar aonde os dados de um LV são fisicamente armazenados, mas essa possibilidade não é necessária para o uso do dia a dia. Pelo contrário: quando o conjunto de componentes físicos de um VG evolui, as localizações de armazenagem física correspondentes a um LV em particular podem ser migradas entre discos (enquanto se mantém dentro de PVs atribuídos ao VG, é claro).</para>
      </section>
      <section id="sect.lvm-setup">
        <title>Configurando um LVM</title>

	<para>Vamos agora seguir, passo a passo, o processo de configurar um LVM para um caso de uso típico: nós queremos simplificar uma situação complexa de armazenagem. Uma situação dessas geralmente acontece após alguma longa e complicada história de  medidas temporárias acumuladas. Para propósitos de ilustração, nós vamos considerar um servidor aonde a armazenagem precisa ter alterações com o passar do tempo, terminando em um labirinto de partições disponíveis, divididas em vários discos parcialmente usados. Em termos mais concretos, as seguintes partições estão disponíveis:</para>
        <itemizedlist>
          <listitem>
	    <para>no disco <filename>sdb</filename>, uma partição <filename>sdb2</filename>, 4 GB;</para>
          </listitem>
          <listitem>
	    <para>no disco <filename>sdc</filename>, uma partição <filename>sdc3</filename>, 3 GB;</para>
          </listitem>
          <listitem>
	    <para>o disco <filename>sdd</filename>, 4 GB, está completamente disponível;</para>
          </listitem>
          <listitem>
	    <para>no disco <filename>sdf</filename>, uma partição <filename>sdf1</filename>, 4 GB; e uma partição <filename>sdf2</filename>, 5 GB.</para>
          </listitem>
        </itemizedlist>

	<para>Complementando, vamos assumir que os discos <filename>sdb</filename> e <filename>sdf</filename> são mais rápidos do que os outros dois.</para>

	<para>Nosso objetivo é configurar três volumes lógicos para três diferentes aplicações: um servidor de arquivos necessitando 5 GB de espaço de armazenagem, um banco de dados (1 GB) e algum espaço para cópias de segurança (12 GB). Os dois primeiros precisam de bom desempenho, mas cópias de segurança são menos críticas em termos de velocidade de acesso. Todos essas limitações impedem o uso de partições propriamente; usando LVM pode-se abstrair o tamanho físico dos dispositivos, então o único limite é o total de espaço disponível.</para>

	<para>As ferramentas necessárias estão no pacote <emphasis role="pkg">lvm2</emphasis> e suas dependência. Quando os mesmos estiverem instalados, configurarar o LVM terá três etapas, cobrindo três níveis de conceitos.</para>

	<para>Primeiro, nós preparamos o volumes físicos utilizando <command>pvcreate</command>:</para>

        <screen id="screen.pvcreate"><computeroutput># </computeroutput><userinput>pvdisplay</userinput>
<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2</userinput>
<computeroutput>  Physical volume "/dev/sdb2" successfully created
# </computeroutput><userinput>pvdisplay</userinput>
<computeroutput>  "/dev/sdb2" is a new physical volume of "4.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/sdb2
  VG Name               
  PV Size               4.00 GiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I

# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</userinput>
<computeroutput>  Physical volume "/dev/sdc3" successfully created
  Physical volume "/dev/sdd" successfully created
  Physical volume "/dev/sdf1" successfully created
  Physical volume "/dev/sdf2" successfully created
# </computeroutput><userinput>pvdisplay -C</userinput>
<computeroutput>  PV         VG   Fmt  Attr PSize PFree
  /dev/sdb2       lvm2 ---  4.00g 4.00g
  /dev/sdc3       lvm2 ---  3.09g 3.09g
  /dev/sdd        lvm2 ---  4.00g 4.00g
  /dev/sdf1       lvm2 ---  4.10g 4.10g
  /dev/sdf2       lvm2 ---  5.22g 5.22g
</computeroutput></screen>

	<para>Até agora tudo bem; note que o PV (volume físico) pode ser configurado em um disco inteiro assim como em partições individuais do mesmo. Como demonstrado acima, o comando <command>pvdisplay</command> lista os PVs existentes, com dois possíveis formatos de saída.</para>

	<para>Agora vamos montar esses elementos físicos em VGs usando <command>vgcreate</command>. Nós vamos reunir apenas PVs dos discos rápidos em um VG <filename>vg_critical</filename>; o outro VG, <filename>vg_normal</filename>, irá incluir também elementos mais lentos.</para>

        <screen id="screen.vgcreate"><computeroutput># </computeroutput><userinput>vgdisplay</userinput>
<computeroutput>  No volume groups found
# </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1</userinput>
<computeroutput>  Volume group "vg_critical" successfully created
# </computeroutput><userinput>vgdisplay</userinput>
<computeroutput>  --- Volume group ---
  VG Name               vg_critical
  System ID             
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               8.09 GiB
  PE Size               4.00 MiB
  Total PE              2071
  Alloc PE / Size       0 / 0   
  Free  PE / Size       2071 / 8.09 GiB
  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp

# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</userinput>
<computeroutput>  Volume group "vg_normal" successfully created
# </computeroutput><userinput>vgdisplay -C</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize  VFree 
  vg_critical   2   0   0 wz--n-  8.09g  8.09g
  vg_normal     3   0   0 wz--n- 12.30g 12.30g
</computeroutput></screen>

	<para>Aqui novamente, os comandos são bem simples (e <command>vgdisplay</command> propõem dois formatos de saída). Note que é perfeitamente possível usar duas partições de um mesmo disco físico em dois VGs diferentes. Note também que nós usamos um prefixo <filename>vg_</filename> para nomear nossos VGs, mas isso não é nada mais que uma convenção.</para>

	<para>Nós agora temos dois "discos virtuais", com o tamanho de 8 GB e 12 GB, respectivamente.Vamos transformá-los em "partições virtuais" (LVs). Isto envolve o comando <command>lvcreate</command>, e uma sintaxe um pouco mais complexa:</para>

        <screen id="screen.lvcreate"><computeroutput># </computeroutput><userinput>lvdisplay</userinput>
<computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical</userinput>
<computeroutput>  Logical volume "lv_files" created
# </computeroutput><userinput>lvdisplay</userinput>
<computeroutput>  --- Logical volume ---
  LV Path                /dev/vg_critical/lv_files
  LV Name                lv_files
  VG Name                vg_critical
  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT
  LV Write Access        read/write
  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400
  LV Status              available
  # open                 0
  LV Size                5.00 GiB
  Current LE             1280
  Segments               2
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0

# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical</userinput>
<computeroutput>  Logical volume "lv_base" created
# </computeroutput><userinput>lvcreate -n lv_backups -L 12G vg_normal</userinput>
<computeroutput>  Logical volume "lv_backups" created
# </computeroutput><userinput>lvdisplay -C</userinput>
<computeroutput>  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_base    vg_critical -wi-a---  1.00g                                           
  lv_files   vg_critical -wi-a---  5.00g                                           
  lv_backups vg_normal   -wi-a--- 12.00g</computeroutput></screen>

	<para>São necessários dois parâmetros para a criação de volumes lógicos; eles tem que ser passados ao <command>lvcreate</command> como opções. O nome do LV a ser criado é especificado com a opção <literal>-n</literal>, e seu tamanho geralmente é dado usando a opção <literal>-L</literal>. Nós também precisamos dizer ao comando qual o VG a ser operado, é claro, sendo portanto o último parâmetro da linha de comando.</para>

        <sidebar>
          <title><emphasis>Aprofundamento</emphasis> <command>lvcreate</command> opções</title>

	  <para>O comando <command>lvcreate</command> possui diversas opções que permitem manipular como o LV é criado.</para>

	  <para>Vamos primeiro descrever a opção <literal>-l</literal>, com a qual o tamanho do LV pode ser dados como um número de blocos (como o oposto das unidades “humanas” que nós usamos acima). Esses blocos (chamados de PEs, <emphasis>physical extents</emphasis>, nos termos LVM) são unidades contíguas de espaço de armazenamento em PVs, e elas não podem ser divididas entre LVs. Se alguém quiser definir espaço de armazenamento para um LV com alguma precisão, por exemplo para usar todo o espaço disponível, a opção <literal>-l</literal> provavelmente será preferida, ao invés da <literal>-L</literal>.</para>

	  <para>Também é possível sugerir uma localização física de um LV, para que suas extensões sejam armazenadas em um PV em particular (enquanto se mantém dentro dos atribuídos ao VG, é claro). Como nós sabemos que o <filename>sdb</filename> é mais rápido que o <filename>sdf</filename>, nós talvez queiramos armazenar o <filename>lv_base</filename> lá, caso nós queiramos dar uma vantagem ao servidor de banco de dados, em comparação com o servidor de arquivos. A linha de comando se torna: <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command>. Note que esse comando pode falhar se o PV não tiver extensões livres suficientes. Em nosso exemplo, nós provavelmente teríamos que criar <filename>lv_base</filename> antes de <filename>lv_files</filename> para evitar essa situação – ou liberar algum espaço em <filename>sdb2</filename> com o comando <command>pvmove</command>.</para>
        </sidebar>

	<para>Volumes lógicos, quando criados, são representados como dispositivos de blocos no <filename>/dev/mapper/</filename>:</para>

        <screen><computeroutput># </computeroutput><userinput>ls -l /dev/mapper</userinput>
<computeroutput>total 0
crw------- 1 root root 10, 236 Jun 10 16:52 control
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2
# </computeroutput><userinput>ls -l /dev/dm-*</userinput>
<computeroutput>brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0
brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1
brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2
</computeroutput></screen>

        <sidebar>
          <title><emphasis>NOTA</emphasis> Auto-detectando volumes LVM</title>

          <para>Quando o computador é inicializado, a unidade de serviço do systemd <filename>lvm2-activation</filename> executa o  <command>vgchange -aay</command> para "ativar" os grupos de volume; ele faz uma busca nos dispositivos disponíveis; aqueles que tiverem sido inicializados como volumes físicos para o LVM são registrados em um subsistema LVM, aqueles que pertencem aos grupos de volume são montados, e os volumes lógicos relevantes são iniciados e tornados disponíveis. Não existe, portanto, necessidade de editar arquivos de configuração quando se cria ou modifica volumes LVM.</para>

	  <para>Note, contudo, que o layout dos elementos LVM (volumes físicos e lógicos, e grupos de volume) tem cópia de segurança em <filename>/etc/lvm/backup</filename>, que pode ser útil em caso de um problema (ou apenas para da uma espiada embaixo do capô).</para>
        </sidebar>

	<para>Para simplificar, links simbólicos são convenientemente criados em diretórios que coincidem com os VGs:</para>

        <screen><computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical</userinput>
<computeroutput>total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0
# </computeroutput><userinput>ls -l /dev/vg_normal</userinput>
<computeroutput>total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</computeroutput></screen>

	<para>Os LVs então podem ser utilizados exatamente como partições padrão:</para>

        <screen><computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups</userinput>
<computeroutput>mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 3145728 4k blocks and 786432 inodes
Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d
[...]
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </computeroutput><userinput>mkdir /srv/backups</userinput>
<computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups</userinput>
<computeroutput># </computeroutput><userinput>df -h /srv/backups</userinput>
<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_normal-lv_backups   12G   30M   12G   1% /srv/backups
# </computeroutput><userinput>[...]</userinput>
<computeroutput>[...]
# </computeroutput><userinput>cat /etc/fstab</userinput>
<computeroutput>[...]
/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2
/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2
/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</computeroutput></screen>

	<para>Do ponto de vista das aplicações, a miríade de pequenas partições foi abstraída em um grande volume de 12 GB, com um nome amigável.</para>
      </section>
      <section id="sect.lvm-over-time">
        <title>LVM ao longo do tempo</title>

	<para>Mesmo que a habilidade de agregar partições ou discos físicos seja conveniente, essa não é a principal vantagem trazida pelo LVM. A flexibilidade que ele trás é especialmente notada com o passar do tempo, quando as necessidades se desenvolvem. Em nosso exemplo, vamos assumir que novos e grandes arquivos tem que ser armazenados, e que o LV dedicado ao servidor de arquivos é muito pequeno para acomodá-los. Como nós não usamos todo o espaço disponível em <filename>vg_critical</filename>, nós podemos crescer o <filename>lv_files</filename>. Para esse propósito, nós iremos usar o comando <command>lvresize</command>, e então o <command>resize2fs</command> para adaptar o sistema de arquivos em conformidadde:</para>

        <screen><computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>
<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files
# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>
<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 5.00g
# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   2   2   0 wz--n- 8.09g 2.09g
# </computeroutput><userinput>lvresize -L 7G vg_critical/lv_files</userinput>
<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).
  Logical volume lv_files successfully resized
# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>
<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 7.00g
# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>
<computeroutput>resize2fs 1.42.12 (29-Aug-2014)
Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.

# </computeroutput><userinput>df -h /srv/files/</userinput>
<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</computeroutput></screen>

        <sidebar>
          <title><emphasis>ATENÇÃO</emphasis> Redimensionando sistemas de arquivos</title>

	  <para>Nem todos os sistemas de arquivos podem ser redimensionados em tempo real; redimensionar um volume podem portanto requere primeiro desmontar o sistema de arquivos e remontá-lo depois. Claro que, se alguém quiser diminuir o espaço alocado para um LV, o sistema de arquivos tem que ser diminuído primeiro; a ordem é revertida quando o redimensionamento segue na outra direção: o volume lógico tem que ser cultivado antes do sistema de arquivos existente nele. Isso é bastante simples, já que em nenhum momento o tamanho do sistema de  arquivos tem que ser maior que o dispositivo de bloco aonde ele reside (seja esse dispositivo uma partição física ou um volume lógico).</para>

	  <para>Os sistemas de arquivo ext3, ext4 and xfs podem ser cultivados em tempo real, sem serem desmontados; encolhé-los requer uma desmontagem. O sistema de arquivos reiserfs permite o redimensionamento em tempo real nas duas direções. O venerável ext2 não permite, e sempre requer o desmonte.</para>
        </sidebar>

	<para>Nós poderíamos proceder de maneira similar para estender o volume que hospeda o banco de dados, apenas se nós tivermos alcançado o limite de espaço disponível do VG:</para>

        <screen><computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>
<computeroutput>Sist. Arq.           Tam. Usado Disp. Uso% Montado em
/dev/mapper/vg_critical-lv_base 1008M  854M  104M  90% /srv/base
# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree 
  vg_critical   2   2   0 wz--n- 8.09g 92.00m</computeroutput></screen>

	<para>Não importa, já que o LVM permite a adição de volumes físico em grupos de volumes existentes. Por exemplo, talvez nós percebamos que a partição <filename>sdb1</filename>, que era até agora usada fora do LVM, apenas contém arquivos que poderiam ser movidos para <filename>lv_backups</filename>. Nós podemos agora reciclá-la e integrá-la ao grupo de volume, e assim recuperar algum espaço disponível. Esse é o propósito do comando <command>vgextend</command>. Claro que, a partição tem que ser preparada antes como um volume físico. Uma vez que o VG tenha sido estendido, nós podemos usar comandos similares aos anteriores para cultivar o volume lógico e então o sistema de arquivos:</para>

        <screen><computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>
<computeroutput>  Physical volume "/dev/sdb1" successfully created
# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>
<computeroutput>  Volume group "vg_critical" successfully extended
# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   3   2   0 wz--n- 9.09g 1.09g
# </computeroutput><userinput>[...]</userinput>
<computeroutput>[...]
# </computeroutput><userinput>df -h /srv/base/</userinput>
<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</computeroutput></screen>

        <sidebar>
          <title><emphasis>Aprofundamento</emphasis> LVM avançado</title>

	  <para>O LVM também serve para usos mais avançados, aonde muitos detalhes podem ser especificados a mão. Por exemplo, um administrador pode ajustar o tamanho dos blocos que compõem os volumes físicos e lógicos, assim como seus layouts físicos. Também é possível mover blocos entre PVs, por exemplo para um ajuste fino no desempenho ou, de maneira mais mundana, liberar um PV quando alguém precisa extrair o disco físico correspondente de um VG (seja para designá-lo para outro VG ou removê-lo do LVM totalmente). As páginas de manual que descrevem os comandos geralmente são claras e detalhadas. Um bom ponto de partida é a página de manual <citerefentry><refentrytitle>lvm</refentrytitle> <manvolnum>8</manvolnum></citerefentry>.</para>
        </sidebar>
      </section>
    </section>
    <section id="sect.raid-or-lvm">
      <title>RAID ou LVM?</title>

      <para>Tanto o RAID quanto o LVM irão trazer vantagens indiscutíveis assim que se deixar o simples caso de um computador de mesa com um único disco rígido, aonde o padrão de uso não muda com o tempo. Contudo, RAID e LVM vão em direções diferentes, com objetivos divergentes, e é legítimo questionar qual deles deve ser adotado. A resposta mais apropriada irá, é claro, depender das necessidades atuais e previstas.</para>

      <para>Existem alguns casos simples aonde a questão realmente não surge. Se a necessidade é salvaguardar dados contra falhas de hardware, então, obviamente, o RAID será configurado em uma array redundante de discos, já que o LVM realmente não é designado para esse problema. Se, por outro lado, a necessidade é por um esquema de armazenamento flexível aonde os volumes são feitos independente do layout físico dos disco, o RAID não ajuda muito e o LVM será a escolha natural.</para>

      <sidebar>
        <title><emphasis>NOTA</emphasis> Se o desempenho importa…</title>

	<para>Se a velocidade de entrada/saída é essencial, especialmente em termos de tempo de acesso, o uso do LVM e/ou RAID em uma das muitas combinações pode ter algum impacto no desempenho, e isso pode influenciar nas decisões sobre qual escolher. Contudo, essas diferenças de desempenho são realmente pequenas, e só serão mensuráveis  em alguns casos de uso. Se desempenho importa, o melhor ganho a ser obtido seria no uso de mídia não rotativa (<indexterm><primary>SSD</primary></indexterm><emphasis>solid-state drives</emphasis> ou SSDs); seu custo por megabyte é maior que os discos rígidos padrão, e sua capacidade geralmente é menor, mas ele fornece desempenho excelente para acessos aleatórios. Se o padrão de uso inclui muitas operações de entrada/saída espalhadas por todo o sistema de arquivos, por exemplo, para bancos de dados aonde consultas complexas são executadas rotineiramente, então a vantagem de executá-las em um SSD de longe superam tudo o que pode ser ganho escolhendo LVM sobre RAID ou o contrário. Nessas situações, a escolha deveria ser determinada por outras considerações ao invés de velocidade pura, já que o aspecto desempenho é mais facilmente lidado usando SSDs.</para>
      </sidebar>

      <para>O terceiro caso de uso notável é quando alguém apenas quer agregar dois discos em um volume, seja por questões de desempenho, ou para ter um único sistema de arquivos que seja maior que qualquer dos discos disponíveis. Esse caso pode ser resolvido pelo RAID-0 (ou mesmo um linear-RAID) e por um volume LVM. Quando nesta situação, e salvo restrições extras (por exemplo, manter em sintonia com o resto dos computadores se eles usam apenas RAID), a configuração de escolha irá geralmente ser LVM. A configuração inicial é um pouco mais complexa, mas esse incremento na complexidade mais do que compensado pela flexibilidade extra que o LVM trás caso as necessidades mudem ou se novos discos precisem ser adicionados.</para>

      <para>Então, é claro, temos uma caso de uso realmente interessante, aonde o sistema de armazenamento precisa ser feito tanto para resistência de falha de hardware quanto flexível quando se trata de alocação de volume. Nem RAID nem LVM podem atender esses dois requisitos por conta própria; não tem problema, é aqui que nós usamos os dois ao mesmo tempo — ou melhor, um em cima do outro. O esquema que tem tudo, mas só se tornou um padrão quando o RAID e o LVM alcançaram a maturidade, é para garantir a redundância de dados, primeiro pelo agrupamento de discos em um pequeno número de grandes arrays RAID, e para usar essas arrays RAID como volumes físicos LVM; partições lógicas serão então esculpidas a partir desses LVs para sistemas de arquivos. O ponto forte dessa configuração é que, quando um disco falha, apenas um pequeno número das arrays RAID precisará ser reconstru[ida, limitando assim o tempo gasto pelo administrador para recuperação.</para>

      <para>Vamos ver um exemplo concreto: o departamento de relações públicas da Falcot Corp precisa de uma estação de trabalho para edição de vídeo, mas o orçamento do departamento não permite investir em hardware de ponta para a finalidade. Uma decisão é tomada para favorecer o hardware que é específico para a natureza gráfica do trabalho (monitor e placa de vídeo), e ficar com o hardware genérico para armazenamento. No entanto, como é amplamente conhecido, o vídeo digital tem sim alguns requisitos especiais para seu armazenamento: a quantidade de dados a ser armazenado é grande, e a taxa de transferência para leitura e escrita desses dados é importante para o desempenho geral do sistema (mais que o tempo de acesso típico, por exemplo). Essas restrições precisam ser preenchidas com hardware genérico, neste caso com dois discos rígidos SATA de 300 GB; os dados do sistema também tem que ser resistentes a falha de hardware, assim como alguns dos dados do usuário. Videoclipes editados tem que realmente estar seguros, mas vídeo com edição pendente é menos critico, já que eles ainda estão nas fitas de vídeo.</para>

      <para>O RAID-1 e o LVM são combinados para satisfazer essas restrições. Os discos são anexados a duas controladoras SATA diferentes, para otimizar acesso paralelo e reduzir o risco de falhas simultâneas, e eles portanto aparecem como <filename>sda</filename> e <filename>sdc</filename>. Eles são particionados de forma idêntica, seguindo o seguinte esquema:</para>

      <screen><computeroutput># </computeroutput><userinput>fdisk -l /dev/sda</userinput>
<computeroutput>
Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x00039a9f

Device    Boot     Start       End   Sectors Size Id Type
/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect
/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris
/dev/sda3        4000185 586099395 582099210 298G 5  Extended
/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect
/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect
/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</computeroutput></screen>
      <itemizedlist>
        <listitem>
	  <para>As primeiras partições em ambos os discos (por volta de 1 GB) são montadas em um volume RAID-1, <filename>md0</filename>. Este espelho é diretamente usado para armazenar o sistema de arquivos raiz.</para>
        </listitem>
        <listitem>
	  <para>As partições <filename>sda2</filename> e <filename>sdc2</filename> são usadas como partição swap, provendo um total de 2 GB de espaço swap. Com 1 GB de RAM, a estação de trabalho encontra uma quantidade confortável de memoria disponível.</para>
        </listitem>
        <listitem>
	  <para>As partições <filename>sda5</filename> e <filename>sdc5</filename>, assim como a <filename>sda6</filename> e <filename>sdc6</filename>, são montadas em dois novos volumes RAID-1 de 100 GB cada, <filename>md1</filename> e <filename>md2</filename>. Os dois espelhos são iniciados como volumes físicos para o LVM, e atribuídos para o grupo de volume <filename>vg_raid</filename>. Esse VG , portanto, contém 200 GB de espaço seguro.</para>
        </listitem>
        <listitem>
	  <para>As partições que sobraram, <filename>sda7</filename> e <filename>sdc7</filename>,são diretamente usadas como volumes físicos, e associadas a outro VG chamado <filename>vg_bulk</filename>, o qual portanto terminará com aproximadamente 200 GB de espaço.</para>
        </listitem>
      </itemizedlist>

      <para>Uma vez que os VGs sejam criados, eles podem ser particionados de maneira bem flexível. É preciso ter em mente que os LVs criados em <filename>vg_raid</filename> serão preservados mesmo que um dos discos falhe, o que não será o caso para LVs criados em <filename>vg_bulk</filename>; por outro lado, o último será alocado em paralelo nos dois discos, o que permite altas velocidades de leitura ou escrita para arquivos grandes.</para>

      
      <para>Portanto nós iremos criar os LVs <filename>lv_usr</filename>, <filename>lv_var</filename> e <filename>lv_home</filename> no <filename>vg_raid</filename>, para hospedar os sistemas de arquivos correspondentes; outro grande LV, <filename>lv_movies</filename>, será usado para hospedar as versões definitivas dos filmes após a edição. O outro VG será dividido em um grande <filename>lv_rushes</filename>, para dados tirados de câmeras digitais de vídeo, e um <filename>lv_tmp</filename> para arquivos temporários. A localização da área de trabalho é uma escolha menos simples de fazer: enquanto bom desempenho é necessário para esse volume, vale o risco de perda de trabalho se uma falha de disco ocorrer durante uma sessão de edição? Dependendo da resposta  essa pergunta, o LV relevante será criado em um VG ou em outro.</para>

      <para>Nós agora temos tanto alguma redundância para dados importantes quanto muita flexibilidade no modo como o espaço disponível é dividido entre as aplicações. Para novo software que for instalado mais tarde (para edição de clips de áudio, por exemplo), o LV que hospeda <filename>/usr/</filename> pode ser aumentado sem dor de cabeça.</para>

      <sidebar>
        <title><emphasis>NOTA</emphasis> Por que três volumes RAID-1?</title>

	<para>Nós poderíamos ter configurado somente um volume RAID-1, para servir como volume físico para <filename>vg_raid</filename>. Por que criar três deles, então?</para>

	<para>A razão para a primeira divisão (<filename>md0</filename> vs. os outros) é para segurança de dados: dados escritos nos dois elementos de um espelho RAID-1 são exatamente os mesmos, e é portanto possível ignorar a camada RAID e montar um dos discos diretamente. No caso de um bug no núcleo, por exemplo, ou se o metadado do LVM se for corrompido, ainda é possível inicializar um sistema mínimo para acessar dados críticos como o layout de discos nos volumes RAID e LVM; o metadado pode então ser reconstruído e os arquivos podem ser acessados novamente, de modo que o sistema pode ser trazido de volta ao seu estado nominal.</para>

	<para>A razão para a segunda divisão (<filename>md1</filename> vs. <filename>md2</filename>) não é tão bem definida, e mais relacionada ao reconhecimento que o futuro é incerto. Quando a estação de trabalho é montada na primeira vez, os requisitos exatos de armazenamento não são necessariamente conhecidos com uma precisão perfeita; eles podem também evoluir ao longo do tempo. Em nosso caso, nós não podemos saber com antecedência os requisitos de espaço de armazenamento real para cenas de vídeo e video clips completos. Se um clip em particular precisa de uma grande quantidade de cenas, e o VG dedicado a dados redundantes está cheio menos que a metade, nós podemos reusar algum de seu espaço desnecessário. Nós podemos remover um dos volumes físicos, digamos o <filename>md2</filename>, de <filename>vg_raid</filename> e então ou atribuí-lo ao <filename>vg_bulk</filename> diretamente (se a duração esperada da operação é curta o suficiente para que nós possamos viver com a temporária queda no desempenho), ou desfazer a configuração RAID em <filename>md2</filename> e integrar seus componentes <filename>sda6</filename> e <filename>sdc6</filename> no VG maior (o que aumenta para 200 GB ao invés de 100 GB); o volume lógico <filename>lv_rushes</filename> pode então ser aumentado de acordo com os requisitos.</para>
      </sidebar>
    </section>
  </section>
  <section id="sect.virtualization">
    <title>Virtualização</title>
    <indexterm><primary>virtualização</primary></indexterm> 

    <para>Virtualização é um dos maiores avanços nos anos recentes da computação. O termo cobre várias abstrações e técnicas de simulação de computadores virtuais com um grau de variabilidade de independência do hardware real. Um servidor físico pode então hospedar vários sistemas que trabalham ao mesmo tempo e em isolamento. As aplicações são muitas, e geralmente derivam a partir desse isolamento: ambientes de teste com configurações variáveis por exemplo, ou separação de serviços hospedados entre diferentes máquinas virtuais para segurança.</para>

    <para>Existem múltiplas soluções de virtualização, cada uma com seus prós e contras. Este livro focará no Xen, LXC e KVM, mas outras implementações dignas de nota são as seguintes:</para>
    <indexterm><primary><emphasis>VMWare</emphasis></primary></indexterm>
    <indexterm><primary><emphasis>Bochs</emphasis></primary></indexterm>
    <indexterm><primary><emphasis>QEMU</emphasis></primary></indexterm>
    <indexterm><primary><emphasis>VirtualBox</emphasis></primary></indexterm>
    <indexterm><primary><emphasis>KVM</emphasis></primary></indexterm>
    <indexterm><primary><emphasis>LXC</emphasis></primary></indexterm>
    <itemizedlist>
      <listitem>
	<para>O QEMU é um emulador de software para um computador completo; o desempenho está longe da velocidade que se poderia alcançar rodando nativamente, mas ele permite rodar sistemas operacionais não modificados ou experimentais no hardware emulado. Ele também permite a emulação de uma arquitetura de hardware diferente: por exemplo, um sistema <emphasis>amd64</emphasis> pode emular um computador <emphasis>arm</emphasis>. O QEMU é software livre. <ulink type="block" url="http://www.qemu.org/" /></para>
      </listitem>
      <listitem>
	<para>Bochs é outra máquina virtual livre, mas somente emula as arquiteturas x86 (i386 e amd64).</para>
      </listitem>
      <listitem>
	<para>VMWare é uma máquina virtual proprietária; sendo uma das mais antigas que se tem por ai, também é uma das mais amplamente conhecidas. Ela funciona com princípios similares aos do QEMU. A VMWare propõe recursos avançados tais como "snapshotting" de uma máquina virtual em execução. <ulink type="block" url="http://www.vmware.com/" /></para>
      </listitem>
      <listitem>
        <para>VirtualBox é uma máquina virtual que é quase toda software livre (alguns componentes extra estão disponíveis sob licença proprietária). Infelizmente ele está na seção Debian “contrib” devido ao fato que ele inclui alguns arquivos pré-compilados que não podem ser reconstruídos sem um compilador proprietário.  Embora ele seja mais jovem que a VMWare e restrito as arquiteturas i386 e amd64, ele ainda inclui alguns "snapshotting" e outras características interessantes. <ulink type="block" url="http://www.virtualbox.org/" /></para>
      </listitem>
    </itemizedlist>
    <section id="sect.xen">
      <title>Xen</title>

      <para>Xen <indexterm><primary>Xen</primary></indexterm> é uma solução de “paravirtualização”. Ele introduz uma fina camada de abstração, chamada de “hypervisor”, entre o hardware e os sistemas superiores; Ele age como um árbitro que controla o acesso ao hardware feito pelas máquinas virtuais. Entretanto, ele apenas lida com algumas das instruções, o resto é executado diretamente pelo hardware em nome dos sistemas. A principal vantagem é que o desempenho não se degradada, e os sistemas rodam perto da velocidade nativa; a desvantagem é que os núcleos dos sistemas operacionais que alguém deseja usar em um "hypervisor" Xen precisam ser adaptados para rodar o Xen.</para>

      <para>Vamos gastar algum tempo com termos. O "hypervisor" é a camada mais baixa, que roda diretamente no hardware, até mesmo abaixo do núcleo. Esse "hypervisor" pode dividir o resto do software entre vários <emphasis>domínios</emphasis>, que podem ser vistos como muitas máquinas virtuais. Um desses domínios (o primeiro que for iniciado) é conhecido como <emphasis>dom0</emphasis>, e tem um papel especial, já que apenas esse domínio pode controlar o "hypervisor" e a execução de outros domínios. Esses outros domínios são conhecidos como <emphasis>domU</emphasis>. Em outras palavras, e a partir do ponto de vista do usuário, o <emphasis>dom0</emphasis> coincide com o “hospedeiro” de outros sistemas de virtualização, enquanto que o <emphasis>domU</emphasis> pode ser visto como um "convidado" (“guest”).</para>

      <sidebar>
        <title><emphasis>CULTURA</emphasis> Xen e as várias versões do Linux</title>

	<para>O Xen foi inicialmente desenvolvido como um conjunto de patches que viviam fora da árvore oficial, e não integrados ao núcleo Linux. Ao mesmo tempo, vários sistemas de virtualização próximos (incluindo o KVM) necessitavam de algumas funções genéricas relacionadas a virtualização para facilitar suas integrações, e o núcleo Linux ganhou esse conjunto de funções (conhecidas como interface <emphasis>paravirt_ops</emphasis> ou <emphasis>pv_ops</emphasis>). Como os patches Xen estavam duplicando algumas das funcionalidades dessa interface, eles não podiam ser aceitos oficialmente.</para>

	<para>A Xensource, a companhia por trás do Xen, portanto, tinha que portar o Xen para esse novo "framework", para que os patches do Xen pudessem ser incorporados ao núcleo Linux oficial. Isso significa um monte de códigos reescritos, e embora a  Xensource logo tivesse uma versão em funcionamento com base na interface paravirt_ops, os patches eram apenas progressivamente incorporados no núcleo oficial. A fusão foi concluída no Linux 3.0. <ulink type="block" url="http://wiki.xenproject.org/wiki/XenParavirtOps" /></para>

	<para>Como a <emphasis role="distribution">Jessie</emphasis> é baseada na versão 3.16 do núcleo Linux, os pacotes padrão <emphasis role="pkg">linux-image-686-pae</emphasis> e <emphasis role="pkg">linux-image-amd64</emphasis> incluem o código necessário, e os patches específicos para <emphasis role="distribution">Squeeze</emphasis> e versões anteriores do Debian não são mais necessárias. <ulink type="block" url="http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix" /></para>
      </sidebar>

      <sidebar>
        <title><emphasis>NOTA</emphasis> Arquiteturas compatíveis com Xen</title>

        <para>O Xen, atualmente, só está disponível para as arquiteturas i386, amd64, arm64 e armhf.</para>
      </sidebar>

      <sidebar>
        <title><emphasis>CULTURA</emphasis> Xen e núcleos não-Linux</title>

	<para>O Xen requer modificações em todos os sistemas operacionais em que alguém queira rodá-lo; nem todos os núcleos tem o mesmo nível de maturidade sobre esse assunto. Muitos são totalmente funcionais, tanto como dom0 quanto domU: Linux 3.0 e posteriores, NetBSD 4.0 e posteriores, e OpenSolaris. Outros apenas funcionam como domU. Você pode checar o status de cada sistema operacional no wiki do Xen:  <ulink type="block" url="http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen" /> <ulink type="block" url="http://wiki.xenproject.org/wiki/DomU_Support_for_Xen" /></para>

	<para>Contudo, se o Xen puder contar com funções de hardware dedicadas a virtualização (que apenas estão presentes em processadores mais recentes), até mesmo sistemas operacionais não modificados podem rodar como domU (incluindo o Windows).</para>
      </sidebar>

      <para>Utilizar o Xen com o Debian necessita de três componentes:</para>
      <itemizedlist>
        <listitem>
	  <para>O "hypervisor" ele próprio. De acordo com o hardware disponível, o pacote apropriado será <emphasis role="pkg">xen-hypervisor-4.4-amd64</emphasis>, <emphasis role="pkg">xen-hypervisor-4.4-armhf</emphasis>, ou <emphasis role="pkg">xen-hypervisor-4.4-arm64</emphasis>.</para>
        </listitem>
        <listitem>
	  <para>Um núcleo que rode nesse "hypervisor". Qualquer núcleo mais recente que o 3.0 irá servir, incluindo a versão 3.16 presente na <emphasis role="distribution">Jessie</emphasis>.</para>
        </listitem>
        <listitem>
	  <para>A arquitetura i386 também requer uma biblioteca padrão com os patches apropriados para aproveitar o Xen; ela está no pacote <emphasis role="pkg">libc6-xen</emphasis>.</para>
        </listitem>
      </itemizedlist>

      <para>Para evitar o aborrecimento de selecionar esses componentes manualmente, a conveniência de alguns pacotes (tais com o  <emphasis role="pkg">xen-linux-system-amd64</emphasis>) foram colocados a disposição; todos eles puxam, em uma boa combinação, os pacotes "hypervisor" e núcleo apropriados. O "hypervisor" também trás o <emphasis role="pkg">xen-utils-4.4</emphasis>, que contém ferramentas para controlar o "hypervisor" a partir do dom0. Este, por sua vez, trás a biblioteca padrão apropriada. Durante a instalação de tudo isso, scripts de configuração também criam uma nova entrada no menu do carregador de inicialização Grub, a fim de iniciar o núcleo escolhido em um dom0 Xen. Note, contudo, que essa entrada geralmente não é definida para ser a primeira da lista, e portanto, não será selecionada por padrão. Se esse não é o comportamento desejado, os comandos a seguir irão mudar isso:</para>

      <screen><computeroutput># </computeroutput><userinput>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen
</userinput><computeroutput># </computeroutput><userinput>update-grub
</userinput></screen>

      <para>Uma vez que esses pré-requisitos estejam instalados, o próximo passo é testar o comportamento do próprio dom0; isso envolve uma reinicialização do "hypervisor" e do núcleo Xen. O sistema deverá inicializar da maneira usual, com algumas mensagens extras no console, durante os passo iniciais da inicialização.</para>

      <para>Agora é o momento de realmente instalar sistemas úteis nos sistemas domU, usando as ferramentas do <emphasis role="pkg">xen-tools</emphasis>. esse pacote provê o comando <command>xen-create-image</command>, que automatiza a tarefa em grande parte. O único parâmetro mandatório é o <literal>--hostname</literal>, dando um nome ao domU; outras opções são importantes, mas podem ser armazenadas no arquivo de configuração <filename>/etc/xen-tools/xen-tools.conf</filename>, e assim, a ausência dessas opções na linha de comando não dispara um error. É, portanto, importante ou checar o conteúdo desse arquivo antes de criar imagens, ou usar parâmetros extras na invocação do <command>xen-create-image</command>. Parâmetros que são importantes de notar são os seguintes:</para>
      <itemizedlist>
        <listitem>
	  <para><literal>--memory</literal>, para definir o quantidade de RAM dedicada para o sistema recentemente criado;</para>
        </listitem>
        <listitem>
	  <para><literal>--size</literal> e <literal>--swap</literal>, para definir o tamanho dos "discos virtuais" disponíveis para o domU;</para>
        </listitem>
        <listitem>
	  <para><literal>--debootstrap</literal>, para fazer com que o novo sistema seja instalado com o <command>debootstrap</command>; neste caso, a opção <literal>--dist</literal> irá também ser usada mais geralmente (com um nome de distribuição como a <emphasis role="distribution">jessie</emphasis>).</para>

          <sidebar>
            <title><emphasis>Aprofundamento</emphasis> Instalando um sistema não Debian em um domU</title>

	    <para>Em caso de sistemas não-Linux, um certo cuidado deve ser tomado ao definir qual domU o núcleo deve usar, usando a opção <literal>--kernel</literal>.</para>
          </sidebar>
        </listitem>
        <listitem>
	  <para><literal>--dhcp</literal> define que a configuração de rede do domU deve ser obtida por DHCP enquanto <literal>--ip</literal> permite a definição estática do endereço IP.</para>
        </listitem>
        <listitem>
	  <para>Por fim, um método de armazenamento tem que ser escolhido para as imagens a serem criadas (aquelas que serão vistas como unidades de disco rígido a partir do domU). O método mais simples, que corresponde a opção <literal>--dir</literal>, é criar um um arquivo no dom0 para cada dispositivo que o domU deveria fornecer. Para sistemas que usam o LVM, a alternativa é usar a opção <literal>--lvm</literal>, seguida pelo nome do grupo de volume; <command>xen-create-image</command> irá então criar um novo volume lógico dentro desse grupo, e esse volume lógico se tornará disponível para o domU como uma unidade de disco rígido.</para>

          <sidebar>
            <title><emphasis>NOTA</emphasis> Armazenamento em domU</title>

	    <para>Discos rígidos inteiros também podem ser exportados para o domU, assim como as partições, arrays RAID ou volumes lógicos LVM pré-existentes . Entretanto, essas operações não são automatizadas pelo <command>xen-create-image</command>. Logo, editando o arquivo de configuração de imagem do Xen irá pôr em ordem o arquivo após sua criação inicial com o <command>xen-create-image</command>.</para>
          </sidebar>
        </listitem>
      </itemizedlist>

      <para>Assim que essas escolhas são feitas, podemos criar uma imagem para o nosso futuro Xen domU:</para>

      <screen><computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</userinput>
<computeroutput>
[...]
General Information
--------------------
Hostname       :  testxen
Distribution   :  jessie
Mirror         :  http://ftp.debian.org/debian/
Partitions     :  swap            128Mb (swap)
                  /               2G    (ext3)
Image type     :  sparse
Memory size    :  128Mb
Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64
Initrd path    :  /boot/initrd.img-3.16.0-4-amd64
[...]
Logfile produced at:
         /var/log/xen-tools/testxen.log

Installation Summary
---------------------
Hostname        :  testxen
Distribution    :  jessie
MAC Address     :  00:16:3E:8E:67:5C
IP-Address(es)  :  dynamic
RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b
Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ
</computeroutput></screen>

      <para>Agora temos uma máquina virtual, mas atualmente não está sendo executada (e portanto somente utilizando espaço de disco do dom0). Obviamente, podemos criar mais imagens, possivelmente com parâmetros diferentes.</para>

      <para>Antes de ligarmos essas máquinas virtuais, nós precisamos definir como elas serão acessadas. Elas podem, é claro, serem consideradas como máquinas isoladas, apenas acessadas através de seus consoles de sistema, mas isso raramente coincide com o padrão de uso. Na maioria das vezes, um domU será considerado um servidor remoto, e apenas acessado através de uma rede. No entanto, seria bem inconveniente adicionar uma placa de rede para cada; e por isso é que o Xen permite a criação de interfaces virtuais, que cada domínio possa ver e usar da forma padrão. Note que essas interfaces, mesmo que elas sejam virtuais, só serão úteis uma vez conectadas a uma rede, mesmo que virtual. O Xen tem vários modelos de rede para isso:</para>
      <itemizedlist>
        <listitem>
	  <para>O modelo mais simples é o modelo de ponte <emphasis>bridge</emphasis>; todos as placas de rede eth0 (tanto no caso do dom0 quanto nos sistemas domU) se comportam como se fossem diretamente conectadas em um switch de rede.</para>
        </listitem>
        <listitem>
	  <para>Em seguida vem o modelo <emphasis>routing</emphasis>, onde o dom0 se comporta como um roteador que se põem entre sistemas domU e a rede (física) externa.</para>
        </listitem>
        <listitem>
	  <para>Finalmente, no modelo <emphasis>NAT</emphasis>, o dom0 novamente está entre os sistemas domU e o resto da rede, mas os sistemas domU não são diretamente acessíveis por fora, e todo o tráfego vai através de uma tradução de endereços de rede (NAT) para o dom0.</para>
        </listitem>
      </itemizedlist>

      <para>Estes três nós de rede envolvem várias interfaces com nome incomuns, tais como <filename>vif*</filename>, <filename>veth*</filename>, <filename>peth*</filename> e <filename>xenbr0</filename>. O "hypervisor" Xen organiza-os de acordo com qualquer que seja o layout definido, sob o controle das ferramentas de espaço do usuário. Como o NAT e os modelos de roteamento adaptam-se apenas a casos particulares, nós só iremos abordar o modelo de "bridging".</para>

      <para>A configuração padrão dos pacotes Xen não altera a configuração de rede de todo o sistema. No entanto, o daemon  <command>xend</command> é configurado para integrar interfaces de rede virtual com qualquer bridge de rede pré-existente (com <filename>xenbr0</filename> tendo precedência se várias dessas bridges existirem). Nós temos, portanto, de definir uma bridge em <filename>/etc/network/interfaces</filename> (o que requer a instalação do pacote <emphasis role="pkg">bridge-utils</emphasis>, o que explica porque o pacote <emphasis role="pkg">xen-utils-4.4</emphasis> o recomenda) para substituir a entrada eth0 existente:</para>

      <programlisting>auto xenbr0
iface xenbr0 inet dhcp
    bridge_ports eth0
    bridge_maxwait 0
    </programlisting>

      <para>Depois de reinicializar o computador, para termos certeza que a bridge é criada automaticamente, nós podemos agora iniciar o domU com as ferramentas de controle do Xen, em particular o comando <command>xl</command>. Esse comando permite diferentes manipulações nos domínios, incluindo listando-os e, iniciando/parando eles.</para>

      <screen><computeroutput># </computeroutput><userinput>xl list</userinput>
<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   463     1     r-----      9.8
# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>
<computeroutput>Parsing config from /etc/xen/testxen.cfg
# </computeroutput><userinput>xl list</userinput>
<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   366     1     r-----     11.4
testxen                                      1   128     1     -b----      1.1</computeroutput></screen>

      <sidebar>
        <title><emphasis>FERRAMENTA</emphasis> Escolha da toolstacks para gerenciar a VM Xen</title>
        <indexterm><primary><command>xm</command></primary></indexterm>
        <indexterm><primary><command>xe</command></primary></indexterm>
        <para>No Debian 7 e lançamentos anteriores, o <command>xm</command> era a referência de ferramenta de linha de comando a ser usada para gerenciar máquinas virtuais Xen. Ele agora foi substituido pelo <command>xl</command> que é quase completamente retro-compatível. Mas essas não são as únicas ferramentas disponíveis: <command>virsh</command> da libvirt e <command>xe</command> da XAPI do XenServer (oferta comercial do Xen) são ferramentas alternativas.</para>
      </sidebar>

      <sidebar>
        <title><emphasis>ATENÇÃO</emphasis> Somente utilize um domU por imagem!</title>

	<para>Enquanto é claro que é possível ter vários sistemas domU rodando em paralelo, eles todos precisarão usar suas próprias imagens, já que cada domU é feito para acreditar que ele roda em sue próprio hardware (fora a pequena parte do núcleo que conversa com o "hypervisor"). Em particular, não é possível para dois sistemas domU rodando simultaneamente, compartilhar espaço de armazenamento. Se os sistemas domU não estiverem rodando ao mesmo tempo, é, no entanto, bem possível reutilizar uma única partição de troca (swap), ou a partição de hospeda o sistema de arquivos <filename>/home</filename>.</para>
      </sidebar>

      <para>Note que o <filename>testxen</filename> domU usa memória real, retirada da RAM, que estaria de outra forma disponível para o dom0, não a memória simulada. Portanto, cuidados devem ser tomados quando se constrói um servidor objetivando hospedar instâncias Xen, disponibilizando a RAM física de acordo.</para>

      <para>Voilà! Nossa máquina virtual está iniciando. Nós podemos acessá-la de uma de duas maneiras. A maneira usual é se conectar a ela “remotamente” através da rede, como nós nos conectaríamos a uma máquina real; isso geralmente irá requerer a configuração de um servidor DHCP ou alguma configuração de DNS. A outra maneira, que pode ser a única maneira se a configuração de rede estiver incorreta, é usar o console <filename>hvc0</filename>, com o comando <command>xl console</command>:</para>

      <screen><computeroutput># </computeroutput><userinput>xl console testxen</userinput>
<computeroutput>[...]

Debian GNU/Linux 8 testxen hvc0

testxen login: </computeroutput></screen>

      <para>Então pode-se abrir uma sessão, tal como se faria caso se estivesse sentado em frente ao teclado da máquina virtual. Desconectar-se desse console é possível através da combinação de teclas <keycombo action="simul"><keycap>Control</keycap> <keycap>]</keycap></keycombo>.</para>

      <sidebar>
        <title><emphasis>DICA</emphasis> Obtendo o console imediatamente</title>

	<para>Às vezes existe o desejo de iniciar um sistema domU e ir diretamente para o seu console; é por isso que o comando <command>xl create</command> tem uma opção <literal>-c</literal>. Iniciar um domU com esse interruptor irá exibir todas as mensagens de inicialização do sistema.</para>
      </sidebar>

      <sidebar>
        <title><emphasis>FERRAMENTA</emphasis> OpenXenManager</title>

	<para>O OpenXenManager (do pacote <emphasis role="pkg">openxenmanager</emphasis>) é uma interface gráfica que permite o gerenciamento remoto de domínios Xen via a API do Xen. Pode-se assim controlar domínios Xen remotamente. Ele provê a maiorias dos recursos do comando <command>xl</command>.</para>
      </sidebar>

      <para>Uma vez que o domU está ativo, ele pode ser usado como qualquer outro servidor (a final de contas é um sistema GNU/Linux ). Contudo, seu status de máquina virtual permite algumas características extras. Por exemplo, um domU pode, temporariamente, ser pausado e então retomado através dos comandos <command>xl pause</command> e <command>xl unpause</command>. Note que embora um domU pausado não use qualquer recurso do processador, sua memória alocada ainda está em uso. Talvez possa ser interessante considerar os comandos <command>xl save</command> e <command>xl restore</command>: ao salvar o domU libera-se os recursos que eram usados previamente por esse domU, incluindo a RAM. Quando restaurado (ou retirado da pausa, por assim dizer), um domU não nota nada além da passagem do tempo. Se um domU estava rodando quando o dom0 é desligado,os scripts empacotados automaticamente salvam o domU, e o restaurarão na próxima inicialização. Isso irá, é claro, implicar na inconveniência padrão que ocorre quando se hiberna um computador laptop, por exemplo; em particular, se o domU é suspenso por muito tempo, as conexões de rede podem expirar. Note também que o Xen é, até agora, incompatível com uma grande parte do gerenciamento de energia do ACPI, o que impede suspensão do sistema hospedeiro (dom0).</para>

      <sidebar>
        <title><emphasis>DOCUMENTAÇÃO</emphasis> opções <command>xl</command></title>

	<para>A maioria dos subcomandos <command>xl</command> esperam um ou mais argumentos, muitas vezes um nome domU. Esses argumentos estão bem descritos na página de manual <citerefentry><refentrytitle>xl</refentrytitle> <manvolnum>1</manvolnum></citerefentry>.</para>
      </sidebar>

      <para>Interromper ou reinicializar um domU pode ser feito tanto a partir de dentro do domU (com o comando <command>shutdown</command>) quanto a partir do dom0, com o <command>xl shutdown</command> ou <command>xl reboot</command>.</para>

      <sidebar>
        <title><emphasis>APROFUNDAMENTO</emphasis> Xen avançado</title>

	<para>O Xen tem muito mais recursos do que nós podemos descrever nesses poucos parágrafos. Em particular, o sistema é muito dinâmico, e muitos parâmetros para um domínio (como a quantidade de memoria alocada, os discos rígidos visíveis, o comportamento do agendador de tarefas, e assim por diante) podem ser ajustados até mesmo quando esse domínio está rodando. Um domU pode até mesmo ser migrado entre servidores sem ser desligado, e sem perder suas conexões de rede! Para todos esses aspectos avançados, a principal fonte de informação é a documentação oficial do Xen. <ulink type="block" url="http://www.xen.org/support/documentation.html" /></para>
      </sidebar>
    </section>
    <section id="sect.lxc">
      <title>LXC</title>
      <indexterm><primary>LXC</primary></indexterm> 

      <para>Mesmo que seja usado para construir “máquinas virtuais”, o LXC não é, estritamente falando, um sistema de virtualização, mas um sistema que isola grupos de processos uns dos outros mesmo que eles todos rodem na mesma máquina. Ele tira proveito de um conjunto de evoluções recentes do núcleo Linux, coletivamente conhecidas como <emphasis>control groups</emphasis>, de maneira que diferentes conjuntos de processos chamados de  “groups” tem diferentes visões de certos aspectos do sistema global. Os mais notáveis dentre esses aspectos são os identificadores de processo, a configuração de rede, e os pontos de montagem. Tais grupos de processos isolados não terão qualquer acesso a outros processos do sistema, e esses acessos ao sistema de arquivos podem ser restritos a um subconjunto específico. Eles também podem ter sua própria interface de rede e tabela de roteamento, e também podem ser configurados para ver apenas um subconjunto de dispositivos disponíveis presentes no sistema.</para>

      <para>Esses recursos podem ser combinados para isolar toda uma família de processos iniciando a partir do processo <command>init</command>, e o conjunto resultante se parece muito com uma máquina virtual. O nome oficial para tal configuração é um “container” (daí o apelido LXC: <emphasis>LinuX Containers</emphasis>), mas uma diferença bastante importanto com as máquinas virtuais “reais”, tais como as providas pelo Xen ou KVM é que não há um segundo núcleo; o container usa o mesmo núcleo que o sistema hospedeiro. Isso tem tanto prós quanto contras: as vantagens incluem excelente desempenho devido à total falta de sobrecarga, e o fato que o núcleo tem uma visão global de todos processos rodando no sistema, então o agendamento pode ser mais eficiente do que seria se dois núcleos independentes fossem agendar diferentes conjuntos de tarefas. Líder entre os inconvenientes está a impossibilidade de rodar um núcleo diferente em um container (seja uma versão diferente do Linux ou um sistema operacional diferente por completo).</para>

      <sidebar>
        <title><emphasis>NOTA</emphasis> limites de isolamento do LXC</title>

	<para>Contêineres LXC não provêm o mesmo nível de isolamento conseguido por emuladores ou virtualizadores. Em particular:</para>
        <itemizedlist>
          <listitem>
	    <para>já que o núcleo é compartilhado entre o sistema hospedeiro e os contêineres, processos restritos ao contêineres ainda podem acessar mensagens do núcleo, o qual pode levar ao vazamento de informação se as mensagem forem emitidas pelo contêiner;</para>
          </listitem>
          <listitem>
	    <para>por razões parecidas, se o contêiner é comprometido e se uma vulnerabilidade do núcleo é explorada, os outros contêineres podem ser afetados também;</para>
          </listitem>
          <listitem>
	    <para>no sistema de arquivos, o núcleo verifica as permissões de acordo com os identificadores numéricos para usuários e grupos; esses identificadores podem designar diferentes usuários e grupos dependendo do container, o que deve ser mantido em mente se as partes com permissão de escrita do sistema de arquivos são compartilhadas entre containers.</para>
          </listitem>
        </itemizedlist>
      </sidebar>

      <para>Como nós estamos lidando com isolamento e não virtualização simples, a criação de containers LXC é mais complexa do que simplesmente rodar o debian-installer em uma máquina virtual. Nós iremos descrever alguns pré-requisitos e, em seguida, iremos para a configuração de rede; para então depois, sermos capazes de realmente criar o sistema para ser rodado no container.</para>
      <section>
        <title>Etapas Preliminares</title>

	<para>O pacote <emphasis role="pkg">lxc</emphasis> contém as ferramentas necessárias para executar o LXC, e devem portanto serem instaladas.</para>

	<para>O LXC também necessita do sistema de configuração <emphasis>control groups</emphasis>, o qual é um sistema de arquivos virtual que é montado no <filename>/sys/fs/cgroup</filename>. Como o Debian 8 optou pelo systemd, que também faz uso de "control groups", isso agora é feito automaticamente no momento da inicialização, sem configurações adicionais.</para>
      </section>
      <section id="sect.lxc.network">
        <title>Configuração de Rede</title>

	<para>O objetivo de instalar o LXC é configurar máquinas virtuais; enquanto nós poderíamos, é claro, mantê-las isoladas da rede, e apenas nos comunicarmos com elas através do sistema de arquivos, a maioria dos casos de uso envolve dar, ao menos, um mínimo acesso de rede para os containers. No caso típico, cada container irá ter uma interface de rede virtual, conectada à rede real através de uma bridge. Essa interface virtual pode ser plugada tanto diretamente na interface de rede física do hospedeiro (e nesse caso o container está diretamente na rede), ou em outra interface virtual definida no hospedeiro (e o hospedeiro pode então filtrar  ou rotear o tráfego). Em ambos os casos, o pacote <emphasis role="pkg">bridge-utils</emphasis> será necessário.</para>

	<para>O caso simples é a penas uma questão de editar o <filename>/etc/network/interfaces</filename>, e mover a configuração da interface física (por exemplo <literal>eth0</literal>) para a interface bridge (usualmente <literal>br0</literal>), e configurar a ligação entre elas. Por exemplo, se o arquivo de configuração da interface de rede inicialmente contém entradas como as seguintes:</para>

        <programlisting>auto eth0
iface eth0 inet dhcp</programlisting>

	<para>Devem ser desabilitados e substituídos pelo seguinte:</para>

        <programlisting>#auto eth0
#iface eth0 inet dhcp

auto br0
iface br0 inet dhcp
  bridge-ports eth0</programlisting>

	<para>O efeito dessa configuração será similar ao que seria obtido se os containers fossem máquinas plugadas na mesma rede física como o hospedeiro. A configuração “bridge” gerencia o trânsito dos quadros Ethernet entre todas as interfaces "bridged", o que inclui a  <literal>eth0</literal> física, assim como as interfaces definidas para os containers.</para>

	<para>Em casos onde essa configuração não pode ser usada (por exemplo, se nenhum endereço IP público pode ser atribuído aos containers), uma interface virtual <emphasis>tap</emphasis> será criada e conectada à brigde. A topologia de rede equivalente torna-se então de um host com uma segunda placa de rede conectada em um switch separado, com os containers também conectados nesse switch. O host tem então que atuar como um gateway para os containers caso eles sejam feitos para se comunicar com o mundo exterior.</para>

	<para>Em adição ao <emphasis role="pkg">bridge-utils</emphasis>, essa “rica” configuração requer o pacote <emphasis role="pkg">vde2</emphasis>; o arquivo <filename>/etc/network/interfaces</filename> então torna-se:</para>

        <programlisting># Interface eth0 is unchanged
auto eth0
iface eth0 inet dhcp

# Virtual interface 
auto tap0
iface tap0 inet manual
  vde2-switch -t tap0

# Bridge for containers
auto br0
iface br0 inet static
  bridge-ports tap0
  address 10.0.0.1
  netmask 255.255.255.0</programlisting>

	<para>A rede então pode ser configurada tanto estaticamente nos contêineres, quanto dinamicamente com um servidor DHCP rodando no host.
Tal servidor DHCP deverá ser configurado para responder as consultas na interface <literal>br0</literal>.</para>
      </section>
      <section>
        <title>Configurando o Sistema</title>

	<para>Deixe-nos agora configurar o sistema de arquivos a ser usado pelo container. Uma vez que essa “máquina virtual” não irá rodar diretamente no hardware, alguns ajustes são necessários quando comparados a um sistema de arquivos padrão, especialmente quando o núcleo, dispositivos e consoles estão em questão. Felizmente, o <emphasis role="pkg">lxc</emphasis> inclui scripts que praticamente automatizam essa configuração. Por exemlo, os comandos a seguir (que requerem os pacotes <emphasis role="pkg">debootstrap</emphasis> e <emphasis role="pkg">rsync</emphasis>) irão instalar um container Debian:</para>

        <screen><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-create -n testlxc -t debian
</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap
Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... 
Downloading debian minimal ...
I: Retrieving Release 
I: Retrieving Release.gpg 
[...]
Download complete.
Copying rootfs to /var/lib/lxc/testlxc/rootfs...
[...]
Root password is 'sSiKhMzI', please change !
root@mirwiz:~# </computeroutput>
        </screen>

	<para>Note que o sistema de arquivo é inicialmente criado em <filename>/var/cache/lxc</filename>, então é movido para o seu diretório de destino. Isto proporciona a criação de contêineres idênticos mais rapidamente, já que somente um cópia é necessária.</para>

	<para>Note que o modelo  de script de criação do debian aceita uma opção <option>--arch</option> para especificar a arquitetura do sistema a ser instalado e uma opção <option>--release</option> caso você queira instalar alguma coisa a mais que a atual versão estável do Debian. Você pode também definir a variável de ambiente <literal>MIRROR</literal> para apontar para um espelho Debian local.</para>

	<para>O sistema de arquivos recém criado agora contém um sistema Debian mínimo, e por padrão o container não tem interface de rede (além da loopback). Como isso não é realmente o que queremos, nós iremos editar o arquivo de configuração do container (<filename>/var/lib/lxc/testlxc/config</filename>) e adicionar algumas entradas <literal>lxc.network.*</literal>:</para>

        <programlisting>lxc.network.type = veth
lxc.network.flags = up
lxc.network.link = br0
lxc.network.hwaddr = 4a:49:43:49:79:20</programlisting>

	<para>Essas entradas significam, respectivamente, que uma interface virtual será criada no container; que ela irá, automaticamente, ser levantada quando o dito container for iniciado; que ela será, automaticamente, ser conectada a brigde <literal>br0</literal> no hospedeiro; e que seu endereço MAC será o como especificado. Caso essa última entrada estaja faltando ou desabilitada, um endereço MAC aleatório será gerado.</para>

	<para>Outra entrada útil nesse arquivo é a configuração de uma nome para o hospedeiro:</para>

<programlisting>lxc.utsname = testlxc</programlisting>

      </section>
      <section>
        <title>Inicializando o Contêiner</title>

	<para>Agora que nossa imagem da máquina virtual está pronta, vamos inicializar o contêiner:</para>

        <screen role="scale" width="94"><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-start --daemon --name=testlxc
</userinput><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-console -n testlxc
</userinput><computeroutput>Debian GNU/Linux 8 testlxc tty1

testlxc login: </computeroutput><userinput>root</userinput><computeroutput>
Password: 
Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
root@testlxc:~# </computeroutput><userinput>ps auxwf</userinput>
<computeroutput>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init
root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald
root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D
root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux
root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux
root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux
root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     
root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \_ -bash
root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \_ ps auxfw
root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102
root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e
root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux
root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux
root@testlxc:~# </computeroutput></screen>

	<para>Nós agora estamos dentro do container; nosso acesso aos processos é restrito apenas aqueles que foram iniciados a partir do próprio container, e nosso acesso ao sistema de arquivos é similarmente restrito ao subconjunto do sistema de arquivos completo dedicado (<filename>/var/lib/lxc/testlxc/rootfs</filename>). Nós podemos sair do console com <keycombo action="simul"><keycap>Control</keycap> <keycap>a</keycap></keycombo> <keycombo><keycap>q</keycap></keycombo>.</para>

	<para>Note que nós executamos o container como um processo em segundo plano, graças a opção <option>--daemon</option> do <command>lxc-start</command>. Nós podemos interromper o container com um comando como <command>lxc-stop --name=testlxc</command>.</para>

	<para>O pacote <emphasis role="pkg">lxc</emphasis> contém um script de inicialização que pode iniciar automaticamente um ou vários containers quando a máquina inicia (ele faz uso do <command>lxc-autostart</command> que inicia os containers que tem a opção <literal>lxc.start.auto</literal> definida como 1). Controle mais afinado da ordem de início é possível com <literal>lxc.start.order</literal> e <literal>lxc.group</literal>: por padrão, o script de inicialização primeiro inicia os containers que são parte do grupo <literal>onboot</literal> e então os containers que não fazem parte de nenhum grupo. Em ambos os casos, a ordem dentro de um grupo é definida pela opção <literal>lxc.start.order</literal>.</para>

        <sidebar>
          <title><emphasis>APROFUNDAMENTO</emphasis> Virtualização em massa</title>

	  <para>Como o LXC é um sistema de isolamento muito peso leve, ele pode ser particularmente adaptado par uma maciça hospedagem de servidores virtuais. A configuração de rede provavelmente será um pouco mais avançada do que a que nós descrevemos acima, mas a configuração “rica”,  usando as interfaces <literal>tap</literal> e <literal>veth</literal> deverá ser suficiente em muitos casos.</para>

	  <para>Também pode fazer sentido compartilhar parte do sistema de arquivos, como os subdiretórios <filename>/usr</filename> e <filename>/lib</filename>, a fim de evitar a duplicação de software que possa ser comum a vários containers. Isso irá, geralmente, ser alcançado com as entradas <literal>lxc.mount.entry</literal> no arquivo de configuração dos containers. Um interessante efeito colateral é que os processos irão então usar menos memória física, já que o núcleo é capaz de detectar que os programas são compartilhados. O custo marginal de um container extra pode então ser reduzido a espaço de disco dedicado para seus dados específicos, e alguns processos extras que o núcleo tem que agendar e gerenciar.</para>

	  <para>Nós não descrevemos todas as opções disponíveis, é claro; informações mais completas podem ser obtidas a partir das páginas de manual <citerefentry> <refentrytitle>lxc</refentrytitle> <manvolnum>7</manvolnum> </citerefentry> e <citerefentry> <refentrytitle>lxc.container.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> e aquelas que elas referenciam.</para>
        </sidebar>
      </section>
    </section>
    <section>
      <title>Virtualização com KVM</title>
      <indexterm><primary>KVM</primary></indexterm>

      <para>KVM, que significa <emphasis>Kernel-based Virtual Machine</emphasis>, é primeiro, e antes de tudo, um módulo do núcleo que fornece a maior parte da infraestrutura que pode ser usada por um virtualizador, mas não é por si só um virtualizador. O  real control para a virtualização é tratado por um aplicativo com base no  QEMU. Não se preocupe se essa seção menciona os comandos <command>qemu-*</command>: ela continua sendo sobre KVM.</para>

      <para>Ao contrário de outros sistemas de virtualização, o KVM foi incorporado ao núcleo Linux desde o seu início. Seus desenvolvedores escolheram tirar vantagem do conjunto de instruções do processador dedicado a virtualização (Intel-VT e AMD-V), o que mantém o KVM leve, elegante e sem fome por recursos. A contraparte, claro, é que o KVM não funciona em qualquer computador, mas apenas naqueles com os processadores apropriados. Para computadores baseados em x86, você pode verificar que você tem tal processador procurando por “vmx” ou “svm” nas flags da CPU listadas em <filename>/proc/cpuinfo</filename>.</para>

      <para>Com a Red Hat ativamente suportando seu desenvolvimento, o KVM se tornou mais ou menos a referência na virtualização do Linux.</para>
      <section>
        <title>Etapas Preliminares</title>
        <indexterm><primary><command>virt-install</command></primary></indexterm>

	<para>Ao contrário de ferramentas como o VirtualBox, o KVM em si não inclui nenhuma interface de usuário para a criação de gerenciamento de máquinas virtuais. O pacote <emphasis role="pkg">qemu-kvm</emphasis> apenas fornece um executável capaz de iniciar uma máquina virtual, assim como um script de inicialização que carrega os módulos do núcleo apropriados.</para>
        <indexterm><primary>libvirt</primary></indexterm>
        <indexterm><primary><emphasis role="pkg">virt-manager</emphasis></primary></indexterm>

	<para>Felizmente, a Red Hat também fornece outro conjunto de ferramentas para resolver esse problema, tendo desenvolvido a biblioteca <emphasis>libvirt</emphasis> e as ferramentas do <emphasis>gerenciador de máquinas virtuais</emphasis> associadas. A libvirt permite o gerenciamento de máquinas virtuais de maneira uniforme, independentemente do sistema de virtualização envolvido nos bastidores (ela atualmente suporta QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare e UML). O <command>virtual-manager</command> é uma interface gráfica que usa a libvirt para criar e gerenciar máquinas virtuais.</para>
        <indexterm><primary><emphasis role="pkg">virtinst</emphasis></primary></indexterm>

	<para>Nós primeiro instalamos os pacotes necessários, com <command>apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</command>. O <emphasis role="pkg">libvirt-bin</emphasis> fornece o daemon <command>libvirtd</command>, que permite o gerenciamento (potencialmente remoto) de máquinas virtuais rodando no host, e inicia as VMs necessárias quando o host inicializa. Além disso, esse pacote fornece a ferramenta de linha de comando <command>virsh</command>, que permite o controle das máquinas gerenciadas pelo <command>libvirtd</command>.</para>

	<para>O pacote <emphasis role="pkg">virtinst</emphasis> fornece o <command>virt-install</command>, o qual permite a criação de  máquinas virtuais a partir da linha de comando. E finalmente, o <emphasis role="pkg">virt-viewer</emphasis> que permite o acessar o console gráfico das VM's.</para>
      </section>
      <section>
        <title>Configuração de Rede</title>

	<para>Assim como no Xen e no LXC, a configuração de rede mais freqüente envolve uma brigde agrupando as intefaces de rede das máquinas virtuais (see <xref linkend="sect.lxc.network" />).</para>

	<para>Alternativamente, e na configuração padrão fornecida pelo KVM, um endereço privado é atribuído a máquina virtual (no intervalo 192.168.122.0/24), e o NAT é configurado para que a VM possa acessar a rede externa.</para>

	<para>O restante desta seção assume que o host tem uma interface física <literal>eth0</literal> e uma bridge <literal>br0</literal>, e que a primeira está conectada a última.</para>
      </section>
      <section>
        <title>Instalação com <command>virt-install</command></title>
        <indexterm><primary><command>virt-install</command></primary></indexterm>

	<para>A criação de uma máquina virtual é muito similar a instalação de um sistema normal, exceto que as características da máquina virtual são descritas em uma linha de comando aparentemente interminável.</para>

	<para>Em termos práticos, isso significa que nós iremos usar o instalador Debian, inicializando a máquina virtual pelo drive DVD-ROM virtual que é mapeado para uma imagem de DVD do Debian armazenada no sistema do host. A VM irá exportar seu console gráfico pelo protocolo VNC (see <xref linkend="sect.remote-desktops" /> for details), o que irá nos permitir controlar o processo de instalação.</para>

	<para>Nós primeiro precisamos avisar o libvirtd aonde armazenar as imagens de disco, a não ser que a localização padrão (<filename>/var/lib/libvirt/images/</filename>) seja boa.</para>

        <screen><computeroutput>root@mirwiz:~# </computeroutput><userinput>mkdir /srv/kvm</userinput>
<computeroutput>root@mirwiz:~# </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>
<computeroutput>Pool srv-kvm created

root@mirwiz:~# </computeroutput></screen>

        <sidebar>
          <title><emphasis>DICA</emphasis> Adicione seu usuário ao grupo libvirt</title>
          <para>Todos os exemplos nesta seção assumem que você está executando comandos como root. Efetivamente, se você quer controlar um serviço ("daemon") libvirt local, você precisa ou ser root ou ser membro do grupo <literal>libvirt</literal> (o que por padrão não é o caso). Assim, se você quer evitar ficar usando direitos de root com frequência, você pode adicionar a si próprio ao grupo <literal>libvirt</literal> e rodar os vários comandos com a identidade de seu usuário.</para>
        </sidebar>

	<para>Vamos agora iniciar o processo de instalação da máquina virtual, e ter um olhar mais atento nas mais importantes opções do <command>virt-install</command>. Esse comando registra a máquina virtual e seus parâmetros no libvirtd, e então,  a inicia, para que a sua instalação possa prosseguir.</para>

        <screen><computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id="virtinst.connect"></co>
               --virt-type kvm           <co id="virtinst.type"></co>
               --name testkvm            <co id="virtinst.name"></co>
               --ram 1024                <co id="virtinst.ram"></co>
               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <co id="virtinst.disk"></co>
               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <co id="virtinst.cdrom"></co>
               --network bridge=br0      <co id="virtinst.network"></co>
               --vnc                     <co id="virtinst.vnc"></co>
               --os-type linux           <co id="virtinst.os"></co>
               --os-variant debianwheezy
</userinput><computeroutput>
Starting install...
Allocating 'testkvm.qcow'             |  10 GB     00:00
Creating domain...                    |    0 B     00:00
Guest installation complete... restarting guest.
</computeroutput></screen>
        <calloutlist>
          <callout arearefs="virtinst.connect">
	    <para>A opção <literal>--connect</literal> especifica o “hypervisor” a ser usado. Sua forma é a de uma URL contendo o sistema de virtualização (<literal>xen://</literal>, <literal>qemu://</literal>, <literal>lxc://</literal>, <literal>openvz://</literal>, <literal>vbox://</literal>, e assim por diante) e a máquina que deve hospedar a VM (isso pode ser deixado vazio, no caso de  ser um hospedeiro local). Além disso, no caso do QEMU/KVM, cada usuário pode gerenciar máquinas virtuais trabalhando com permissões restritas, e o caminho da URL permite diferenciar máquinas do “sistema” (<literal>/system</literal>) de outras (<literal>/session</literal>).</para>
          </callout>
          <callout arearefs="virtinst.type">
	    <para>Como o KVM é gerenciado da mesma maneira que o QEMU, o <literal>--virt-type kvm</literal> permite especificar o uso do KVM, mesmo que a URL se pareça com a do QEMU.</para>
          </callout>
          <callout arearefs="virtinst.name">
	    <para>A opção <literal>--name</literal> define um nome (específico) para a máquina virtual.</para>
          </callout>
          <callout arearefs="virtinst.ram">
	    <para>A opção <literal>--ram</literal> permite especificar a quantidade de RAM (em MB) a ser alocada para a máquina virtual.</para>
          </callout>
          <callout arearefs="virtinst.disk">
	    <para>A <literal>--disk</literal> especifica a localização do arquivo de imagem que irá representar nosso disco rígido da máquina virtual; esse arquivo é criado, se não  estiver presente, com tamanho(em GB) especificado pelo parâmetro <literal>size</literal>.  O parâmetro <literal>format</literal> permite escolher, entre várias maneiras, o armazenamento do arquivo de imagem. O formato padrão (<literal>raw</literal>) é um arquivo único que corresponde exatamente ao tamanho do disco e seu conteúdo. Nós pegamos um formato mais avançado aqui, que é específico para o QEMU e permite iniciar com um pequeno arquivo que só cresce quando a máquina virtual realmente começa a usar espaço.</para>
          </callout>
          <callout arearefs="virtinst.cdrom">
	    <para>A opção <literal>--cdrom</literal> é usada para indicar aonde encontrar o disco ótico para usar na instalação. O caminho pode ser tanto um caminho local para um arquivo ISO, uma URL aonde o arquivo pode ser obtido, ou um arquivo de dispositivo de um drive físico de CD-ROM (por exemplo <literal>/dev/cdrom</literal>).</para>
          </callout>
          <callout arearefs="virtinst.network">
	    <para>A <literal>--network</literal> especifica como a placa de rede virtual se integra na configuração de rede do hospedeiro. O comportamento padrão (o qual nós explicitamente forçamos em nosso exemplo) é para integrá-la em qualquer bridge de rede  préexistente. Se tal bridge não existe, a máquina virtual irá apenas alcançar a rede física através de NAT, ficando em um intervalo de endereço da sub-rede privada (192.168.122.0/24).</para>
          </callout>
          <callout arearefs="virtinst.vnc">
	    <para><literal>--vnc</literal> determina que o console gráfico de ser disponibilizado usando o VNC. O comprotamento padrão para o servidor VNC associado é para apenas escutar na interface local; se um cliente VNC tiver que ser executado em um host diferente, para estabelecer a conexão será necessário a configuração de um túnel SSH (veja <xref linkend="sect.ssh-port-forwarding" />). Alternativamente, a <literal>--vnclisten=0.0.0.0</literal> pode ser usada para que o servidor VNC seja acessível a partir de todas as interfaces; note que se  você fizer isso, você realmente deve projetar seu firewall de maneira apropriada.</para>
          </callout>
          <callout arearefs="virtinst.os">
	    <para>As opções <literal>--os-type</literal> e <literal>--os-variant</literal> permitem a otimização de alguns parâmetros de máquina virtual, com base em algumas das conhecidas características do sistema operacional ali mencionadas.</para>
          </callout>
        </calloutlist>

	<para>Nesse ponto, a máquina virtual está rodando, e nós precisamos nos conectar ao console gráfico para prosseguir com o processo de instalação. Se a operação prévia foi executada a partir de um ambiente gráfico, essa conexão deve ser iniciada automaticamente. Se não, ou se nós operamos remotamente, o <command>virt-viewer</command> pode ser rodado a partir de qualquer ambiente gráfico para abrir o console gráfico (note que a senha do root do host remoto é pedida duas vezes porque a operação requer 2 conexões SSH):</para>

        <screen><computeroutput>$ </computeroutput><userinput>virt-viewer --connect qemu+ssh://root@<replaceable>server</replaceable>/system testkvm
</userinput><computeroutput>root@server's password: 
root@server's password: </computeroutput></screen>

	<para>Quando o processo de instalação terminar, a máquina virtual é reiniciada, e agora pronta para o uso.</para>
      </section>
      <section>
        <title>Gerenciando Máquina com <command>virsh</command></title>
        <indexterm><primary><command>virsh</command></primary></indexterm>

	<para>Agora que  instalação está feita, vamos ver como lidar com as máquinas virtuais disponíveis. A primeira coisa a tentar é pedir ao <command>libvirtd</command> a lista de máquinas virtuais que ele gerencia:</para>

        <screen><computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all
 Id Name                 State
----------------------------------
  - testkvm              shut off
</userinput></screen>

	<para>Vamos iniciar nossa máquina virtual de teste:</para>

        <screen><computeroutput># </computeroutput><userinput>virsh -c qemu:///system start testkvm
</userinput><computeroutput>Domain testkvm started</computeroutput></screen>

	<para>Nós podemos agora pegar as instruções de conexão para o console gráfico (o visor VNC retornado pode ser dado como parâmetro para o <command>vncviewer</command>):</para>

        <screen><computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm
</userinput><computeroutput>:0</computeroutput></screen>

	<para>Outros subcomandos disponíveis para o <command>virsh</command> incluem:</para>
        <itemizedlist>
          <listitem>
	    <para><literal>reboot</literal> reinicia uma máquina virtual;</para>
          </listitem>
          <listitem>
	    <para><literal>shutdown</literal> para ativar um desligamento limpo;</para>
          </listitem>
          <listitem>
	    <para><literal>destroy</literal>, para parar abruptamente;</para>
          </listitem>
          <listitem>
	    <para><literal>suspend</literal> para pausar a mesma;</para>
          </listitem>
          <listitem>
	    <para><literal>resume</literal> para despausar a mesma;</para>
          </listitem>
          <listitem>
	    <para><literal>autostart</literal> para habilitar (ou desabilitar, com a opção <literal>--disable</literal>) o início da máquina virtual automaticamente quando o hospedeiro é iniciado;</para>
          </listitem>
          <listitem>
	    <para><literal>undefine</literal> para remover todos os registros de uma máquina virtual do <command>libvirtd</command>.</para>
          </listitem>
        </itemizedlist>

	<para>Todos esses subcomandos têm como parâmetro a identificação da máquina virtual.</para>
      </section>
      <section>
        <title>Instalando um sistema baseado em RPM no Debian com o yum</title>

	<para>Se a máquina virtual está destinada a rodar um Debian (ou um dos seus derivados), o sistema pode ser inicializado com o <command>debootstrap</command>, como descrito acima. Mas se a máquina virtual for para ser instalada em um sistema baseado em RPM (como o Fedora, CentOS ou Scientific Linux), a configuração terá de ser feita usando o utilitário <command>yum</command> (disponível pelo pacote de mesmo nome).</para>
	
        <para>O procedimento necessita do uso do <command>rpm</command> para extrair um conjunto inicial de arquivos, incluindo, notávelmente, os arquivos configuração do <command>yum</command>, e então chamar o <command>yum</command> para extrair o conjunto de pacotes remanecentes. Mas como nós chamamos o <command>yum</command> a partir do lado de fora do chroot, nós precisamos fazer algumas mudanças temporárias. No exemplo abaixo, o chroot alvo é <filename>/srv/centos</filename>.</para>

        <screen><computeroutput># </computeroutput><userinput>rootdir="/srv/centos"
</userinput><computeroutput># </computeroutput><userinput>mkdir -p "$rootdir" /etc/rpm
</userinput><computeroutput># </computeroutput><userinput>echo "%_dbpath /var/lib/rpm" &gt; /etc/rpm/macros.dbpath
</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root "$rootdir" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!
rpm: However assuming you know what you are doing...
warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY
# </computeroutput><userinput>sed -i -e "s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g" $rootdir/etc/yum.repos.d/*.repo
</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core
</userinput><computeroutput>[...]
# </computeroutput><userinput>sed -i -e "s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g" $rootdir/etc/yum.repos.d/*.repo
</userinput></screen>
      </section>
    </section>
  </section>
  <section id="sect.automated-installation">
    <title>Instalação Automatizada</title>
    <indexterm><primary>implantação</primary></indexterm>
    <indexterm><primary>instalação</primary><secondary>instalação automatizada</secondary></indexterm>

    <para>Os administradores da Falcot Corp, como muitos administradores de grandes serviços de TI, precisam de ferramentas para instalar (ou reinstalar) rapidamente, e automaticamente se possível, suas novas máquinas.</para>

    <para>Essas exigências podem ser atendidas por uma ampla gama de soluções. Por um lado, ferramentas genéricas como a SystemImager lidam com isso criando uma imagem baseada em uma máquina modelo, e então, implantam essa imagem nos sistemas alvo; no extremo oposto do espectro, o instalador Debian padrão pode ser pré alimentado com um arquivo de configuração contendo as repostas das questões perguntadas durante o processo de instalação. Como um tipo de meio termo, uma ferramenta híbrida como a FAI (<emphasis>Fully Automatic Installer</emphasis>) instala máquinas usando o sistema de empacotamento, mas ela também usa sua própria infraestrutura para tarefas que são mais específicas para implantações em massa (como iniciação, particionamento, configuração e assim por diante).</para>

    <para>Cada uma dessas soluções tem seus prós e contras: o SystemImager trabalha de maneira independente de qualquer sistema de empacotamento em particular, o que permite a ele gerenciar grandes conjuntos de máquinas usando várias distribuições Linux distintas. Ele também inclui um sistema de atualização que não requer uma reinstalação, mas esse sistema de atualização só será confiável se as máquinas não forem modificadas de forma independente; em outras palavras, o usuário não pode atualizar nenhum software por conta própria, ou instalar qualquer outro software. De maneira similar, atualizações de segurança não podem ser automatizadas, porque elas tem que passar pela imagem de referência centralizada mantida pelo SystemImager. Essa solução também requer que as máquinas alvo sejam homogêneas, caso contrário muitas imagens diferentes teriam que ser mantidas e gerenciadas (uma imagem i386 não caberia em uma máquina powerpc, e assim por diante).</para>

    <para>Por outro lado, uma instalação automatizada usando o debian-installer pode ser adaptada para as especificações de cada máquina: o instalador irá buscar o núcleo apropriado e pacotes de software nos repositórios relevantes, detectar o hardware disponível, particionar todo o disco rígido para tirar vantagem de todo o espaço disponível, instalar o sistema Debian correspondente, e configurar um gerenciador de inicialização de maneira apropriada. Contudo, o instalador padrão irá apenas instalar as versões padrão do Debian, com o sistema base e um conjunto de "tarefas" pré selecionadas; isso exclui a instalação de um sistema específico com aplicações não empacotáveis. Preencher essa necessidade em particular requer customizar o instalador… Felizmente, o instalador é muito modular, e existem ferramentas para automatizar a maior parte do trabalho necessário para essa customização, a mais importante o simple-CDD (CDD sendo um acrônimo para <emphasis>Custom Debian Derivative</emphasis>). Mas mesmo a solução simple-CDD, entretanto, apenas lida com instalações iniciais; mas isso geralmente não é um problema, já que as ferramentas APT permitem uma implantação eficiente de atualizações posteriormente.</para>

    <para>Nós iremos apenas dar uma olhada grosseira no FAI, e pular o SystemImager de uma vez (que não está mais no Debian), a fim de focar mais atentamente no debian-installer e simple-CDD, que são mais interessantes num contexto somente Debian.</para>
    <section id="sect.fai">
      <title>Instalador Completamente Automático (FAI)</title>
      <indexterm><primary>Instalador Completamente Automático (FAI)</primary></indexterm>

      <para><foreignphrase>Instalador Completamente Automático (Fully Automatic Installer)</foreignphrase> é provavelmente o mais antigo sistema de implantação automatizada para Debian, o que explica seu status como uma referência; mas sua natureza muito flexível apenas compensa a complexidade que ele envolve.</para>

      <para>O FAI requer um sistema de serivdor para armazenar informação da implantação e permitir que as máquinas alvo inicializem a partir da rede. Esse servidor requer o pacote <emphasis role="pkg">fai-server</emphasis> (ou <emphasis role="pkg">fai-quickstart</emphasis>, que também traz  os elementos necessários para uma configuração padrão).</para>

      <para>O FAI usa uma abordagem específica para definir os vários perfis instaláveis. Em vez de simplesmente duplicar uma instalação de referência, o FAI é um instalador de pleno direito (full-fledged), totalmente configurável através de um conjunto de arquivos e scripts armazenados no servidor; a localização padrão <filename>/srv/fai/config/</filename> não é criado automaticamente, então o administrador precisa criá-lo juntamente com os arquivos relevantes. Na maioria das vezes, esses arquivos serão customizados a partir de arquivos exemplo disponíveis na documentação do pacote <emphasis role="pkg">fai-doc</emphasis>, mais particularmente no diretório <filename>/usr/share/doc/fai-doc/examples/simple/</filename>.</para>

      <para>Uma vez que os perfis estejam definidos, o comando <command>fai-setup</command> gera os elementos necessários para iniciar uma instalação FAI; isso significa principalmente preparar uo atualizar um sistema mínimo (NFS-root) usado durante a instalação. Uma alternativa é gerar um CD de inicialização dedicado com o <command>fai-cd</command>.</para>

      <para>Para criar todos esses arquivos de configuração é necessário algum entendimento da maneira a qual o FAI funciona. Um processo de instalação típico é feito dos passos seguintes:</para>
      <itemizedlist>
        <listitem>
	  <para>pegar um núcleo da rede, e iniciá-lo;</para>
        </listitem>
        <listitem>
	  <para>montar um sistema de arquivo raiz de um NFS;</para>
        </listitem>
        <listitem>
	  <para>executar <command>/usr/sbin/fai</command>, o qual controla o resto do processo (os próximos passos portanto são iniciados por este roteiro);</para>
        </listitem>
        <listitem>
	  <para>copiar o espaço de configuração do servidor para <filename>/fai/</filename>;</para>
        </listitem>
        <listitem>
	  <para>rodando <command>fai-class</command>. Os scripts <filename>/fai/class/[0-9][0-9]*</filename> são executados em turnos, e retornam nomes de “classes” que se aplicam a máquina que está sendo instalada; essa informação irá servir como base para as etapas seguintes. Isso permite alguma flexibilidade na definição de serviços a  serem instalados e configurados.</para>
        </listitem>
        <listitem>
	  <para>buscando várias variáveis de configuração, dependendo das classes relevantes;</para>
        </listitem>
        <listitem>
	  <para>particionar os discos e formatar as partições com base nas informações fornecidas em <filename>/fai/disk_config/<replaceable>class</replaceable></filename>;</para>
        </listitem>
        <listitem>
	  <para>montar essas partições;</para>
        </listitem>
        <listitem>
	  <para>instalar o sistema base;</para>
        </listitem>
        <listitem>
	  <para>preparar o banco de dados Debconf com <command>fai-debconf</command>;</para>
        </listitem>
        <listitem>
	  <para>buscar a lista de pacotes disponíveis para o APT;</para>
        </listitem>
        <listitem>
	  <para>instalar os pacotes listados em <filename>/fai/package_config/<replaceable>class</replaceable></filename>;</para>
        </listitem>
        <listitem>
	  <para>executar os scripts de pós configuração, <filename>/fai/scripts/<replaceable>class</replaceable>/[0-9][0-9]*</filename>;</para>
        </listitem>
        <listitem>
	  <para>gravar os registros de instalação, desmontar as partições e reinicializar o computador.</para>
        </listitem>
      </itemizedlist>
    </section>
    <section id="sect.d-i-preseeding">
      <title>Preseeding Debian-Installer</title>
      <indexterm><primary>preseed</primary></indexterm>
      <indexterm><primary>pré-configuração</primary></indexterm>

      <para>No final das contas, a melhor ferramenta para instalar sistemas Debian deve ser, logicamente, o instalador Debian oficial. Isso é porque, desde sua concepção, o debian-installer tem sido projetado para o uso automatizado, tirando vantagem da infraestrutura fornecida pelo <emphasis role="pkg">debconf</emphasis>. Esse último permite, por um lado, reduzir o número de perguntas feitas (perguntas ocultas irão usar as respostas padrão fornecidas), e por outro lado, para fornecer respostas padrão separadamente, para que a instalação possa ser não-interativa. Essa última característica é conhecida como <emphasis>preseeding</emphasis>.</para>

      <sidebar>
        <title><emphasis>INDO ALÉM</emphasis> Debconf com um banco de dados centralizado</title>
        <indexterm><primary><command>debconf</command></primary></indexterm>

	<para>O Preseeding permite fornecer um conjunto de respostas às perguntas do Debconf no momento da instalação, mas essas respostas são estáticas e não evoluem com o passar do tempo. Como máquinas já instaladas talvez precisem de atualização, e novas respostas talvez venham a ser necessárias, o arquivo de configuração <filename>/etc/debconf.conf</filename> pode ser configurado para que o Debconf use fontes de dados externas (como um servidor de diretório LDAP, ou um arquivo remoto acessado via NFS ou Samba). Várias fontes de dados externas podem ser definidas ao mesmo tempo, e elas se complementam. O banco de dados local ainda é usado (para acesso leitura-escrita), mas os bancos de dados remotos são, geralmente, restritos a leitura. A página de manual <citerefentry><refentrytitle>debconf.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> descreve todas as possibilidades em detalhes. (você precisa do pacote <emphasis role="pkg">debconf-doc</emphasis>).</para>
      </sidebar>
      <section>
        <title>Usando um Arquivo Preseed</title>

	<para>Existem vários lugares aonde o instalador pode obter um arquivo preseeding:</para>
        <itemizedlist>
          <listitem>
	    <para>Dentro do initrd ,usado para iniciar a máquina; neste caso, o preseeding acontece bem no início da instalação, e todas as perguntas podem ser evitadas. O arquivo apenas precisa ser chamado <filename>preseed.cfg</filename> e armazenado dentro da raiz do initrd.</para>
          </listitem>
          <listitem>
	    <para>na mídia de inicialização (CD ou dispositivo USB); o preseeding então acontece assim que a mídia é montada, o que significa ser logo após as perguntas sobre idioma e layout do teclado. O parâmetro de inicialização <literal>preseed/file</literal> pode ser usado para indicar a localização do arquivo preseeding (por exemplo, <filename>/cdrom/preseed.cfg</filename> quando a instalação é feita por um CD-ROM, ou <filename>/hd-media/preseed.cfg</filename> no caso de um dispositivo USB).</para>
          </listitem>
          <listitem>
	    <para>a partir da rede; o preseeding então apenas acontece após a rede ser configurada (automaticamente); o parâmetro de inicialização relevante é então <literal>preseed/url=http://<replaceable>server</replaceable>/preseed.cfg</literal>.</para>
          </listitem>
        </itemizedlist>

	<para>De relance, incluir o arquivo preseeding dentro do initrd parece ser a solução mais interessante; no entanto, ela raramente é usada na prática, porque gerar um initrd instalador é bem complexo. As outras duas soluções são muito mais comuns, especialmente quando os parâmetros de inicialização fornecem outra maneira de fazer "preseed" das respostas para as primeiras perguntas do processo de instalação. A maneira usual de evitar o incômodo de digitar esses parâmetro de inicialização a manualmente a cada instalação é salvá-los na configuração do <command>isolinux</command> (no caso do CD-ROM) ou <command>syslinux</command> (dispositivo USB).</para>
      </section>
      <section>
        <title>Criando um Arquivo Preseed</title>

	<para>Um arquivo preseed é um arquivo de texto puro, aonde cada linha contém a resposta para uma pergunta do Debconf. A linha é dividida em quatro campos separados por espaço em branco (espaços ou tabs), com em, por exemplo, <literal>d-i mirror/suite string stable</literal>:</para>
        <itemizedlist>
          <listitem>
	    <para>o primeiro campo é o “dono” da pergunta; “d-i” é usado para perguntas relevantes para o instalador, mas ele também pode ser um nome de pacote para perguntas vindas a partir de pacotes Debian;</para>
          </listitem>
          <listitem>
	    <para>o segundo campo é um identificador para a pergunta;</para>
          </listitem>
          <listitem>
	    <para>terceiro, o tipo de pergunta;</para>
          </listitem>
          <listitem>
	    <para>o quarto e último campo contém o valor para a resposta. Note que ele tem que ser separador do terceiro campo com um único espaço; se existir mais de um, os caracteres espaço seguintes serão considerados parte do valor.</para>
          </listitem>
        </itemizedlist>

	<para>A maneira mais simples de escrever um arquivo preseed é instalar o sistema manualmente. Então o <command>debconf-get-selections --installer</command> irá prover as respostas com relação ao instalador. Respostas sobre outros pacotes podem ser obtidas com <command>debconf-get-selections</command>. No entanto, uma solução mais limpa é escrever o arquivo preseed manualmente, iniciando a partir de um exemplo e da documentação de referência: com tal abordagem, apenas perguntas aonde a resposta padrão precisa ser sobrescrita podem se submeter ao preseeded; usando o parâmetro de inicialização <literal>priority=critical</literal> irá instruir o Debconf a apenas perguntar questões criticas, e usar a resposta padrão para as outras.</para>

        <sidebar>
          <title><emphasis>DOCUMENTAÇÃO</emphasis> Apêndice do guia de instalação</title>

	  <para>O guia de instalação, disponível online, inclui documentação detalhada sobre o uso de um arquivo preseed em um apêndice. Ele também inclui um arquivo de amostra detalhado e comentado, que pode servir como base para customizações locais. <ulink type="block" url="https://www.debian.org/releases/jessie/amd64/apb.html" /> <ulink type="block" url="https://www.debian.org/releases/jessie/example-preseed.txt" /></para>
        </sidebar>
      </section>
      <section>
        <title>Criando uma Mídia de Inicialização Customizada</title>

	<para>Saber aonde armazenar um arquivo preseed é muito bom, mas a localização não é tudo: é preciso, de uma forma ou de outra, alterar a mídia de inicialização de instalação para mudar os parâmetros de inicialização e adicionar o arquivo preseed.</para>
        <section>
          <title>Inicializando a Partir da Rede</title>

	  <para>Quando um computador é inicializado a partir da rede, o servidor que envia os elementos de inicialização também define os parâmetros de inicialização. Assim, a alteração precisa ser feita na configuração PXE do servidor de inicialização; mais especificamente, no seu arquivo de configuração <filename>/tftpboot/pxelinux.cfg/default</filename>. Configurar a inicialização pela rede é um pré-requisito; veja o Guia de Instalação para detalhes. <ulink type="block" url="https://www.debian.org/releases/jessie/amd64/ch04s05.html" /></para>
        </section>
        <section>
          <title>Preparando um Dispositivo USB Inicializável</title>

	  <para>Uma vez que um dispositivo inicializável tenha sido preparado (veja <xref linkend="sect.install-usb" />), algumas operações extras são necessárias. Assumindo que o conteúdo do dispositivo está disponível em <filename>/media/usbdisk/</filename>:</para>
          <itemizedlist>
            <listitem>
	      <para>copiar o arquivo preseed para <filename>/media/usbdisk/preseed.cfg</filename></para>
            </listitem>
            <listitem>
	      <para>editar <filename>/media/usbdisk/syslinux.cfg</filename> e adicionar os parâmetros de inicialização necessários (veja o exemplo abaixo).</para>
            </listitem>
          </itemizedlist>

          <example>
            <title>arquivo syslinux.cfg e parâmetros preseeding</title>

            <programlisting>default vmlinuz
append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --</programlisting>
          </example>
        </section>
        <section>
          <title>Criando uma Imagem de CD-ROM</title>
          <indexterm><primary>debian-cd</primary></indexterm>

	  <para>Um dispositivo USB é uma mídia de leitura-escrita, então foi fácil para nós adicionar um arquivo lá e alterar alguns parâmetros. No caso do CD-ROM, a operação é mais complexa, já que nós precisamos refazer uma imagem ISO completa. Essa tarefa é feita pelo <emphasis role="pkg">debian-cd</emphasis>, mas essa ferramenta é um pouco mais complicada de usar: ela precisa de um espelho local, e requer o entendimento de todas as opções fornecidas pelo <filename>/usr/share/debian-cd/CONF.sh</filename>; mesmo assim, o <command>make</command> tem que ser invocado várias vezes. <filename>/usr/share/debian-cd/README</filename> é, portanto, uma leitura muito recomendada.</para>

	  <para>Dito isso, o debian-cd sempre opera de maneira similar: um diretório “imagem” com o conteúdo exato do CD-ROM é gerado, e então convertido em um arquivo ISO com uma ferramental tal como a <command>genisoimage</command>, <command>mkisofs</command> ou <command>xorriso</command>. O diretório da imagem é finalizado após o passo <command>make image-trees</command> do debian-cd. Nesse ponto, nós inserimos o arquivo preseed dentro do diretório apropriado (usualmente <filename>$TDIR/$CODENAME/CD1/</filename>, $TDIR e $CODENAME sendo os parâmetros definidos pelo arquivo de configuração <filename>CONF.sh</filename>). O CD-ROM usa <command>isolinux</command> como seu carregador de inicialização, e seu arquivo de configuração tem que ser adaptado a partir do que o debian-cd gerou, a fim de inserir os parâmetros de inicialização necessários (o arquivo específico é <filename>$TDIR/$CODENAME/boot1/isolinux/isolinux.cfg</filename>). Então o processo “normal” pode ser retomado e nós podemos continuar a gerar a imagem ISO com  <command>make image CD=1</command> (ou <command>make images</command> se for para gerar vários CD-ROMs).</para>
        </section>
      </section>
    </section>
    <section id="sect.simple-cdd">
      <title>Simple-CDD: A Solução Tudo-Em-Um</title>
      <indexterm><primary>simple-cdd</primary></indexterm>

      <para>Simplesmente usar um arquivo preseed não é o suficiente para preencher todos os requerimentos que possam aparecer em grandes implantações. Mesmo que seja possível executar alguns scripts no final de processos normais de instalação, a seleção de um conjunto de pacotes a instalar ainda não é muito flexível (basicamente, apenas “tarefas” podem ser selecionadas); mais importante, isso apenas permite a instalação de pacotes Debian oficiais e impede os gerados localmente.</para>

      <para>Por outro lado, o debian-cd é capaz de integrar pacotes externos, e o debian-installer pode ser estendido através da inserção de novas etapas no processo de instalação. Pela combinação dessas capacidades, devesse ser possível criar um instalador customizado que preencha nossas necessidades; deve até ser possível configurar alguns serviços após o desempacotamento dos pacotes requeridos. Felizmente, isso não é mera hipótese, já que é exatamente isso que o Simple-CDD (do pacote <emphasis role="pkg">simple-cdd</emphasis>) faz.</para>

      <para>O propósito do Simple-CDD é permitir que qualquer um crie, com facilidade, uma distribuição derivada do Debian, pela seleção de um subconjunto dos pacotes disponíveis, pré configuração deles com o Debconf, adição de software específico, e execução de scripts customizados no final do processo de instalação. Isso confirma a filosofia “sistema operacional universal”, já que qualquer um pode adaptar o Debian à sua própria necessidade.</para>
      <section>
        <title>Criando Perfis</title>

	<para>O Simple-CDD define “perfis” que coincidem com o conceito “classes” FAI e uma máquina pode ter vários perfis (determinados no momento da instalação). Um perfil é definido por um conjunto de arquivos <filename>profiles/<replaceable>profile</replaceable>.*</filename>:</para>
        <itemizedlist>
          <listitem>
	    <para>o arquivo <filename>.description</filename> contém uma descrição de uma linha para o perfil;</para>
          </listitem>
          <listitem>
	    <para>o arquivo <filename>.packages</filename> lista os pacotes que irão ser instalados automaticamente caso o perfil seja selecionado;</para>
          </listitem>
          <listitem>
	    <para>o arquivo <filename>.downloads</filename> lista os pacotes que serão armazenados na mídia de instalação, mas não necessariamente instalados;</para>
          </listitem>
          <listitem>
	    <para>o arquivo <filename>.preseed</filename> contém as informações preseeding para as perguntas do Debconf (para o instalador e/ou para os pacotes);</para>
          </listitem>
          <listitem>
	    <para>o arquivo <filename>.postinst</filename> contém um script que será executado no final do processo de instalação;</para>
          </listitem>
          <listitem>
	    <para>por fim, o arquivo <filename>.conf</filename> permite alterar alguns parâmetros do Simple-CDD com base nos perfis a serem incluídos em uma imagem.</para>
          </listitem>
        </itemizedlist>

	<para>O perfil <literal>padrão</literal> tem uma função em particular, já que ele está sempre selecionado; el contém o mínimo necessário para o Simple-CDD funcionar. A única coisa que geralmente é customizada nesse perfil é o parâmetro preseed <literal>simple-cdd/profiles</literal>: isso permite evitar a pergunta, introduzida pelo Simple-CDD, sobre quais perfis instalar.</para>

	<para>Note também que os comandos precisam ser invocados a partir do diretório pai do diretório <filename>profiles</filename>.</para>
      </section>
      <section>
        <title>Configurando e Usando o <command>build-simple-cdd</command></title>
        <indexterm><primary><command>build-simple-cdd</command></primary></indexterm>

        <sidebar>
          <title><emphasis>OLHADA RÁPIDA</emphasis> Arquivo de configuração detalhado</title>

	  <para>Um exemplo do arquivo de configuração do Simple-CDD, com todos os parâmetros possíveis, está incluído no pacote (<filename>/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz</filename>). Ele pode sr usado como ponto de partida ao criar um arquivo de configuração customizado.</para>
        </sidebar>

	<para>O Simple-CDD requer muitos parâmetros para operar plenamente. Eles irão, na maioria das vezes, estar reunidos em um arquivo de configuração,  o qual pode ser informado ao <command>build-simple-cdd</command> com a opção <literal>--conf</literal>, mas eles também podem ser especificados via parâmetros dedicados dados ao <command>build-simple-cdd</command>. Aqui está uma visão geral de como esse comando se comporta, e como seus parâmetros são usados:</para>
        <itemizedlist>
          <listitem>
	    <para>o parâmetro <literal>profiles</literal> lista os perfis que serão incluídos na imagem CD-ROM gerada;</para>
          </listitem>
          <listitem>
	    <para>com base na lista de pacotes requeridos, o Simple-CDD baixa os arquivos apropriados do servidor mencionado em <literal>server</literal>, e os reúne em um espelho parcial (que mais tarde será dado ao debian-cd);</para>
          </listitem>
          <listitem>
	    <para>os pacotes customizados mencionados em <literal>local_packages</literal> também são integrados neste espelho local;</para>
          </listitem>
          <listitem>
	    <para>o debian-cd é então executado (dentro de uma local padrão que pode ser configurada com a variável <literal>debian_cd_dir</literal>), com a lista de pacotes para integrar;</para>
          </listitem>
          <listitem>
	    <para>uma vez que o debian-cd tenha preparado seu diretório, o Simple-CDD aplica algumas mudanças nesse diretório:</para>
            <itemizedlist>
              <listitem>
		<para>arquivos contendo os perfis são adicionados em um subdiretório <filename>simple-cdd</filename> (que irá terminar no CD-ROM);</para>
              </listitem>
              <listitem>
		<para>outros arquivos listados no parâmetro <literal>all_extras</literal> também são adicionados;</para>
              </listitem>
              <listitem>
		<para>os parâmetros de inicialização são ajustados a fim de habilitar o preseeding. Perguntas com relação a idioma e país podem ser evitadas se a informação requerida está armazenada nas variáveis <literal>language</literal> e <literal>country</literal>.</para>
              </listitem>
            </itemizedlist>
          </listitem>
          <listitem>
	    <para>o debian-cd então gera a imagem ISO final.</para>
          </listitem>
        </itemizedlist>
      </section>
      <section>
        <title>Gerando uma imagem ISO</title>

	<para>Uma vez que nós tenhamos escrito um arquivo de configuração e definido nossos perfis, a etapa restante é invocar <command>build-simple-cdd --conf simple-cdd.conf</command>. Após alguns minutos, nós teremos a imagem requerida em <filename>images/debian-8.0-amd64-CD-1.iso</filename>.</para>
      </section>
    </section>
  </section>
  <section id="sect.monitoring">
    <title>Monitoramento</title>

    <para>O monitoramento é um termo genérico e as várias atividades envolvidas tem vários objetivos: por um lado, seguir o uso dos recursos fornecidos pela máquina permite antecipar a saturação e os subsequentes necessidades de upgrades; por outro lado, alertar o administrador assim que um serviço fica indisponível ou não está funcionando de maneira apropriada significa que os problemas que estão acontecendo podem ser consertados mais rapidamente.</para>

    <para>O <emphasis>Munin</emphasis> cobre a primeira área, exibindo gráficos de valores históricos de inúmeros parâmetros (RAM usada, espaço de disco ocupado, carga do processador, tráfego de rede,carga do Apache/MySQL, e assim por diante). O <emphasis>Nagios</emphasis> cobre a segunda área,regularmente checando que os serviços estão funcionando  e disponíveis, e enviando alertas através dos canais apropriados (e-mails, mensagens de texto e assim por diante). Os dois tem um design modular, o que torna fácil criar novas extensões para monitorar parâmetros específicos ou serviços.</para>

    <sidebar>
      <title><emphasis>ALTERNATIVA</emphasis> Zabbix, uma ferramenta de monitoramento integrada</title>
      <indexterm><primary>Zabbix</primary></indexterm>

      <para>Embora o Munin e o Nagios sejam os mais populares, eles não são as únicas opções no campo de monitoramento, e cada um deles apenas lida com metade da tarefa (um com gráficos, outro com alertas). O Zabbix, por outro lado, integra ambas as partes de monitoramento; ele também tem uma interface web para configurar a maioria dos aspectos comuns. Ele cresceu aos trancos e barrancos durante os últimos anos, e pode agora ser considerado um concorrente viável. No servidor de monitoramento, você instalaria o <emphasis role="pkg">zabbix-server-pgsql</emphasis> (ou <emphasis role="pkg">zabbix-server-mysql</emphasis>), possivelmente junto com o <emphasis role="pkg">zabbix-frontend-php</emphasis> para ter uma interface web. Nas máquinas a serem monitoradas você instalaria o <emphasis role="pkg">zabbix-agent</emphasis>, para alimentar o servidor com os dados. <ulink type="block" url="http://www.zabbix.com/" /></para>
    </sidebar>

    <sidebar>
      <title><emphasis>ALTERNATIVA</emphasis> Icinga, uma ramificação do Nagios</title>
      <indexterm><primary>Icinga</primary></indexterm>

      <para>Estimulados por divergências nas opiniões que se referem ao modelo de desenvolvimento do Nagios (que é controlado por uma companhia), alguns desenvolvedores fizeram um "fork" do Nagios e usaram Icinga como seu novo nome. O Icinga ainda é compatível — até agora — com as configurações e extensões do Nagios, porém ele também adiciona funcionalidades extras. <ulink type="block" url="http://www.icinga.org/" /></para>
    </sidebar>
    <section id="sect.munin">
      <title>Configurando o Munin</title>
      <indexterm><primary>Munin</primary></indexterm>

      <para>O propósito do Munin é monitorar muitas máquinas; logo, é bem natural que ele use uma arquitetura cliente/servidor. A máquina ("host") central — que faz o gráfico ("grapher") — coleta dados de todas as máquinas ("hosts") monitoradas, e gera gráficos com os históricos.</para>
      <section>
        <title>Configurando As Máquinas A Serem Monitoradas</title>

	<para>O primeiro passo é instalar o pacote <emphasis role="pkg">munin-node</emphasis>. O serviço ("daemon") instalado por esse pacote escuta na porta 4949 e envia de volta os dados coletados por todas as extensões ativas. Cada extensão é um programa simples que retorna um descrição dos dados coletados, assim como o último valor medido. As extensões são armazenadas em <filename>/usr/share/munin/plugins/</filename>, mas apenas aquelas com uma ligação simbólica em <filename>/etc/munin/plugins/</filename> são realmente usadas.</para>

	<para>Quando o pacote é instalado, um conjunto de extensões ativas é determinado com base nos softwares disponíveis e na configuração atual da máquina. Contudo, essa auto configuração depende da funcionalidade que cada extensão deve fornecer, e geralmente é uma boa ideia rever e ajustar os resultados manualmente. Navegar pela <ulink url="http://gallery.munin-monitoring.org">Plugin Gallery (galeria de extensões)</ulink> pode ser interessante mesmo que nem todas as extensões tenham uma documentação compreensível. Entretanto, todas as extensões são scripts e a maioria é bem simples e bem comentado. Navegar por <filename>/etc/munin/plugins/</filename> é portanto uma boa maneira de se ter uma ideia sobre para que cada extensão serve e determinar quais devem ser removidas. Similarmente, habilitar uma extensão interessante encontrada em <filename>/usr/share/munin/plugins/</filename> é uma simples questão de configurar uma ligação simbólica com <command>ln -sf /usr/share/munin/plugins/<replaceable>extensão</replaceable> /etc/munin/plugins/</command>. Note que quando o nome de uma extensão termina com um sublinhado “_”, a extensão precisa de um parâmetro. Esse parâmetro tem que ser armazenado no nome da ligação simbólica; por exemplo, a extensão “if_”  tem que ser habilitada como uma ligação simbólica <filename>if_eth0</filename> para monitorar o tráfego de rede na interface eth0.</para>

	<para>Uma vez que todas as extensões estejam configuradas corretamente, a configuração do serviço ("daemon") tem que ser atualizada para descrever o controle de acesso aos dados coletados. Isso envolve a diretiva <literal>allow</literal> no arquivo <filename>/etc/munin/munin-node.conf</filename>. A configuração padão é <literal>allow ^127\.0\.0\.1$</literal>, e apenas permite acesso a máquina local. Um administrador geralmente irá adicionar uma linha similar contendo o endereço IP da máquina que gera o gráfico ("grapher host"), e então reiniciar o serviço com <command>service munin-node restart</command>.</para>

        <sidebar>
          <title><emphasis>APROFUNDANDO</emphasis> Criando extensões locais</title>

	  <para>O Munin inclui documentação detalhada sobre como as extensões devem se comportar e como desenvolver novas extensões. <ulink type="block" url="http://munin-monitoring.org/wiki/plugins" /></para>

	  <para>Uma extensão é melhor testada quando executada nas mesmas condições encontradas quando iniciada pelo munin-node; isso pode ser simulado rodando <command>munin-run <replaceable>extensão</replaceable></command> como root. Um segundo parâmetro potecial dado a esse comando (tal como <literal>config</literal>) é passado para a extensão como um parâmetro.</para>

	  <para>Quando uma extensão é invocada com o parâmetro <literal>config</literal>, ela tem que descrever a si própria pelo retorno de um conjunto de campos:</para>

          <screen><computeroutput>$ </computeroutput><userinput>sudo munin-run load config
</userinput><computeroutput>graph_title Load average
graph_args --base 1000 -l 0
graph_vlabel load
graph_scale no
graph_category system
load.label load
graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run "immediately").
load.info 5 minute load average
</computeroutput></screen>

	  <para>Os vários campos disponíveis são descritos pela  Referência das Extensões (“Plugin reference”) disponível como parte do Guia do Munin (“Munin guide”). <ulink type="block" url="http://munin.readthedocs.org/en/latest/reference/plugin.html" /></para>

	  <para>Quando invocada sem um parâmetro, a extensão apenas retorna os últimos valores medidos; por exemplo, executando <command>sudo munin-run load</command> poderia retornar <literal>load.value 0.12</literal>.</para>

	  <para>Finalmente, quando uma extensão é invocada com parâmetro <literal>autoconf</literal>, ela deveria retornar “yes” (e um status de término 0) ou “no” (com um status de término 1) de acordo sobre se a extensão deve ser habilitada nesta máquina ("host").</para>
        </sidebar>
      </section>
      <section>
        <title>Configurando a Máquina que faz o Gráfico ("Grapher")</title>

	<para>O “grapher” é simplesmente o computador que agrega os dados e gera os gráficos correspondentes. O software necessário está no pacote <emphasis role="pkg">munin</emphasis>. A configuração padrão roda o <command>munin-cron</command> (uma vez a cada 5 minutos), que reune dados de todas as máquinas ("hosts") listados em <filename>/etc/munin/munin.conf</filename> (apenas a máquina ("host") local é listada por padrão), salva os dados históricos em arquivos RRD (<emphasis>Round Robin Database</emphasis>, um arquivo com formato desenvolvido par armazenar dados que variam com o tempo) armazenados em <filename>/var/lib/munin/</filename> e gera uma página HTML com os gráficos em <filename>/var/cache/munin/www/</filename>.</para>

	<para>Portanto todas as máquinas monitoradas tem que estar listadas no arquivo de configuração <filename>/etc/munin/munin.conf</filename>. Cada máquina é listada como uma seção completa, com um nome correspondendo com a máquina e pelo menos uma entrada com <literal>endereço</literal> dando o endereço IP correspondente.</para>

        <programlisting>[ftp.falcot.com]
    address 192.168.0.12
    use_node_name yes</programlisting>

	<para>Seções podem ser mais complexas, e descrever gráficos extras que poderiam ser criados pela combinação de dados vindos de várias máquinas. Os exemplos fornecidos no arquivo de configuração são um bom ponto de partida para customizações.</para>

	<para>O último passo é publicar as páginas geradas; isso envolve a configuração de um servidor web para que o conteúdo de <filename>/var/cache/munin/www/</filename> seja disponibilizado em um site web. O acesso a esse site web geralmente será restrito, pelo uso de um mecanismo de autenticação ou controle de acesso baseado em IP. Veja <xref linkend="sect.http-web-server" /> para os detalhes relevantes.</para>
      </section>
    </section>
    <section id="sect.nagios">
      <title>Configurando o Nagios</title>
      <indexterm><primary>Nagios</primary></indexterm>

      <para>Diferentemente do Munin, o Nagios não necessariamente requer a instalação de alguma coisa nas máquinas (""hosts") monitoradas; na maioria das vezes, o Nagios é usado para conferir a disponibilidade de serviços de rede. Por exemplo, o Nagios pode se conectar em um servidor web e conferir que determinada página web pode ser obtida dentro de um determinado tempo.</para>
      <section>
        <title>Instalando</title>

	<para>O primeiro passo na configuração do Nagios é a instalação dos pacotes <emphasis role="pkg">nagios3</emphasis>, <emphasis role="pkg">nagios-plugins</emphasis> e <emphasis role="pkg">nagios3-doc</emphasis>. Instalando esses pacotes é configurada a interface web e criado o primeiro usuário, <literal>nagiosadmin</literal> (para o qual é perguntado uma senha). Adicionar outros usuários é uma simples questão de inseri-los no arquivo <filename>/etc/nagios3/htpasswd.users</filename> com o comando do Apache <command>htpasswd</command>. Se nenhuma pergunta do Debconf foi exibida durante a instalação, <command>dpkg-reconfigure nagios3-cgi</command> pode ser usado para definir a senha do <literal>nagiosadmin</literal>.</para>

	<para>Apontar o navegador para <literal>http://<replaceable>servidor</replaceable>/nagios3/</literal> exibe a interface web; em  particular, note que o Nagios já monitora alguns parâmetros da máquina aonde ele roda. Contudo, algumas funcionalidades interativas, tais como adicionar comentários a uma máquina ("host") não funciona. Esses recursos estão desabilitados pela configuração padrão do Nagios, que é muito restritiva por razões de segurança.</para>

	<para>Como ducomentado em <filename>/usr/share/doc/nagios3/README.Debian</filename>, habilitar alguns recursos envolve a editar o <filename>/etc/nagios3/nagios.cfg</filename> e configurar o parâmetro <literal>check_external_commands</literal> para “1”. Nós tambéms precisamos configurar permissões de escrita para o diretório usado pelo Nagios, através de comandos como os seguintes:</para>

        <screen><computeroutput># </computeroutput><userinput>service nagios3 stop</userinput>
<computeroutput>[...]
# </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios3/rw
</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios3
</userinput><computeroutput># </computeroutput><userinput>service nagios3 start</userinput>
<computeroutput>[...]</computeroutput></screen>
      </section>
      <section>
        <title>Configurando</title>

	<para>A interface web do Nagios é bem legal, mas elas não permite configurações, nem pode ser usada para adicionar máquinas ("hosts") monitorados e serviços. Toda a configuração é gerenciada através de arquivos referenciados pelo arquivo de configuração central, <filename>/etc/nagios3/nagios.cfg</filename>.</para>

	<para>Não se deve mergulhar nesses arquivos sem algum entendimento dos conceitos do Nagios. A configuração lista objetos dos seguintes tipos:</para>
        <itemizedlist>
          <listitem>
	    <para>um <emphasis>host</emphasis> é a máquina a ser monitorada;</para>
          </listitem>
          <listitem>
	    <para>um <emphasis>hostgroup</emphasis> é um conjunto de máquinas que devem ser agrupadas para exibição, ou para fatorar alguns elementos comuns de configuração;</para>
          </listitem>
          <listitem>
	    <para>Um <emphasis>service</emphasis> é um elemento testável relacionado a uma máquina ou um grupo de máquinas. Ele irá, muito frequentemente, ser uma checagem para um serviço de rede, mas ele também envolve a checagem de que alguns parâmetros estão dentro de um intervalo aceitável (por exemplo, espaço livre em disco ou carga do processador);</para>
          </listitem>
          <listitem>
	    <para>um <emphasis>servicegroup</emphasis> é um conjunto de serviços que devem ser agrupados para exibição;</para>
          </listitem>
          <listitem>
	    <para>um <emphasis>contact</emphasis> é uma pessoa que pode receber alertas;</para>
          </listitem>
          <listitem>
	    <para>um <emphasis>contactgroup</emphasis> é um grupo de tais pessoas;</para>
          </listitem>
          <listitem>
	    <para>um <emphasis>timeperiod</emphasis> é um intervalo de tempo durante o qual alguns serviços tem que ser checados;</para>
          </listitem>
          <listitem>
	    <para>um <emphasis>command</emphasis> é a linha de comando invocada para checar um dado serviço.</para>
          </listitem>
        </itemizedlist>

	<para>De acordo com seu tipo, cada objeto tem um número de propriedades que podem ser customizadas. Um lista completa seria muito longa para ser incluída, mas as propriedades mais importantes são as relações entre os objetos.</para>

	<para>Um <emphasis>service</emphasis> (serviço) usa um <emphasis>command</emphasis> (commando) para checar o estado de uma funcionalidade em um <emphasis>host</emphasis> (máquina) (ou um <emphasis>hostgroup</emphasis>) dentro de um <emphasis>timeperiod</emphasis> (intervalo de tempo). Em caso de um problema, o Nagios envia um alerta para todos os membros de <emphasis>contactgroup</emphasis> (grupo de contatos) ligados ao serviço. É enviado um alerta a cada membro de acordo com o canal descrito no objeto <emphasis>contact</emphasis> (contato) correspondente.</para>

	<para>Um sistema de herança permite o fácil compartilhamento de um conjunto de propriedades por entre muitos objetos sem a duplicação de informação. Além disso, a configuração inicial inclui um número de objetos padrão; em muitos casos, a definição de novas máquinas, serviços e contatos é uma simples questão de derivação a partir dos objetos genéricos fornecidos. Os arquivos em <filename>/etc/nagios3/conf.d/</filename> são uma boa fonte de informação sobre como eles funcionam.</para>

	<para>Os administradores da Falcot Corp usam a seguinte configuração:</para>

        <example>
          <title>arquivo <filename>/etc/nagios3/conf.d/falcot.cfg</filename></title>

          <programlisting>define contact{
    name                            generic-contact
    service_notification_period     24x7
    host_notification_period        24x7
    service_notification_options    w,u,c,r
    host_notification_options       d,u,r
    service_notification_commands   notify-service-by-email
    host_notification_commands      notify-host-by-email
    register                        0 ; Template only
}
define contact{
    use             generic-contact
    contact_name    rhertzog
    alias           Raphael Hertzog
    email           hertzog@debian.org
}
define contact{
    use             generic-contact
    contact_name    rmas
    alias           Roland Mas
    email           lolando@debian.org
}

define contactgroup{
    contactgroup_name     falcot-admins
    alias                 Falcot Administrators
    members               rhertzog,rmas
}

define host{
    use                   generic-host ; Name of host template to use
    host_name             www-host
    alias                 www.falcot.com
    address               192.168.0.5
    contact_groups        falcot-admins
    hostgroups            debian-servers,ssh-servers
}
define host{
    use                   generic-host ; Name of host template to use
    host_name             ftp-host
    alias                 ftp.falcot.com
    address               192.168.0.6
    contact_groups        falcot-admins
    hostgroups            debian-servers,ssh-servers
}

# 'check_ftp' command with custom parameters
define command{
    command_name          check_ftp2
    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35
}

# Generic Falcot service
define service{
    name                  falcot-service
    use                   generic-service
    contact_groups        falcot-admins
    register              0
}

# Services to check on www-host
define service{
    use                   falcot-service
    host_name             www-host
    service_description   HTTP
    check_command         check_http
}
define service{
    use                   falcot-service
    host_name             www-host
    service_description   HTTPS
    check_command         check_https
}
define service{
    use                   falcot-service
    host_name             www-host
    service_description   SMTP
    check_command         check_smtp
}

# Services to check on ftp-host
define service{
    use                   falcot-service
    host_name             ftp-host
    service_description   FTP
    check_command         check_ftp2
}</programlisting>
        </example>

	<para>Ess arquivo de configuração descreve duas máquinas monitoradas. A primeira é o servidor web, e a checagem é feita nas portas HTTP (80) e HTTP-seguro (443). O Nagios também checa se um servidor SMTP está rodando na porta 25. A segunda máquina é um servidor FTP, e a checagem inclui garantir que uma resposta venha em 20 segundos. Além desse intervalo, um <emphasis>warning</emphasis> é emitido; além de 30 segundos, o alerta é considerado crítico. A interface web do Nagios também mostra que um serviço SSH é monitorado: isso vem de máquinas pertencentes ao grupo de máquinas <literal>ssh-servers</literal>. O serviço padrão correspondente é definido em <filename>/etc/nagios3/conf.d/services_nagios2.cfg</filename>.</para>

	<para>Note o uso da herança: um objeto é feito para herdar de outro objeto através de “use <replaceable>parent-name</replaceable>”. O obejto pai tem que ser identificável, o que requer dar a ele uma prorpiedade “name <replaceable>identifier</replaceable>”. Se o objeto pai não se destina a ser um objeto real, mas apenas servir como um pai, dar-lhe uma propriedade “register 0” informa ao Nagios para não considerá-lo, e assim ignorar a falta de alguns parâmetros que de outra forma seriam necessários.</para>

        <sidebar>
          <title><emphasis>DOCUMENTAÇÃO</emphasis> Lista de propriedades do objeto</title>

	  <para>Uma compreensão mais profunda das várias maneiras pelas quais o Nagios pode ser configurado pode ser obtida a partir da documentação fornecida pelo pacote <emphasis role="pkg">nagios3-doc</emphasis>. Essa documentação é diretamente acessível pela interface web, com o link “Documentation” no topo do canto esquerdo. ela inclui uma lista de todos dos tipos de objetos, com todas as propriedades que eles podem ter. ela também explica como criar novas extensões.</para>
        </sidebar>

        <sidebar>
          <title><emphasis>INDO ALÉM</emphasis> Testes remotos com NRPE</title>

	  <para>Muitas extensões Nagios permitem conferir alguns parâmetros locais de uma máquina ("host"); se muitas máquinas precisam dessas conferências enquanto uma instalação central as reune, a extensão NRPE (<emphasis>Nagios Remote Plugin Executor</emphasis>) precisa ser implantada. O pacote <emphasis role="pkg">nagios-nrpe-plugin</emphasis> precisa ser instalado no servidor Nagios, e o <emphasis role="pkg">nagios-nrpe-server</emphasis> nas máquinas ("hosts") aonde os testes locais precisam ser rodados. Esse último pega sua configuração de <filename>/etc/nagios/nrpe.cfg</filename>. Esse arquivo deve listar os testes que podem ser iniciados remotamente, e os endereços IP  das máquinas que tem permissão para iniciá-los. Pelo lado do Nagios, habilitar esses testes remotos é uma simples questão de adicionar os serviços correspondentes usando o novo comando <emphasis>check_nrpe</emphasis>.</para>
        </sidebar>
      </section>
    </section>
  </section>
</chapter>
