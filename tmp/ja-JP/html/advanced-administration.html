<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html
    xmlns="http://www.w3.org/1999/xhtml"><head><meta
        http-equiv="Content-Type"
        content="text/html; charset=UTF-8" /><title
        xmlns:d="http://docbook.org/ns/docbook">第 12 章 高度な管理</title><link
        rel="stylesheet"
        type="text/css"
        href="Common_Content/css/default.css" /><link
        rel="stylesheet"
        media="print"
        href="Common_Content/css/print.css"
        type="text/css" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="generator"
        content="publican v4.3.2" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="package"
        content="Debian-debian-handbook-9-ja-JP-1.0-1" /><meta
        name="keywords"
        content="RAID, LVM, FAI, Preseed, 監視, 仮想化, Xen, LXC" /><link
        rel="home"
        href="index.html"
        title="Debian 管理者ハンドブック" /><link
        rel="up"
        href="index.html"
        title="Debian 管理者ハンドブック" /><link
        rel="prev"
        href="sect.rtc-services.html"
        title="11.8. リアルタイムコミュニケーションサービス" /><link
        rel="next"
        href="sect.virtualization.html"
        title="12.2. 仮想化" /><meta
        xmlns=""
        name="flattr:id"
        content="4pz9jq" /><link
        xmlns=""
        rel="canonical"
        href="https://debian-handbook.info/browse/ja-JP/stable/advanced-administration.html" /></head><body><div
        id="banner"><a
          href="http://debian-handbook.info/get/"><span
            class="text">Download the ebook</span></a></div><p
        id="title"><a
          class="left"
          href="http://www.debian.org"><img
            alt="Product Site"
            src="Common_Content/images//image_left.png" /></a><a
          class="right"
          href="index.html"><img
            alt="Documentation Site"
            src="Common_Content/images//image_right.png" /></a></p><ul
        class="docnav top"><li
          class="previous"><a
            accesskey="p"
            href="sect.rtc-services.html"><strong>戻る</strong></a></li><li
          class="home">Debian 管理者ハンドブック</li><li
          class="next"><a
            accesskey="n"
            href="sect.virtualization.html"><strong>次へ</strong></a></li></ul><div
        xml:lang="ja-JP"
        class="chapter"
        lang="ja-JP"><div
          class="titlepage"><div><div><h1
                class="title"><a
                  xmlns=""
                  id="advanced-administration"></a>第 12 章 高度な管理</h1></div></div></div><div
          class="toc"><dl
            class="toc"><dt><span
                class="section"><a
                  href="advanced-administration.html#sect.raid-and-lvm">12.1. RAID と LVM</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.raid-soft">12.1.1. ソフトウェア RAID</a></span></dt><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.lvm">12.1.2. LVM</a></span></dt><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.raid-or-lvm">12.1.3. RAID それとも LVM?</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.virtualization.html">12.2. 仮想化</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.virtualization.html#sect.xen">12.2.1. Xen</a></span></dt><dt><span
                    class="section"><a
                      href="sect.virtualization.html#sect.lxc">12.2.2. LXC</a></span></dt><dt><span
                    class="section"><a
                      href="sect.virtualization.html#id-1.15.5.14">12.2.3. KVM を使った仮想化</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.automated-installation.html">12.3. 自動インストール</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.fai">12.3.1. Fully Automatic Installer (FAI)</a></span></dt><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.d-i-preseeding">12.3.2. Debian-Installer の事前設定</a></span></dt><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.simple-cdd">12.3.3. Simple-CDD、一体型の解決策</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.monitoring.html">12.4. 監視</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.monitoring.html#sect.munin">12.4.1. Munin のセットアップ</a></span></dt><dt><span
                    class="section"><a
                      href="sect.monitoring.html#sect.nagios">12.4.2. Nagios のセットアップ</a></span></dt></dl></dd></dl></div><div
          class="highlights"><div
            class="para">
		この章では、前章までに説明した一部の側面を異なる視点からもう一度取り上げます。すなわち、1 台のコンピュータにインストールするのではなく、大規模な配備システムについて学びます。さらに、初回インストール時に RAID や LVM ボリュームを作成するのではなく、手作業でこれを行う方法について学びます。こうすることで初回インストール時の選択を訂正することが可能です。最後に、監視ツールと仮想化技術について議論します。その結果として、この章はより熟練した管理者を対象にしており、ホームネットワークに責任を負う個人を対象にしていません。
	</div></div><div
          class="section"><div
            class="titlepage"><div><div><h2
                  class="title"><a
                    xmlns=""
                    id="sect.raid-and-lvm"></a>12.1. RAID と LVM</h2></div></div></div><div
            class="para">
			<a
              class="xref"
              href="installation.html">第 4 章「<em>インストール</em>」</a>ではインストーラの視点から RAID と LVM の技術を説明し、インストーラを使って初回インストール時に RAID や LVM を簡単に配備する方法について説明しました。初回インストールが終わったら、管理者は再インストールという手間のかかる最終手段を行使することなく、より大きなストレージ領域の要求に対処しなければいけません。すなわち、管理者は RAID と LVM ボリュームを操作するために必要なツールを理解しなければいけません。
		</div><div
            class="para">
			RAID と LVM は両方ともマウントされたボリュームを物理的に対応する物 (実際のハードディスクドライブまたはそのパーティション) から抽象化する技術です。さらに、RAID は冗長性を導入することでデータをハードディスク障害から守り、LVM はボリューム管理をより柔軟にしてディスクの実サイズに依存しないようにします。どちらの場合であっても、最終的にシステムには新しいブロックデバイスが追加されます。追加されたブロックデバイスはファイルシステムやスワップ領域を作成するために使われますが、必ずしも単独の物理ディスクに対応付けられるものではありません。RAID と LVM は全く異なる生い立ちを持っていますが、両者の機能は多少重複しています。このため、両者は一緒に言及されることが多いです。
		</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>PERSPECTIVE</em></span> Btrfs が LVM と RAID を結び付ける</strong></p></div></div></div><div
              class="para">
			LVM と RAID は 2 種類の全く別のカーネルサブシステムで、どちらもディスクブロックデバイスとそのファイルシステムの間のやり取りを担当します。<span
                class="emphasis"><em>btrfs</em></span> は最初 Oracle で開発された新しいファイルシステムで、LVM と RAID の機能を結び付けると主張しています。<span
                class="emphasis"><em>btrfs</em></span> は開発がまだ完了していない (一部の機能がまだ実装されていない) ため「実験中」とタグ付けされていますが、ほとんどうまく機能し、既に本番環境で使われています。<div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://btrfs.wiki.kernel.org/">http://btrfs.wiki.kernel.org/</a></div>
		</div><div
              class="para">
			<span
                class="emphasis"><em>btrfs</em></span> の特筆すべき機能に、任意の時点におけるファイルシステムツリーのスナップショットを取る機能があります。このスナップショットコピーは初期状態ではいかなるディスク領域も使いません、コピー内容の 1 つが修正された際にデータが複製されます。また、このファイルシステムはファイルを透過的に圧縮することが可能で、さらにチェックサムを用いて保存されているデータの完全性を保証します。
		</div></div><div
            class="para">
			RAID と LVM のどちらの場合も、カーネルはハードディスクドライブやパーティションに対応するブロックデバイスファイルとよく似たブロックデバイスファイルを提供します。アプリケーションやカーネルの別の部分がそのようなデバイスのあるブロックにアクセスを要求する場合、適切なサブシステムが要求されたブロックを物理層のブロックに対応付けます。設定に依存して、アプリケーション側から見たブロックは単独か複数の物理ディスクに保存されます。このブロックの物理的場所は論理デバイス内のブロックの位置と直接的に対応するものではないかもしれません。
		</div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      xmlns=""
                      id="sect.raid-soft"></a>12.1.1. ソフトウェア RAID</h3></div></div></div><a
              id="id-1.15.4.6.2"
              class="indexterm"></a><div
              class="para">
				RAID は <span
                class="emphasis"><em>Redundant Array of Independent Disks</em></span> を意味します。RAID システムの目標はハードディスク障害の際にデータ損失を防ぐことです。一般原則は極めて単純です。すなわち、データは設定できる冗長性のレベルに基づいて単独ではなく複数のディスクに保存されます。冗長性の度合いに依存して、たとえ予想外のディスク障害が起きた場合でも、データを残りのディスクから損失なく再構成することが可能です。
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>CULTURE</em></span> <span
                          class="foreignphrase"><em
                            class="foreignphrase">independent</em></span> それとも <span
                          class="foreignphrase"><em
                            class="foreignphrase">inexpensive</em></span>?</strong></p></div></div></div><div
                class="para">
				当初、RAID の I は <span
                  class="emphasis"><em>inexpensive</em></span> を意味していました。なぜなら、RAID は高価な高性能ディスクへ投資することなくデータの安全性を劇的に高めることが可能だったからです。しかしながらおそらく心証的な懸念から、現在 RAID の I は通例 <span
                  class="emphasis"><em>independent</em></span> を意味するものとされます。<span
                  class="emphasis"><em>independent</em></span> には安価であることに対する悪い印象がないからです。
			</div></div><div
              class="para">
				RAID は専用ハードウェア (SCSI や SATA コントローラカードに統合された RAID モジュール) またはソフトウェア抽象化 (カーネル) を使って実装することが可能です。ハードウェアかソフトウェアかに関わらず、十分な冗長性を備えた RAID システムはディスク障害があっても利用できる状態を透過的に継続することが可能です。従って、スタックの上層 (アプリケーション) はディスク障害にも関わらず、引き続きデータにアクセスできます。もちろん「信頼性低下状態」は性能に影響をおよぼし、冗長性を低下させます。このため、もう一つ別のディスク障害が起きるとデータを失うことになります。このため実践的には、管理者は信頼性低下状態を障害の起きたディスクが交換されるまでの間だけに留めるように努力します。新しいディスクが配備されると、RAID システムは要求されたデータを再構成することが可能です。こうすることで信頼性の高い状態に戻ります。信頼性低下状態か再構成状態にある RAID アレイのアクセス速度は低下する可能性がありますが、この点を除けばアプリケーションがディスク障害に気が付くことはないでしょう。
			</div><div
              class="para">
				RAID がハードウェアで実装された場合、その設定は通常 BIOS セットアップツールによってなされます。カーネルは RAID アレイを標準的な物理ディスクとして機能する単一のディスクとみなします。RAID アレイのデバイス名は (ドライバに依存して) 違うかもしれません。
			</div><div
              class="para">
				本書ではソフトウェア RAID だけに注目します。
			</div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.raid-levels"></a>12.1.1.1. さまざまな RAID レベル</h4></div></div></div><div
                class="para">
					実際のところ RAID の種類は 1 種類だけではなく、そのレベルによって識別される複数の種類があります。すなわち、設計と提供される冗長性の度合いが異なる複数の RAID レベルが存在します。より冗長性を高くすれば、より障害に強くなります。なぜなら、より多くのディスクで障害が起きても、システムを動かし続けることができるからです。これに応じて、与えられた一連のディスクに対して利用できる領域が小さくなります。すなわち、あるサイズのデータを保存するために必要なディスク領域のサイズが多くなります。
				</div><div
                class="variablelist"><dl
                  class="variablelist"><dt><span
                      class="term">リニア RAID</span></dt><dd><div
                      class="para">
								カーネルの RAID サブシステムを使えば「リニア RAID」を作ることも可能ですが、「リニア RAID」は適切な RAID ではありません。なぜなら、「リニア RAID」には冗長性が一切ないからです。カーネルはただ単純に複数のディスク端同士を統合し、統合されたボリュームを 1 つの仮想ディスク (1 つのブロックデバイス) として提供するだけです。これが「リニア RAID」のすべてです。「リニア RAID」を使うのは極めてまれな場合に限られます (後から使用例を説明します)。なぜなら、冗長性がないということは 1 つのディスクの障害が統合されたボリューム全体を駄目にすること、ひいてはすべてのデータを駄目にすることを意味するからです。
							</div></dd><dt><span
                      class="term">RAID-0</span></dt><dd><div
                      class="para">
								同様に RAID-0 にも冗長性がありません。しかしながら、RAID-0 は順番通り単純に物理ディスクを連結する構成ではありません。すなわち、物理ディスクは<span
                        class="emphasis"><em>ストライプ状</em></span>に分割され、仮想デバイスのブロックは互い違いになった物理ディスクのストライプに保存されます。たとえば 2 台のディスクから構成されている RAID-0 セットアップでは、偶数を付番されたブロックは最初の物理ディスクに保存され、奇数を付番されたブロックは 2 番目の物理ディスクに保存されます。
							</div><div
                      class="para">
								RAID-0 システムを使っても信頼性は向上しません。なぜなら、システムの信頼性すなわちデータの可用性は (リニア RAID と同様に) ディスク障害があればすぐに脅かされるからです。しかしながら、RAID-0 システムを使うことで性能は向上します。すなわち隣接した巨大なデータにシーケンシャルアクセスする場合、カーネルは両方のディスクから並行して読み込む (書き込む) ことが可能です。これによりデータの転送率が増加します。とは言うものの、RAID-0 が使われる機会は減り、代わりに LVM (後から説明します) が使われるようになっています。
							</div></dd><dt><span
                      class="term">RAID-1</span></dt><dd><div
                      class="para">
								RAID-1 は「RAID ミラーリング」としても知られ、最も簡単で最も広く使われています。RAID-1 の標準的な構成では、同じサイズの 2 台の物理ディスクを使い、物理ディスクと同じサイズの論理ボリュームが利用できるようになります。データを両方のディスクに保存するため、「ミラー」と呼ばれています。一方のディスクに障害があっても、他方のディスクからデータを利用することが可能です。もちろん、非常に重要なデータ用に RAID-1 を 2 台以上の構成にすることも可能ですが、これはハードウェア費用と利用できる保存領域の比率に直接的な影響をおよぼします。
							</div><div
                      class="sidebar"><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>NOTE</em></span> ディスクとクラスタサイズ</strong></p></div></div></div><div
                        class="para">
								異なるサイズの 2 台のディスクをミラーでセットアップする場合、サイズの大きい側のディスクは完全に利用されません。なぜなら、大きい側のディスクに含まれるデータは最も小さいディスクに含まれるデータと同じデータだからです。このため RAID-1 ボリュームで提供される利用できる領域のサイズは RAID アレイの最も小さなディスクのサイズと同じになります。冗長性を異なる方法で確保しているより高い RAID レベルの RAID ボリュームに対しても同じことが言えます。
							</div><div
                        class="para">
								それ故、(RAID-0 と「リニア RAID」以外の) RAID アレイをセットアップする場合、資源の無駄を防ぐためにアレイを構成するディスクはそのサイズが完全に同じか近いものを使うことが重要です。
							</div></div><div
                      class="sidebar"><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>NOTE</em></span> 予備ディスク</strong></p></div></div></div><div
                        class="para">
								冗長性を持たせた RAID レベルでは、必要なディスク数よりも多くのディスクで RAID アレイを構成させることが可能です。追加的ディスクは主要ディスクに障害が起きた場合に予備として使われます。たとえば、2 台のディスクと 1 台の予備ディスクのミラー構成では、最初の 2 台のうちの 1 台に障害が起きた場合、カーネルは自動的に (そして素早く) 予備ディスクを使ってミラーを再構成し、再構成の完了後に冗長性が再確保されます。すなわち、重要なデータに対するもう一つの安全装置として予備ディスクを使うことが可能ということです。
							</div><div
                        class="para">
								この方式が単純に 3 台のディスクに対して最初からミラーリングを行うよりも優れているとされることに疑問を持つかもしれません。「予備ディスク」を設定する利点は複数の RAID ボリュームで予備ディスクを共有することが可能という点です。たとえば、1 台のディスク障害に対する冗長性を確保した 3 つのミラーされたボリュームを構成するには、ディスクを 7 台 (3 つのペアと 1 台の共有された予備) 用意するだけですみます。これに対して各ボリュームに 3 台のディスクを用意する場合には 9 台のディスクが必要です。
							</div></div><div
                      class="para">
								RAID-1 は高価であるにも関わらず (良くても物理ストレージ領域のたった半分しか使えないにも関わらず)、広く実運用されています。RAID-1 は簡単に理解でき、簡単にバックアップできます。なぜなら両方のディスクが全く同じ内容を持っているため、片方を一時的に取り外しても運用システムに影響をおよぼさないからです。通常 RAID-1 を使うことで、読み込み性能は好転します。なぜなら、カーネルはデータの半分をそれぞれのディスクから並行して読むことができるからです。これに対して、書き込み性能はそれほど悪化しません。N 台のディスクからなる RAID-1 アレイの場合、データは N-1 台のディスク障害に対して保護されます。
							</div></dd><dt><span
                      class="term">RAID-4</span></dt><dd><div
                      class="para">
								RAID-4 は広く使われていません。RAID-4 は実データを保存するために N 台のディスクを使い、冗長性情報を保存するために 1 台の「パリティ」ディスクを使います。「パリティ」ディスクに障害が起きた場合、システムは他の N 台からデータを再構成することが可能です。N 台のデータディスクのうち、最大で 1 台に障害が起きた場合、残りの N-1 台と「パリティ」ディスクには、要求されたデータを再構成するために十分な情報が含まれます。
							</div><div
                      class="para">
								RAID-4 は高価過ぎるというわけではありません。なぜならディスク 1 台につきたった N 分の 1 台分の追加費用で済むからです。また RAID-4 を使うと読み込み性能が大きく低下するというわけでもありません。しかしながら、RAID-4 は書き込み性能に深刻な影響をおよぼします。加えて、N 台の実データ用ディスクのどのディスクに書き込んでもパリティディスクに対する書き込みが発生するので、パリティディスクは実データ用ディスクに比べて書き込み回数が増えます。その結果、パリティディスクは極めて寿命が短くなります。RAID-4 アレイのデータは (N+1 台のディスクのうち) 1 台の障害に対して保護されます。
							</div></dd><dt><span
                      class="term">RAID-5</span></dt><dd><div
                      class="para">
								RAID-5 は RAID-4 の非対称性問題を対処したものです。すなわち、パリティブロックは N+1 台のディスクに分散して保存され、特定のディスクが特定の役割を果たすことはありません。
							</div><div
                      class="para">
								読み込みと書き込み性能は RAID-4 と同様です。繰り返しになりますが、RAID-5 システムは (N+1 台のディスクのうち) 最大で 1 台までに障害が起きても動作します。
							</div></dd><dt><span
                      class="term">RAID-6</span></dt><dd><div
                      class="para">
								RAID-6 は RAID-5 の拡張と考えられます。RAID-6 では、N 個の連続するブロックに対して 2 個の冗長性ブロックを使います。この N+2 個のブロックは N+2 台のディスクに分散して保存されます。
							</div><div
                      class="para">
								RAID-6 は RAID-4 と RAID-5 に比べて少し高価ですが、RAID-6 を使うことで安全性はさらに高まります。なぜなら、(N+2 台中の) 最大で 2 台までの障害に対してデータを守ることが可能だからです。書き込み操作は 1 つのデータブロックと 2 つの冗長性ブロックを書き込むことに対応しますから、RAID-6 の書き込み性能は RAID-4 と RAID-5 に比べてさらに悪化します。
							</div></dd><dt><span
                      class="term">RAID-1+0</span></dt><dd><div
                      class="para">
								厳密に言えば RAID-1+0 は RAID レベルではなく、2 種類の RAID 分類を積み重ねたものです。RAID-1+0 を使うには 2×N 台のディスクが必要で、最初に 2 台ずつのペアから N 台の RAID-1 ボリュームを作ります。N 台の RAID-1 ボリュームは「リニア RAID」か LVM (次第にこちらを選ぶケースが増えています) のどちらか一方を使って 1 台に統合されます。LVM を使うと純粋な RAID ではなくなりますが、LVM を使っても問題はありません。
							</div><div
                      class="para">
								RAID-1+0 は複数のディスク障害を乗り切ることが可能です。具体的に言えば、上に挙げた 2×N アレイの場合、最大で N 台までの障害に耐えます。ただし、各 RAID-1 ペアを構成するディスクの両方に障害が発生してはいけません。
							</div><div
                      class="sidebar"><a
                        xmlns=""
                        id="sidebar.raid-10"></a><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>GOING FURTHER</em></span> RAID-10</strong></p></div></div></div><div
                        class="para">
								通常 RAID-10 は RAID-1+0 の同意語と考えられますが、Linux では特別に RAID-10 をより一般的な構成を可能にするものとして定めています。RAID-10 では、システムが各ブロックを 2 種類の異なるディスクに保存することが可能です。奇数台のディスク構成の場合でも、ブロックのコピーは設定可能なモデルに従って分散して保存されます。
							</div><div
                        class="para">
								RAID-10 の性能は選択した再分割モデルと冗長性の度合い、そして論理ボリュームの作業負荷に依存して変化します。
							</div></div></dd></dl></div><div
                class="para">
					RAID レベルを選ぶ際には、各用途からの制限および要求を考慮する必要があるのは明らかです。1 台のコンピュータに異なる設定を持つ複数の RAID アレイを配置することが可能である点に注意してください。
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.raid-setup"></a>12.1.1.2. RAID の設定</h4></div></div></div><a
                id="id-1.15.4.6.9.2"
                class="indexterm"></a><div
                class="para">
					RAID ボリュームを設定するには <span
                  class="pkg pkg">mdadm</span> パッケージが必要です。<span
                  class="pkg pkg">mdadm</span> パッケージには RAID アレイを作成したり操作するための <code
                  class="command">mdadm</code> コマンド、システムの他の部分に RAID アレイを統合するためのスクリプトやツール、監視システムが含まれます。
				</div><div
                class="para">
					以下の例では、多数のディスクを持つサーバをセットアップします。ディスクの一部は既に利用されており、残りは RAID をセットアップするために利用できるようになっています。最初の状態で、以下のディスクとパーティションが存在します。
				</div><div
                xmlns:d="http://docbook.org/ns/docbook"
                class="itemizedlist"><ul><li
                    class="listitem"><div
                      class="para">
							<code
                        class="filename">sdb</code> ディスク (4 GB) は全領域を利用できます。
						</div></li><li
                    class="listitem"><div
                      class="para">
							<code
                        class="filename">sdc</code> ディスク (4 GB) は全領域を利用できます。
						</div></li><li
                    class="listitem"><div
                      class="para">
							<code
                        class="filename">sdd</code> ディスクは <code
                        class="filename">sdd2</code> パーティション (約 4 GB) だけを利用できます。
						</div></li><li
                    class="listitem"><div
                      class="para">
							<code
                        class="filename">sde</code> ディスク (4 GB) は全領域を利用できます。
						</div></li></ul></div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>NOTE</em></span> 既存の RAID ボリュームの識別</strong></p></div></div></div><div
                  class="para">
					<code
                    class="filename">/proc/mdstat</code> ファイルには既存のボリュームとその状態が書かれています。新しい RAID ボリュームを作成する場合、既存のボリュームと同じ名前を付けないように注意してください。
				</div></div><div
                class="para">
					RAID-0 とミラー (RAID-1) の 2 つのボリュームを作るために上記の物理ディスクを使います。それでは RAID-0 ボリュームから作っていきましょう。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</code></strong>
<code
                  class="computeroutput">mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
# </code><strong
                  class="userinput"><code>mdadm --query /dev/md0</code></strong>
<code
                  class="computeroutput">/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md0</code></strong>
<code
                  class="computeroutput">/dev/md0:
        Version : 1.2
  Creation Time : Wed May  6 09:24:34 2015
     Raid Level : raid0
     Array Size : 8387584 (8.00 GiB 8.59 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:24:34 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

     Chunk Size : 512K

           Name : mirwiz:0  (local to host mirwiz)
           UUID : bb085b35:28e821bd:20d697c9:650152bb
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       1       8       32        1      active sync   /dev/sdc
# </code><strong
                  class="userinput"><code>mkfs.ext4 /dev/md0</code></strong>
<code
                  class="computeroutput">mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 2095104 4k blocks and 524288 inodes
Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </code><strong
                  class="userinput"><code>mkdir /srv/raid-0</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mount /dev/md0 /srv/raid-0</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/raid-0</code></strong>
<code
                  class="computeroutput">ファイルシス   サイズ  使用  残り 使用% マウント位置
/dev/md0         7.9G   18M  7.4G    1% /srv/raid-0
</code></pre><div
                class="para">
					<code
                  class="command">mdadm --create</code> コマンドには複数のパラメータが必要です。具体的に言えば、作成するボリュームの名前 (<code
                  class="filename">/dev/md*</code>、MD は <span
                  class="foreignphrase"><em
                    class="foreignphrase">Multiple Device</em></span> を意味します)、RAID レベル、ディスク数 (普通この値は RAID-1 とそれ以上のレベルでのみ意味があるにも関わらず、これは必須オプションです)、RAID を構成する物理デバイスを指定する必要があります。RAID デバイスを作成したら、RAID デバイスを通常のパーティションを取り扱うのと同様のやり方で取り扱うことが可能です。すなわち、ファイルシステムを作成したり、ファイルシステムをマウントしたりすることが可能です。ここで作成する RAID-0 ボリュームに <code
                  class="filename">md0</code> と名前を付けたのは偶然に過ぎない点に注意してください。アレイに付けられた番号と冗長性の度合いを関連付ける必要はありません。また、<code
                  class="filename">/dev/md0</code> の代わりに <code
                  class="filename">/dev/md/linear</code> のようなパラメータを <code
                  class="command">mdadm</code> に渡すことで、名前付き RAID アレイを作成することも可能です。
				</div><div
                class="para">
					同様のやり方で RAID-1 を作成します。注意するべき違いは作成後に説明します。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%
Continue creating array? </code><strong
                  class="userinput"><code>y</code></strong>
<code
                  class="computeroutput">mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md1 started.
# </code><strong
                  class="userinput"><code>mdadm --query /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
        Version : 1.2
  Creation Time : Wed May  6 09:30:19 2015
     Raid Level : raid1
     Array Size : 4192192 (4.00 GiB 4.29 GB)
  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:30:40 2015
          State : clean, resyncing (PENDING) 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       1       8       64        1      active sync   /dev/sde
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
          State : clean
[...]
</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>TIP</em></span> RAID、ディスク、パーティション</strong></p></div></div></div><div
                  class="para">
					上の例で示した通り、RAID デバイスはディスクパーティションに作成することが可能です。必ずディスク全体を使わなければいけないというわけではありません。
				</div></div><div
                class="para">
					いくつかの注意点があります。最初に、<code
                  class="command">mdadm</code> は物理デバイス同士のサイズが異なる点を指摘しています。さらに、このことによりサイズが大きい側のデバイスの一部の領域が使えなくなるため、確認が求められています。
				</div><div
                class="para">
					さらに重要なことは、ミラーの状態に注意することです。RAID ミラーの正常な状態とは、両方のディスクが全く同じ内容を持っている状態です。しかしながら、ボリュームを最初に作成した直後の RAID ミラーは正常な状態であることを保証されません。このため、RAID サブシステムは RAID ミラーの正常な状態を保証するために、RAID デバイスが作成されたらすぐに同期化作業を始めます。しばらくの後 (必要な時間はディスクの実サイズに依存します)、RAID アレイは「active」または「clean」状態に移行します。同期化作業中、ミラーは信頼性低下状態で、冗長性は保証されない点に注意してください。同期化作業中にディスク障害が起きると、すべてのデータを失うことにつながる恐れがあります。しかしながら、最近作成された RAID アレイの最初の同期化作業の前に大量の重要なデータがこの RAID アレイに保存されていることはほとんどないでしょう。信頼性低下状態であっても <code
                  class="filename">/dev/md1</code> を利用することが可能で、ファイルシステムを作成したり、データのコピーを取ったりすることが可能という点に注意してください。
				</div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>TIP</em></span> 信頼性低下状態でミラーを開始する</strong></p></div></div></div><div
                  class="para">
					RAID-1 ミラーを構成する 2 台のディスクの両方をすぐに使えないことが時々あります。たとえば、ミラーを構成するディスクの片方にミラーに移動したいデータが既に保存されている場合です。このような場合、<code
                    class="command">mdadm</code> に渡すデバイスファイル引数の片方をデバイスファイルの代わりに <code
                    class="filename">missing</code> にすることで、意図的に信頼性低下状態の RAID-1 アレイを作成することも可能です。ミラーに移動したいデータを含むディスクからデータを「ミラー」にコピーした後、そのディスクをアレイに追加することが可能です。追加作業が終われば、同期化作業が行われ、ミラーに移動したかったデータの冗長性が確保されます。
				</div></div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>TIP</em></span> 同期化作業を行わずにミラーをセットアップする</strong></p></div></div></div><div
                  class="para">
					通常 RAID-1 ボリュームは新しいディスクとして使うために作成され、RAID-1 ボリュームの作成直後にはデータが保存されていないと考えられます。すなわち、RAID-1 ボリュームの初期内容に価値はなく、RAID-1 で保護したい重要なデータは RAID-1 ボリュームの作成後に書き込まれるデータというわけです。
				</div><div
                  class="para">
					そう考えると、RAID-1 ボリュームにデータが書き込まれる前に RAID-1 ボリュームを構成するディスクの内容が同期されるという点について疑問に思うかもしれません。RAID-1 ボリュームに書き込んでいないデータは後から読み込まれることもないのにも関わらず、なぜ RAID-1 ボリュームの作成時に RAID-1 ボリュームを構成するディスクの内容の同期化作業が必要なのでしょうか?
				</div><div
                  class="para">
					幸いなことに、RAID-1 を構成するディスクの内容の同期化作業は <code
                    class="literal">--assume-clean</code> オプションを <code
                    class="command">mdadm</code> に渡せば避けることが可能です。しかしながら、初期データが読まれる場合、<code
                    class="literal">--assume-clean</code> オプションを使うと問題があります (たとえば、物理ディスク上にファイルシステムが既に存在している場合、問題があります)。このため、デフォルトでこのオプションは有効化されません。
				</div></div><div
                class="para">
					RAID-1 アレイを構成するディスクの 1 台に障害が発生した場合、何が起きるかを見て行きましょう。<code
                  class="command">mdadm</code> に <code
                  class="literal">--fail</code> オプションを付けることで、ディスク障害を模倣することが可能です。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --fail /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: set /dev/sde faulty in /dev/md1
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Update Time : Wed May  6 09:39:39 2015
          State : clean, degraded 
 Active Devices : 1
Working Devices : 1
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 19

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       0        0        2      removed

       1       8       64        -      faulty   /dev/sde</code></pre><div
                class="para">
					RAID-1 ボリュームの内容はまだアクセスすることが可能ですが (そして、RAID-1 ボリュームがマウントされていた場合、アプリケーションはディスク障害に気が付きませんが)、データの安全性はもはや保証されません。つまり <code
                  class="filename">sdd</code> ディスクにも障害が発生した場合、データは失われます。この危険性を避けるために、障害の発生したディスクを新しいディスク <code
                  class="filename">sdf</code> に交換します。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --add /dev/sdf</code></strong>
<code
                  class="computeroutput">mdadm: added /dev/sdf
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
   Raid Devices : 2
  Total Devices : 3
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:48:49 2015
          State : clean, degraded, recovering 
 Active Devices : 1
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 1

 Rebuild Status : 28% complete

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 26

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      spare rebuilding   /dev/sdf

       1       8       64        -      faulty   /dev/sde
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Update Time : Wed May  6 09:49:08 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 41

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf

       1       8       64        -      faulty   /dev/sde</code></pre><div
                class="para">
					繰り返しになりますが、ボリュームはまだアクセスすることが可能とは言うもののボリュームが信頼性低下状態ならば、カーネルは自動的に再構成作業を実行します。再構成作業が終了したら、RAID アレイは正常状態に戻ります。ここで、システムに <code
                  class="filename">sde</code> ディスクをアレイから削除することを伝えることが可能です。削除することで、2 台のディスクからなる古典的な RAID ミラーになります。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --remove /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: hot removed /dev/sde from /dev/md1
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf</code></pre><div
                class="para">
					この後、今後サーバの電源を切った際にドライブを物理的に取り外したり、ハードウェア設定がホットスワップに対応しているならばドライブをホットリムーブすることが可能です。一部の SCSI コントローラ、多くの SATA ディスク、USB や Firewire で接続された外部ドライブなどはホットスワップに対応しています。
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.backup-raid-config"></a>12.1.1.3. 設定のバックアップ</h4></div></div></div><div
                class="para">
					RAID ボリュームに関連するメタデータのほとんどはアレイを構成するディスク上に直接保存されています。このため、カーネルはアレイとその構成要素を検出し、システムの起動時に自動的にアレイを組み立てることが可能です。しかしながら、この設定をバックアップすることを推奨します。なぜなら、この検出機構は不注意による間違いを防ぐものではないからです。そして、注意して取り扱うべき状況ではまさに検出機構がうまく働かないことが見込まれます。上の例で、<code
                  class="filename">sde</code> ディスク障害が本物で (模倣でない)、<code
                  class="filename">sde</code> ディスクを取り外す前にシステムを再起動した場合、<code
                  class="filename">sde</code> ディスクは再起動中に検出され、システムに復帰します。カーネルは 3 つの物理ディスクを検出し、それぞれのディスクが同じ RAID ボリュームの片割れであると主張します。さらに別の混乱する状況が考えられます。2 台のサーバで使われていた RAID ボリュームを片方のサーバに集約することを考えてみましょう。ディスクが移動される前、各アレイは正常に実行されていました。カーネルはアレイを検出して、適切なペアを組み立てることが可能です。しかし、片方のサーバに移動されたディスクが前のサーバでは <code
                  class="filename">md1</code> に組み込まれており、さらに新しいサーバが既に <code
                  class="filename">md1</code> という名前のアレイを持っていた場合、どちらか一方の名前が変えられます。
				</div><div
                class="para">
					このため、参考情報に過ぎないとは言うものの、設定を保存することは重要です。設定を保存する標準的な方法は <code
                  class="filename">/etc/mdadm/mdadm.conf</code> ファイルを編集することです。以下に例を示します。
				</div><div
                class="example"><a
                  xmlns=""
                  id="example.mdadm-conf"></a><p
                  class="title"><strong>例 12.1 <code
                      class="command">mdadm</code> 設定ファイル</strong></p><div
                  class="example-contents"><pre
                    class="programlisting"># mdadm.conf
#
# このファイルに関する詳細は mdadm.conf(5) を参照してください。
#

# デフォルト (組み込み) 状態ならば、MD スーパーブロックを持つパーティション
# (/proc/partitions) とコンテナをすべてスキャンします。以下のようにスキャンする
# デバイスを指定することも可能です。必要ならばワイルドカードを使ってください。
DEVICE /dev/sd*

# デバイスの自動作成時に使う Debian の標準的なパーミッションを指定します
CREATE owner=root group=disk mode=0660 auto=yes

# 新規アレイの所属先にローカルシステムを自動登録します
HOMEHOST &lt;system&gt;

# 監視デーモンに root を警告メールの送信先として通知します
MAILADDR root

# 既存の MD アレイの定義
ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464

# この設定ファイルは mkconf 3.2.5-3 により
# Fri, 18 Jan 2013 00:21:01 +0900 に自動生成されました</pre></div></div><div
                class="para">
					最も役に立つ設定項目の 1 つに <code
                  class="literal">DEVICE</code> オプションがあります。これは起動時にシステムが RAID ボリュームの構成情報を自動的に探すデバイスをリストします。上の例では、値をデフォルト値 <code
                  class="literal">partitions containers</code> からデバイスファイルを明示したリストに置き換えました。なぜなら、パーティションだけでなくすべてのディスクをボリュームとして使うように決めたからです。
				</div><div
                class="para">
					上の例における最後の 2 行を使うことで、カーネルはアレイに割り当てるボリューム番号を安全に選ぶことが可能です。ディスク本体に保存されたメタ情報はボリュームをもう一度組み上げるのに十分ですが、ボリューム番号を定義する (そして <code
                  class="filename">/dev/md*</code> デバイス名にマッチすることを確認する) には不十分です。
				</div><div
                class="para">
					幸いなことに、以下のコマンドを実行すればこの行を自動的に生成することが可能です。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --misc --detail --brief /dev/md?</code></strong>
<code
                  class="computeroutput">ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</code></pre><div
                class="para">
					最後の 2 行の内容はボリュームを構成するディスクのリストに依存しません。このため、障害の発生したディスクを新しいディスクに交換した際に、これをもう一度生成する必要はありません。逆に、RAID アレイを作成および削除した際に、必ずこの設定ファイルを注意深く更新する必要があります。
				</div></div></div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      xmlns=""
                      id="sect.lvm"></a>12.1.2. LVM</h3></div></div></div><a
              id="id-1.15.4.7.2"
              class="indexterm"></a><a
              id="id-1.15.4.7.3"
              class="indexterm"></a><div
              class="para">
				LVM (<span
                class="emphasis"><em>論理ボリュームマネージャ</em></span>) は物理ディスクから論理ボリュームを抽象化するもう一つの方法で、信頼性を増加させるのではなく柔軟性を増加させることに注目しています。LVM を使うことで、アプリケーションから見る限り透過的に論理ボリュームを変更することが可能です。LVM を使うことで、たとえば新しいディスクを追加し、データを新しいディスクに移行し、古いディスクを削除することがボリュームをアンマウントせずに可能です。
			</div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.lvm-concepts"></a>12.1.2.1. LVM の概念</h4></div></div></div><div
                class="para">
					LVM の柔軟性は 3 つの概念から構成された抽象化レベルによって達成されます。
				</div><div
                class="para">
					1 番目の概念は PV (<span
                  class="emphasis"><em>物理ボリューム</em></span>) です。PV はハードウェアに最も近い要素です。具体的に言えば、PV はディスクのパーティション、ディスク全体、その他の任意のブロックデバイス (たとえば、RAID アレイ) などの物理的要素を指します。物理的要素を LVM の PV に設定した場合、物理的要素へのアクセスは必ず LVM を介すべきという点に注意してください。そうでなければ、システムが混乱します。
				</div><div
                class="para">
					2 番目の概念は VG (<span
                  class="emphasis"><em>ボリュームグループ</em></span>) です。複数の PV は VG にクラスタ化することが可能です。VG は仮想的かつ拡張できるディスクに例えられます。VG は概念的な要素で、<code
                  class="filename">/dev</code> 階層のデバイスファイルに現れません。そのため、VG を直接的に操作する危険はありません。
				</div><div
                class="para">
					3 番目の概念は LV (<span
                  class="emphasis"><em>論理ボリューム</em></span>) です。LV は VG の中の 1 つの塊です。さらに VG をディスクに例えたのと同様の考え方を使うと、LV はパーティションに例えられます。LV はブロックデバイスとして <code
                  class="filename">/dev</code> に現れ、他の物理パーティションと同様に取り扱うことが可能です (一般的に言えば、LV にファイルシステムやスワップ領域を作成することが可能です)。
				</div><div
                class="para">
					ここで重要な事柄は VG を LV に分割する場合に物理的要素 (PV) はいかなる制約も要求しないという点です。1 つの PV (たとえばディスク) から構成される VG を複数の LV に分割できます。同様に、複数の PV から構成される VG を 1 つの大きな LV として提供することも可能です。制約事項がたった 1 つしかないのは明らかです。それはある VG から分割された LV のサイズの合計はその VG を構成する PV のサイズの合計を超えることができないという点です。
				</div><div
                class="para">
					しかしながら、ある VG を構成する PV 同士の性能を同様のものにしたり、その VG から分割された LV 同士に求められる性能を同様のものにしたりすることは通常理に適った方針です。たとえば、利用できるハードウェアに高速な PV と低速な PV がある場合、高速な PV から構成される VG と低速な PV から構成される VG に分けると良いでしょう。こうすることで、高速な PV から構成される VG から分割された LV を高速なデータアクセスを必要とするアプリケーションに割り当て、低速な PV から構成される VG から分割された LV を負荷の少ない作業用に割り当てることが可能です。
				</div><div
                class="para">
					いかなる場合でも、LV は特定の PV を使用するわけではないという点を覚えておいてください。ある LV に含まれるデータの物理的な保存場所を操作することも可能ですが、普通に使っている限りその必要はありません。逆に、VG を構成する PV 群の構成要素が変化した場合、ある LV に含まれるデータの物理的な保存場所は対象の LV の分割元である VG の中ひいてはその VG を構成する PV 群の構成要素の中を移動することがあります (もちろん、データの移動先は対象の LV の分割元の VG を構成する PV 群の構成要素の中に限られます)。
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.lvm-setup"></a>12.1.2.2. LVM の設定</h4></div></div></div><div
                class="para">
					典型的な用途に対する LVM の設定過程を、段階的に見て行きましょう。具体的に言えば、複雑なストレージの状況を単純化したい場合を見ていきます。通常、長く複雑な一時的措置を繰り返した挙句の果てに、この状況に陥ることがあります。説明目的で、徐々にストレージを変更する必要のあったサーバを考えます。このサーバでは、PV として利用できるパーティションが複数の一部使用済みディスクに分散しています。より具体的に言えば、以下のパーティションを PV として利用できます。
				</div><div
                xmlns:d="http://docbook.org/ns/docbook"
                class="itemizedlist"><ul><li
                    class="listitem"><div
                      class="para">
							<code
                        class="filename">sdb</code> ディスク上の <code
                        class="filename">sdb2</code> パーティション (4 GB)。
						</div></li><li
                    class="listitem"><div
                      class="para">
							<code
                        class="filename">sdc</code> ディスク上の <code
                        class="filename">sdc3</code> パーティション (3 GB)。
						</div></li><li
                    class="listitem"><div
                      class="para">
							<code
                        class="filename">sdd</code> ディスク (4 GB) は全領域を利用できます。
						</div></li><li
                    class="listitem"><div
                      class="para">
							<code
                        class="filename">sdf</code> ディスク上の <code
                        class="filename">sdf1</code> パーティション (4 GB) および <code
                        class="filename">sdf2</code> パーティション (5 GB)。
						</div></li></ul></div><div
                class="para">
					加えて、<code
                  class="filename">sdb</code> と <code
                  class="filename">sdf</code> が他の 2 台に比べて高速であると仮定しましょう。
				</div><div
                class="para">
					今回の目標は、3 種類の異なるアプリケーション用に 3 つの LV を設定することです。具体的に言えば、5 GB のストレージ領域が必要なファイルサーバ、データベース (1 GB)、バックアップ用の領域 (12 GB) 用の LV を設定することです。ファイルサーバとデータベースは高い性能を必要とします。しかし、バックアップはアクセス速度をそれほど重要視しません。これらの要件により、各アプリケーションに設定する LV の使用する PV が決定されます。さらに LVM を使いますので、PV の物理的サイズからくる制限はありません。このため、PV 群として利用できる領域のサイズの合計だけが制限となります。
				</div><div
                class="para">
					LVM の設定に必要なツールは <span
                  class="pkg pkg">lvm2</span> パッケージとその依存パッケージに含まれています。これらのパッケージをインストールしたら、3 つの手順を踏んで LVM を設定します。各手順は LVM の概念の 3 つの抽象化レベルに対応します。
				</div><div
                class="para">
					最初に、<code
                  class="command">pvcreate</code> を使って PV を作成します。
				</div><a
                xmlns=""
                id="screen.pvcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvdisplay</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvcreate /dev/sdb2</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdb2" successfully created
# </code><strong
                  class="userinput"><code>pvdisplay</code></strong>
<code
                  class="computeroutput">  "/dev/sdb2" is a new physical volume of "4.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/sdb2
  VG Name               
  PV Size               4.00 GiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I

# </code><strong
                  class="userinput"><code>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdc3" successfully created
  Physical volume "/dev/sdd" successfully created
  Physical volume "/dev/sdf1" successfully created
  Physical volume "/dev/sdf2" successfully created
# </code><strong
                  class="userinput"><code>pvdisplay -C</code></strong>
<code
                  class="computeroutput">  PV         VG   Fmt  Attr PSize PFree
  /dev/sdb2       lvm2 ---  4.00g 4.00g
  /dev/sdc3       lvm2 ---  3.09g 3.09g
  /dev/sdd        lvm2 ---  4.00g 4.00g
  /dev/sdf1       lvm2 ---  4.10g 4.10g
  /dev/sdf2       lvm2 ---  5.22g 5.22g
</code></pre><div
                class="para">
					ここまでは順調です。PV はディスク全体およびディスク上の各パーティションに対して設定することが可能という点に注意してください。上に示した通り、<code
                  class="command">pvdisplay</code> コマンドは既存の PV をリストします。出力フォーマットは 2 種類あります。
				</div><div
                class="para">
					<code
                  class="command">vgcreate</code> を使って、これらの PV から VG を構成しましょう。高速なディスクの PV から <code
                  class="filename">vg_critical</code> VG を構成します。さらに、これ以外の低速なディスクの PV から <code
                  class="filename">vg_normal</code> VG を構成します。
				</div><a
                xmlns=""
                id="screen.vgcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>vgdisplay</code></strong>
<code
                  class="computeroutput">  No volume groups found
# </code><strong
                  class="userinput"><code>vgcreate vg_critical /dev/sdb2 /dev/sdf1</code></strong>
<code
                  class="computeroutput">  Volume group "vg_critical" successfully created
# </code><strong
                  class="userinput"><code>vgdisplay</code></strong>
<code
                  class="computeroutput">  --- Volume group ---
  VG Name               vg_critical
  System ID             
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               8.09 GiB
  PE Size               4.00 MiB
  Total PE              2071
  Alloc PE / Size       0 / 0   
  Free  PE / Size       2071 / 8.09 GiB
  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp

# </code><strong
                  class="userinput"><code>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</code></strong>
<code
                  class="computeroutput">  Volume group "vg_normal" successfully created
# </code><strong
                  class="userinput"><code>vgdisplay -C</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize  VFree 
  vg_critical   2   0   0 wz--n-  8.09g  8.09g
  vg_normal     3   0   0 wz--n- 12.30g 12.30g
</code></pre><div
                class="para">
					繰り返しになりますが、<code
                  class="command">vgdisplay</code> コマンドはかなり簡潔です (そして <code
                  class="command">vgdisplay</code> には 2 種類の出力フォーマットがあります)。同じ物理ディスク上にある 2 つの PV から 2 つの異なる VG を構成することが可能である点に注意してください。また、<code
                  class="filename">vg_</code> 接頭辞を VG の名前に使っていますが、これは慣例に過ぎない点に注意してください。
				</div><div
                class="para">
					これでサイズが約 8 GB と約 12 GB の 2 台の「仮想ディスク」(VG) を手に入れたことになります。それでは仮想ディスク (VG) を「仮想パーティション」(LV) に分割しましょう。これを行うには <code
                  class="command">lvcreate</code> コマンドを少し複雑な構文で実行します。
				</div><a
                xmlns=""
                id="screen.lvcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>lvdisplay</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>lvcreate -n lv_files -L 5G vg_critical</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_files" created
# </code><strong
                  class="userinput"><code>lvdisplay</code></strong>
<code
                  class="computeroutput">  --- Logical volume ---
  LV Path                /dev/vg_critical/lv_files
  LV Name                lv_files
  VG Name                vg_critical
  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT
  LV Write Access        read/write
  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400
  LV Status              available
  # open                 0
  LV Size                5.00 GiB
  Current LE             1280
  Segments               2
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0

# </code><strong
                  class="userinput"><code>lvcreate -n lv_base -L 1G vg_critical</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_base" created
# </code><strong
                  class="userinput"><code>lvcreate -n lv_backups -L 12G vg_normal</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_backups" created
# </code><strong
                  class="userinput"><code>lvdisplay -C</code></strong>
<code
                  class="computeroutput">  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_base    vg_critical -wi-a---  1.00g                                           
  lv_files   vg_critical -wi-a---  5.00g                                           
  lv_backups vg_normal   -wi-a--- 12.00g</code></pre><div
                class="para">
					LV を作成する場合、2 種類のパラメータが必要です。このため、必ず 2 種類のパラメータをオプションとして <code
                  class="command">lvcreate</code> に渡します。作成する LV の名前を <code
                  class="literal">-n</code> オプションで指定し、サイズを <code
                  class="literal">-L</code> オプションで指定します。また、操作対象の VG をコマンドに伝えることが必要です。これはもちろん最後のコマンドラインパラメータです。
				</div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>GOING FURTHER</em></span> <code
                            class="command">lvcreate</code> のオプション</strong></p></div></div></div><div
                  class="para">
					<code
                    class="command">lvcreate</code> コマンドは複数のオプションを取り、作成する LV を微調整することが可能です。
				</div><div
                  class="para">
					最初に <code
                    class="literal">-l</code> オプションについて説明しましょう。<code
                    class="literal">-l</code> オプションを使った場合 LV のサイズをブロック数 (上の例で用いた「人間にとって分かりやすい」単位ではありません) で指定することが可能です。ブロックとは (LVM の用語で PE すなわち<span
                    class="emphasis"><em>物理エクステント</em></span>と呼ばれています) PV 中のストレージ領域の連続した単位です。ブロックは LV 中に分散されています。ある LV 用のストレージ領域を正確に定義したい場合、たとえば利用できる領域のすべてを使いたい場合、<code
                    class="literal">-l</code> オプションのほうが <code
                    class="literal">-L</code> オプションよりも使いやすいでしょう。
				</div><div
                  class="para">
					LV の物理的な位置を示唆することも可能です。こうすることで、LV の PE は特定の PV 上 (もちろん、VG を構成する PV 上に限ります) に作成されます。<code
                    class="filename">sdb</code> は <code
                    class="filename">sdf</code> よりも高速なので、<code
                    class="filename">lv_base</code> を <code
                    class="filename">sdb</code> 上に作成することでファイルサーバよりもデータベースサーバが高速にアクセスできるようにするには、以下のコマンドラインを使います。すなわち <code
                    class="command">lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</code> です。指定した PV に十分な空き PE がない場合、このコマンドは失敗する可能性があります。今回の例でこのような失敗を防ぐには <code
                    class="filename">lv_files</code> の前に <code
                    class="filename">lv_base</code> を作成するか、<code
                    class="command">pvmove</code> コマンドを使って <code
                    class="filename">sdb2</code> に多少の領域を空ける必要があるかもしれません。
				</div></div><div
                class="para">
					LV が作成され、ブロックデバイスファイルとして <code
                  class="filename">/dev/mapper/</code> に現れます。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>ls -l /dev/mapper</code></strong>
<code
                  class="computeroutput">合計 0
crw------- 1 root root 10, 236  6月 10 16:52 control
lrwxrwxrwx 1 root root       7  6月 10 17:05 vg_critical-lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root       7  6月 10 17:05 vg_critical-lv_files -&gt; ../dm-0
lrwxrwxrwx 1 root root       7  6月 10 17:05 vg_normal-lv_backups -&gt; ../dm-2
# </code><strong
                  class="userinput"><code>ls -l /dev/dm-*</code></strong>
<code
                  class="computeroutput">brw-rw---T 1 root disk 253, 0  6月 10 17:05 /dev/dm-0
brw-rw---- 1 root disk 253, 1  6月 10 17:05 /dev/dm-1
brw-rw---- 1 root disk 253, 2  6月 10 17:05 /dev/dm-2
</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>NOTE</em></span> LVM ボリュームの自動検出</strong></p></div></div></div><div
                  class="para">
					コンピュータの起動時に、<code
                    class="filename">lvm2-activation</code> systemd サービスユニットは <code
                    class="command">vgchange -aay</code> を実行して VG を「始動」します。具体的に言えば、<code
                    class="filename">lvm2-activation</code> systemd サービスユニットは利用できるデバイスを探します。そして LVM サブシステムに LVM 用の PV として初期化されたデバイスが登録され、PV から構成される VG が開始され、VG から分割された LV が開始され、LV が利用できるようになります。このため、LVM ボリュームを作成したり変更する際に設定ファイルを編集する必要はありません。
				</div><div
                  class="para">
					しかしながら、LVM 要素 (PV、LV、GV) の配置図は <code
                    class="filename">/etc/lvm/backup</code> にバックアップされ、問題が起きた時 (見えないところで何が行われているかを確認したい時) に有益です。
				</div></div><div
                class="para">
					ブロックデバイスファイルを分かり易くするために、VG に対応するディレクトリの中に便利なシンボリックリンクが作成されます。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>ls -l /dev/vg_critical</code></strong>
<code
                  class="computeroutput">合計 0
lrwxrwxrwx 1 root root 7  6月 10 17:05 lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root 7  6月 10 17:05 lv_files -&gt; ../dm-0
# </code><strong
                  class="userinput"><code>ls -l /dev/vg_normal</code></strong>
<code
                  class="computeroutput">合計 0
lrwxrwxrwx 1 root root 7  6月 10 17:05 lv_backups -&gt; ../dm-2</code></pre><div
                class="para">
					LV は標準的なパーティションと全く同様に取り扱われます。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mkfs.ext4 /dev/vg_normal/lv_backups</code></strong>
<code
                  class="computeroutput">mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 3145728 4k blocks and 786432 inodes
Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d
[...]
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </code><strong
                  class="userinput"><code>mkdir /srv/backups</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mount /dev/vg_normal/lv_backups /srv/backups</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/backups</code></strong>
<code
                  class="computeroutput">ファイルシス                     サイズ  使用  残り 使用% マウント位置
/dev/mapper/vg_normal-lv_backups    12G   30M   12G    1% /srv/backups
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>cat /etc/fstab</code></strong>
<code
                  class="computeroutput">[...]
/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2
/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2
/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</code></pre><div
                class="para">
					アプリケーションにしてみれば、無数の小さなパーティションがわかり易い名前を持つ 1 つの大きな 12 GB のボリュームにまとめられたことになります。
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.lvm-over-time"></a>12.1.2.3. 経時変化に伴う LVM の利便性</h4></div></div></div><div
                class="para">
					LVM のパーティションや物理ディスクを統合する機能は便利ですが、これは LVM のもたらす主たる利点ではありません。時間経過に伴い LVM のもたらす柔軟性が特に重要になる時とは LV のサイズを増加させる必要が生じた時でしょう。ここまでの例を使い、LV に新たに巨大なファイルを保存したいけれども、ファイルサーバ用の LV はこの巨大なファイルを保存するには狭すぎると仮定しましょう。<code
                  class="filename">vg_critical</code> から分割できる全領域はまだ使い切られていないので、<code
                  class="filename">lv_files</code> のサイズを増やすことが可能です。LV のサイズを増やすために <code
                  class="command">lvresize</code> コマンドを使い、LV のサイズの変化にファイルシステムを対応させるために <code
                  class="command">resize2fs</code> を使います。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/files/</code></strong>
<code
                  class="computeroutput">ファイルシス                     サイズ  使用  残り 使用% マウント位置
/dev/mapper/vg_critical-lv_files   5.0G  4.6G  146M   97% /srv/files
# </code><strong
                  class="userinput"><code>lvdisplay -C vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 5.00g
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   2   2   0 wz--n- 8.09g 2.09g
# </code><strong
                  class="userinput"><code>lvresize -L 7G vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).
  Logical volume lv_files successfully resized
# </code><strong
                  class="userinput"><code>lvdisplay -C vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 7.00g
# </code><strong
                  class="userinput"><code>resize2fs /dev/vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">resize2fs 1.42.12 (29-Aug-2014)
Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.

# </code><strong
                  class="userinput"><code>df -h /srv/files/</code></strong>
<code
                  class="computeroutput">ファイルシス                     サイズ  使用  残り 使用% マウント位置
/dev/mapper/vg_critical-lv_files   6.9G  4.6G  2.1G   70% /srv/files</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>CAUTION</em></span> ファイルシステムのサイズ変更</strong></p></div></div></div><div
                  class="para">
					すべてのファイルシステムがオンラインでサイズを変更できるわけではありません。このため、ボリュームのサイズ変更前にファイルシステムをアンマウントし、ボリュームのサイズ変更後に再マウントしなければいけません。もちろん、ボリュームのサイズを小さくする場合、ボリューム上のファイルシステムのサイズを小さくした後にボリュームのサイズを小さくしなければいけません。ボリュームのサイズを大きくする場合、ボリュームのサイズを大きくした後にボリューム上のファイルシステムを大きくしなければいけません。これはかなりわかり易いです。なぜなら、ブロックデバイス上に存在するファイルシステムのサイズをブロックデバイスよりも大きくすることは絶対に不可能だからです (この原則はボリュームが物理パーティションか LV かに依存しません)。
				</div><div
                  class="para">
					ext3、ext4、xfs ファイルシステムはオンラインでサイズを増加させることすなわちアンマウントすることなくサイズを増加させることが可能です。しかし、サイズを減少させる場合はアンマウントを必要とします。reiserfs はオンラインでサイズを増加および減少することが可能です。ext2 は増加も減少も可能ですが、アンマウントを必要とします。
				</div></div><div
                class="para">
					同様の方法でデータベースをホストしている <code
                  class="filename">lv_base</code> のサイズを増加させます。以下の通り <code
                  class="filename">lv_base</code> の分割元である <code
                  class="filename">vg_critical</code> から分割できる領域は既にほぼ使い切った状態になっています。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/base/</code></strong>
<code
                  class="computeroutput">ファイルシス                    サイズ  使用  残り 使用% マウント位置
/dev/mapper/vg_critical-lv_base  1008M  854M  104M   90% /srv/base
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree 
  vg_critical   2   2   0 wz--n- 8.09g 92.00m</code></pre><div
                class="para">
					でもご安心ください。LVM を使っていれば新しい PV を既存の VG を構成する PV の 1 つとして追加することが可能です。たとえば、今までは LVM の外で管理されていた <code
                  class="filename">sdb1</code> パーティションには、<code
                  class="filename">lv_backups</code> に移動しても問題のないアーカイブだけが含まれていた点に気が付いたとしましょう。このため、<code
                  class="filename">sdb1</code> パーティションを <code
                  class="filename">vg_critical</code> を構成する PV の 1 つとして再利用することが可能です。こうすることで、<code
                  class="filename">vg_critical</code> から <code
                  class="filename">lv_base</code> に分割される領域のサイズを増やすことが可能です。これが <code
                  class="command">vgextend</code> コマンドの目的です。もちろん、事前に <code
                  class="filename">sdb1</code> パーティションを PV として準備しなければいけません。<code
                  class="filename">vg_critical</code> を拡張したら、先と同様のコマンドを使って先に <code
                  class="filename">lv_base</code> のサイズを増加させ、その後に <code
                  class="filename">lv_base</code> 上のファイルシステムのサイズを増加させます。
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvcreate /dev/sdb1</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdb1" successfully created
# </code><strong
                  class="userinput"><code>vgextend vg_critical /dev/sdb1</code></strong>
<code
                  class="computeroutput">  Volume group "vg_critical" successfully extended
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   3   2   0 wz--n- 9.09g 1.09g
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>df -h /srv/base/</code></strong>
<code
                  class="computeroutput">ファイルシス                    サイズ  使用  残り 使用% マウント位置
/dev/mapper/vg_critical-lv_base   2.0G  854M  1.1G   45% /srv/base</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>GOING FURTHER</em></span> LVM の上級活用</strong></p></div></div></div><div
                  class="para">
					LVM にはさらに上級の使い方があり、多くの設定事項を手作業で指定することが可能です。たとえば、管理者は PV と LV のブロックサイズおよびボリュームの物理的な配置を微調整することが可能です。また、ブロックを PV 間で移動することも可能です。これは、たとえば性能を微調整したり、よりありふれたケースではある物理ディスクに対応する PV を VG の構成要素から外したりするため (PV を別の VG に移動したり、完全に LVM から取り外したりするため) に行われます。コマンドを説明しているマニュアルページは基本的に明快で詳細です。手始めに、<span
                    class="citerefentry"><span
                      class="refentrytitle">lvm</span>(8)</span> マニュアルページを参照することをお勧めします。
				</div></div></div></div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      xmlns=""
                      id="sect.raid-or-lvm"></a>12.1.3. RAID それとも LVM?</h3></div></div></div><div
              class="para">
				1 番目の利用形態は用途が時間的に変化しない 1 台のハードディスクを備えたデスクトップコンピュータのような単純な利用形態です。この場合 RAID と LVM はどちらも疑う余地のない利点をもたらします。しかしながら、RAID と LVM は目標を分岐させて別々の道を歩んでいます。どちらを使うべきか悩むのは間違っていることではありません。最も適切な答えはもちろん現在の要求と将来に予測される要求に依存します。
			</div><div
              class="para">
				いくつかの状況では、疑問の余地がないくらい簡単に答えを出すことが可能です。2 番目の利用形態はハードウェア障害からデータを保護することが求められる利用形態です。この場合、ディスクの冗長性アレイ上に RAID をセットアップするのは明らかです。なぜなら LVM はこの種の問題への対応策を全く用意していないからです。逆に、柔軟なストレージ計画が必要でディスクの物理的な配置に依存せずにボリュームを構成したい場合、RAID はあまり役に立たず LVM を選ぶのが自然です。
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>NOTE</em></span> 性能が重要な場合</strong></p></div></div></div><div
                class="para">
				特にアクセス速度という意味の入出力速度が最重要な場合を考えてみましょう。LVM および RAID はどのような組み合わせで使っても性能にある程度の影響をおよぼします。このため、どの組み合わせを採用するかが議題に挙げられるかもしれません。しかしながら、どんな組み合わせを使っても性能差は極めて少なく、この程度の性能差が無視できない場合は極めて少ないと言えるでしょう。性能が重要な場合、実現できる最良の改善方針は非回転ストレージメディア (<a
                  id="id-1.15.4.8.4.2.1"
                  class="indexterm"></a><span
                  class="emphasis"><em>ソリッドステートドライブ</em></span>すなわち SSD) を使うことです。SSD のメガバイト当たりの費用は標準的なハードディスクドライブよりも高価で、SSD の容量は通常小さいですが、SSD はランダムアクセスで素晴らしい性能を発揮します。ファイルシステムに広く分散された位置から数多くの入出力を行うような場合 (たとえば複雑な問い合わせが定期的に実行されるデータベースの場合)、SSD 上にデータベースを置くほうが RAID over LVM または LVM over RAID 上にデータベースを置くよりも良好な性能が手に入ります。このような場合、純粋な速度だけでなく他の要素も検討した上で採用の可否を決定するべきです。なぜなら、性能が必要な場合に SSD を採用することは最も安直な解決策だからです。
			</div></div><div
              class="para">
				3 番目に注目すべき利用形態は単に 2 つのディスクを 1 つのボリュームにまとめるような利用形態です。性能が欲しかったり、利用できるディスクのどれよりも大きな単一のファイルシステムにしたい場合にこの利用形態が採用されます。この場合、RAID-0 (またはリニア RAID) か LVM ボリュームを使って対処できます。この状況では、追加的な制約事項 (たとえば、他のコンピュータが RAID だけを使っている場合に RAID を使わなければいけないなどの制約事項) がなければ、通常 LVM を選択すると良いでしょう。LVM の最初のセットアップは RAID に比べて複雑ですが、LVM は複雑度を少し増加させるだけで要求が変った場合や新しいディスクを追加する必要ができた場合に対処可能な追加的な柔軟性を大きく上昇させます。
			</div><div
              class="para">
				そしてもちろん、最後の本当に興味深い利用形態はストレージシステムにハードウェア障害に対する耐性を持たせさらにボリューム分割に対する柔軟性を持たせる必要がある場合の利用形態です。RAID と LVM のどちらも片方だけで両方の要求を満足させることは不可能です。しかし心配ありません。この要求を満足させるには RAID と LVM の両方を同時に使用する方針、正確に言えば一方の上に他方を構成する方針を採用すれば良いでのです。RAID と LVM の高い成熟度のおかげでほぼ標準になりつつある方針に従うならば、最初にディスクを少数の大きな RAID アレイにグループ分けすることでデータの冗長性を確保します。さらにそれらの RAID アレイを LVM の PV として使います。そして、ファイルシステム用の VG から分割された LV を論理パーティションとして使います。この標準的な方針の優れた点は、ディスク障害が起きた場合に再構築しなければいけない RAID アレイの数が少ない点です。このため、管理者は復旧に必要な時間を減らすことが可能です。
			</div><div
              class="para">
				ここで具体例を見てみましょう。たとえば Falcot Corp の広報課は動画編集用にワークステーションを必要としていますが、広報課の予算の都合上、最初から高性能のハードウェアに投資することは不可能です。このため、グラフィック性能を担うハードウェア (モニタとビデオカード) に大きな予算を割き、ストレージ用には一般的なハードウェアを使うことが決定されました。しかしながら、広く知られている通りデジタルビデオ用のストレージはある種の条件を必要とします。すなわち、保存されるデータのサイズが大きく、このデータを読み込みおよび書き込みする際の処理速度がシステム全体の性能にとって重要 (たとえば、平均的なアクセス時間よりも重要) という条件です。この条件を一般的なハードウェアを使って満足させる必要があります。今回の場合 2 台の SATA ハードディスクドライブを使います。さらに、システムデータと一部のユーザデータはハードウェア障害に対する耐性を持たせる必要があります。編集済みのビデオクリップを保護する必要はありますが、編集前のビデオ素材をそれほど気にする必要はありません。なぜなら、編集前のビデオ素材はまだビデオテープに残されているからです。
			</div><div
              class="para">
				前述の条件を満足させるために RAID-1 と LVM を組み合わせます。ディスクの並行アクセスを最適化し、そして障害が同時に発生する危険性を減らすために、各ディスクは 2 つの異なる SATA コントローラに接続されています。このため、各ディスクは <code
                class="filename">sda</code> と <code
                class="filename">sdc</code> として現れます。どちらのディスクも以下に示したパーティショニング方針に従ってパーティショニングされます。
			</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>fdisk -l /dev/sda</code></strong>
<code
                class="computeroutput">
Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x00039a9f

Device    Boot     Start       End   Sectors Size Id Type
/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect
/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris
/dev/sda3        4000185 586099395 582099210 298G 5  Extended
/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect
/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect
/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</code></pre><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
						<code
                      class="filename">sda1</code> と <code
                      class="filename">sdc1</code> パーティション (約 1 GB) から RAID-1 ボリューム <code
                      class="filename">md0</code> を構成します。<code
                      class="filename">md0</code> はルートファイルシステムを保存するために直接的に使われます。
					</div></li><li
                  class="listitem"><div
                    class="para">
						<code
                      class="filename">sda2</code> と <code
                      class="filename">sdc2</code> パーティションから swap パーティションを作成します。スワップ領域のサイズは合計で 2 GB になります。RAM のサイズ 1 GB と合わせれば、ワークステーションで利用できるメモリサイズは十分な量と言えます。
					</div></li><li
                  class="listitem"><div
                    class="para">
						<code
                      class="filename">sda5</code> と <code
                      class="filename">sdc5</code> パーティションおよび <code
                      class="filename">sda6</code> と <code
                      class="filename">sdc6</code> パーティションからそれぞれ約 100 GB の 2 つの新しい RAID-1 ボリューム <code
                      class="filename">md1</code> と <code
                      class="filename">md2</code> を構成します。<code
                      class="filename">md1</code> と <code
                      class="filename">md2</code> は LVM の PV として初期化され、これらの PV から <code
                      class="filename">vg_raid</code> VG を構成します。<code
                      class="filename">vg_raid</code> は約 200 GB の安全な領域になります。
					</div></li><li
                  class="listitem"><div
                    class="para">
						残りのパーティションである <code
                      class="filename">sda7</code> と <code
                      class="filename">sdc7</code> はそのまま LVM の PV として初期化され、これらの PV から <code
                      class="filename">vg_bulk</code> VG を構成します。<code
                      class="filename">vg_bulk</code> はおよそ 200 GB の領域になります。
					</div></li></ul></div><div
              class="para">
				VG を作成したら、VG をとても柔軟な方法で LV に分割することが可能です。<code
                class="filename">vg_raid</code> から分割された LV は 1 台のディスク障害に対して耐性を持ちますが、<code
                class="filename">vg_bulk</code> から分割された LV はディスク障害に対する耐性を持たない点を忘れないでください。逆に、<code
                class="filename">vg_bulk</code> は両方のディスクにわたって割り当てられるので、<code
                class="filename">vg_bulk</code> から分割された LV に保存された巨大なファイルの読み書き速度は高速化されるでしょう。
			</div><div
              class="para">
				<code
                class="filename">vg_raid</code> から <code
                class="filename">lv_usr</code>、<code
                class="filename">lv_var</code>、<code
                class="filename">lv_home</code> を分割し、各 LV に応じたファイルシステムをホストさせます。さらに、<code
                class="filename">vg_raid</code> からもう一つの大きな LV である <code
                class="filename">lv_movies</code> を分割し、<code
                class="filename">lv_movies</code> に編集済みの最終版の映像をホストさせます。また、<code
                class="filename">vg_bulk</code> からデジタルビデオカメラから取り出したデータ用の大きな <code
                class="filename">lv_rushes</code> と一時ファイル用の <code
                class="filename">lv_tmp</code> を分割します。<code
                class="filename">vg_raid</code> と <code
                class="filename">vg_bulk</code> のどちらから作業領域用の LV を分割するかは簡単に決められるものではありません。つまり、作業領域用の LV は良い性能を必要としますが、編集作業中にディスク障害が起きた場合に作業内容を保護する必要があるでしょうか? この質問の回答次第で、作業領域用の LV を <code
                class="filename">vg_raid</code> か <code
                class="filename">vg_bulk</code> のどちらの VG に作成するかが決まります。
			</div><div
              class="para">
				これで、重要なデータ用に多少の冗長性と、利用できる領域が用途ごとにどのように分割されるかに関する大きな柔軟性が確保されました。後から (たとえば音声クリップの編集用に) 新しいソフトウェアをインストールする場合も、<code
                class="filename">/usr/</code> をホストしている LV のサイズを簡単に増加することが可能です。
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>NOTE</em></span> なぜ 3 種類の RAID-1 ボリュームが必要なのでしょうか?</strong></p></div></div></div><div
                class="para">
				RAID-1 ボリュームを 1 つだけ作成し、作成した PV から <code
                  class="filename">vg_raid</code> を構成し、<code
                  class="filename">vg_raid</code> から保護したい内容用の LV を分割することも可能でした。それにも関わらず、なぜ 3 種類の RAID-1 ボリュームを作成したのでしょうか?
			</div><div
                class="para">
				最初の分割 (<code
                  class="filename">md0</code> とその他) の根本的理由はデータの安全性を考慮したためです。つまり RAID-1 ミラーを構成する要素に書き込まれるデータは要素同士で全く同じだからです。そのため RAID 層を迂回し、RAID-1 ミラーを構成する 1 台のディスクだけを直接マウントすることが可能です。すなわち、カーネルにバグがあったり LVM メタ情報が破壊されたりした場合でも、RAID と LVM ボリュームに含まれるディスクの配置などの重要なデータにアクセスするために最小限のシステムを起動することが可能ということです。そして、このメタ情報を再構成したりファイルにアクセスしたりすることが可能です。こうすることで、システムを正常状態に戻すことが可能です。
			</div><div
                class="para">
				2 番目の分割 (<code
                  class="filename">md1</code> と <code
                  class="filename">md2</code>) の根本的理由は明確というわけではありませんが、将来の不明確さを認めていることに関連します。動画編集用ワークステーションを最初に組み上げる時点で、要求される正確なストレージサイズを完全な精度で知ることは不可能です。それどころか、ストレージサイズは時間経過に従い増加するかもしれません。今回の場合、ビデオ素材用の <code
                  class="filename">lv_rushes</code> と編集済みビデオクリップ用の <code
                  class="filename">lv_movies</code> に対して実際に要求されるストレージサイズを事前に知ることはできません。とても大きな量の素材を必要とするクリップを <code
                  class="filename">lv_rushes</code> に保存する必要があり、さらに冗長性データ用 VG である <code
                  class="filename">vg_raid</code> のまだ半分以上が未使用状態ならば、<code
                  class="filename">vg_raid</code> から未使用領域を再利用することが可能です。具体的には <code
                  class="filename">vg_raid</code> の構成要素から片方の PV (たとえば <code
                  class="filename">md2</code>) を削除し、<code
                  class="filename">md2</code> を <code
                  class="filename">vg_bulk</code> を構成する PV として初期化するか (予想される作業時間が一時的な性能の低下を許容できる程度に十分短い場合に限ります)、<code
                  class="filename">md2</code> の RAID-1 セットアップを破棄してその構成要素である <code
                  class="filename">sda6</code> と <code
                  class="filename">sdc6</code> を <code
                  class="filename">vg_bulk</code> を構成する PV として初期化する (この場合 100 GB ではなく 200 GB の増加になります) ことが可能です。そして <code
                  class="filename">lv_rushes</code> を必要に応じて増加させることが可能です。
			</div></div></div></div></div><ul
        class="docnav"><li
          class="previous"><a
            accesskey="p"
            href="sect.rtc-services.html"><strong>戻る</strong>11.8. リアルタイムコミュニケーションサービス</a></li><li
          class="up"><a
            accesskey="u"
            href="#"><strong>上に戻る</strong></a></li><li
          class="home"><a
            accesskey="h"
            href="index.html"><strong>ホーム</strong></a></li><li
          class="next"><a
            accesskey="n"
            href="sect.virtualization.html"><strong>次へ</strong>12.2. 仮想化</a></li></ul></body></html>
