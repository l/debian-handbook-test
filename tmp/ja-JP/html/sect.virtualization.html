<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html
    xmlns="http://www.w3.org/1999/xhtml"><head><meta
        http-equiv="Content-Type"
        content="text/html; charset=UTF-8" /><title
        xmlns:d="http://docbook.org/ns/docbook">12.2. 仮想化</title><link
        rel="stylesheet"
        type="text/css"
        href="Common_Content/css/default.css" /><link
        rel="stylesheet"
        media="print"
        href="Common_Content/css/print.css"
        type="text/css" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="generator"
        content="publican v4.3.2" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="package"
        content="Debian-debian-handbook-9-ja-JP-1.0-1" /><meta
        name="keywords"
        content="RAID, LVM, FAI, Preseed, 監視, 仮想化, Xen, LXC" /><link
        rel="home"
        href="index.html"
        title="Debian 管理者ハンドブック" /><link
        rel="up"
        href="advanced-administration.html"
        title="第 12 章 高度な管理" /><link
        rel="prev"
        href="advanced-administration.html"
        title="第 12 章 高度な管理" /><link
        rel="next"
        href="sect.automated-installation.html"
        title="12.3. 自動インストール" /><meta
        xmlns=""
        name="flattr:id"
        content="4pz9jq" /><link
        xmlns=""
        rel="canonical"
        href="https://debian-handbook.info/browse/ja-JP/stable/sect.virtualization.html" /></head><body><div
        id="banner"><a
          href="http://debian-handbook.info/get/"><span
            class="text">Download the ebook</span></a></div><p
        id="title"><a
          class="left"
          href="http://www.debian.org"><img
            alt="Product Site"
            src="Common_Content/images//image_left.png" /></a><a
          class="right"
          href="index.html"><img
            alt="Documentation Site"
            src="Common_Content/images//image_right.png" /></a></p><ul
        class="docnav top"><li
          class="previous"><a
            accesskey="p"
            href="advanced-administration.html"><strong>戻る</strong></a></li><li
          class="home">Debian 管理者ハンドブック</li><li
          class="next"><a
            accesskey="n"
            href="sect.automated-installation.html"><strong>次へ</strong></a></li></ul><div
        class="section"><div
          class="titlepage"><div><div><h2
                class="title"><a
                  xmlns=""
                  id="sect.virtualization"></a>12.2. 仮想化</h2></div></div></div><a
          id="id-1.15.5.2"
          class="indexterm"></a><div
          class="para">
			仮想化は最近のコンピューティングにおける最も大きな進歩の 1 つです。仮想化という用語は、実際のハードウェアに対するさまざまな独立性の度合いを持つ仮想コンピュータを模倣するさまざまな抽象化と技術を指します。1 台の物理的なサーバが同時かつ隔離された状態で動く複数のシステムをホストすることが可能です。仮想化アプリケーションは数多く存在し、隔離された仮想システムを使うことができます。たとえば、さまざまに設定されたテスト環境を作ったり、安全性を確保する目的で異なる仮想マシン間でホストされたサービスを分離したりすることが可能です。
		</div><div
          class="para">
			複数の仮想化ソリューションが存在し、それぞれが利点と欠点を持っています。本書では Xen、LXC、KVM に注目しますが、他にも以下のような注目すべき実装が存在します。
		</div><a
          id="id-1.15.5.5"
          class="indexterm"></a><a
          id="id-1.15.5.6"
          class="indexterm"></a><a
          id="id-1.15.5.7"
          class="indexterm"></a><a
          id="id-1.15.5.8"
          class="indexterm"></a><a
          id="id-1.15.5.9"
          class="indexterm"></a><a
          id="id-1.15.5.10"
          class="indexterm"></a><div
          xmlns:d="http://docbook.org/ns/docbook"
          class="itemizedlist"><ul><li
              class="listitem"><div
                class="para">
					QEMU は完全なコンピュータを模倣するソフトウェアエミュレータです。このため QEMU の性能はネイティブに実行した場合の速度には遠くおよびませんが、QEMU を使うことで修正されていなかったり実験的なオペレーティングシステムをエミュレートされたハードウェア上で実行することが可能です。さらに QEMU は異なるハードウェアアーキテクチャをエミュレートすることが可能です。たとえば、<span
                  class="emphasis"><em>amd64</em></span> システムで <span
                  class="emphasis"><em>arm</em></span> コンピュータをエミュレートすることが可能です。QEMU はフリーソフトウェアです。<div
                  xmlns=""
                  class="url">→ <a
                    xmlns="http://www.w3.org/1999/xhtml"
                    href="http://www.qemu.org/">http://www.qemu.org/</a></div>
				</div></li><li
              class="listitem"><div
                class="para">
					Bochs は自由な仮想マシンですが、x86 アーキテクチャ (i386 と amd64) だけをエミュレートすることが可能です。
				</div></li><li
              class="listitem"><div
                class="para">
					VMWare はプロプライエタリの仮想マシンです。VMWare はこの分野で最も古く、最も広く使われているソフトウェアの 1 つです。VMWare は QEMU とよく似た原理で動いています。VMWare には実行中の仮想マシンのスナップショットなどの高度な機能が含まれています。<div
                  xmlns=""
                  class="url">→ <a
                    xmlns="http://www.w3.org/1999/xhtml"
                    href="http://www.vmware.com/">http://www.vmware.com/</a></div>
				</div></li><li
              class="listitem"><div
                class="para">
					VirtualBox はほぼ自由なソフトウェアの仮想マシンです (一部の追加的な構成要素はプロプライエタリライセンスの下で利用できます)。残念なことに VirtualBox は Debian の「contrib」セクションにあります。なぜなら VirtualBox にはいくつかのコンパイル済みファイルが含まれ、このファイルを再ビルドするにはプロプライエタリコンパイラが必要だからです。また、今のところ VirtualBox を含めることができるのは Debian 不安定版に限られます。なぜなら Oracle の方針に従うと Debian 安定版に VirtualBox を含めることはできないからです (<a
                  href="https://bugs.debian.org/794466">#794466</a> を参照してください)。VirtualBox は VMWare よりも歴史が浅く、i386 と amd64 アーキテクチャだけをサポートします。しかしながら、VirtualBox はスナップショットやその他の興味深い機能を備えています。<div
                  xmlns=""
                  class="url">→ <a
                    xmlns="http://www.w3.org/1999/xhtml"
                    href="http://www.virtualbox.org/">http://www.virtualbox.org/</a></div>
				</div></li></ul></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    xmlns=""
                    id="sect.xen"></a>12.2.1. Xen</h3></div></div></div><div
            class="para">
				Xen<a
              id="id-1.15.5.12.2.1"
              class="indexterm"></a> は「準仮想化」ソリューションです。Xen には薄い抽象化層が含まれ、この抽象化層は「ハイパーバイザ」と呼ばれ、ハードウェアとその上にあるシステムの間に位置します。さらにハイパーバイザは審判員として振る舞い、仮想マシンからハードウェアへのアクセスを制御します。しかしながら、Xen ハイパーバイザは命令のほんの一部だけを取り扱い、残りは Xen ハイパーバイザではなくハードウェアによって直接的に実行されます。こうすることによる主な有効性は性能が低下せず、システムがネイティブ速度に迫る性能を発揮するという点です。一方で欠点は Xen ハイパーバイザ上でオペレーティングシステムを実行するには実行されるオペレーティングシステムのカーネルを修正しなければいけないという点です。
			</div><div
            class="para">
				用語の解説に少し時間を割きましょう。Xen ハイパーバイザはカーネルよりも下層の最も低い層に位置し、ハードウェア上で直接動きます。Xen ハイパーバイザは残りのソフトウェアをいくつかの<span
              class="emphasis"><em>ドメイン</em></span>に分割することが可能で、<span
              class="emphasis"><em>ドメイン</em></span>は多数の仮想マシンと考えられます。これらのドメインの 1 つ (最初に起動されたもの) は <span
              class="emphasis"><em>dom0</em></span> と呼ばれ、特別な役割を担います。なぜなら、<span
              class="emphasis"><em>dom0</em></span> だけが Xen ハイパーバイザを制御することが可能だからです。他のドメインは <span
              class="emphasis"><em>domU</em></span> として知られています。ユーザ視点で言い換えれば、<span
              class="emphasis"><em>dom0</em></span> は他の仮想システムにおける「ホスト」、これに対して <span
              class="emphasis"><em>domU</em></span> は「ゲスト」になります。
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>CULTURE</em></span> Xen と Linux のさまざまなバージョン</strong></p></div></div></div><div
              class="para">
				当初 Xen は Linux 公式ツリーの外部パッチとして開発され、Linux カーネルに組み込まれていませんでした。これと同時期に、複数の次世代仮想化システム (KVM など) は Linux カーネルへの組み込みを簡単にするためにいくつかの包括的な仮想化関連関数を必要としており、さらに Linux カーネルが (<span
                class="emphasis"><em>paravirt_ops</em></span> または <span
                class="emphasis"><em>pv_ops</em></span> インターフェースとして知られる) 一連の仮想化関連関数を獲得しました。Xen のパッチはこのインターフェースのいくつかの機能を複製していたため、Xen のパッチは公式に受け入れられませんでした。
			</div><div
              class="para">
				このため、Xen を影で支える会社の Xensource は新しく Linux カーネルに取り込まれた仮想化関連関数を使って Xen を移植しなければいけませんでした。この移植作業により、Xen のパッチを公式の Linux カーネルに取り込むことが可能になりました。この移植作業は多くのコードを書き換えることを意味していました。Xensource はすぐに paravirt_ops インターフェースを使って評価版を作ったにも関わらず、Xen のパッチを公式カーネルにマージする作業はゆっくりと進みました。マージか完了したのは Linux 3.0 です。<div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://wiki.xenproject.org/wiki/XenParavirtOps">http://wiki.xenproject.org/wiki/XenParavirtOps</a></div>
			</div><div
              class="para">
				<span
                class="distribution distribution">Jessie</span> は Linux カーネルのバージョン 3.16 に基づくため、標準的な <span
                class="pkg pkg">linux-image-686-pae</span> と <span
                class="pkg pkg">linux-image-amd64</span> パッケージには Xen を動作させるために必要なコードが含まれます。Debian の <span
                class="distribution distribution">Squeeze</span> およびそれ以前のバージョンで必要とされていたディストリビューションに特有のパッチ作業はもはや必要ありません。<div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix">http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix</a></div>
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>NOTE</em></span> Xen 互換のアーキテクチャ</strong></p></div></div></div><div
              class="para">
				現在のところ、Xen を利用できるのは i386、amd64、arm64、armhf アーキテクチャだけです。
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>CULTURE</em></span> Xen と非 Linux カーネル</strong></p></div></div></div><div
              class="para">
				Xen 上でオペレーティングシステムを動作させるには、いかなるオペレーティングシステムであってもそれを修正する必要があります。さらに、すべてのオペレーティングシステムのカーネルが修正点に関して同じ程度の成熟度を持っているとは限りません。多くのオペレーティングシステムは dom0 および domU として完全に動作します。具体的に言えば、Linux 3.0 とそれ以降、NetBSD 4.0 とそれ以降、OpenSolaris は dom0 および domU として完全に動作します。また、他のオペレーティングシステムは dom0 としては動作せず domU として動作します。dom0 および domU として動作するオペレーティングシステムの状況を確認するには Xen のウィキに含まれる該当ページをご覧ください。<div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen">http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen</a></div><div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://wiki.xenproject.org/wiki/DomU_Support_for_Xen">http://wiki.xenproject.org/wiki/DomU_Support_for_Xen</a></div>
			</div><div
              class="para">
				しかしながら、Xen で仮想化専用のハードウェア機能 (最近のプロセッサだけが搭載した機能) に依存するオペレーティングシステムを動作させる場合や、修正されていないオペレーティングシステム (Windows など) を動作させる場合、そのオペレーティングシステムは domU としてのみ動作します。
			</div></div><div
            class="para">
				Debian の下で Xen を使うには 3 つの要素が必要です。
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						Xen ハイパーバイザ自身。適切なパッケージは利用できるハードウェアによって決まります。すなわち <span
                    class="pkg pkg">xen-hypervisor-4.4-amd64</span>、<span
                    class="pkg pkg">xen-hypervisor-4.4-armhf</span>、<span
                    class="pkg pkg">xen-hypervisor-4.4-arm64</span> のうちどれか 1 つが必要です。
					</div></li><li
                class="listitem"><div
                  class="para">
						ハイパーバイザ上で実行するカーネル。バージョン 3.0 より新しい Linux カーネルが動作します。<span
                    class="distribution distribution">Jessie</span> に含まれる Linux カーネルのバージョンは 3.16 なのでこれも動作します。
					</div></li><li
                class="listitem"><div
                  class="para">
						さらに i386 アーキテクチャでは、Xen を活用するための適切なパッチを組み込んだ標準的なライブラリが必要です。このライブラリは <span
                    class="pkg pkg">libc6-xen</span> パッケージに含まれます。
					</div></li></ul></div><div
            class="para">
				複数の構成要素を手作業で選択するという煩わしさを避けるために、いくつかの便利なパッケージ (<span
              class="pkg pkg">xen-linux-system-amd64</span> など) が用意されています。これらのパッケージをインストールすることで、適切な Xen ハイパーバイザとカーネルパッケージが既知の良い組み合わせで導入されます。ここで導入される Xen ハイパーバイザには <span
              class="pkg pkg">xen-utils-4.4</span> が含まれます。<span
              class="pkg pkg">xen-utils-4.4</span> パッケージには dom0 からハイパーバイザを操作するためのツールが含まれます。同様に、<span
              class="pkg pkg">xen-utils-4.4</span> パッケージには適切な標準的ライブラリが含まれます。すべてのインストール中に、設定スクリプトは Grub ブートローダメニューに新しいエントリを作成します。こうすることで Xen dom0 から選択されたカーネルを開始することが可能です。しかしながら、通常このエントリはリストの最初に置かれないため、デフォルトで選択されません。この点に注意してください。これを望まない場合、以下のコマンドを使って変更してください。
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen
</code></strong><code
              class="computeroutput"># </code><strong
              class="userinput"><code>update-grub
</code></strong></pre><div
            class="para">
				これらの前提要件をインストールしたら、次に dom0 の挙動をテストします。テストを行うには、Xen ハイパーバイザと Xen カーネルの再起動が必要です。システムは標準的な方法で起動するべきです。初期化の早い段階でコンソールにいくつかの追加的メッセージが表示されます。
			</div><div
            class="para">
				これで、実用システムを domU システムに実際にインストールできるようになりました。これを行うには <span
              class="pkg pkg">xen-tools</span> に含まれるツールを使います。<span
              class="pkg pkg">xen-tools</span> パッケージには <code
              class="command">xen-create-image</code> コマンドが含まれます。<code
              class="command">xen-create-image</code> コマンドはインストール作業の大部分を自動化します。必須のパラメータは <code
              class="literal">--hostname</code> だけで、このパラメータは domU の名前を設定します。他のオプションは重要ですが、オプションを <code
              class="filename">/etc/xen-tools/xen-tools.conf</code> 設定ファイルに保存することが可能です。そして、コマンドラインでオプションを指定しなくてもエラーは起きません。このため、イメージを作る前にこのファイルの内容を確認するか、<code
              class="command">xen-create-image</code> の実行時に追加的パラメータを使うことが重要です。以下に注目すべき重要なパラメータを示します。
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--memory</code>。新たに作成する domU システム専用の RAM のサイズを指定します。
					</div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--size</code> と <code
                    class="literal">--swap</code>。domU で利用できる「仮想ディスク」のサイズを定義します。
					</div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--debootstrap</code>。<code
                    class="command">debootstrap</code> を使って新しいシステムをインストールします。このオプションを使う場合、<code
                    class="literal">--dist</code> オプション (ディストリビューションの名前、たとえば <span
                    class="distribution distribution">jessie</span>) を一緒に使うことが多いです。
					</div><div
                  class="sidebar"><div
                    class="titlepage"><div><div><p
                          class="title"><strong><span
                              class="emphasis"><em>GOING FURTHER</em></span> 非 Debian システムを domU にインストール</strong></p></div></div></div><div
                    class="para">
						非 Linux システムを domU にインストールする場合、<code
                      class="literal">--kernel</code> オプションを使って、domU で使うカーネルを定義しなければいけない点に注意してください。
					</div></div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--dhcp</code>。domU のネットワーク設定を DHCP で取得することを宣言します。対して、<code
                    class="literal">--ip</code> は静的 IP アドレスを定義します。
					</div></li><li
                class="listitem"><div
                  class="para">
						最後に、作成されるイメージ (domU からはハードディスクドライブに見えるイメージ) の保存方法を選択します。最も簡単な方法は、<code
                    class="literal">--dir</code> オプションを使い、各 domU を格納するデバイス用のファイルを dom0 上に作成する方法です。LVM を使っているシステムでは、<code
                    class="literal">--lvm</code> オプションを使い、VG の名前を指定しても良いでしょう。この場合 <code
                    class="command">xen-create-image</code> は指定された VG から新しい LV を分割し、この LV をハードディスクドライブとして domU から利用できるようにします。
					</div><div
                  class="sidebar"><div
                    class="titlepage"><div><div><p
                          class="title"><strong><span
                              class="emphasis"><em>NOTE</em></span> domU 内のストレージ</strong></p></div></div></div><div
                    class="para">
						ハードディスク全体、パーティション、RAID アレイ、既存の LVM の LV を domU に書き出すことも可能です。<code
                      class="command">xen-create-image</code> を使ってもこれらの操作を自動化することは不可能ですが、<code
                      class="command">xen-create-image</code> を使って Xen イメージの設定ファイルを作成した後にその設定ファイルを編集することで対応する操作が可能です。
					</div></div></li></ul></div><div
            class="para">
				これらを選んだ後、将来の Xen domU 用のイメージを作成することが可能です。
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</code></strong>
<code
              class="computeroutput">
[...]
General Information
--------------------
Hostname       :  testxen
Distribution   :  jessie
Mirror         :  http://ftp.debian.org/debian/
Partitions     :  swap            128Mb (swap)
                  /               2G    (ext3)
Image type     :  sparse
Memory size    :  128Mb
Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64
Initrd path    :  /boot/initrd.img-3.16.0-4-amd64
[...]
Logfile produced at:
         /var/log/xen-tools/testxen.log

Installation Summary
---------------------
Hostname        :  testxen
Distribution    :  jessie
MAC Address     :  00:16:3E:8E:67:5C
IP-Address(es)  :  dynamic
RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b
Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ
</code></pre><div
            class="para">
				これで仮想マシンが作成されましたが、仮想マシンはまだ実行されていません (このため dom0 のハードディスク上の領域が使われているだけです)。もちろん、異なるパラメータを使ってより多くのイメージを作成することが可能です。
			</div><div
            class="para">
				仮想マシンを起動する前に、仮想マシンにアクセスする方法を定義します。もちろん仮想マシンは隔離されたマシンですから、仮想マシンにアクセスする唯一の方法はシステムコンソールだけです。しかし、システムコンソールだけで要求を満足できることはほとんどないと言っても過言ではありません。ほとんどの時間、domU はリモートサーバとして機能し、ネットワークを通じてのみアクセスされます。しかしながら、各 domU 専用のネットワークカードを追加するのはかなり不便です。このため Xen は仮想インターフェースの作成機能を備えています。各ドメインは仮想インターフェースを参照し、標準的な方法で使うことが可能です。これらのネットワークカードは仮想的なものですが、ネットワークに接続されている状況下でのみ役に立つという点に注意してください。Xen は以下に挙げる複数のネットワークモデルを備えています。
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						最も単純なモデルは <span
                    class="emphasis"><em>bridge</em></span> モデルです。この場合、すべての eth0 ネットワークカードが (dom0 と domU システムに含まれるものも含めて) 直接的にイーサネットスイッチに接続されているかのように振る舞います。
					</div></li><li
                class="listitem"><div
                  class="para">
						2 番目に単純なモデルは <span
                    class="emphasis"><em>routing</em></span> モデルです。これは dom0 が domU システムと (物理) 外部ネットワークの間に位置するルータとして振る舞うモデルです。
					</div></li><li
                class="listitem"><div
                  class="para">
						最後が <span
                    class="emphasis"><em>NAT</em></span> モデルです。これは dom0 が domU システムとその他のネットワークの間に位置するモデルですが、domU システムに外部から直接アクセスすることは不可能です。dom0 の行ういくつかのネットワークアドレス変換がトラフィックを仲介します。
					</div></li></ul></div><div
            class="para">
				これら 3 種類のネットワークノードは <code
              class="filename">vif*</code>、<code
              class="filename">veth*</code>、<code
              class="filename">peth*</code>、<code
              class="filename">xenbr0</code> などの独特な名前を付けられた数多くのインターフェースと関係を持ちます。Xen ハイパーバイザは定義された配置に従いユーザ空間ツールの制御の下でインターフェースを準備します。NAT と routing モデルは特定の場合にのみ適合します。このためわれわれは bridge モデルを使います。
			</div><div
            class="para">
				Xen パッケージの標準的な設定はシステム全体のネットワーク設定を変更しません。しかしながら、<code
              class="command">xend</code> デーモンは既存のネットワークブリッジの中に仮想ネットワークインターフェースを組み込むように設定されています (複数のブリッジが存在する場合 <code
              class="filename">xenbr0</code> を優先します)。このためここでは <code
              class="filename">/etc/network/interfaces</code> の中にブリッジをセットアップして (<span
              class="pkg pkg">bridge-utils</span> パッケージをインストールする必要があります。このため <span
              class="pkg pkg">bridge-utils</span> パッケージは <span
              class="pkg pkg">xen-utils-4.4</span> パッケージの推奨パッケージになっています)、既存の eth0 エントリを置き替えます。
			</div><pre
            class="programlisting">auto xenbr0
iface xenbr0 inet dhcp
    bridge_ports eth0
    bridge_maxwait 0
</pre><div
            class="para">
				再起動して、ブリッジが自動的に作成されることを確認します。この後 Xen 制御ツール、特に <code
              class="command">xl</code> コマンドを使って domU を起動することが可能です。また、<code
              class="command">xl</code> を使ってドメインを表示、起動、終了するなどの操作を行うことが可能です。
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xl list</code></strong>
<code
              class="computeroutput">Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   463     1     r-----      9.8
# </code><strong
              class="userinput"><code>xl create /etc/xen/testxen.cfg</code></strong>
<code
              class="computeroutput">Parsing config from /etc/xen/testxen.cfg
# </code><strong
              class="userinput"><code>xl list</code></strong>
<code
              class="computeroutput">Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   366     1     r-----     11.4
testxen                                      1   128     1     -b----      1.1</code></pre><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>TOOL</em></span> Xen VM を管理するツールスタックの選択</strong></p></div></div></div><a
              id="id-1.15.5.12.24.2"
              class="indexterm"></a><a
              id="id-1.15.5.12.24.3"
              class="indexterm"></a><div
              class="para">
				Debian 7 および Debian 7 よりも古いリリースでは、<code
                class="command">xm</code> が Xen 仮想マシンの管理に使うための標準的なコマンドラインツールでした。現在 <code
                class="command">xm</code> はほぼ後方互換性を持つ <code
                class="command">xl</code> によって置き換えられました。しかし、利用できるツールは <code
                class="command">xm</code> および <code
                class="command">xl</code> だけではありません。代替ツールとしては、libvirt を操作する <code
                class="command">virsh</code> と XenServer (Xen の商用版) の XAPI を操作する <code
                class="command">xe</code> が存在します。
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>CAUTION</em></span> 1 つのイメージに 1 台以上の domU を割り当てないでください!</strong></p></div></div></div><div
              class="para">
				もちろん複数の domU システムを並列実行させることは可能ですが、各 domU システムは専用のイメージを必要とします。なぜなら、各 domU は自分に割り当てられたハードウェアを専有するという仮定に基づいて実行されるからです (ハイパーバイザとやり取りするカーネルの一部分は別です)。特に、ストレージ領域を共有する目的で 2 つの domU システムを同時に起動することは不可能です。2 つの domU システムが同時に起動していなければ、両者はストレージ領域を共有して、単独のスワップパーティションや <code
                class="filename">/home</code> ファイルシステムをホストしているパーティションを再利用することが可能です。
			</div></div><div
            class="para">
				<code
              class="filename">testxen</code> domU は仮想メモリではなく RAM から取った物理メモリを使います。このメモリ領域は <code
              class="filename">testxen</code> domU が起動していなければ dom0 が使えるメモリ領域だったという点に注意してください。このため、サーバを作ることが Xen インスタンスをホストすることを意味する場合、それに応じて十分なサイズの物理 RAM が必要になるという点に注意が必要です。
			</div><div
            class="para">
				おめでとうございます! 仮想マシンが開始されました。仮想マシンにアクセスするには 2 種類の方法があります。通常の方法は、真のマシンに接続するのと同様に、ネットワークを介して「リモートで」仮想マシンに接続することです。そしてこれを行うには、通常別の DHCP サーバや DNS 設定をセットアップすることが必要です。別の方法は <code
              class="command">xl console</code> コマンドから <code
              class="filename">hvc0</code> コンソールを使う方法です。ネットワーク設定が正しくない場合にはこれが唯一の方法です。
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xl console testxen</code></strong>
<code
              class="computeroutput">[...]

Debian GNU/Linux 8 testxen hvc0

testxen login: </code></pre><div
            class="para">
				仮想マシンのキーボードの前に座っているかのごとくセッションを開くことが可能です。このコンソールからデタッチするには、<span
              class="keycap"><strong>Control</strong></span>+<span
              class="keycap"><strong>]</strong></span> キーの組み合わせを使用します。
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>TIP</em></span> すぐにコンソールを開始する</strong></p></div></div></div><div
              class="para">
				domU システムの開始直後にコンソールを始めたい場合があります。そしてこの希望に応えるために <code
                class="command">xl create</code> コマンドは <code
                class="literal">-c</code> オプションを備えています。<code
                class="literal">-c</code> オプションを付けて domU を開始すれば、システム起動時に表示されるすべてのメッセージを見ることが可能です。
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>TOOL</em></span> OpenXenManager</strong></p></div></div></div><div
              class="para">
				OpenXenManager (<span
                class="pkg pkg">openxenmanager</span> パッケージに含まれます) はグラフィカルインターフェースです。これ使うことで Xen API を介して Xen ドメインのリモート管理することが可能です。このため、Xen ドメインをリモートで制御することが可能です。OpenXenManager は <code
                class="command">xl</code> コマンドの機能のほとんどを備えています。
			</div></div><div
            class="para">
				domU の起動完了後、domU は他のサーバと同様に使うことが可能です (domU は結局 GNU/Linux システムに過ぎません)。しかしながら、domU の仮想マシンの状態はいくつかの追加的機能を備えています。たとえば、<code
              class="command">xl pause</code> と <code
              class="command">xl unpause</code> コマンドを使って domU を一時的に停止したり再開することが可能です。一時的に停止された domU は全くプロセッサを使いませんが、割り当てられたメモリを解放しません。<code
              class="command">xl save</code> と <code
              class="command">xl restore</code> コマンドを考慮することは興味深いかもしれません。なぜなら <code
              class="command">xl save</code> で domU を保存すれば domU の使っていた RAM などの資源が解放されるからです。また、<code
              class="command">xl restore</code> で domU を元に戻す時 (ついでに言えば <code
              class="command">xl unpause</code> で再開する時)、domU は時間が経過したことに全く気が付きません。dom0 を停止した時に domU が動いていた場合、パッケージに含まれるスクリプトが自動的に <code
              class="command">xl save</code> で domU を保存し、dom0 の次回起動時に自動的に <code
              class="command">xl restore</code> で domU を再開します。もちろんこれにはラップトップコンピュータをハイバネートする場合と同様の標準的な不便さがあります。特に、domU が長い間一時停止されていた場合、ネットワーク接続が切断される可能性があります。今現在 Xen は ACPI 電源管理のほとんどに互換性がない点にも注意してください。このため、ホスト (dom0) システムを一時停止することは不可能です。
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>DOCUMENTATION</em></span> <code
                        class="command">xl</code> のオプション</strong></p></div></div></div><div
              class="para">
				<code
                class="command">xl</code> サブコマンドのほとんどは domU の名前などの 1 つか複数個の引数を取ります。これらの引数は <span
                class="citerefentry"><span
                  class="refentrytitle">xl</span>(1)</span> マニュアルページは詳しく説明されています。
			</div></div><div
            class="para">
				domU を停止したり再起動するには、domU の内部から (<code
              class="command">shutdown</code> コマンドを使って) 行ったり、dom0 から <code
              class="command">xl shutdown</code> または <code
              class="command">xl reboot</code> を使って行うことも可能です。
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>GOING FURTHER</em></span> Xen の上級活用</strong></p></div></div></div><div
              class="para">
				Xen はここで示すことができた数項だけにとどまらない多くの機能を持っています。特に、Xen はシステムを動的に変更することが可能です。すなわちドメインに対する多くのパラメータ (割り当てメモリサイズ、見えるハードドライブ、タスクスケジューラの挙動など) をドメインの実行中に調整することが可能です。domU はシャットダウンすることもネットワーク接続を失うこともなくサーバ間を移動することさえ可能です! すべての上級活用法に関して、最良の情報源は公式の Xen 文書です。<div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://www.xen.org/support/documentation.html">http://www.xen.org/support/documentation.html</a></div>
			</div></div></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    xmlns=""
                    id="sect.lxc"></a>12.2.2. LXC</h3></div></div></div><a
            id="id-1.15.5.13.2"
            class="indexterm"></a><div
            class="para">
				LXC は「仮想マシン」を作るために使われるにも関わらず、厳密に言うと仮想システムではなく、同じホスト上で実行されるプロセスのグループを隔離するためのシステムです。LXC は近年 Linux カーネルに対して行われた数々の機能の利点を活用しています。これらの機能はまとめて <span
              class="emphasis"><em>control groups</em></span> として知られています。<span
              class="emphasis"><em>control groups</em></span> を使うことにより、「グループ」と呼ばれるさまざまなプロセス群に対してシステム全体の特定の側面の状態を強制することが可能です。中でも最も注目すべき側面はプロセス ID、ネットワーク接続、マウントポイントです。隔離されたプロセスのグループはシステムの他のプロセスにアクセスできませんし、グループによるファイルシステムへのアクセスを特定の一部に限定することが可能です。さらにグループにネットワークインターフェースとルーティングテーブルを設定することにより、グループがシステム上の利用できるデバイスの一部だけを見えるように設定することが可能です。
			</div><div
            class="para">
				これらの機能を組み合わせることで、<code
              class="command">init</code> プロセスから起動されたすべてのプロセスファミリーを隔離することが可能です。その結果、仮想マシンにとてもよく似たものが作られます。このようなセットアップの正式名称が「コンテナ」です (LXC の名称 <span
              class="emphasis"><em>LinuX Containers</em></span> はこれに由来しています)。Xen や KVM が提供する「真の」仮想マシンとのより重要な違いは仮想マシン用のカーネルがない点です。このため、コンテナはホストシステムと全く同じカーネルを使います。これには利点と欠点があります。すなわち、利点はオーバーヘッドが全くないことで素晴らしい性能を得ることが可能という点とカーネルはシステムで実行しているすべてのプロセスを見ることが可能という点です。このため 2 つの独立したカーネルが異なるタスクセットでスケジュールを行うよりも効果的なスケジューリングが可能です。欠点の最たるものはコンテナの中で異なるカーネルを動作させることが不可能という点です (異なる Linux バージョンや異なるオペレーティングシステムを同時に動かすことができません)。
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>NOTE</em></span> LXC の隔離制限</strong></p></div></div></div><div
              class="para">
				LXC コンテナは負荷の大きなエミュレータやバーチャライザが備える隔離機能を備えていません。たとえば以下のような機能を備えていません。
			</div><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
						カーネルはホストシステムとコンテナによって共有されているため、コンテナ内に隔離されているプロセスはカーネルメッセージにアクセスできます。このことにより、メッセージがコンテナによって発せられた場合、情報が漏洩する可能性があります。
					</div></li><li
                  class="listitem"><div
                    class="para">
						同様の理由で、コンテナが不正アクセスされカーネルの脆弱性が悪用された場合、他のコンテナが影響を受ける可能性があります。
					</div></li><li
                  class="listitem"><div
                    class="para">
						ファイルシステムについて、カーネルはユーザとグループの数値的な識別子に従ってパーミッションを確認します。これらの識別子の意味するユーザとグループはコンテナごとに異なります。ファイルシステムの書き込み可能な部分がコンテナ同士で共有されている場合、この点を覚えておくべきです。
					</div></li></ul></div></div><div
            class="para">
				LXC による隔離は単純な仮想化と異なるため、LXC コンテナを設定することは仮想マシン上で単純に debian-installer を実行するよりも複雑な作業です。このため、いくつかの必要条件を説明した後、ネットワーク設定を行います。こうすることで、コンテナの中で実行するシステムを実際に作成することが可能です。
			</div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.13.7"></a>12.2.2.1. 準備段階</h4></div></div></div><div
              class="para">
					<span
                class="pkg pkg">lxc</span> パッケージには LXC を実行するために必要なツールが含まれるため、必ずこのパッケージをインストールしなければいけません。
				</div><div
              class="para">
					LXC を使うには <span
                class="emphasis"><em>control groups</em></span> 設定システムが必要で、<code
                class="filename">/sys/fs/cgroup</code> に仮想ファイルシステムをマウントする必要があります。Debian 8 からは init システムとして systemd が採用されており、systemd は <span
                class="emphasis"><em>control groups</em></span> に依存しているため、設定せずとも <code
                class="filename">/sys/fs/cgroup</code> は起動時に自動でマウントされます。
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="sect.lxc.network"></a>12.2.2.2. ネットワークの設定</h4></div></div></div><div
              class="para">
					LXC をインストールする目的は仮想マシンをセットアップすることです。もちろん、仮想マシンをネットワークから隔離するように設定したり、ファイルシステムを介してのみ情報をやり取りするように設定することも可能ですが、コンテナに対して少なくとも最低限のネットワークアクセスを提供するように設定するのが一般的です。典型的な場合、各コンテナにはブリッジを介して実際のネットワークに接続された仮想ネットワークインターフェースが備えられています。この仮想インターフェースは、直接ホスト上の物理ネットワークインターフェースに接続されているか (この場合、コンテナは直接ネットワークに接続されています)、ホスト上に定義された他の仮想インターフェースに接続されています (ホストからトラフィックをフィルタしたり配送することが可能です)。どちらの場合も、<span
                class="pkg pkg">bridge-utils</span> パッケージが必要です。
				</div><div
              class="para">
					最も簡単なやり方は <code
                class="filename">/etc/network/interfaces</code> を編集することです。物理インターフェース (たとえば <code
                class="literal">eth0</code>) に関する設定をブリッジインターフェース (通常 <code
                class="literal">br0</code>) に変え、物理とブリッジインターフェース間のリンクを設定します。たとえば、最初にネットワークインターフェース設定ファイルが以下のようなエントリを持っていたとします。
				</div><pre
              class="programlisting">auto eth0
iface eth0 inet dhcp</pre><div
              class="para">
					このエントリを無効化し、以下の通り書き換えます。
				</div><pre
              class="programlisting">#auto eth0
#iface eth0 inet dhcp

auto br0
iface br0 inet dhcp
  bridge_ports eth0</pre><div
              class="para">
					この設定により、コンテナをホストと同じ物理ネットワークに接続されたマシンとして考えた場合と、同様の効果が得られます。この「ブリッジ」設定はすべてのブリッジされたインターフェース間のイーサネットフレームの通過を管理します。これには物理的な <code
                class="literal">eth0</code> およびコンテナ用に定義されたインターフェースが含まれます。
				</div><div
              class="para">
					この設定を使うことができない場合 (たとえば、公開 IP アドレスをコンテナに割り当てることができない場合)、仮想 <span
                class="emphasis"><em>tap</em></span> インターフェースを作成し、これをブリッジに接続します。これと等価なネットワークトポロジーは、ホストの 2 番目のネットワークカードが分離されたスイッチに接続されている状態です。コンテナはこのスイッチに接続されています。コンテナが外部と通信するには、ホストがコンテナ用のゲートウェイとして振る舞わなければいけません。
				</div><div
              class="para">
					この「ぜいたくな」設定を行うには <span
                class="pkg pkg">bridge-utils</span> と <span
                class="pkg pkg">vde2</span> パッケージが必要です。<code
                class="filename">/etc/network/interfaces</code> ファイルは以下のようになります。
				</div><pre
              class="programlisting"># eth0 インターフェースは同じものを使います
auto eth0
iface eth0 inet dhcp

# 仮想インターフェース
auto tap0
iface tap0 inet manual
  vde2-switch -t tap0

# コンテナ用のブリッジ
auto br0
iface br0 inet static
  bridge_ports tap0
  address 10.0.0.1
  netmask 255.255.255.0</pre><div
              class="para">
					コンテナのネットワークは静的またはコンテナのホスト上で動く DHCP サーバを使って動的に設定されます。また、DHCP サーバを <code
                class="literal">br0</code> インターフェースを介した問い合わせに応答するように設定する必要があります。
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.13.9"></a>12.2.2.3. システムのセットアップ</h4></div></div></div><div
              class="para">
					それではコンテナがファイルシステムを使うようにファイルシステムを設定しましょう。コンテナという「仮想マシン」はハードウェア上で直接的に実行されないため、標準的なファイルシステムに比べていくつかの微調整を必要とします。これは特にカーネル、デバイス、コンソールが該当します。幸いなことに、<span
                class="pkg pkg">lxc</span> にはこの設定をほぼ自動化するスクリプトが含まれます。たとえば、以下のコマンド (<span
                class="pkg pkg">debootstrap</span> と <span
                class="pkg pkg">rsync</span> パッケージが必要です) で Debian コンテナがインストールされます。
				</div><pre
              class="screen"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-create -n testlxc -t debian
</code></strong><code
                class="computeroutput">debootstrap は /usr/sbin/debootstrap です
Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... 
Downloading debian minimal ...
I: Retrieving Release 
I: Retrieving Release.gpg 
[...]
Download complete.
Copying rootfs to /var/lib/lxc/testlxc/rootfs...
[...]
Root password is 'sSiKhMzI', please change !
root@mirwiz:~# </code>
</pre><div
              class="para">
					ファイルシステムは最初に <code
                class="filename">/var/cache/lxc</code> の中に作成され、その後目的のディレクトリに移動されます。こうすることで、同一のコンテナが極めて素早く作成されます。なぜなら、単純にコピーするだけだからです。
				</div><div
              class="para">
					この debian テンプレート作成スクリプトは、インストールされるシステムのアーキテクチャを指定する <code
                class="option">--arch</code> オプションと、現在の Debian 安定版以外の物をインストールしたい場合に指定する <code
                class="option">--release</code> オプションを取ります。また、<code
                class="literal">MIRROR</code> 環境変数を設定してローカル Debian アーカイブミラーを指定することも可能です。
				</div><div
              class="para">
					これで、新規に作成されたファイルシステムが最低限の Debian システムを含むようになりました。デフォルト状態だとこのコンテナにはネットワークインターフェースがありません (ループバックインターフェースすらありません)。これは全く望むべき状態ではないため、コンテナの設定ファイル (<code
                class="filename">/var/lib/lxc/testlxc/config</code>) を編集し、いくつかの <code
                class="literal">lxc.network.*</code> エントリを追加します。
				</div><pre
              class="programlisting">lxc.network.type = veth
lxc.network.flags = up
lxc.network.link = br0
lxc.network.hwaddr = 4a:49:43:49:79:20</pre><div
              class="para">
					これらのエントリの意味するところはそれぞれ、仮想インターフェースはコンテナによって作られます。そして仮想インターフェースはコンテナが開始された時に自動的に利用できる状態にされます。そして仮想インターフェースはホストの <code
                class="literal">br0</code> ブリッジに自動的に接続されます。さらに仮想インターフェースの MAC アドレスは指定したものになります。最後のエントリを削除するか無効化した場合、ランダムな MAC アドレスが生成されます。
				</div><div
              class="para">
					以下のようにすることで、設定ファイル内でホスト名を設定することも可能です。
				</div><pre
              class="programlisting">lxc.utsname = testlxc</pre></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.13.10"></a>12.2.2.4. コンテナの開始</h4></div></div></div><div
              class="para">
					これで仮想マシンイメージの準備が整いました。それではコンテナを開始しましょう。
				</div><pre
              class="screen scale"
              width="94"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-start --daemon --name=testlxc
</code></strong><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-console -n testlxc
</code></strong><code
                class="computeroutput">Debian GNU/Linux 8 testlxc tty1

testlxc login: </code><strong
                class="userinput"><code>root</code></strong><code
                class="computeroutput">
Password: 
Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
root@testlxc:~# </code><strong
                class="userinput"><code>ps auxwf</code></strong>
<code
                class="computeroutput">USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init
root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald
root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D
root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux
root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux
root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux
root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     
root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \_ -bash
root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \_ ps auxfw
root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102
root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e
root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux
root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux
root@testlxc:~# </code></pre><div
              class="para">
					これでコンテナの中に入りました。プロセスへのアクセスはコンテナ自身によって開始されたものだけに制限されていることがわかります。同様に、ファイルシステムへのアクセスも testlxc コンテナ専用に割り当てられた完全なファイルシステムの一部分 (<code
                class="filename">/var/lib/lxc/testlxc/rootfs</code>) に制限されています。コンソールを終了するには <span
                class="keycap"><strong>Control</strong></span>+<span
                class="keycap"><strong>a</strong></span> <span
                class="keycap"><strong>q</strong></span> を使います。
				</div><div
              class="para">
					<code
                class="command">lxc-start</code> に <code
                class="option">--daemon</code> オプションを渡したおかげで、コンテナがバックグラウンドプロセスとして実行されていることに注意してください。コンテナを中止するには <code
                class="command">lxc-stop --name=testlxc</code> などのコマンドを使います。
				</div><div
              class="para">
					<span
                class="pkg pkg">lxc</span> パッケージには、ホストの起動時に自動的に 1 つまたは複数のコンテナを開始するための初期化スクリプトが含まれます (この初期化スクリプトは <code
                class="literal">lxc.start.auto</code> オプションが 1 に設定されているコンテナを起動する <code
                class="command">lxc-autostart</code> に依存しています)。起動順序を非常に細かく制御するには <code
                class="literal">lxc.start.order</code> と <code
                class="literal">lxc.group</code> を使います。デフォルトの場合、初期化スクリプトは <code
                class="literal">onboot</code> グループに所属するコンテナを起動し、その後いかなるグループにも所属しないコンテナを起動します。どちらの場合も、グループ内の起動順序を制御するには <code
                class="literal">lxc.start.order</code> オプションを使います。
				</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>GOING FURTHER</em></span> 大量の仮想化</strong></p></div></div></div><div
                class="para">
					LXC は非常に軽量の隔離システムですから、LXC を使って仮想サーバを大量にホストすることが可能です。この場合のネットワーク設定は上に述べた物よりも少し高度なものになるかもしれませんが、多くの場合 <code
                  class="literal">tap</code> と <code
                  class="literal">veth</code> インターフェースを用いた「ぜいたくな」設定を使えば事足ります。
				</div><div
                class="para">
					ファイルシステムの一部 (たとえば <code
                  class="filename">/usr</code> と <code
                  class="filename">/lib</code> などのサブツリー) を共有することは合理的です。こうすることで、複数のコンテナで共通に必要なソフトウェアを複製することを避けることが可能です。通常これを設定するには、コンテナ設定ファイルに含まれる <code
                  class="literal">lxc.mount.entry</code> エントリを使います。さらに興味深い副作用として、より少ない物理メモリでプロセスを動かすことが可能になります。なぜなら、カーネルはプログラムが共有されていることを検出できるからです。このことにより 1 つのコンテナを追加するためのコストをコンテナに特有のデータに割り当てられたディスク領域と、カーネルがスケジュールと管理に使ういくつかの追加的プロセスだけに減らすことが可能です。
				</div><div
                class="para">
					もちろん、ここではすべての利用できるオプションを説明していません。このため、より広範囲におよぶ情報を入手するには、<span
                  class="citerefentry"><span
                    class="refentrytitle">lxc</span>(7)</span> と <span
                  class="citerefentry"><span
                    class="refentrytitle">lxc.container.conf</span>(5)</span> マニュアルページおよびこれらのマニュアルページから参照されている文書を参照してください。
				</div></div></div></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    xmlns=""
                    id="id-1.15.5.14"></a>12.2.3. KVM を使った仮想化</h3></div></div></div><a
            id="id-1.15.5.14.2"
            class="indexterm"></a><div
            class="para">
				KVM は <span
              class="emphasis"><em>Kernel-based Virtual Machine</em></span> を意味しており、仮想化システムの使うほとんどの基礎構造を提供する最初で最高のカーネルモジュールです。しかしながら、LVM 自身は仮想化システムではありません。仮想化の実際の制御を行うには QEMU に基づくアプリケーションを使います。この節で <code
              class="command">qemu-*</code> コマンドがあっても心配しないでください。なぜならこのコマンドは KVM に関連するものだからです。
			</div><div
            class="para">
				他の仮想化システムと異なり、KVM は最初から Linux カーネルにマージされていました。KVM の開発者はプロセッサが備える仮想化専用命令セット (Intel-VT と AMD-V) を有効活用することを選びました。仮想化専用命令セットを活用することで、KVM は軽量で簡潔でリソースを大量に消費しないものになっています。もちろんその代償として KVM にも欠点があります。それはすべてのコンピュータが KVM を動かせるわけではなく、適切なプロセッサを備えたコンピュータでなければ KVM を動かせないという点です。x86 ベースのコンピュータで <code
              class="filename">/proc/cpuinfo</code> 内の CPU フラグに「vmx」または「svm」が含まれている場合、そのプロセッサは KVM を動かすことができることを意味します。
			</div><div
            class="para">
				Red Hat が KVM の開発を活発に支援したことで、KVM は事実上 Linux 仮想化の基準点になりました。
			</div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.6"></a>12.2.3.1. 準備段階</h4></div></div></div><a
              id="id-1.15.5.14.6.2"
              class="indexterm"></a><div
              class="para">
					VirtualBox などのツールと異なり、KVM は仮想マシンを作成管理するためのユーザインターフェースを含みません。仮想マシンを開始することが可能な実行ファイルおよび適切なカーネルモジュールを読み込むための初期化スクリプトを含むパッケージが <span
                class="pkg pkg">qemu-kvm</span> パッケージです。
				</div><a
              id="id-1.15.5.14.6.4"
              class="indexterm"></a><a
              id="id-1.15.5.14.6.5"
              class="indexterm"></a><div
              class="para">
					幸いなことに、Red Hat は <span
                class="emphasis"><em>libvirt</em></span> ライブラリおよび関連する<span
                class="emphasis"><em>仮想マシンマネージャ</em></span>ツールを開発することで、この問題に対処するためのツールを提供しています。libvirt により仮想マシンを管理する方法が統一され、仮想マシンの管理方法が裏で動く仮想システムに依存しなくなります (libvirt は現在 QEMU、KVM、Xen、LXC、OpenVZ、VirtualBox、VMWare、UML をサポートしています)。<code
                class="command">virtual-manager</code> は仮想マシンを作成管理するために libvirt を使うグラフィルインターフェースです。
				</div><a
              id="id-1.15.5.14.6.7"
              class="indexterm"></a><div
              class="para">
					最初に <code
                class="command">apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</code> を使って、必要なパッケージをインストールします。<span
                class="pkg pkg">libvirt-bin</span> には、<code
                class="command">libvirtd</code> デーモンが含まれます。<code
                class="command">libvirtd</code> デーモンを使うことでホストで実行されている仮想マシンを (潜在的にリモートで) 管理したり、ホスト起動時に要求された VM を開始したりすることが可能です。加えて、<span
                class="pkg pkg">libvirt-bin</span> パッケージは <code
                class="command">virsh</code> コマンドラインツールを提供します。<code
                class="command">virsh</code> を使うことで、<code
                class="command">libvirtd</code> の管理するマシンを操作することが可能です。
				</div><div
              class="para">
					<span
                class="pkg pkg">virtinst</span> パッケージには <code
                class="command">virt-install</code> コマンドが含まれます。<code
                class="command">virt-install</code> を使うことで、コマンドラインから仮想マシンを作成することが可能になります。最後に、<span
                class="pkg pkg">virt-viewer</span> を使うことで、仮想マシンのグラフィカルコンソールにアクセスすることが可能になります。
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.7"></a>12.2.3.2. ネットワークの設定</h4></div></div></div><div
              class="para">
					Xen や LXC と同様に、最もよく使われるネットワーク設定は仮想マシンのネットワークインターフェースをグループ化するブリッジです (<a
                class="xref"
                href="sect.virtualization.html#sect.lxc.network">第 12.2.2.2 節「ネットワークの設定」</a>を参照してください)。
				</div><div
              class="para">
					ネットワーク設定には別の方法もあります。KVM の提供するデフォルト設定の中では、仮想マシンに (192.168.122.0/24 の範囲内に) プライベートアドレスが割り当てられており、さらに NAT が設定されています。この設定により仮想マシンは外部ネットワークにアクセスすることが可能です。
				</div><div
              class="para">
					この節の残りでは、ホストが <code
                class="literal">eth0</code> 物理インターフェースと <code
                class="literal">br0</code> ブリッジを備え、<code
                class="literal">eth0</code> が <code
                class="literal">br0</code> に接続されていることを仮定します。
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.8"></a>12.2.3.3. <code
                      class="command">virt-install</code> を使ったインストール</h4></div></div></div><a
              id="id-1.15.5.14.8.2"
              class="indexterm"></a><div
              class="para">
					仮想マシンの作成は普通のシステムをインストールするのとよく似ています。違いは、仮想マシンの性質をコマンドラインから非常に長々と指定する点です。
				</div><div
              class="para">
					具体的に言えば、これはホストシステムに保存された Debian DVD イメージを挿入された仮想 DVD-ROM ドライブから仮想マシンを起動することにより Debian インストーラを使うことを意味します。仮想マシンは VNC プロトコル (詳しくは<a
                class="xref"
                href="sect.remote-login.html#sect.remote-desktops">第 9.2.2 節「リモートグラフィカルデスクトップの利用」</a>を参照してください) を介してグラフィカルコンソールに表示されます。これによりインストール作業を操作することが可能になります。
				</div><div
              class="para">
					最初にディスクイメージの保存先を libvirtd に伝える必要があります。デフォルトの保存先 (<code
                class="filename">/var/lib/libvirt/images/</code>) でも構わないならばこれは必要ありません。
				</div><pre
              class="screen"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>mkdir /srv/kvm</code></strong>
<code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>virsh pool-create-as srv-kvm dir --target /srv/kvm</code></strong>
<code
                class="computeroutput">Pool srv-kvm created

root@mirwiz:~# </code></pre><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>TIP</em></span> libvirt グループに対するユーザの追加</strong></p></div></div></div><div
                class="para">
					この節で挙げたすべての例では、root がコマンドを実行しています。実際、あるユーザがローカルの libvirt デーモンを操作するには、root になるか <code
                  class="literal">libvirt</code> グループのメンバーになって (デフォルト状態では <code
                  class="literal">libvirt</code> グループはユーザの初期参加グループに設定されていません) コマンドを実行する必要があります。このユーザに root 権限を頻繁に使わせることを避けたい場合、そのユーザを <code
                  class="literal">libvirt</code> グループに追加して、本人の権限でさまざまなコマンドを実行するように設定することが可能です。
				</div></div><div
              class="para">
					それでは仮想マシンのインストール作業を開始し、<code
                class="command">virt-install</code> の最も重要なオプションを詳細に見て行きましょう。<code
                class="command">virt-install</code> は仮想マシンとそのパラメータを libvirtd に登録し、インストールを進めるために仮想マシンを開始します。
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virt-install --connect qemu:///system  <span
                    id="virtinst.connect"><img
                      class="callout"
                      src="Common_Content/images/1.png"
                      alt="1" /></span>
               --virt-type kvm           <span
                    id="virtinst.type"><img
                      class="callout"
                      src="Common_Content/images/2.png"
                      alt="2" /></span>
               --name testkvm            <span
                    id="virtinst.name"><img
                      class="callout"
                      src="Common_Content/images/3.png"
                      alt="3" /></span>
               --ram 1024                <span
                    id="virtinst.ram"><img
                      class="callout"
                      src="Common_Content/images/4.png"
                      alt="4" /></span>
               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <span
                    id="virtinst.disk"><img
                      class="callout"
                      src="Common_Content/images/5.png"
                      alt="5" /></span>
               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <span
                    id="virtinst.cdrom"><img
                      class="callout"
                      src="Common_Content/images/6.png"
                      alt="6" /></span>
               --network bridge=br0      <span
                    id="virtinst.network"><img
                      class="callout"
                      src="Common_Content/images/7.png"
                      alt="7" /></span>
               --vnc                     <span
                    id="virtinst.vnc"><img
                      class="callout"
                      src="Common_Content/images/8.png"
                      alt="8" /></span>
               --os-type linux           <span
                    id="virtinst.os"><img
                      class="callout"
                      src="Common_Content/images/9.png"
                      alt="9" /></span>
               --os-variant debianwheezy
</code></strong><code
                class="computeroutput">
Starting install...
Allocating 'testkvm.qcow'             |  10 GB     00:00
Creating domain...                    |    0 B     00:00
Guest installation complete... restarting guest.
</code></pre><div
              class="calloutlist"><table
                border="0"
                summary="Callout list"><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.connect"><img
                          class="callout"
                          src="Common_Content/images/1.png"
                          alt="1" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--connect</code> オプションは使用する「ハイパーバイザ」を指定します。これは仮想システムを表す URL (<code
                        class="literal">xen://</code>、<code
                        class="literal">qemu://</code>、<code
                        class="literal">lxc://</code>、<code
                        class="literal">openvz://</code>、<code
                        class="literal">vbox://</code> など) と VM をホストするマシン (ローカルホストの場合、空でも構いません) の形をしています。QEMU/KVM の場合、これに加えて各ユーザは制限されたパーミッションで稼働する仮想マシンを管理できます。この場合 URL パスは「システム」マシン (<code
                        class="literal">/system</code>) かその他 (<code
                        class="literal">/session</code>) かで識別されます。
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.type"><img
                          class="callout"
                          src="Common_Content/images/2.png"
                          alt="2" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--virt-type kvm</code> を指定することで KVM を使うことが可能です。<code
                        class="literal">--connect</code> で指定した URL を一見すると QEMU が使われるように見えますが、これは KVM は QEMU と同じ方法で管理されているためです。
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.name"><img
                          class="callout"
                          src="Common_Content/images/3.png"
                          alt="3" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--name</code> オプションは仮想マシンの (一意的な) 名前を定義します。
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.ram"><img
                          class="callout"
                          src="Common_Content/images/4.png"
                          alt="4" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--ram</code> オプションは仮想マシンに割り当てる RAM の量 (MB 単位) を指定します。
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.disk"><img
                          class="callout"
                          src="Common_Content/images/5.png"
                          alt="5" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--disk</code> オプションは仮想マシンのハードディスクとして利用するイメージファイルの場所を指定します。このファイルが存在しなければ、<code
                        class="literal">size</code> パラメータで指定されたサイズ (GB 単位) のイメージファイルが作成されます。<code
                        class="literal">format</code> パラメータはイメージファイルを保存するさまざまな方法を選択します。デフォルトフォーマット (<code
                        class="literal">raw</code>) はディスクサイズと内容が全く同じ単一ファイルです。ここではより先進的なフォーマット qcow2 を選びました。qcow2 は QEMU 専用のフォーマットです。qcow2 フォーマットのファイルは作成時のサイズは小さいのですが、仮想マシンが領域を実際に利用することになった時にサイズが増加します。
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.cdrom"><img
                          class="callout"
                          src="Common_Content/images/6.png"
                          alt="6" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--cdrom</code> オプションはインストール時に利用する光学ディスクの場所を指定するために使われます。場所には ISO ファイルのローカルパス、ファイル取得先の URL、物理 CD-ROM ドライブのデバイスファイル (例 <code
                        class="literal">/dev/cdrom</code>) のどれか 1 つを使うことが可能です。
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.network"><img
                          class="callout"
                          src="Common_Content/images/7.png"
                          alt="7" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--network</code> オプションはホストネットワーク設定の中に仮想ネットワークを統合する方法を指定します。デフォルトは既存のネットワークブリッジに仮想ネットワークを統合する方法です (例では明示的にこの挙動を指定しています)。指定したブリッジが存在しない場合、仮想マシンが到達できるネットワークは NAT を介した物理ネットワークだけに限定されるので、仮想マシンはプライベートサブネット範囲 (192.168.122.0/24) に含まれるアドレスを割り当てられます。
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.vnc"><img
                          class="callout"
                          src="Common_Content/images/8.png"
                          alt="8" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--vnc</code> は VNC を使ってグラフィカルコンソールを利用できるようにすることを意味します。VNC サーバに対するデフォルトの挙動を使った場合、ローカルインターフェースだけがリッスンされます。さらに仮想マシンを操作する VNC クライアントを別のホスト上で実行する場合、VNC 接続を確立するには SSH トンネルを設定する必要があります (<a
                        class="xref"
                        href="sect.remote-login.html#sect.ssh-port-forwarding">第 9.2.1.3 節「ポート転送を使った暗号化トンネルの作成」</a>を参照してください)。別の方法として、VNC サーバをすべてのインターフェースを介して利用できるようにするために、<code
                        class="literal">--vnclisten=0.0.0.0</code> を使うことも可能です。しかしこの方針を取る場合、ファイアウォールを適切に設計するべきという点に注意してください。
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.os"><img
                          class="callout"
                          src="Common_Content/images/9.png"
                          alt="9" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--os-type</code> と <code
                        class="literal">--os-variant</code> オプションは、指定されたオペレーティングシステムの備える既知の機能に基づいて、仮想マシンのいくつかのパラメータを最適化するためのものです。
						</div></td></tr></table></div><div
              class="para">
					<code
                class="command">virt-install</code> を実行した時点で仮想マシンが実行されます。インストール作業に進むためには、グラフィカルコンソールに接続する必要があります。上の操作をグラフィカルデスクトップ環境から行った場合、自動的に接続が開始されます。そうでない場合、グラフィカルコンソールを開くために <code
                class="command">virt-viewer</code> を任意のグラフィカル環境から実行します (この時にリモートホストの root パスワードが 2 回尋ねられる点に注意してください。なぜなら、この操作には 2 つの SSH 接続を必要とするからです)。
				</div><pre
              class="screen"><code
                class="computeroutput">$ </code><strong
                class="userinput"><code>virt-viewer --connect qemu+ssh://root@<em
                    class="replaceable">server</em>/system testkvm
</code></strong><code
                class="computeroutput">root@server's password: 
root@server's password: </code></pre><div
              class="para">
					インストール作業が終了したら、仮想マシンが再起動されます。これで仮想マシンを利用する準備が整いました。
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.9"></a>12.2.3.4. <code
                      class="command">virsh</code> を使ったマシンの管理</h4></div></div></div><a
              id="id-1.15.5.14.9.2"
              class="indexterm"></a><div
              class="para">
					これでインストールが終了しました。利用できる仮想マシンを取り扱う方法に移りましょう。最初に <code
                class="command">virsh</code> を使って <code
                class="command">libvirtd</code> が管理している仮想マシンのリストを確認します。
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system list --all
</code></strong><code
                class="computeroutput"> Id Name                 State
----------------------------------
  - testkvm              shut off
</code></pre><div
              class="para">
					それではテスト用仮想マシンを起動しましょう。
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system start testkvm
</code></strong><code
                class="computeroutput">Domain testkvm started</code></pre><div
              class="para">
					そして、グラフィカルコンソールへの接続命令を出します (接続する VNC 画面を <code
                class="command">vncviewer</code> へのパラメータの形で指定することが可能です)。
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system vncdisplay testkvm
</code></strong><code
                class="computeroutput">:0</code></pre><div
              class="para">
					その他の利用できる <code
                class="command">virsh</code> サブコマンドには以下のものがあります。
				</div><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">reboot</code>。仮想マシンを再起動します。
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">shutdown</code>。仮想マシンを正常にシャットダウンします。
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">destroy</code>。仮想マシンを無理やり停止します。
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">suspend</code>。仮想マシンを一時停止します。
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">resume</code>。一時停止された仮想マシンを再開します。
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">autostart</code>。ホスト起動時にこの仮想マシンを自動的に起動することを有効化します (または <code
                      class="literal">--disable</code> オプションを付けて無効化します)。
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">undefine</code>。仮想マシンのすべての痕跡を <code
                      class="command">libvirtd</code> から削除します。
						</div></li></ul></div><div
              class="para">
					ここに挙げたすべてのサブコマンドは仮想マシン識別子をパラメータとして受け取ります。
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.10"></a>12.2.3.5. yum を使い RPM に基づくシステムを Debian の中にインストールする</h4></div></div></div><div
              class="para">
					仮想マシンが Debian (または Debian 派生物) を実行することを意図している場合、上で述べた通り <code
                class="command">debootstrap</code> を使ってシステムを初期化することが可能です。しかし、仮想マシンに RPM に基づくシステム (Fedora、CentOS、Scientific Linux など) をインストールする場合、<code
                class="command">yum</code> ユーティリティ (同名のパッケージに含まれます) を使ってシステムをセットアップする必要があります。
				</div><div
              class="para">
					RPM に基づくシステムをインストールする際には、特に <code
                class="command">yum</code> 設定ファイルなどのファイルの初期セットを展開するために <code
                class="command">rpm</code> を使い、その後パッケージの残りのセットを展開するために <code
                class="command">yum</code> を呼び出す必要があります。しかし、chroot の外から <code
                class="command">yum</code> を呼び出しているため、一時的な修正が必要です。以下に載せた例では、対象の chroot 先は <code
                class="filename">/srv/centos</code> です。
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>rootdir="/srv/centos"
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>mkdir -p "$rootdir" /etc/rpm
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>echo "%_dbpath /var/lib/rpm" &gt; /etc/rpm/macros.dbpath
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>rpm --nodeps --root "$rootdir" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</code></strong><code
                class="computeroutput">rpm: RPM should not be used directly install RPM packages, use Alien instead!
rpm: However assuming you know what you are doing...
warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY
# </code><strong
                class="userinput"><code>sed -i -e "s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g" $rootdir/etc/yum.repos.d/*.repo
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>yum --assumeyes --installroot $rootdir groupinstall core
</code></strong><code
                class="computeroutput">[...]
# </code><strong
                class="userinput"><code>sed -i -e "s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g" $rootdir/etc/yum.repos.d/*.repo
</code></strong></pre></div></div></div><ul
        class="docnav"><li
          class="previous"><a
            accesskey="p"
            href="advanced-administration.html"><strong>戻る</strong>第 12 章 高度な管理</a></li><li
          class="up"><a
            accesskey="u"
            href="#"><strong>上に戻る</strong></a></li><li
          class="home"><a
            accesskey="h"
            href="index.html"><strong>ホーム</strong></a></li><li
          class="next"><a
            accesskey="n"
            href="sect.automated-installation.html"><strong>次へ</strong>12.3. 自動インストール</a></li></ul></body></html>
