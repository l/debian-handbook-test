<?xml version='1.0' encoding='utf-8' ?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [
]>
<chapter id="advanced-administration">
  <chapterinfo>
    <mediaobject condition="pdf">
      <imageobject>
        <imagedata fileref="images/chap-advanced-administration.png" scalefit="1" />
      </imageobject>
    </mediaobject>
    <keywordset>
      <keyword>RAID</keyword>
      <keyword>LVM</keyword>
      <keyword>FAI</keyword>
      <keyword>Preseed</keyword>
      <keyword>監視</keyword>
      <keyword>仮想化</keyword>
      <keyword>Xen</keyword>
      <keyword>LXC</keyword>
    </keywordset>
  </chapterinfo>
  <title>高度な管理</title>
  <highlights>
    <para>この章では、前章までに説明した一部の側面を異なる視点から再度取り上げます。すなわち、1 台のコンピュータにインストールするのではなく、大規模な配備システムについて学びます。さらに、初回インストール時に RAID や LVM ボリュームを作成するのではなく、手作業でこれを行う方法について学びます。こうすることで初回インストール時の選択を訂正することが可能です。最後に、監視ツールと仮想化技術について議論します。その結果として、この章はより熟練した管理者を対象にしており、ホームネットワークに責任を負う個人を対象にしていません。</para>
  </highlights>
  <section id="sect.raid-and-lvm">
    <title>RAID と LVM</title>

    <para><xref linkend="installation" />ではインストーラの視点から RAID と LVM の技術を説明し、インストーラを使って初回インストール時に RAID や LVM を簡単に配備する方法について説明しました。初回インストールが終わったら、管理者は再インストールという手間のかかる最終手段を行使することなく、より大きなストレージ領域の要求に対処しなければいけません。すなわち、管理者は RAID と LVM ボリュームを操作するために必要なツールを理解しなければいけません。</para>

    <para>RAID と LVM は両方ともマウントされたボリュームを物理的に対応する物 (実際のハードディスクドライブまたはそのパーティション) から抽象化する技術です。さらに、RAID は冗長性を導入することでデータをハードディスク障害から守り、LVM はボリューム管理をより柔軟にしてディスクの実サイズに依存しないようにします。どちらの場合であっても、最終的にシステムには新しいブロックデバイスが追加されます。追加されたブロックデバイスはファイルシステムやスワップ領域を作成するために使われますが、必ずしも単独の物理ディスクに対応付けられるものではありません。RAID と LVM は全く異なる生い立ちを持っていますが、両者の機能は多少重複しています。このため、両者は一緒に言及されることが多いです。</para>

    <sidebar>
      <title><emphasis>PERSPECTIVE</emphasis> Btrfs が LVM と RAID を結び付ける</title>

      <para>LVM と RAID は 2 種類の全く別のカーネルサブシステムで、どちらもディスクブロックデバイスとそのファイルシステムの間のやり取りを担当します。<emphasis>btrfs</emphasis> は最初 Oracle で開発された新しいファイルシステムで、LVM と RAID の機能を結び付けると主張しています。<emphasis>btrfs</emphasis> は開発がまだ完了していない (一部の機能がまだ実装されていない) ため「実験中」とタグ付けされていますが、ほとんどうまく機能し、既に本番環境で使われています。<ulink type="block" url="http://btrfs.wiki.kernel.org/" /></para>

      <para><emphasis>btrfs</emphasis> の特筆すべき機能に、任意の時点におけるファイルシステムツリーのスナップショットを取る機能があります。このスナップショットコピーは初期状態ではいかなるディスク領域も使いません、コピー内容の 1 つが修正された際にデータが複製されます。また、このファイルシステムはファイルを透過的に圧縮することが可能で、さらにチェックサムを用いて保存されているデータの完全性を保証します。</para>
    </sidebar>

    <para>RAID と LVM のどちらの場合も、カーネルはハードディスクドライブやパーティションに対応するブロックデバイスファイルとよく似たブロックデバイスファイルを提供します。アプリケーションやカーネルの別の部分がそのようなデバイスのあるブロックにアクセスを要求する場合、適切なサブシステムが要求されたブロックを物理層のブロックに対応付けます。設定に依存して、アプリケーション側から見たブロックは単独か複数の物理ディスクに保存されます。このブロックの物理的場所は論理デバイス内のブロックの位置と直接的に対応するものではないかもしれません。</para>
    <section id="sect.raid-soft">
      <title>ソフトウェア RAID</title>
      <indexterm><primary>RAID</primary></indexterm>

      <para>RAID は <emphasis>Redundant Array of Independent Disks</emphasis> を意味します。RAID システムの目標はハードディスク障害の際にデータ損失を防ぐことです。一般原則は極めて単純です。すなわち、データは設定できる冗長性のレベルに基づいて単独ではなく複数のディスクに保存されます。冗長性の度合いに依存して、たとえ予想外のディスク障害が起きた場合でも、データを残りのディスクから損失なく再構成することが可能です。</para>

      <sidebar>
        <title><emphasis>CULTURE</emphasis> <foreignphrase>independent</foreignphrase> それとも <foreignphrase>inexpensive</foreignphrase>?</title>

	<para>当初、RAID の I は <emphasis>inexpensive</emphasis> を意味していました。なぜなら、RAID は高価な高性能ディスクへ投資することなくデータの安全性を劇的に高めることが可能だったからです。しかしながらおそらく心証的な懸念から、現在 RAID の I は通例 <emphasis>independent</emphasis> を意味するものとされます。<emphasis>independent</emphasis> には安価であることに対する悪い印象がないからです。</para>
      </sidebar>

      <para>RAID は専用ハードウェア (SCSI や SATA コントローラカードに統合された RAID モジュール) またはソフトウェア抽象化 (カーネル) を使って実装することが可能です。ハードウェアかソフトウェアかに関わらず、十分な冗長性を備えた RAID システムはディスク障害があっても利用できる状態を透過的に継続することが可能です。従って、スタックの上層 (アプリケーション) はディスク障害にも関わらず、引き続きデータにアクセスできます。もちろん「信頼性低下状態」は性能に影響をおよぼし、冗長性を低下させます。このため、もう一つ別のディスク障害が起きるとデータを失うことになります。このため実践的には、管理者は信頼性低下状態を障害の起きたディスクが交換されるまでの間だけに留めるように努力します。新しいディスクが配備されると、RAID システムは要求されたデータを再構成することが可能です。こうすることで信頼性の高い状態に戻ります。信頼性低下状態か再構成状態にある RAID アレイのアクセス速度は低下する可能性がありますが、この点を除けばアプリケーションがディスク障害に気が付くことはないでしょう。</para>

      <para>RAID がハードウェアで実装された場合、その設定は通常 BIOS セットアップツールによってなされます。カーネルは RAID アレイを標準的な物理ディスクとして機能する単一のディスクとみなします。RAID アレイのデバイス名は (ドライバに依存して) 違うかもしれません。</para>

      <para>本書ではソフトウェア RAID だけに注目します。</para>

      <section id="sect.raid-levels">
        <title>さまざまな RAID レベル</title>

	<para>実際のところ RAID の種類は 1 種類だけではなく、そのレベルによって識別される複数の種類があります。すなわち、設計と提供される冗長性の度合いが異なる複数の RAID レベルが存在します。より冗長性を高くすれば、より障害に強くなります。なぜなら、より多くのディスクで障害が起きても、システムを動かし続けることができるからです。これに応じて、与えられた一連のディスクに対して利用できる領域が小さくなります。すなわち、あるサイズのデータを保存するために必要なディスク領域のサイズが多くなります。</para>
        <variablelist>
          <varlistentry>
            <term>リニア RAID</term>
            <listitem>
	      <para>カーネルの RAID サブシステムを使えば「リニア RAID」を作ることも可能ですが、「リニア RAID」は適切な RAID ではありません。なぜなら、「リニア RAID」には冗長性が一切ないからです。カーネルはただ単純に複数のディスク端同士を統合し、統合されたボリュームを 1 つの仮想ディスク (1 つのブロックデバイス) として提供するだけです。これが「リニア RAID」のすべてです。「リニア RAID」を使うのは極めてまれな場合に限られます (後から使用例を説明します)。なぜなら、冗長性がないということは 1 つのディスクの障害が統合されたボリューム全体を駄目にすること、ひいてはすべてのデータを駄目にすることを意味するからです。</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>RAID-0</term>
            <listitem>
	      <para>同様に RAID-0 にも冗長性がありません。しかしながら、RAID-0 は順番通り単純に物理ディスクを連結する構成ではありません。すなわち、物理ディスクは<emphasis>ストライプ状</emphasis>に分割され、仮想デバイスのブロックは互い違いになった物理ディスクのストライプに保存されます。たとえば 2 台のディスクから構成されている RAID-0 セットアップでは、偶数を付番されたブロックは最初の物理ディスクに保存され、奇数を付番されたブロックは 2 番目の物理ディスクに保存されます。</para>

	      <para>RAID-0 システムを使っても信頼性は向上しません。なぜなら、システムの信頼性すなわちデータの可用性は (リニア RAID と同様に) ディスク障害があればすぐに脅かされるからです。しかしながら、RAID-0 システムを使うことで性能は向上します。すなわち隣接した巨大なデータにシーケンシャルアクセスする場合、カーネルは両方のディスクから平行して読み込む (書き込む) ことが可能です。これによりデータの転送率が増加します。とは言うものの、RAID-0 が使われる機会は減り、代わりに LVM (後から説明します) が使われるようになっています。</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>RAID-1</term>
            <listitem>
	      <para>RAID-1 は「RAID ミラーリング」としても知られ、最も簡単で最も広く使われています。RAID-1 の標準的な構成では、同じサイズの 2 台の物理ディスクを使い、物理ディスクと同じサイズの論理ボリュームが利用できるようになります。データを両方のディスクに保存するため、「ミラー」と呼ばれています。一方のディスクに障害があっても、他方のディスクからデータを利用することが可能です。もちろん、非常に重要なデータ用に RAID-1 を 2 台以上の構成にすることも可能ですが、これはハードウェア費用と利用できる保存領域の比率に直接的な影響をおよぼします。</para>

              <sidebar>
                <title><emphasis>NOTE</emphasis> ディスクとクラスタサイズ</title>

		<para>異なるサイズの 2 台のディスクをミラーでセットアップする場合、サイズの大きい側のディスクは完全に利用されません。なぜなら、大きい側のディスクに含まれるデータは最も小さいディスクに含まれるデータと同じデータだからです。このため RAID-1 ボリュームで提供される利用できる領域のサイズは RAID アレイの最も小さなディスクのサイズと同じになります。冗長性を異なる方法で確保しているより高い RAID レベルの RAID ボリュームに対しても同じことが言えます。</para>

		<para>それ故、(RAID-0 と「リニア RAID」以外の) RAID アレイをセットアップする場合、資源の無駄を防ぐためにアレイを構成するディスクはそのサイズが完全に同じか近いものを使うことが重要です。</para>
              </sidebar>

              <sidebar>
                <title><emphasis>NOTE</emphasis> 予備ディスク</title>

		<para>冗長性を持たせた RAID レベルでは、必要なディスク数よりも多くのディスクで RAID アレイを構成させることが可能です。追加的ディスクは主要ディスクに障害が起きた場合に予備として使われます。たとえば、2 台のディスクと 1 台の予備ディスクのミラー構成では、最初の 2 台のうちの 1 台に障害が起きた場合、カーネルは自動的に (そして素早く) 予備ディスクを使ってミラーを再構成し、再構成の完了後に冗長性が再確保されます。すなわち、重要なデータに対するもう一つの安全装置として予備ディスクを使うことが可能ということです。</para>

		<para>この方式が単純に 3 台のディスクに対して最初からミラーリングを行うよりも優れているとされることに疑問を持つかもしれません。「予備ディスク」を設定する利点は複数の RAID ボリュームで予備ディスクを共有することが可能という点です。たとえば、1 台のディスク障害に対する冗長性を確保した 3 つのミラーされたボリュームを構成するには、ディスクを 7 台 (3 つのペアと 1 台の共有された予備) 用意するだけですみます。これに対して各ボリュームに 3 台のディスクを用意する場合には 9 台のディスクが必要です。</para>
              </sidebar>

	      <para>RAID-1 は高価であるにも関わらず (良くても物理ストレージ領域のたった半分しか使えないにも関わらず)、広く実運用されています。RAID-1 は簡単に理解でき、簡単にバックアップできます。なぜなら両方のディスクが全く同じ内容を持っているため、片方を一時的に取り外しても運用システムに影響をおよぼさないからです。通常 RAID-1 を使うことで、読み込み性能は好転します。なぜなら、カーネルはデータの半分をそれぞれのディスクから平行して読むことができるからです。これに対して、書き込み性能はそれほど悪化しません。N 台のディスクからなる RAID-1 アレイの場合、データは N-1 台のディスク障害に対して保護されます。</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>RAID-4</term>
            <listitem>
	      <para>RAID-4 は広く使われていません。RAID-4 は実データを保存するために N 台のディスクを使い、冗長性情報を保存するために 1 台の「パリティ」ディスクを使います。「パリティ」ディスクに障害が起きた場合、システムは他の N 台からデータを再構成することが可能です。N 台のデータディスクのうち、最大で 1 台に障害が起きた場合、残りの N-1 台と「パリティ」ディスクには、要求されたデータを再構成するために十分な情報が含まれます。</para>

	      <para>RAID-4 は高価過ぎるというわけではありません。なぜならディスク 1 台につきたった N 分の 1 台分の追加費用で済むからです。また RAID-4 を使うと読み込み性能が大きく低下するというわけでもありません。しかしながら、RAID-4 は書き込み性能に深刻な影響をおよぼします。加えて、N 台の実データ用ディスクのどのディスクに書き込んでもパリティディスクに対する書き込みが発生するので、パリティディスクは実データ用ディスクに比べて書き込み回数が増えます。その結果、パリティディスクは極めて寿命が短くなります。RAID-4 アレイのデータは (N+1 台のディスクのうち) 1 台の障害に対して保護されます。</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>RAID-5</term>
            <listitem>
	      <para>RAID-5 は RAID-4 の非対称性問題を対処したものです。すなわち、パリティブロックは N+1 台のディスクに分散して保存され、特定のディスクが特定の役割を果たすことはありません。</para>

	      <para>読み込みと書き込み性能は RAID-4 と同様です。繰り返しになりますが、RAID-5 システムは (N+1 台のディスクのうち) 最大で 1 台までに障害が起きても動作します。</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>RAID-6</term>
            <listitem>
	      <para>RAID-6 は RAID-5 の拡張と考えられます。RAID-6 では、N 個の連続するブロックに対して 2 個の冗長性ブロックを使います。この N+2 個のブロックは N+2 台のディスクに分散して保存されます。</para>

	      <para>RAID-6 は RAID-4 と RAID-5 に比べて少し高価ですが、RAID-6 を使うことで安全性はさらに高まります。なぜなら、(N+2 台中の) 最大で 2 台までの障害に対してデータを守ることが可能だからです。書き込み操作は 1 つのデータブロックと 2 つの冗長性ブロックを書き込むことに対応しますから、RAID-6 の書き込み性能は RAID-4 と RAID-5 に比べてさらに悪化します。</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>RAID-1+0</term>
            <listitem>
	      <para>厳密に言えば RAID-1+0 は RAID レベルではなく、2 種類の RAID 分類を積み重ねたものです。RAID-1+0 を使うには 2×N 台のディスクが必要で、最初に 2 台ずつのペアから N 台の RAID-1 ボリュームを作ります。N 台の RAID-1 ボリュームは「リニア RAID」か LVM (次第にこちらを選ぶケースが増えています) のどちらか一方を使って 1 台に統合されます。LVM を使うと純粋な RAID ではなくなりますが、LVM を使っても問題はありません。</para>

	      <para>RAID-1+0 は複数のディスク障害を乗り切ることが可能です。具体的に言えば、上に挙げた 2×N アレイの場合、最大で N 台までの障害に耐えます。ただし、各 RAID-1 ペアを構成するディスクの両方に障害が発生してはいけません。</para>

              <sidebar id="sidebar.raid-10">
                <title><emphasis>GOING FURTHER</emphasis> RAID-10</title>

		<para>通常 RAID-10 は RAID-1+0 の同意語と考えられますが、Linux では特別に RAID-10 をより一般的な構成を可能にするものとして定めています。RAID-10 では、システムが各ブロックを 2 種類の異なるディスクに保存することが可能です。奇数台のディスク構成の場合でも、ブロックのコピーは設定可能なモデルに従って分散して保存されます。</para>

		<para>RAID-10 の性能は選択した再分割モデルと冗長性の度合い、そして論理ボリュームの作業負荷に依存して変化します。</para>
              </sidebar>
            </listitem>
          </varlistentry>
        </variablelist>

	<para>RAID レベルを選ぶ際には、各用途からの制限および要求を考慮する必要があるのは明らかです。1 台のコンピュータに異なる設定を持つ複数の RAID アレイを配置することが可能である点に注意してください。</para>
      </section>
      <section id="sect.raid-setup">
        <title>RAID の設定</title>
        <indexterm><primary><emphasis role="pkg">mdadm</emphasis></primary></indexterm> 

	<para>RAID ボリュームを設定するには <emphasis role="pkg">mdadm</emphasis> パッケージが必要です。<emphasis role="pkg">mdadm</emphasis> パッケージには RAID アレイを作成したり操作するための <command>mdadm</command> コマンド、システムの他の部分に RAID アレイを統合するためのスクリプトやツール、監視システムが含まれます。</para>

	<para>以下の例では、多数のディスクを持つサーバをセットアップします。ディスクの一部は既に利用されており、残りは RAID をセットアップするために利用できるようになっています。最初の状態で、以下のディスクとパーティションが存在します。</para>
        <itemizedlist>
          <listitem>
	    <para><filename>sdb</filename> ディスク (4 GB) は全領域を利用できます。</para>
          </listitem>
          <listitem>
	    <para><filename>sdc</filename> ディスク (4 GB) は全領域を利用できます。</para>
          </listitem>
          <listitem>
	    <para><filename>sdd</filename> ディスクは <filename>sdd2</filename> パーティション (約 4 GB) だけを利用できます。</para>
          </listitem>
          <listitem>
	    <para><filename>sde</filename> ディスク (4 GB) は全領域を利用できます。</para>
          </listitem>
        </itemizedlist>

        <sidebar>
          <title><emphasis>NOTE</emphasis> 既存の RAID ボリュームの識別</title>

	  <para><filename>/proc/mdstat</filename> ファイルには既存のボリュームとその状態が書かれています。新しい RAID ボリュームを作成する場合、既存のボリュームと同じ名前を付けないように注意してください。</para>
        </sidebar>

	<para>RAID-0 とミラー (RAID-1) の 2 つのボリュームを作るために上記の物理ディスクを使います。それでは RAID-0 ボリュームから作っていきましょう。</para>

        <screen><computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</userinput>
<computeroutput>mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
# </computeroutput><userinput>mdadm --query /dev/md0</userinput>
<computeroutput>/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.
# </computeroutput><userinput>mdadm --detail /dev/md0</userinput>
<computeroutput>/dev/md0:
        Version : 1.2
  Creation Time : Wed May  6 09:24:34 2015
     Raid Level : raid0
     Array Size : 8387584 (8.00 GiB 8.59 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:24:34 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

     Chunk Size : 512K

           Name : mirwiz:0  (local to host mirwiz)
           UUID : bb085b35:28e821bd:20d697c9:650152bb
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       1       8       32        1      active sync   /dev/sdc
# </computeroutput><userinput>mkfs.ext4 /dev/md0</userinput>
<computeroutput>mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 2095104 4k blocks and 524288 inodes
Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </computeroutput><userinput>mkdir /srv/raid-0</userinput>
<computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0</userinput>
<computeroutput># </computeroutput><userinput>df -h /srv/raid-0</userinput>
<computeroutput>ファイルシス   サイズ  使用  残り 使用% マウント位置
/dev/md0         7.9G   18M  7.4G    1% /srv/raid-0
</computeroutput></screen>

	<para><command>mdadm --create</command> コマンドには複数のパラメータが必要です。具体的に言えば、作成するボリュームの名前 (<filename>/dev/md*</filename>、MD は <foreignphrase>Multiple Device</foreignphrase> を意味します)、RAID レベル、ディスク数 (普通この値は RAID-1 とそれ以上のレベルでのみ意味があるにも関わらず、これは必須オプションです)、RAID を構成する物理デバイスを指定する必要があります。RAID デバイスを作成したら、RAID デバイスを通常のパーティションを取り扱うのと同様のやり方で取り扱うことが可能です。すなわち、ファイルシステムを作成したり、ファイルシステムをマウントしたりすることが可能です。ここで作成する RAID-0 ボリュームに <filename>md0</filename> と名前を付けたのは偶然に過ぎない点に注意してください。アレイに付けられた番号と冗長性の度合いを関連付ける必要はありません。また、<filename>/dev/md0</filename> の代わりに <filename>/dev/md/linear</filename> のようなパラメータを <command>mdadm</command> に渡すことで、名前付き RAID アレイを作成することも可能です。</para>

	<para>同様のやり方で RAID-1 を作成します。注意するべき違いは作成後に説明します。</para>

        <screen><computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</userinput>
<computeroutput>mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%
Continue creating array? </computeroutput><userinput>y</userinput>
<computeroutput>mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md1 started.
# </computeroutput><userinput>mdadm --query /dev/md1</userinput>
<computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
        Version : 1.2
  Creation Time : Wed May  6 09:30:19 2015
     Raid Level : raid1
     Array Size : 4192192 (4.00 GiB 4.29 GB)
  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:30:40 2015
          State : clean, resyncing (PENDING) 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       1       8       64        1      active sync   /dev/sde
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
          State : clean
[...]
</computeroutput></screen>

        <sidebar>
          <title><emphasis>TIP</emphasis> RAID、ディスク、パーティション</title>

	  <para>上の例で示した通り、RAID デバイスはディスクパーティションに作成することが可能です。必ずディスク全体を使わなければいけないというわけではありません。</para>
        </sidebar>

	<para>いくつかの注意点があります。最初に、<command>mdadm</command> は物理デバイス同士のサイズが異なる点を指摘しています。さらに、このことによりサイズが大きい側のデバイスの一部の領域が使えなくなるため、確認が求められています。</para>

	<para>さらに重要なことは、ミラーの状態に注意することです。RAID ミラーの正常な状態とは、両方のディスクが全く同じ内容を持っている状態です。しかしながら、ボリュームを最初に作成した直後の RAID ミラーは正常な状態であることを保証されません。このため、RAID サブシステムは RAID ミラーの正常な状態を保証するために、RAID デバイスが作成されたらすぐに同期化作業を始めます。しばらくの後 (必要な時間はディスクの実サイズに依存します)、RAID アレイは「active」または「clean」状態に移行します。同期化作業中、ミラーは信頼性低下状態で、冗長性は保証されない点に注意してください。同期化作業中にディスク障害が起きると、すべてのデータを失うことにつながる恐れがあります。しかしながら、最近作成された RAID アレイの最初の同期化作業の前に大量の重要なデータがこの RAID アレイに保存されていることはほとんどないでしょう。信頼性低下状態であっても <filename>/dev/md1</filename> を利用することが可能で、ファイルシステムを作成したり、データのコピーを取ったりすることが可能という点に注意してください。</para>

        <sidebar>
          <title><emphasis>TIP</emphasis> 信頼性低下状態でミラーを開始する</title>

	  <para>RAID-1 ミラーを構成する 2 台のディスクの両方をすぐに使えないことが時々あります。たとえば、ミラーを構成するディスクの片方にミラーに移動したいデータが既に保存されている場合です。このような場合、<command>mdadm</command> に渡すデバイスファイル引数の片方をデバイスファイルの代わりに <filename>missing</filename> にすることで、意図的に信頼性低下状態の RAID-1 アレイを作成することも可能です。ミラーに移動したいデータを含むディスクからデータを「ミラー」にコピーした後、そのディスクをアレイに追加することが可能です。追加作業が終われば、同期化作業が行われ、ミラーに移動したかったデータの冗長性が確保されます。</para>
        </sidebar>

        <sidebar>
          <title><emphasis>TIP</emphasis> 同期化作業を行わずにミラーをセットアップする</title>

	  <para>通常 RAID-1 ボリュームは新しいディスクとして使うために作成され、RAID-1 ボリュームの作成直後にはデータが保存されていないと考えられます。すなわち、RAID-1 ボリュームの初期内容に価値はなく、RAID-1 で保護したい重要なデータは RAID-1 ボリュームの作成後に書き込まれるデータというわけです。</para>

	  <para>そう考えると、RAID-1 ボリュームにデータが書き込まれる前に RAID-1 ボリュームを構成するディスクの内容が同期されるという点について疑問に思うかもしれません。RAID-1 ボリュームに書き込んでいないデータは後から読み込まれることもないのにも関わらず、なぜ RAID-1 ボリュームの作成時に RAID-1 ボリュームを構成するディスクの内容の同期化作業が必要なのでしょうか?</para>

	  <para>幸いなことに、RAID-1 を構成するディスクの内容の同期化作業は <literal>--assume-clean</literal> オプションを <command>mdadm</command> に渡せば避けることが可能です。しかしながら、初期データが読まれる場合、<literal>--assume-clean</literal> オプションを使うと問題があります (たとえば、物理ディスク上にファイルシステムが既に存在している場合、問題があります)。このため、デフォルトでこのオプションは有効化されません。</para>
        </sidebar>

	<para>RAID-1 アレイを構成するディスクの 1 台に障害が発生した場合、何が起きるかを見て行きましょう。<command>mdadm</command> に <literal>--fail</literal> オプションを付けることで、ディスク障害を模倣することが可能です。</para>

        <screen><computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde</userinput>
<computeroutput>mdadm: set /dev/sde faulty in /dev/md1
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
    Update Time : Wed May  6 09:39:39 2015
          State : clean, degraded 
 Active Devices : 1
Working Devices : 1
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 19

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       0        0        2      removed

       1       8       64        -      faulty   /dev/sde</computeroutput></screen>

	<para>RAID-1 ボリュームの内容はまだアクセスすることが可能ですが (そして、RAID-1 ボリュームがマウントされていた場合、アプリケーションはディスク障害に気が付きませんが)、データの安全性はもはや保証されません。つまり <filename>sdd</filename> ディスクにも障害が発生した場合、データは失われます。この危険性を避けるために、障害の発生したディスクを新しいディスク <filename>sdf</filename> に交換します。</para>

        <screen><computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>
<computeroutput>mdadm: added /dev/sdf
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
   Raid Devices : 2
  Total Devices : 3
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:48:49 2015
          State : clean, degraded, recovering 
 Active Devices : 1
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 1

 Rebuild Status : 28% complete

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 26

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      spare rebuilding   /dev/sdf

       1       8       64        -      faulty   /dev/sde
# </computeroutput><userinput>[...]</userinput>
<computeroutput>[...]
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
    Update Time : Wed May  6 09:49:08 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 41

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf

       1       8       64        -      faulty   /dev/sde</computeroutput></screen>

	<para>繰り返しになりますが、ボリュームはまだアクセスすることが可能とは言うもののボリュームが信頼性低下状態ならば、カーネルは自動的に再構成作業を実行します。再構成作業が終了したら、RAID アレイは正常状態に戻ります。ここで、システムに <filename>sde</filename> ディスクをアレイから削除することを伝えることが可能です。削除することで、2 台のディスクからなる古典的な RAID ミラーになります。</para>

        <screen><computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>
<computeroutput>mdadm: hot removed /dev/sde from /dev/md1
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf</computeroutput></screen>

	<para>この後、今後サーバの電源を切った際にドライブを物理的に取り外したり、ハードウェア設定がホットスワップに対応しているならばドライブをホットリムーブすることが可能です。一部の SCSI コントローラ、多くの SATA ディスク、USB や Firewire で接続された外部ドライブなどはホットスワップに対応しています。</para>
      </section>
      <section id="sect.backup-raid-config">
        <title>設定のバックアップ</title>

	<para>RAID ボリュームに関連するメタデータのほとんどはアレイを構成するディスク上に直接保存されています。このため、カーネルはアレイとその構成要素を検出し、システムの起動時に自動的にアレイを組み立てることが可能です。しかしながら、この設定をバックアップすることを推奨します。なぜなら、この検出機構は不注意による間違いを防ぐものではないからです。そして、注意して取り扱うべき状況ではまさに検出機構がうまく働かないことが見込まれます。上の例で、<filename>sde</filename> ディスク障害が本物で (模倣でない)、<filename>sde</filename> ディスクを取り外す前にシステムを再起動した場合、<filename>sde</filename> ディスクは再起動中に検出され、システムに復帰します。カーネルは 3 つの物理ディスクを検出し、それぞれのディスクが同じ RAID ボリュームの片割れであると主張します。さらに別の混乱する状況が考えられます。2 台のサーバで使われていた RAID ボリュームを片方のサーバに集約することを考えてみましょう。ディスクが移動される前、各アレイは正常に実行されていました。カーネルはアレイを検出して、適切なペアを組み立てることが可能です。しかし、片方のサーバに移動されたディスクが前のサーバでは <filename>md1</filename> に組み込まれており、さらに新しいサーバが既に <filename>md1</filename> という名前のアレイを持っていた場合、どちらか一方の名前が変えられます。</para>

	<para>このため、参考情報に過ぎないとは言うものの、設定を保存することは重要です。設定を保存する標準的な方法は <filename>/etc/mdadm/mdadm.conf</filename> ファイルを編集することです。以下に例を示します。</para>

        <example id="example.mdadm-conf">
          <title><command>mdadm</command> 設定ファイル</title>

          <programlisting># mdadm.conf
#
# このファイルに関する詳細は mdadm.conf(5) を参照してください。
#

# デフォルト (組み込み) 状態ならば、MD スーパーブロックを持つパーティション
# (/proc/partitions) とコンテナをすべてスキャンします。以下のようにスキャンする
# デバイスを指定することも可能です。必要ならばワイルドカードを使ってください。
DEVICE /dev/sd*

# デバイスの自動作成時に使う Debian の標準的なパーミッションを指定します
CREATE owner=root group=disk mode=0660 auto=yes

# 新規アレイの所属先にローカルシステムを自動登録します
HOMEHOST &lt;system&gt;

# 監視デーモンに root を警告メールの送信先として通知します
MAILADDR root

# 既存の MD アレイの定義
ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464

# この設定ファイルは mkconf 3.2.5-3 により
# Fri, 18 Jan 2013 00:21:01 +0900 に自動生成されました</programlisting>
        </example>

	<para>最も役に立つ設定項目の 1 つに <literal>DEVICE</literal> オプションがあります。これは起動時にシステムが RAID ボリュームの構成情報を自動的に探すデバイスをリストします。上の例では、値をデフォルト値 <literal>partitions containers</literal> からデバイスファイルを明示したリストに置き換えました。なぜなら、パーティションだけでなくすべてのディスクをボリュームとして使うように決めたからです。</para>

	<para>上の例における最後の 2 行を使うことで、カーネルはアレイに割り当てるボリューム番号を安全に選ぶことが可能です。ディスク本体に保存されたメタ情報はボリュームを再度組み上げるのに十分ですが、ボリューム番号を定義する (そして <filename>/dev/md*</filename> デバイス名にマッチすることを確認する) には不十分です。</para>

	<para>幸いなことに、以下のコマンドを実行すればこの行を自動的に生成することが可能です。</para>

        <screen><computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>
<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</computeroutput></screen>

	<para>最後の 2 行の内容はボリュームを構成するディスクのリストに依存しません。このため、障害の発生したディスクを新しいディスクに交換した際に、これを改めて生成する必要はありません。逆に、RAID アレイを作成および削除した際に、必ずこの設定ファイルを注意深く更新する必要があります。</para>
      </section>
    </section>
    <section id="sect.lvm">
      <title>LVM</title>
      <indexterm><primary>LVM</primary></indexterm>
      <indexterm><primary>論理ボリュームマネージャ</primary></indexterm>

      <para>LVM (<emphasis>論理ボリュームマネージャ</emphasis>) は物理ディスクから論理ボリュームを抽象化するもう一つの方法で、信頼性を増加させるのではなく柔軟性を増加させることに注目しています。LVM を使うことで、アプリケーションから見る限り透過的に論理ボリュームを変更することが可能です。LVM を使うことで、たとえば新しいディスクを追加し、データを新しいディスクに移行し、古いディスクを削除することがボリュームをアンマウントせずに可能です。</para>
      <section id="sect.lvm-concepts">
        <title>LVM の概念</title>

	<para>LVM の柔軟性は 3 つの概念から構成された抽象化レベルによって達成されます。</para>

	<para>1 番目の概念は PV (<emphasis>物理ボリューム</emphasis>) です。PV はハードウェアに最も近い要素です。具体的に言えば、PV はディスクのパーティション、ディスク全体、その他の任意のブロックデバイス (たとえば、RAID アレイ) などの物理的要素を指します。物理的要素を LVM の PV に設定した場合、物理的要素へのアクセスは必ず LVM を介すべきという点に注意してください。そうでなければ、システムが混乱します。</para>

	<para>2 番目の概念は VG (<emphasis>ボリュームグループ</emphasis>) です。複数の PV は VG にクラスタ化することが可能です。VG は仮想的かつ拡張できるディスクに例えられます。VG は概念的な要素で、<filename>/dev</filename> 階層のデバイスファイルに現れません。そのため、VG を直接的に操作する危険はありません。</para>

	<para>3 番目の概念は LV (<emphasis>論理ボリューム</emphasis>) です。LV は VG の中の 1 つの塊です。さらに VG をディスクに例えたのと同様の考え方を使うと、LV はパーティションに例えられます。LV はブロックデバイスとして <filename>/dev</filename> に現れ、他の物理パーティションと同様に取り扱うことが可能です (一般的に言えば、LV にファイルシステムやスワップ領域を作成することが可能です)。</para>

	<para>ここで重要な事柄は VG を LV に分割する場合に物理的要素 (PV) はいかなる制約も要求しないという点です。1 つの PV (たとえばディスク) から構成される VG を複数の LV に分割できます。同様に、複数の PV から構成される VG を 1 つの大きな LV として提供することも可能です。制約事項がたった 1 つしかないのは明らかです。それはある VG から分割された LV のサイズの合計はその VG を構成する PV のサイズの合計を超えることができないという点です。</para>

	<para>しかしながら、ある VG を構成する PV 同士の性能を同様のものにしたり、その VG から分割された LV 同士に求められる性能を同様のものにしたりすることは通常理に適った方針です。たとえば、利用できるハードウェアに高速な PV と低速な PV がある場合、高速な PV から構成される VG と低速な PV から構成される VG に分けると良いでしょう。こうすることで、高速な PV から構成される VG から分割された LV を高速なデータアクセスを必要とするアプリケーションに割り当て、低速な PV から構成される VG から分割された LV を負荷の少ない作業用に割り当てることが可能です。</para>

	<para>いかなる場合でも、LV は特定の PV を使用するわけではないという点を覚えておいてください。ある LV に含まれるデータの物理的な保存場所を操作することも可能ですが、普通に使っている限りその必要はありません。逆に、VG を構成する PV 群の構成要素が変化した場合、ある LV に含まれるデータの物理的な保存場所は対象の LV の分割元である VG の中ひいてはその VG を構成する PV 群の構成要素の中を移動することがあります (もちろん、データの移動先は対象の LV の分割元の VG を構成する PV 群の構成要素の中に限られます)。</para>
      </section>
      <section id="sect.lvm-setup">
        <title>LVM の設定</title>

	<para>典型的な用途に対する LVM の設定過程を、段階的に見て行きましょう。具体的に言えば、複雑なストレージの状況を単純化したい場合を見ていきます。通常、長く複雑な一時的措置を繰り返した挙句の果てに、この状況に陥ることがあります。説明目的で、徐々にストレージを変更する必要のあったサーバを考えます。このサーバでは、PV として利用できるパーティションが複数の一部使用済みディスクに分散しています。より具体的に言えば、以下のパーティションを PV として利用できます。</para>
        <itemizedlist>
          <listitem>
	    <para><filename>sdb</filename> ディスク上の <filename>sdb2</filename> パーティション (4 GB)。</para>
          </listitem>
          <listitem>
	    <para><filename>sdc</filename> ディスク上の <filename>sdc3</filename> パーティション (3 GB)。</para>
          </listitem>
          <listitem>
	    <para><filename>sdd</filename> ディスク (4 GB) は全領域を利用できます。</para>
          </listitem>
          <listitem>
	    <para><filename>sdf</filename> ディスク上の <filename>sdf1</filename> パーティション (4 GB) および <filename>sdf2</filename> パーティション (5 GB)。</para>
          </listitem>
        </itemizedlist>

	<para>加えて、<filename>sdb</filename> と <filename>sdf</filename> が他の 2 台に比べて高速であると仮定しましょう。</para>

	<para>今回の目標は、3 種類の異なるアプリケーション用に 3 つの LV を設定することです。具体的に言えば、5 GB のストレージ領域が必要なファイルサーバ、データベース (1 GB)、バックアップ用の領域 (12 GB) 用の LV を設定することです。ファイルサーバとデータベースは高い性能を必要とします。しかし、バックアップはアクセス速度をそれほど重要視しません。これらの要件により、各アプリケーションに設定する LV の使用する PV が決定されます。さらに LVM を使いますので、PV の物理的サイズからくる制限はありません。このため、PV 群として利用できる領域のサイズの合計だけが制限となります。</para>

	<para>LVM の設定に必要なツールは <emphasis role="pkg">lvm2</emphasis> パッケージとその依存パッケージに含まれています。これらのパッケージをインストールしたら、3 つの手順を踏んで LVM を設定します。各手順は LVM の概念の 3 つの抽象化レベルに対応します。</para>

	<para>最初に、<command>pvcreate</command> を使って PV を作成します。</para>

        <screen id="screen.pvcreate"><computeroutput># </computeroutput><userinput>pvdisplay</userinput>
<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2</userinput>
<computeroutput>  Physical volume "/dev/sdb2" successfully created
# </computeroutput><userinput>pvdisplay</userinput>
<computeroutput>  "/dev/sdb2" is a new physical volume of "4.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/sdb2
  VG Name               
  PV Size               4.00 GiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I

# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</userinput>
<computeroutput>  Physical volume "/dev/sdc3" successfully created
  Physical volume "/dev/sdd" successfully created
  Physical volume "/dev/sdf1" successfully created
  Physical volume "/dev/sdf2" successfully created
# </computeroutput><userinput>pvdisplay -C</userinput>
<computeroutput>  PV         VG   Fmt  Attr PSize PFree
  /dev/sdb2       lvm2 ---  4.00g 4.00g
  /dev/sdc3       lvm2 ---  3.09g 3.09g
  /dev/sdd        lvm2 ---  4.00g 4.00g
  /dev/sdf1       lvm2 ---  4.10g 4.10g
  /dev/sdf2       lvm2 ---  5.22g 5.22g
</computeroutput></screen>

	<para>ここまでは順調です。PV はディスク全体およびディスク上の各パーティションに対して設定することが可能という点に注意してください。上に示した通り、<command>pvdisplay</command> コマンドは既存の PV をリストします。出力フォーマットは 2 種類あります。</para>

	<para><command>vgcreate</command> を使って、これらの PV から VG を構成しましょう。高速なディスクの PV から <filename>vg_critical</filename> VG を構成します。さらに、これ以外の低速なディスクの PV から <filename>vg_normal</filename> VG を構成します。</para>

        <screen id="screen.vgcreate"><computeroutput># </computeroutput><userinput>vgdisplay</userinput>
<computeroutput>  No volume groups found
# </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1</userinput>
<computeroutput>  Volume group "vg_critical" successfully created
# </computeroutput><userinput>vgdisplay</userinput>
<computeroutput>  --- Volume group ---
  VG Name               vg_critical
  System ID             
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               8.09 GiB
  PE Size               4.00 MiB
  Total PE              2071
  Alloc PE / Size       0 / 0   
  Free  PE / Size       2071 / 8.09 GiB
  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp

# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</userinput>
<computeroutput>  Volume group "vg_normal" successfully created
# </computeroutput><userinput>vgdisplay -C</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize  VFree 
  vg_critical   2   0   0 wz--n-  8.09g  8.09g
  vg_normal     3   0   0 wz--n- 12.30g 12.30g
</computeroutput></screen>

	<para>繰り返しになりますが、<command>vgdisplay</command> コマンドはかなり簡潔です (そして <command>vgdisplay</command> には 2 種類の出力フォーマットがあります)。同じ物理ディスク上にある 2 つの PV から 2 つの異なる VG を構成することが可能である点に注意してください。また、<filename>vg_</filename> 接頭辞を VG の名前に使っていますが、これは慣例に過ぎない点に注意してください。</para>

	<para>これでサイズが約 8 GB と約 12 GB の 2 台の「仮想ディスク」(VG) を手に入れたことになります。それでは仮想ディスク (VG) を「仮想パーティション」(LV) に分割しましょう。これを行うには <command>lvcreate</command> コマンドを少し複雑な構文で実行します。</para>

        <screen id="screen.lvcreate"><computeroutput># </computeroutput><userinput>lvdisplay</userinput>
<computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical</userinput>
<computeroutput>  Logical volume "lv_files" created
# </computeroutput><userinput>lvdisplay</userinput>
<computeroutput>  --- Logical volume ---
  LV Path                /dev/vg_critical/lv_files
  LV Name                lv_files
  VG Name                vg_critical
  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT
  LV Write Access        read/write
  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400
  LV Status              available
  # open                 0
  LV Size                5.00 GiB
  Current LE             1280
  Segments               2
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0

# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical</userinput>
<computeroutput>  Logical volume "lv_base" created
# </computeroutput><userinput>lvcreate -n lv_backups -L 12G vg_normal</userinput>
<computeroutput>  Logical volume "lv_backups" created
# </computeroutput><userinput>lvdisplay -C</userinput>
<computeroutput>  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_base    vg_critical -wi-a---  1.00g                                           
  lv_files   vg_critical -wi-a---  5.00g                                           
  lv_backups vg_normal   -wi-a--- 12.00g</computeroutput></screen>

	<para>LV を作成する場合、2 種類のパラメータが必要です。このため、必ず 2 種類のパラメータをオプションとして <command>lvcreate</command> に渡します。作成する LV の名前を <literal>-n</literal> オプションで指定し、サイズを <literal>-L</literal> オプションで指定します。また、操作対象の VG をコマンドに伝えることが必要です。これはもちろん最後のコマンドラインパラメータです。</para>

        <sidebar>
          <title><emphasis>GOING FURTHER</emphasis> <command>lvcreate</command> のオプション</title>

	  <para><command>lvcreate</command> コマンドは複数のオプションを取り、作成する LV を微調整することが可能です。</para>

	  <para>最初に <literal>-l</literal> オプションについて説明しましょう。<literal>-l</literal> オプションを使った場合 LV のサイズをブロック数 (上の例で用いた「人間にとって分かりやすい」単位ではありません) で指定することが可能です。ブロックとは (LVM の用語で PE すなわち<emphasis>物理エクステント</emphasis>と呼ばれています) PV 中のストレージ領域の連続した単位です。ブロックは LV 中に分散されています。ある LV 用のストレージ領域を正確に定義したい場合、たとえば利用できる領域のすべてを使いたい場合、<literal>-l</literal> オプションのほうが <literal>-L</literal> オプションよりも使いやすいでしょう。</para>

	  <para>LV の物理的な位置を示唆することも可能です。こうすることで、LV の PE は特定の PV 上 (もちろん、VG を構成する PV 上に限ります) に作成されます。<filename>sdb</filename> は <filename>sdf</filename> よりも高速なので、<filename>lv_base</filename> を <filename>sdb</filename> 上に作成することでファイルサーバよりもデータベースサーバが高速にアクセスできるようにするには、以下のコマンドラインを使います。すなわち <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command> です。指定した PV に十分な空き PE がない場合、このコマンドは失敗する可能性があります。今回の例でこのような失敗を防ぐには <filename>lv_files</filename> の前に <filename>lv_base</filename> を作成するか、<command>pvmove</command> コマンドを使って <filename>sdb2</filename> に多少の領域を空ける必要があるかもしれません。</para>
        </sidebar>

	<para>LV が作成され、ブロックデバイスファイルとして <filename>/dev/mapper/</filename> に現れます。</para>

        <screen><computeroutput># </computeroutput><userinput>ls -l /dev/mapper</userinput>
<computeroutput>合計 0
crw------- 1 root root 10, 236  6月 10 16:52 control
lrwxrwxrwx 1 root root       7  6月 10 17:05 vg_critical-lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root       7  6月 10 17:05 vg_critical-lv_files -&gt; ../dm-0
lrwxrwxrwx 1 root root       7  6月 10 17:05 vg_normal-lv_backups -&gt; ../dm-2
# </computeroutput><userinput>ls -l /dev/dm-*</userinput>
<computeroutput>brw-rw---T 1 root disk 253, 0  6月 10 17:05 /dev/dm-0
brw-rw---- 1 root disk 253, 1  6月 10 17:05 /dev/dm-1
brw-rw---- 1 root disk 253, 2  6月 10 17:05 /dev/dm-2
</computeroutput></screen>

        <sidebar>
          <title><emphasis>NOTE</emphasis> LVM ボリュームの自動検出</title>

          <para>コンピュータの起動時に、<filename>lvm2-activation</filename> systemd サービスユニットは <command>vgchange -aay</command> を実行して VG を「始動」します。具体的に言えば、<filename>lvm2-activation</filename> systemd サービスユニットは利用できるデバイスを探します。そして LVM サブシステムに LVM 用の PV として初期化されたデバイスが登録され、PV から構成される VG が開始され、VG から分割された LV が開始され、LV が利用できるようになります。このため、LVM ボリュームを作成したり変更する際に設定ファイルを編集する必要はありません。</para>

	  <para>しかしながら、LVM 要素 (PV、LV、GV) の配置図は <filename>/etc/lvm/backup</filename> にバックアップされ、問題が起きた時 (見えないところで何が行われているかを確認したい時) に有益です。</para>
        </sidebar>

	<para>ブロックデバイスファイルを分かり易くするために、VG に対応するディレクトリの中に便利なシンボリックリンクが作成されます。</para>

        <screen><computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical</userinput>
<computeroutput>合計 0
lrwxrwxrwx 1 root root 7  6月 10 17:05 lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root 7  6月 10 17:05 lv_files -&gt; ../dm-0
# </computeroutput><userinput>ls -l /dev/vg_normal</userinput>
<computeroutput>合計 0
lrwxrwxrwx 1 root root 7  6月 10 17:05 lv_backups -&gt; ../dm-2</computeroutput></screen>

	<para>LV は標準的なパーティションと全く同様に取り扱われます。</para>

        <screen><computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups</userinput>
<computeroutput>mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 3145728 4k blocks and 786432 inodes
Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d
[...]
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </computeroutput><userinput>mkdir /srv/backups</userinput>
<computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups</userinput>
<computeroutput># </computeroutput><userinput>df -h /srv/backups</userinput>
<computeroutput>ファイルシス                     サイズ  使用  残り 使用% マウント位置
/dev/mapper/vg_normal-lv_backups    12G   30M   12G    1% /srv/backups
# </computeroutput><userinput>[...]</userinput>
<computeroutput>[...]
# </computeroutput><userinput>cat /etc/fstab</userinput>
<computeroutput>[...]
/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2
/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2
/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</computeroutput></screen>

	<para>アプリケーションにしてみれば、無数の小さなパーティションがわかり易い名前を持つ 1 つの大きな 12 GB のボリュームにまとめられたことになります。</para>
      </section>
      <section id="sect.lvm-over-time">
        <title>経時変化に伴う LVM の利便性</title>

	<para>LVM のパーティションや物理ディスクを統合する機能は便利ですが、これは LVM のもたらす主たる利点ではありません。時間経過に伴い LVM のもたらす柔軟性が特に重要になる時とは LV のサイズを増加させる必要が生じた時でしょう。ここまでの例を使い、LV に新たに巨大なファイルを保存したいけれども、ファイルサーバ用の LV はこの巨大なファイルを保存するには狭すぎると仮定しましょう。<filename>vg_critical</filename> から分割できる全領域はまだ使い切られていないので、<filename>lv_files</filename> のサイズを増やすことが可能です。LV のサイズを増やすために <command>lvresize</command> コマンドを使い、LV のサイズの変化にファイルシステムを対応させるために <command>resize2fs</command> を使います。</para>

        <screen><computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>
<computeroutput>ファイルシス                     サイズ  使用  残り 使用% マウント位置
/dev/mapper/vg_critical-lv_files   5.0G  4.6G  146M   97% /srv/files
# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>
<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 5.00g
# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   2   2   0 wz--n- 8.09g 2.09g
# </computeroutput><userinput>lvresize -L 7G vg_critical/lv_files</userinput>
<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).
  Logical volume lv_files successfully resized
# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>
<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 7.00g
# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>
<computeroutput>resize2fs 1.42.12 (29-Aug-2014)
Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.

# </computeroutput><userinput>df -h /srv/files/</userinput>
<computeroutput>ファイルシス                     サイズ  使用  残り 使用% マウント位置
/dev/mapper/vg_critical-lv_files   6.9G  4.6G  2.1G   70% /srv/files</computeroutput></screen>

        <sidebar>
          <title><emphasis>CAUTION</emphasis> ファイルシステムのサイズ変更</title>

	  <para>すべてのファイルシステムがオンラインでサイズを変更できるわけではありません。このため、ボリュームのサイズ変更前にファイルシステムをアンマウントし、ボリュームのサイズ変更後に再マウントしなければいけません。もちろん、ボリュームのサイズを小さくする場合、ボリューム上のファイルシステムのサイズを小さくした後にボリュームのサイズを小さくしなければいけません。ボリュームのサイズを大きくする場合、ボリュームのサイズを大きくした後にボリューム上のファイルシステムを大きくしなければいけません。これはかなりわかり易いです。なぜなら、ブロックデバイス上に存在するファイルシステムのサイズをブロックデバイスよりも大きくすることは絶対に不可能だからです (この原則はボリュームが物理パーティションか LV かに依存しません)。</para>

	  <para>ext3、ext4、xfs ファイルシステムはオンラインでサイズを増加させることすなわちアンマウントすることなくサイズを増加させることが可能です。しかし、サイズを減少させる場合はアンマウントを必要とします。reiserfs はオンラインでサイズを増加および減少することが可能です。ext2 は増加も減少も可能ですが、アンマウントを必要とします。</para>
        </sidebar>

	<para>同様の方法でデータベースをホストしている <filename>lv_base</filename> のサイズを増加させます。以下の通り <filename>lv_base</filename> の分割元である <filename>vg_critical</filename> から分割できる領域は既にほぼ使い切った状態になっています。</para>

        <screen><computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>
<computeroutput>ファイルシス                    サイズ  使用  残り 使用% マウント位置
/dev/mapper/vg_critical-lv_base  1008M  854M  104M   90% /srv/base
# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree 
  vg_critical   2   2   0 wz--n- 8.09g 92.00m</computeroutput></screen>

	<para>でもご安心ください。LVM を使っていれば新しい PV を既存の VG を構成する PV の 1 つとして追加することが可能です。たとえば、今までは LVM の外で管理されていた <filename>sdb1</filename> パーティションには、<filename>lv_backups</filename> に移動しても問題のないアーカイブだけが含まれていた点に気が付いたとしましょう。このため、<filename>sdb1</filename> パーティションを <filename>vg_critical</filename> を構成する PV の 1 つとして再利用することが可能です。こうすることで、<filename>vg_critical</filename> から <filename>lv_base</filename> に分割される領域のサイズを増やすことが可能です。これが <command>vgextend</command> コマンドの目的です。もちろん、事前に <filename>sdb1</filename> パーティションを PV として準備しなければいけません。<filename>vg_critical</filename> を拡張したら、先と同様のコマンドを使って先に <filename>lv_base</filename> のサイズを増加させ、その後に <filename>lv_base</filename> 上のファイルシステムのサイズを増加させます。</para>

        <screen><computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>
<computeroutput>  Physical volume "/dev/sdb1" successfully created
# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>
<computeroutput>  Volume group "vg_critical" successfully extended
# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   3   2   0 wz--n- 9.09g 1.09g
# </computeroutput><userinput>[...]</userinput>
<computeroutput>[...]
# </computeroutput><userinput>df -h /srv/base/</userinput>
<computeroutput>ファイルシス                    サイズ  使用  残り 使用% マウント位置
/dev/mapper/vg_critical-lv_base   2.0G  854M  1.1G   45% /srv/base</computeroutput></screen>

        <sidebar>
          <title><emphasis>GOING FURTHER</emphasis> LVM の上級活用</title>

	  <para>LVM にはさらに上級の使い方があり、多くの設定事項を手作業で指定することが可能です。たとえば、管理者は PV と LV のブロックサイズおよびボリュームの物理的な配置を微調整することが可能です。また、ブロックを PV 間で移動することも可能です。これは、たとえば性能を微調整したり、よりありふれたケースではある物理ディスクに対応する PV を VG の構成要素から外したりするため (PV を別の VG に移動したり、完全に LVM から取り外したりするため) に行われます。コマンドを説明しているマニュアルページは基本的に明快で詳細です。手始めに、<citerefentry><refentrytitle>lvm</refentrytitle> <manvolnum>8</manvolnum></citerefentry> マニュアルページを参照することをお勧めします。</para>
        </sidebar>
      </section>
    </section>
    <section id="sect.raid-or-lvm">
      <title>RAID それとも LVM?</title>

      <para>1 番目の利用形態は用途が時間的に変化しない 1 台のハードディスクを備えたデスクトップコンピュータのような単純な利用形態です。この場合 RAID と LVM はどちらも疑う余地のない利点をもたらします。しかしながら、RAID と LVM は目標を分岐させて別々の道を歩んでいます。どちらを使うべきか悩むのは間違っていることではありません。最も適切な答えはもちろん現在の要求と将来に予測される要求に依存します。</para>

      <para>いくつかの状況では、疑問の余地がないくらい簡単に答えを出すことが可能です。2 番目の利用形態はハードウェア障害からデータを保護することが求められる利用形態です。この場合、ディスクの冗長性アレイ上に RAID をセットアップするのは明らかです。なぜなら LVM はこの種の問題への対応策を全く用意していないからです。逆に、柔軟なストレージ計画が必要でディスクの物理的な配置に依存せずにボリュームを構成したい場合、RAID はあまり役に立たず LVM を選ぶのが自然です。</para>

      <sidebar>
        <title><emphasis>NOTE</emphasis> 性能が重要な場合</title>

	<para>特にアクセス速度という意味の入出力速度が最重要な場合を考えてみましょう。LVM および RAID はどのような組み合わせで使っても性能にある程度の影響をおよぼします。このため、どの組み合わせを採用するかが議題に挙げられるかもしれません。しかしながら、どんな組み合わせを使っても性能差は極めて少なく、この程度の性能差が無視できない場合は極めて少ないと言えるでしょう。性能が重要な場合、実現できる最良の改善方針は非回転ストレージメディア (<indexterm><primary>SSD</primary></indexterm><emphasis>ソリッドステートドライブ</emphasis>すなわち SSD) を使うことです。SSD のメガバイト当たりの費用は標準的なハードディスクドライブよりも高価で、SSD の容量は通常小さいですが、SSD はランダムアクセスで素晴らしい性能を発揮します。ファイルシステムに広く分散された位置から数多くの入出力を行うような場合 (たとえば複雑な問い合わせが定期的に実行されるデータベースの場合)、SSD 上にデータベースを置くほうが RAID over LVM または LVM over RAID 上にデータベースを置くよりも良好な性能が手に入ります。このような場合、純粋な速度だけでなく他の要素も検討した上で採用の可否を決定するべきです。なぜなら、性能が必要な場合に SSD を採用することは最も安直な解決策だからです。</para>
      </sidebar>

      <para>3 番目に注目すべき利用形態は単に 2 つのディスクを 1 つのボリュームにまとめるような利用形態です。性能が欲しかったり、利用できるディスクのどれよりも大きな単一のファイルシステムにしたい場合にこの利用形態が採用されます。この場合、RAID-0 (またはリニア RAID) か LVM ボリュームを使って対処できます。この状況では、追加的な制約事項 (たとえば、他のコンピュータが RAID だけを使っている場合に RAID を使わなければいけないなどの制約事項) がなければ、通常 LVM を選択すると良いでしょう。LVM の最初のセットアップは RAID に比べて複雑ですが、LVM は複雑度を少し増加させるだけで要求が変った場合や新しいディスクを追加する必要ができた場合に対処可能な追加的な柔軟性を大きく上昇させます。</para>

      <para>そしてもちろん、最後の本当に興味深い利用形態はストレージシステムにハードウェア障害に対する耐性を持たせさらにボリューム分割に対する柔軟性を持たせる必要がある場合の利用形態です。RAID と LVM のどちらも片方だけで両方の要求を満足させることは不可能です。しかし心配ありません。この要求を満足させるには RAID と LVM の両方を同時に使用する方針、正確に言えば一方の上に他方を構成する方針を採用すれば良いでのです。RAID と LVM の高い成熟度のおかげでほぼ標準になりつつある方針に従うならば、最初にディスクを少数の大きな RAID アレイにグループ分けすることでデータの冗長性を確保します。さらにそれらの RAID アレイを LVM の PV として使います。そして、ファイルシステム用の VG から分割された LV を論理パーティションとして使います。この標準的な方針の優れた点は、ディスク障害が起きた場合に再構築しなければいけない RAID アレイの数が少ない点です。このため、管理者は復旧に必要な時間を減らすことが可能です。</para>

      <para>ここで具体例を見てみましょう。たとえば Falcot Corp の広報課は動画編集用にワークステーションを必要としていますが、広報課の予算の都合上、最初から高性能のハードウェアに投資することは不可能です。このため、グラフィック性能を担うハードウェア (モニタとビデオカード) に大きな予算を割き、ストレージ用には一般的なハードウェアを使うことが決定されました。しかしながら、広く知られている通りデジタルビデオ用のストレージはある種の条件を必要とします。すなわち、保存されるデータのサイズが大きく、このデータを読み込みおよび書き込みする際の処理速度がシステム全体の性能にとって重要 (たとえば、平均的なアクセス時間よりも重要) という条件です。この条件を一般的なハードウェアを使って満足させる必要があります。今回の場合 2 台の SATA ハードディスクドライブを使います。さらに、システムデータと一部のユーザデータはハードウェア障害に対する耐性を持たせる必要があります。編集済みのビデオクリップを保護する必要はありますが、編集前のビデオ素材をそれほど気にする必要はありません。なぜなら、編集前のビデオ素材はまだビデオテープに残されているからです。</para>

      <para>前述の条件を満足させるために RAID-1 と LVM を組み合わせます。ディスクの並行アクセスを最適化し、そして障害が同時に発生する危険性を減らすために、各ディスクは 2 つの異なる SATA コントローラに接続されています。このため、各ディスクは <filename>sda</filename> と <filename>sdc</filename> として現れます。どちらのディスクも以下に示したパーティショニング方針に従ってパーティショニングされます。</para>

      <screen><computeroutput># </computeroutput><userinput>fdisk -l /dev/sda</userinput>
<computeroutput>
Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x00039a9f

Device    Boot     Start       End   Sectors Size Id Type
/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect
/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris
/dev/sda3        4000185 586099395 582099210 298G 5  Extended
/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect
/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect
/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</computeroutput></screen>
      <itemizedlist>
        <listitem>
	  <para><filename>sda1</filename> と <filename>sdc1</filename> パーティション (約 1 GB) から RAID-1 ボリューム <filename>md0</filename> を構成します。<filename>md0</filename> はルートファイルシステムを保存するために直接的に使われます。</para>
        </listitem>
        <listitem>
	  <para><filename>sda2</filename> と <filename>sdc2</filename> パーティションから swap パーティションを作成します。スワップ領域のサイズは合計で 2 GB になります。RAM のサイズ 1 GB と合わせれば、ワークステーションで利用できるメモリサイズは十分な量と言えます。</para>
        </listitem>
        <listitem>
	  <para><filename>sda5</filename> と <filename>sdc5</filename> パーティションおよび <filename>sda6</filename> と <filename>sdc6</filename> パーティションからそれぞれ約 100 GB の 2 つの新しい RAID-1 ボリューム <filename>md1</filename> と <filename>md2</filename> を構成します。<filename>md1</filename> と <filename>md2</filename> は LVM の PV として初期化され、これらの PV から <filename>vg_raid</filename> VG を構成します。<filename>vg_raid</filename> は約 200 GB の安全な領域になります。</para>
        </listitem>
        <listitem>
	  <para>残りのパーティションである <filename>sda7</filename> と <filename>sdc7</filename> はそのまま LVM の PV として初期化され、これらの PV から <filename>vg_bulk</filename> VG を構成します。<filename>vg_bulk</filename> はおよそ 200 GB の領域になります。</para>
        </listitem>
      </itemizedlist>

      <para>VG を作成したら、VG をとても柔軟な方法で LV に分割することが可能です。<filename>vg_raid</filename> から分割された LV は 1 台のディスク障害に対して耐性を持ちますが、<filename>vg_bulk</filename> から分割された LV はディスク障害に対する耐性を持たない点を忘れないでください。逆に、<filename>vg_bulk</filename> は両方のディスクにわたって割り当てられるので、<filename>vg_bulk</filename> から分割された LV に保存された巨大なファイルの読み書き速度は高速化されるでしょう。</para>

      
      <para><filename>vg_raid</filename> から <filename>lv_usr</filename>、<filename>lv_var</filename>、<filename>lv_home</filename> を分割し、各 LV に応じたファイルシステムをホストさせます。さらに、<filename>vg_raid</filename> からもう一つの大きな LV である <filename>lv_movies</filename> を分割し、<filename>lv_movies</filename> に編集済みの最終版の映像をホストさせます。また、<filename>vg_bulk</filename> からデジタルビデオカメラから取り出したデータ用の大きな <filename>lv_rushes</filename> と一時ファイル用の <filename>lv_tmp</filename> を分割します。<filename>vg_raid</filename> と <filename>vg_bulk</filename> のどちらから作業領域用の LV を分割するかは簡単に決められるものではありません。つまり、作業領域用の LV は良い性能を必要としますが、編集作業中にディスク障害が起きた場合に作業内容を保護する必要があるでしょうか? この質問の回答次第で、作業領域用の LV を <filename>vg_raid</filename> か <filename>vg_bulk</filename> のどちらの VG に作成するかが決まります。</para>

      <para>これで、重要なデータ用に多少の冗長性と、利用できる領域が用途ごとにどのように分割されるかに関する大きな柔軟性が確保されました。後から (たとえば音声クリップの編集用に) 新しいソフトウェアをインストールする場合も、<filename>/usr/</filename> をホストしている LV のサイズを簡単に増加することが可能です。</para>

      <sidebar>
        <title><emphasis>NOTE</emphasis> なぜ 3 種類の RAID-1 ボリュームが必要なのでしょうか?</title>

	<para>RAID-1 ボリュームを 1 つだけ作成し、作成した PV から <filename>vg_raid</filename> を構成し、<filename>vg_raid</filename> から保護したい内容用の LV を分割することも可能でした。それにも関わらず、なぜ 3 種類の RAID-1 ボリュームを作成したのでしょうか?</para>

	<para>最初の分割 (<filename>md0</filename> とその他) の根本的理由はデータの安全性を考慮したためです。つまり RAID-1 ミラーを構成する要素に書き込まれるデータは要素同士で全く同じだからです。そのため RAID 層を迂回し、RAID-1 ミラーを構成する 1 台のディスクだけを直接マウントすることが可能です。すなわち、カーネルにバグがあったり LVM メタ情報が破壊されたりした場合でも、RAID と LVM ボリュームに含まれるディスクの配置などの重要なデータにアクセスするために最小限のシステムを起動することが可能ということです。そして、このメタ情報を再構成したりファイルにアクセスしたりすることが可能です。こうすることで、システムを正常状態に戻すことが可能です。</para>

	<para>2 番目の分割 (<filename>md1</filename> と <filename>md2</filename>) の根本的理由は明確というわけではありませんが、将来の不明確さを認めていることに関連します。動画編集用ワークステーションを最初に組み上げる時点で、要求される正確なストレージサイズを完全な精度で知ることは不可能です。それどころか、ストレージサイズは時間経過に従い増加するかもしれません。今回の場合、ビデオ素材用の <filename>lv_rushes</filename> と編集済みビデオクリップ用の <filename>lv_movies</filename> に対して実際に要求されるストレージサイズを事前に知ることはできません。とても大きな量の素材を必要とするクリップを <filename>lv_rushes</filename> に保存する必要があり、さらに冗長性データ用 VG である <filename>vg_raid</filename> のまだ半分以上が未使用状態ならば、<filename>vg_raid</filename> から未使用領域を再利用することが可能です。具体的には <filename>vg_raid</filename> の構成要素から片方の PV (たとえば <filename>md2</filename>) を削除し、<filename>md2</filename> を <filename>vg_bulk</filename> を構成する PV として初期化するか (予想される作業時間が一時的な性能の低下を許容できる程度に十分短い場合に限ります)、<filename>md2</filename> の RAID-1 セットアップを破棄してその構成要素である <filename>sda6</filename> と <filename>sdc6</filename> を <filename>vg_bulk</filename> を構成する PV として初期化する (この場合 100 GB ではなく 200 GB の増加になります) ことが可能です。そして <filename>lv_rushes</filename> を必要に応じて増加させることが可能です。</para>
      </sidebar>
    </section>
  </section>
  <section id="sect.virtualization">
    <title>仮想化</title>
    <indexterm><primary>仮想化</primary></indexterm> 

    <para>仮想化は最近のコンピューティングにおける最も大きな進歩の 1 つです。仮想化という用語は、実際のハードウェアに対するさまざまな独立性の度合いを持つ仮想コンピュータを模倣するさまざまな抽象化と技術を指します。1 台の物理的なサーバが同時かつ隔離された状態で動く複数のシステムをホストすることが可能です。仮想化アプリケーションは数多く存在し、隔離された仮想システムを使うことができます。たとえば、さまざまに設定されたテスト環境を作ったり、安全性を確保する目的で異なる仮想マシン間でホストされたサービスを分離したりすることが可能です。</para>

    <para>複数の仮想化ソリューションが存在し、それぞれが利点と欠点を持っています。本書では Xen、LXC、KVM に注目しますが、他にも以下のような注目すべき実装が存在します。</para>
    <indexterm><primary><emphasis>VMWare</emphasis></primary></indexterm>
    <indexterm><primary><emphasis>Bochs</emphasis></primary></indexterm>
    <indexterm><primary><emphasis>QEMU</emphasis></primary></indexterm>
    <indexterm><primary><emphasis>VirtualBox</emphasis></primary></indexterm>
    <indexterm><primary><emphasis>KVM</emphasis></primary></indexterm>
    <indexterm><primary><emphasis>LXC</emphasis></primary></indexterm>
    <itemizedlist>
      <listitem>
	<para>QEMU は完全なコンピュータを模倣するソフトウェアエミュレータです。このため QEMU の性能はネイティブに実行した場合の速度には遠くおよびませんが、QEMU を使うことで修正されていなかったり実験的なオペレーティングシステムをエミュレートされたハードウェア上で実行することが可能です。さらに QEMU は異なるハードウェアアーキテクチャをエミュレートすることが可能です。たとえば、<emphasis>amd64</emphasis> システムで <emphasis>arm</emphasis> コンピュータをエミュレートすることが可能です。QEMU はフリーソフトウェアです。<ulink type="block" url="http://www.qemu.org/" /></para>
      </listitem>
      <listitem>
	<para>Bochs は自由な仮想マシンですが、x86 アーキテクチャ (i386 と amd64) だけをエミュレートすることが可能です。</para>
      </listitem>
      <listitem>
	<para>VMWare はプロプライエタリの仮想マシンです。VMWare はこの分野で最も古く、最も広く使われているソフトウェアの 1 つです。VMWare は QEMU とよく似た原理で動いています。VMWare には実行中の仮想マシンのスナップショットなどの高度な機能が含まれています。<ulink type="block" url="http://www.vmware.com/" /></para>
      </listitem>
      <listitem>
        <para>VirtualBox はほぼ自由なソフトウェアの仮想マシンです (一部の追加的な構成要素はプロプライエタリライセンスの下で利用できます)。残念なことに VirtualBox は Debian の「contrib」セクションにあります。なぜなら VirtualBox にはいくつかのコンパイル済みファイルが含まれ、このファイルを再ビルドするにはプロプライエタリコンパイラが必要だからです。VirtualBox は VMWare よりも歴史が浅く、i386 と amd64 アーキテクチャだけをサポートします。しかしながら、VirtualBox はスナップショットやその他の興味深い機能を備えています。<ulink type="block" url="http://www.virtualbox.org/" /></para>
      </listitem>
    </itemizedlist>
    <section id="sect.xen">
      <title>Xen</title>

      <para>Xen<indexterm><primary>Xen</primary></indexterm> は「準仮想化」ソリューションです。Xen には薄い抽象化層が含まれ、この抽象化層は「ハイパーバイザ」と呼ばれ、ハードウェアとその上にあるシステムの間に位置します。さらにハイパーバイザは審判員として振る舞い、仮想マシンからハードウェアへのアクセスを制御します。しかしながら、Xen ハイパーバイザは命令のほんの一部だけを取り扱い、残りは Xen ハイパーバイザではなくハードウェアによって直接的に実行されます。こうすることによる主な有効性は性能が低下せず、システムがネイティブ速度に迫る性能を発揮するという点です。一方で欠点は Xen ハイパーバイザ上でオペレーティングシステムを実行するには実行されるオペレーティングシステムのカーネルを修正しなければいけないという点です。</para>

      <para>用語の解説に少し時間を割きましょう。Xen ハイパーバイザはカーネルよりも下層の最も低い層に位置し、ハードウェア上で直接動きます。Xen ハイパーバイザは残りのソフトウェアをいくつかの<emphasis>ドメイン</emphasis>に分割することが可能で、<emphasis>ドメイン</emphasis>は多数の仮想マシンと考えられます。これらのドメインの 1 つ (最初に起動されたもの) は <emphasis>dom0</emphasis> と呼ばれ、特別な役割を担います。なぜなら、<emphasis>dom0</emphasis> だけが Xen ハイパーバイザを制御することが可能だからです。他のドメインは <emphasis>domU</emphasis> として知られています。ユーザ視点で言い換えれば、<emphasis>dom0</emphasis> は他の仮想システムにおける「ホスト」、これに対して <emphasis>domU</emphasis> は「ゲスト」になります。</para>

      <sidebar>
        <title><emphasis>CULTURE</emphasis> Xen と Linux のさまざまなバージョン</title>

	<para>当初 Xen は Linux 公式ツリーの外部パッチとして開発され、Linux カーネルに組み込まれていませんでした。これと同時期に、複数の次世代仮想化システム (KVM など) は Linux カーネルへの組み込みを簡単にするためにいくつかの包括的な仮想化関連関数を必要としており、さらに Linux カーネルが (<emphasis>paravirt_ops</emphasis> または <emphasis>pv_ops</emphasis> インターフェースとして知られる) 一連の仮想化関連関数を獲得しました。Xen のパッチはこのインターフェースのいくつかの機能を複製していたため、Xen のパッチは公式に受け入れられませんでした。</para>

	<para>このため、Xen を影で支える会社の Xensource は新しく Linux カーネルに取り込まれた仮想化関連関数を使って Xen を移植しなければいけませんでした。この移植作業により、Xen のパッチを公式の Linux カーネルに取り込むことが可能になりました。この移植作業は多くのコードを書き換えることを意味していました。Xensource はすぐに paravirt_ops インターフェースを使って評価版を作ったにも関わらず、Xen のパッチを公式カーネルにマージする作業はゆっくりと進みました。マージか完了したのは Linux 3.0 です。<ulink type="block" url="http://wiki.xenproject.org/wiki/XenParavirtOps" /></para>

	<para><emphasis role="distribution">Jessie</emphasis> は Linux カーネルのバージョン 3.16 に基づくため、標準的な <emphasis role="pkg">linux-image-686-pae</emphasis> と <emphasis role="pkg">linux-image-amd64</emphasis> パッケージには Xen を動作させるために必要なコードが含まれます。Debian の <emphasis role="distribution">Squeeze</emphasis> およびそれ以前のバージョンで必要とされていたディストリビューションに特有のパッチ作業はもはや必要ありません。<ulink type="block" url="http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix" /></para>
      </sidebar>

      <sidebar>
        <title><emphasis>NOTE</emphasis> Xen 互換のアーキテクチャ</title>

        <para>現在のところ、Xen を利用できるのは i386、amd64、arm64、armhf アーキテクチャだけです。</para>
      </sidebar>

      <sidebar>
        <title><emphasis>CULTURE</emphasis> Xen と非 Linux カーネル</title>

	<para>Xen 上でオペレーティングシステムを動作させるには、いかなるオペレーティングシステムであってもそれを修正する必要があります。さらに、すべてのオペレーティングシステムのカーネルが修正点に関して同じ程度の成熟度を持っているとは限りません。多くのオペレーティングシステムは dom0 および domU として完全に動作します。具体的に言えば、Linux 3.0 とそれ以降、NetBSD 4.0 とそれ以降、OpenSolaris は dom0 および domU として完全に動作します。また、他のオペレーティングシステムは dom0 としては動作せず domU として動作します。dom0 および domU として動作するオペレーティングシステムの状況を確認するには Xen のウィキに含まれる該当ページをご覧ください。<ulink type="block" url="http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen" /><ulink type="block" url="http://wiki.xenproject.org/wiki/DomU_Support_for_Xen" /></para>

	<para>しかしながら、Xen で仮想化専用のハードウェア機能 (最近のプロセッサだけが搭載した機能) に依存するオペレーティングシステムを動作させる場合や、修正されていないオペレーティングシステム (Windows など) を動作させる場合、そのオペレーティングシステムは domU としてのみ動作します。</para>
      </sidebar>

      <para>Debian の下で Xen を使うには 3 つの要素が必要です。</para>
      <itemizedlist>
        <listitem>
	  <para>Xen ハイパーバイザ自身。適切なパッケージは利用できるハードウェアによって決まります。すなわち <emphasis role="pkg">xen-hypervisor-4.4-amd64</emphasis>、<emphasis role="pkg">xen-hypervisor-4.4-armhf</emphasis>、<emphasis role="pkg">xen-hypervisor-4.4-arm64</emphasis> のうちどれか 1 つが必要です。</para>
        </listitem>
        <listitem>
	  <para>ハイパーバイザ上で実行するカーネル。バージョン 3.0 より新しい Linux カーネルが動作します。<emphasis role="distribution">Jessie</emphasis> に含まれる Linux カーネルのバージョンは 3.16 なのでこれも動作します。</para>
        </listitem>
        <listitem>
	  <para>さらに i386 アーキテクチャでは、Xen を活用するための適切なパッチを組み込んだ標準的なライブラリが必要です。このライブラリは <emphasis role="pkg">libc6-xen</emphasis> パッケージに含まれます。</para>
        </listitem>
      </itemizedlist>

      <para>複数の構成要素を手作業で選択するという煩わしさを避けるために、いくつかの便利なパッケージ (<emphasis role="pkg">xen-linux-system-amd64</emphasis> など) が用意されています。これらのパッケージをインストールすることで、適切な Xen ハイパーバイザとカーネルパッケージが既知の良い組み合わせで導入されます。ここで導入される Xen ハイパーバイザには <emphasis role="pkg">xen-utils-4.4</emphasis> が含まれます。<emphasis role="pkg">xen-utils-4.4</emphasis> パッケージには dom0 からハイパーバイザを操作するためのツールが含まれます。同様に、<emphasis role="pkg">xen-utils-4.4</emphasis> パッケージには適切な標準的ライブラリが含まれます。すべてのインストール中に、設定スクリプトは Grub ブートローダメニューに新しいエントリを作成します。こうすることで Xen dom0 から選択されたカーネルを開始することが可能です。しかしながら、通常このエントリはリストの最初に置かれないため、デフォルトで選択されません。この点に注意してください。これを望まない場合、以下のコマンドを使って変更してください。</para>

      <screen><computeroutput># </computeroutput><userinput>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen
</userinput><computeroutput># </computeroutput><userinput>update-grub
</userinput></screen>

      <para>これらの前提要件をインストールしたら、次に dom0 の挙動をテストします。テストを行うには、Xen ハイパーバイザと Xen カーネルの再起動が必要です。システムは標準的な方法で起動するべきです。初期化の早い段階でコンソールにいくつかの追加的メッセージが表示されます。</para>

      <para>これで、実用システムを domU システムに実際にインストールできるようになりました。これを行うには <emphasis role="pkg">xen-tools</emphasis> に含まれるツールを使います。<emphasis role="pkg">xen-tools</emphasis> パッケージには <command>xen-create-image</command> コマンドが含まれます。<command>xen-create-image</command> コマンドはインストール作業の大部分を自動化します。必須のパラメータは <literal>--hostname</literal> だけで、このパラメータは domU の名前を設定します。他のオプションは重要ですが、オプションを <filename>/etc/xen-tools/xen-tools.conf</filename> 設定ファイルに保存することが可能です。そして、コマンドラインでオプションを指定しなくてもエラーは起きません。このため、イメージを作る前にこのファイルの内容を確認するか、<command>xen-create-image</command> の実行時に追加的パラメータを使うことが重要です。以下に注目すべき重要なパラメータを示します。</para>
      <itemizedlist>
        <listitem>
	  <para><literal>--memory</literal>。新たに作成する domU システム専用の RAM のサイズを指定します。</para>
        </listitem>
        <listitem>
	  <para><literal>--size</literal> と <literal>--swap</literal>。domU で利用できる「仮想ディスク」のサイズを定義します。</para>
        </listitem>
        <listitem>
	  <para><literal>--debootstrap</literal>。<command>debootstrap</command> を使って新しいシステムをインストールします。このオプションを使う場合、<literal>--dist</literal> オプション (ディストリビューションの名前、たとえば <emphasis role="distribution">jessie</emphasis>) を一緒に使うことが多いです。</para>

          <sidebar>
            <title><emphasis>GOING FURTHER</emphasis> 非 Debian システムを domU にインストール</title>

	    <para>非 Linux システムを domU にインストールする場合、<literal>--kernel</literal> オプションを使って、domU で使うカーネルを定義しなければいけない点に注意してください。</para>
          </sidebar>
        </listitem>
        <listitem>
	  <para><literal>--dhcp</literal>。domU のネットワーク設定を DHCP で取得することを宣言します。対して、<literal>--ip</literal> は静的 IP アドレスを定義します。</para>
        </listitem>
        <listitem>
	  <para>最後に、作成されるイメージ (domU からはハードディスクドライブに見えるイメージ) の保存方法を選択します。最も簡単な方法は、<literal>--dir</literal> オプションを使い、各 domU を格納するデバイス用のファイルを dom0 上に作成する方法です。LVM を使っているシステムでは、<literal>--lvm</literal> オプションを使い、VG の名前を指定しても良いでしょう。この場合 <command>xen-create-image</command> は指定された VG から新しい LV を分割し、この LV をハードディスクドライブとして domU から利用できるようにします。</para>

          <sidebar>
            <title><emphasis>NOTE</emphasis> domU 内のストレージ</title>

	    <para>ハードディスク全体、パーティション、RAID アレイ、既存の LVM の LV を domU に書き出すことも可能です。<command>xen-create-image</command> を使ってもこれらの操作を自動化することは不可能ですが、<command>xen-create-image</command> を使って Xen イメージの設定ファイルを作成した後にその設定ファイルを編集することで対応する操作が可能です。</para>
          </sidebar>
        </listitem>
      </itemizedlist>

      <para>これらを選んだ後、将来の Xen domU 用のイメージを作成することが可能です。</para>

      <screen><computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</userinput>
<computeroutput>
[...]
General Information
--------------------
Hostname       :  testxen
Distribution   :  jessie
Mirror         :  http://ftp.debian.org/debian/
Partitions     :  swap            128Mb (swap)
                  /               2G    (ext3)
Image type     :  sparse
Memory size    :  128Mb
Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64
Initrd path    :  /boot/initrd.img-3.16.0-4-amd64
[...]
Logfile produced at:
         /var/log/xen-tools/testxen.log

Installation Summary
---------------------
Hostname        :  testxen
Distribution    :  jessie
MAC Address     :  00:16:3E:8E:67:5C
IP-Address(es)  :  dynamic
RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b
Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ
</computeroutput></screen>

      <para>これで仮想マシンが作成されましたが、仮想マシンはまだ実行されていません (このため dom0 のハードディスク上の領域が使われているだけです)。もちろん、異なるパラメータを使ってより多くのイメージを作成することが可能です。</para>

      <para>仮想マシンを起動する前に、仮想マシンにアクセスする方法を定義します。もちろん仮想マシンは隔離されたマシンですから、仮想マシンにアクセスする唯一の方法はシステムコンソールだけです。しかし、システムコンソールだけで要求を満足できることはほとんどないと言っても過言ではありません。ほとんどの時間、domU はリモートサーバとして機能し、ネットワークを通じてのみアクセスされます。しかしながら、各 domU 専用のネットワークカードを追加するのはかなり不便です。このため Xen は仮想インターフェースの作成機能を備えています。各ドメインは仮想インターフェースを参照し、標準的な方法で使うことが可能です。これらのネットワークカードは仮想的なものですが、ネットワークに接続されている状況下でのみ役に立つという点に注意してください。Xen は以下に挙げる複数のネットワークモデルを備えています。</para>
      <itemizedlist>
        <listitem>
	  <para>最も単純なモデルは <emphasis>bridge</emphasis> モデルです。この場合、すべての eth0 ネットワークカードが (dom0 と domU システムに含まれるものも含めて) 直接的にイーサネットスイッチに接続されているかのように振る舞います。</para>
        </listitem>
        <listitem>
	  <para>2 番目に単純なモデルは <emphasis>routing</emphasis> モデルです。これは dom0 が domU システムと (物理) 外部ネットワークの間に位置するルータとして振る舞うモデルです。</para>
        </listitem>
        <listitem>
	  <para>最後が <emphasis>NAT</emphasis> モデルです。これは dom0 が domU システムとその他のネットワークの間に位置するモデルですが、domU システムに外部から直接アクセスすることは不可能です。dom0 の行ういくつかのネットワークアドレス変換がトラフィックを仲介します。</para>
        </listitem>
      </itemizedlist>

      <para>これら 3 種類のネットワークノードは <filename>vif*</filename>、<filename>veth*</filename>、<filename>peth*</filename>、<filename>xenbr0</filename> などの独特な名前を付けられた数多くのインターフェースと関係を持ちます。Xen ハイパーバイザは定義された配置に従いユーザ空間ツールの制御の下でインターフェースを準備します。NAT と routing モデルは特定の場合にのみ適合します。このためわれわれは bridge モデルを使います。</para>

      <para>Xen パッケージの標準的な設定はシステム全体のネットワーク設定を変更しません。しかしながら、<command>xend</command> デーモンは既存のネットワークブリッジの中に仮想ネットワークインターフェースを組み込むように設定されています (複数のブリッジが存在する場合 <filename>xenbr0</filename> を優先します)。このためここでは <filename>/etc/network/interfaces</filename> の中にブリッジをセットアップして (<emphasis role="pkg">bridge-utils</emphasis> パッケージをインストールする必要があります。このため <emphasis role="pkg">bridge-utils</emphasis> パッケージは <emphasis role="pkg">xen-utils-4.4</emphasis> パッケージの推奨パッケージになっています)、既存の eth0 エントリを置き替えます。</para>

      <programlisting>auto xenbr0
iface xenbr0 inet dhcp
    bridge_ports eth0
    bridge_maxwait 0
    </programlisting>

      <para>再起動して、ブリッジが自動的に作成されることを確認します。この後 Xen 制御ツール、特に <command>xl</command> コマンドを使って domU を起動することが可能です。また、<command>xl</command> を使ってドメインを表示、起動、終了するなどの操作を行うことが可能です。</para>

      <screen><computeroutput># </computeroutput><userinput>xl list</userinput>
<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   463     1     r-----      9.8
# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>
<computeroutput>Parsing config from /etc/xen/testxen.cfg
# </computeroutput><userinput>xl list</userinput>
<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   366     1     r-----     11.4
testxen                                      1   128     1     -b----      1.1</computeroutput></screen>

      <sidebar>
        <title><emphasis>TOOL</emphasis> Xen VM を管理するツールスタックの選択</title>
        <indexterm><primary><command>xm</command></primary></indexterm>
        <indexterm><primary><command>xe</command></primary></indexterm>
        <para>Debian 7 および Debian 7 よりも古いリリースでは、<command>xm</command> が Xen 仮想マシンの管理に使うための標準的なコマンドラインツールでした。現在 <command>xm</command> はほぼ後方互換性を持つ <command>xl</command> によって置き換えられました。しかし、利用できるツールは <command>xm</command> および <command>xl</command> だけではありません。代替ツールとしては、libvirt を操作する <command>virsh</command> と XenServer (Xen の商用版) の XAPI を操作する <command>xe</command> が存在します。</para>
      </sidebar>

      <sidebar>
        <title><emphasis>CAUTION</emphasis> 1 つのイメージに 1 台以上の domU を割り当てないでください!</title>

	<para>もちろん複数の domU システムを並列実行させることは可能ですが、各 domU システムは専用のイメージを必要とします。なぜなら、各 domU は自分に割り当てられたハードウェアを専有するという仮定に基づいて実行されるからです (ハイパーバイザとやり取りするカーネルの一部分は別です)。特に、ストレージ領域を共有する目的で 2 つの domU システムを同時に起動することは不可能です。2 つの domU システムが同時に起動していなければ、両者はストレージ領域を共有して、単独のスワップパーティションや <filename>/home</filename> ファイルシステムをホストしているパーティションを再利用することが可能です。</para>
      </sidebar>

      <para><filename>testxen</filename> domU は仮想メモリではなく RAM から取った物理メモリを使います。このメモリ領域は <filename>testxen</filename> domU が起動していなければ dom0 が使えるメモリ領域だったという点に注意してください。このため、サーバを作ることが Xen インスタンスをホストすることを意味する場合、それに応じて十分なサイズの物理 RAM が必要になるという点に注意が必要です。</para>

      <para>おめでとうございます! 仮想マシンが開始されました。仮想マシンにアクセスするには 2 種類の方法があります。通常の方法は、真のマシンに接続するのと同様に、ネットワークを介して「リモートで」仮想マシンに接続することです。そしてこれを行うには、通常別の DHCP サーバや DNS 設定をセットアップすることが必要です。別の方法は <command>xl console</command> コマンドから <filename>hvc0</filename> コンソールを使う方法です。ネットワーク設定が正しくない場合にはこれが唯一の方法です。</para>

      <screen><computeroutput># </computeroutput><userinput>xl console testxen</userinput>
<computeroutput>[...]

Debian GNU/Linux 8 testxen hvc0

testxen login: </computeroutput></screen>

      <para>仮想マシンのキーボードの前に座っているかのごとくセッションを開くことが可能です。このコンソールからデタッチするには、<keycombo action="simul"><keycap>Control</keycap> <keycap>]</keycap></keycombo> キーの組み合わせを使用します。</para>

      <sidebar>
        <title><emphasis>TIP</emphasis> すぐにコンソールを開始する</title>

	<para>domU システムの開始直後にコンソールを始めたい場合があります。そしてこの希望に応えるために <command>xl create</command> コマンドは <literal>-c</literal> オプションを備えています。<literal>-c</literal> オプションを付けて domU を開始すれば、システム起動時に表示されるすべてのメッセージを見ることが可能です。</para>
      </sidebar>

      <sidebar>
        <title><emphasis>TOOL</emphasis> OpenXenManager</title>

	<para>OpenXenManager (<emphasis role="pkg">openxenmanager</emphasis> パッケージに含まれます) はグラフィカルインターフェースです。これ使うことで Xen API を介して Xen ドメインのリモート管理することが可能です。このため、Xen ドメインをリモートで制御することが可能です。OpenXenManager は <command>xl</command> コマンドの機能のほとんどを備えています。</para>
      </sidebar>

      <para>domU の起動完了後、domU は他のサーバと同様に使うことが可能です (domU は結局 GNU/Linux システムに過ぎません)。しかしながら、domU の仮想マシンの状態はいくつかの追加的機能を備えています。たとえば、<command>xl pause</command> と <command>xl unpause</command> コマンドを使って domU を一時的に停止したり再開することが可能です。一時的に停止された domU は全くプロセッサを使いませんが、割り当てられたメモリを解放しません。<command>xl save</command> と <command>xl restore</command> コマンドを考慮することは興味深いかもしれません。なぜなら <command>xl save</command> で domU を保存すれば domU の使っていた RAM などの資源が解放されるからです。また、<command>xl restore</command> で domU を元に戻す時 (ついでに言えば <command>xl unpause</command> で再開する時)、domU は時間が経過したことに全く気が付きません。dom0 を停止した時に domU が動いていた場合、パッケージに含まれるスクリプトが自動的に <command>xl save</command> で domU を保存し、dom0 の次回起動時に自動的に <command>xl restore</command> で domU を再開します。もちろんこれにはラップトップコンピュータをハイバネートする場合と同様の標準的な不便さがあります。特に、domU が長い間一時停止されていた場合、ネットワーク接続が切断される可能性があります。今現在 Xen は ACPI 電源管理のほとんどに互換性がない点にも注意してください。このため、ホスト (dom0) システムを一時停止することは不可能です。</para>

      <sidebar>
        <title><emphasis>DOCUMENTATION</emphasis> <command>xl</command> のオプション</title>

	<para><command>xl</command> サブコマンドのほとんどは domU の名前などの 1 つか複数個の引数を取ります。これらの引数は <citerefentry><refentrytitle>xl</refentrytitle> <manvolnum>1</manvolnum></citerefentry> マニュアルページは詳しく説明されています。</para>
      </sidebar>

      <para>domU を停止したり再起動するには、domU の内部から (<command>shutdown</command> コマンドを使って) 行ったり、dom0 から <command>xl shutdown</command> または <command>xl reboot</command> を使って行うことも可能です。</para>

      <sidebar>
        <title><emphasis>GOING FURTHER</emphasis> Xen の上級活用</title>

	<para>Xen はここで示すことができた数項だけにとどまらない多くの機能を持っています。特に、Xen はシステムを動的に変更することが可能です。すなわちドメインに対する多くのパラメータ (割り当てメモリサイズ、見えるハードドライブ、タスクスケジューラの挙動など) をドメインの実行中に調整することが可能です。domU はシャットダウンすることもネットワーク接続を失うこともなくサーバ間を移動することさえ可能です! すべての上級活用法に関して、最良の情報源は公式の Xen 文書です。<ulink type="block" url="http://www.xen.org/support/documentation.html" /></para>
      </sidebar>
    </section>
    <section id="sect.lxc">
      <title>LXC</title>
      <indexterm><primary>LXC</primary></indexterm> 

      <para>LXC は「仮想マシン」を作るために使われるにも関わらず、厳密に言うと仮想システムではなく、同じホスト上で実行されるプロセスのグループを隔離するためのシステムです。LXC は近年 Linux カーネルに対して行われた数々の機能の利点を活用しています。これらの機能はまとめて <emphasis>control groups</emphasis> として知られています。<emphasis>control groups</emphasis> を使うことにより、「グループ」と呼ばれるさまざまなプロセス群に対してシステム全体の特定の側面の状態を強制することが可能です。中でも最も注目すべき側面はプロセス ID、ネットワーク接続、マウントポイントです。隔離されたプロセスのグループはシステムの他のプロセスにアクセスできませんし、グループによるファイルシステムへのアクセスを特定の一部に限定することが可能です。さらにグループにネットワークインターフェースとルーティングテーブルを設定することにより、グループがシステム上の利用できるデバイスの一部だけを見えるように設定することが可能です。</para>

      <para>これらの機能を組み合わせることで、<command>init</command> プロセスから起動されたすべてのプロセスファミリーを隔離することが可能です。その結果、仮想マシンにとてもよく似たものが作られます。このようなセットアップの正式名称が「コンテナ」です (LXC の名称 <emphasis>LinuX Containers</emphasis> はこれに由来しています)。Xen や KVM が提供する「真の」仮想マシンとのより重要な違いは仮想マシン用のカーネルがない点です。このため、コンテナはホストシステムと全く同じカーネルを使います。これには利点と欠点があります。すなわち、利点はオーバーヘッドが全くないことで素晴らしい性能を得ることが可能という点とカーネルはシステムで実行しているすべてのプロセスを見ることが可能という点です。このため 2 つの独立したカーネルが異なるタスクセットでスケジュールを行うよりも効果的なスケジューリングが可能です。欠点の最たるものはコンテナの中で異なるカーネルを動作させることが不可能という点です (異なる Linux バージョンや異なるオペレーティングシステムを同時に動かすことができません)。</para>

      <sidebar>
        <title><emphasis>NOTE</emphasis> LXC の隔離制限</title>

	<para>LXC コンテナは負荷の大きなエミュレータやバーチャライザが備える隔離機能を備えていません。たとえば以下のような機能を備えていません。</para>
        <itemizedlist>
          <listitem>
	    <para>カーネルはホストシステムとコンテナによって共有されているため、コンテナ内に隔離されているプロセスはカーネルメッセージにアクセスできます。このことにより、メッセージがコンテナによって発せられた場合、情報が漏洩する可能性があります。</para>
          </listitem>
          <listitem>
	    <para>同様の理由で、コンテナが不正アクセスされカーネルの脆弱性が悪用された場合、他のコンテナが影響を受ける可能性があります。</para>
          </listitem>
          <listitem>
	    <para>ファイルシステムについて、カーネルはユーザとグループの数値的な識別子に従ってパーミッションを確認します。これらの識別子の意味するユーザとグループはコンテナごとに異なります。ファイルシステムの書き込み可能な部分がコンテナ同士で共有されている場合、この点を覚えておくべきです。</para>
          </listitem>
        </itemizedlist>
      </sidebar>

      <para>LXC による隔離は単純な仮想化と異なるため、LXC コンテナを設定することは仮想マシン上で単純に debian-installer を実行するよりも複雑な作業です。このため、いくつかの必要条件を説明した後、ネットワーク設定を行います。こうすることで、コンテナの中で実行するシステムを実際に作成することが可能です。</para>
      <section>
        <title>準備段階</title>

	<para><emphasis role="pkg">lxc</emphasis> パッケージには LXC を実行するために必要なツールが含まれるため、必ずこのパッケージをインストールしなければいけません。</para>

	<para>LXC を使うには <emphasis>control groups</emphasis> 設定システムが必要で、<filename>/sys/fs/cgroup</filename> に仮想ファイルシステムをマウントする必要があります。Debian 8 からは init システムとして systemd が採用されており、systemd は <emphasis>control groups</emphasis> に依存しているため、設定せずとも <filename>/sys/fs/cgroup</filename> は起動時に自動でマウントされます。</para>
      </section>
      <section id="sect.lxc.network">
        <title>ネットワークの設定</title>

	<para>LXC をインストールする目的は仮想マシンをセットアップすることです。もちろん、仮想マシンをネットワークから隔離するように設定したり、ファイルシステムを介してのみ情報をやり取りするように設定することも可能ですが、コンテナに対して少なくとも最低限のネットワークアクセスを提供するように設定するのが一般的です。典型的な場合、各コンテナにはブリッジを介して実際のネットワークに接続された仮想ネットワークインターフェースが備えられています。この仮想インターフェースは、直接ホスト上の物理ネットワークインターフェースに接続されているか (この場合、コンテナは直接ネットワークに接続されています)、ホスト上に定義された他の仮想インターフェースに接続されています (ホストからトラフィックをフィルタしたり配送することが可能です)。どちらの場合も、<emphasis role="pkg">bridge-utils</emphasis> パッケージが必要です。</para>

	<para>最も簡単なやり方は <filename>/etc/network/interfaces</filename> を編集することです。物理インターフェース (たとえば <literal>eth0</literal>) に関する設定をブリッジインターフェース (通常 <literal>br0</literal>) に変え、物理とブリッジインターフェース間のリンクを設定します。たとえば、最初にネットワークインターフェース設定ファイルが以下のようなエントリを持っていたとします。</para>

        <programlisting>auto eth0
iface eth0 inet dhcp</programlisting>

	<para>このエントリを無効化し、以下の通り書き換えます。</para>

        <programlisting>#auto eth0
#iface eth0 inet dhcp

auto br0
iface br0 inet dhcp
  bridge_ports eth0</programlisting>

	<para>この設定により、コンテナをホストと同じ物理ネットワークに接続されたマシンとして考えた場合と、同様の効果が得られます。この「ブリッジ」設定はすべてのブリッジされたインターフェース間のイーサネットフレームの通過を管理します。これには物理的な <literal>eth0</literal> およびコンテナ用に定義されたインターフェースが含まれます。</para>

	<para>この設定を使うことができない場合 (たとえば、公開 IP アドレスをコンテナに割り当てることができない場合)、仮想 <emphasis>tap</emphasis> インターフェースを作成し、これをブリッジに接続します。これと等価なネットワークトポロジーは、ホストの 2 番目のネットワークカードが分離されたスイッチに接続されている状態です。コンテナはこのスイッチに接続されています。コンテナが外部と通信するには、ホストがコンテナ用のゲートウェイとして振る舞わなければいけません。</para>

	<para>この「ぜいたくな」設定を行うには <emphasis role="pkg">bridge-utils</emphasis> と <emphasis role="pkg">vde2</emphasis> パッケージが必要です。<filename>/etc/network/interfaces</filename> ファイルは以下のようになります。</para>

        <programlisting># eth0 インターフェースは同じものを使います
auto eth0
iface eth0 inet dhcp

# 仮想インターフェース
auto tap0
iface tap0 inet manual
  vde2-switch -t tap0

# コンテナ用のブリッジ
auto br0
iface br0 inet static
  bridge_ports tap0
  address 10.0.0.1
  netmask 255.255.255.0</programlisting>

	<para>コンテナのネットワークは静的またはコンテナのホスト上で動く DHCP サーバを使って動的に設定されます。また、DHCP サーバを <literal>br0</literal> インターフェースを介した問い合わせに応答するように設定する必要があります。</para>
      </section>
      <section>
        <title>システムのセットアップ</title>

	<para>それではコンテナがファイルシステムを使うようにファイルシステムを設定しましょう。コンテナという「仮想マシン」はハードウェア上で直接的に実行されないため、標準的なファイルシステムに比べていくつかの微調整を必要とします。これは特にカーネル、デバイス、コンソールが該当します。幸いなことに、<emphasis role="pkg">lxc</emphasis> にはこの設定をほぼ自動化するスクリプトが含まれます。たとえば、以下のコマンド (<emphasis role="pkg">debootstrap</emphasis> と <emphasis role="pkg">rsync</emphasis> パッケージが必要です) で Debian コンテナがインストールされます。</para>

        <screen><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-create -n testlxc -t debian
</userinput><computeroutput>debootstrap は /usr/sbin/debootstrap です
Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... 
Downloading debian minimal ...
I: Retrieving Release 
I: Retrieving Release.gpg 
[...]
Download complete.
Copying rootfs to /var/lib/lxc/testlxc/rootfs...
[...]
Root password is 'sSiKhMzI', please change !
root@mirwiz:~# </computeroutput>
        </screen>

	<para>ファイルシステムは最初に <filename>/var/cache/lxc</filename> の中に作成され、その後目的のディレクトリに移動されます。こうすることで、同一のコンテナが極めて素早く作成されます。なぜなら、単純にコピーするだけだからです。</para>

	<para>この debian テンプレート作成スクリプトは、インストールされるシステムのアーキテクチャを指定する <option>--arch</option> オプションと、現在の Debian 安定版以外の物をインストールしたい場合に指定する <option>--release</option> オプションを取ります。また、<literal>MIRROR</literal> 環境変数を設定してローカル Debian アーカイブミラーを指定することも可能です。</para>

	<para>これで、新規に作成されたファイルシステムが最低限の Debian システムを含むようになりました。デフォルト状態だとこのコンテナにはネットワークインターフェースがありません (ループバックインターフェースすらありません)。これは全く望むべき状態ではないため、コンテナの設定ファイル (<filename>/var/lib/lxc/testlxc/config</filename>) を編集し、いくつかの <literal>lxc.network.*</literal> エントリを追加します。</para>

        <programlisting>lxc.network.type = veth
lxc.network.flags = up
lxc.network.link = br0
lxc.network.hwaddr = 4a:49:43:49:79:20</programlisting>

	<para>これらのエントリの意味するところはそれぞれ、仮想インターフェースはコンテナによって作られます。そして仮想インターフェースはコンテナが開始された時に自動的に利用できる状態にされます。そして仮想インターフェースはホストの <literal>br0</literal> ブリッジに自動的に接続されます。さらに仮想インターフェースの MAC アドレスは指定したものになります。最後のエントリを削除するか無効化した場合、ランダムな MAC アドレスが生成されます。</para>

	<para>以下のようにすることで、設定ファイル内でホスト名を設定することも可能です。</para>

<programlisting>lxc.utsname = testlxc</programlisting>

      </section>
      <section>
        <title>コンテナの開始</title>

	<para>これで仮想マシンイメージの準備が整いました。それではコンテナを開始しましょう。</para>

        <screen role="scale" width="94"><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-start --daemon --name=testlxc
</userinput><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-console -n testlxc
</userinput><computeroutput>Debian GNU/Linux 8 testlxc tty1

testlxc login: </computeroutput><userinput>root</userinput><computeroutput>
Password: 
Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
root@testlxc:~# </computeroutput><userinput>ps auxwf</userinput>
<computeroutput>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init
root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald
root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D
root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux
root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux
root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux
root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     
root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \_ -bash
root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \_ ps auxfw
root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102
root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e
root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux
root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux
root@testlxc:~# </computeroutput></screen>

	<para>これでコンテナの中に入りました。プロセスへのアクセスはコンテナ自身によって開始されたものだけに制限されていることがわかります。同様に、ファイルシステムへのアクセスも testlxc コンテナ専用に割り当てられた完全なファイルシステムの一部分 (<filename>/var/lib/lxc/testlxc/rootfs</filename>) に制限されています。コンソールを終了するには <keycombo action="simul"><keycap>Control</keycap> <keycap>a</keycap></keycombo> <keycombo><keycap>q</keycap></keycombo> を使います。</para>

	<para><command>lxc-start</command> に <option>--daemon</option> オプションを渡したおかげで、コンテナがバックグラウンドプロセスとして実行されていることに注意してください。コンテナを中止するには <command>lxc-stop --name=testlxc</command> などのコマンドを使います。</para>

	<para><emphasis role="pkg">lxc</emphasis> パッケージには、ホストの起動時に自動的に 1 つまたは複数のコンテナを開始するための初期化スクリプトが含まれます (この初期化スクリプトは <literal>lxc.start.auto</literal> オプションが 1 に設定されているコンテナを起動する <command>lxc-autostart</command> に依存しています)。起動順序を非常に細かく制御するには <literal>lxc.start.order</literal> と <literal>lxc.group</literal> を使います。デフォルトの場合、初期化スクリプトは <literal>onboot</literal> グループに所属するコンテナを起動し、その後いかなるグループにも所属しないコンテナを起動します。どちらの場合も、グループ内の起動順序を制御するには <literal>lxc.start.order</literal> オプションを使います。</para>

        <sidebar>
          <title><emphasis>GOING FURTHER</emphasis> 大量の仮想化</title>

	  <para>LXC は非常に軽量の隔離システムですから、LXC を使って仮想サーバを大量にホストすることが可能です。この場合のネットワーク設定は上に述べた物よりも少し高度なものになるかもしれませんが、多くの場合 <literal>tap</literal> と <literal>veth</literal> インターフェースを用いた「ぜいたくな」設定を使えば事足ります。</para>

	  <para>ファイルシステムの一部 (たとえば <filename>/usr</filename> と <filename>/lib</filename> などのサブツリー) を共有することは合理的です。こうすることで、複数のコンテナで共通に必要なソフトウェアを複製することを避けることが可能です。通常これを設定するには、コンテナ設定ファイルに含まれる <literal>lxc.mount.entry</literal> エントリを使います。さらに興味深い副作用として、より少ない物理メモリでプロセスを動かすことが可能になります。なぜなら、カーネルはプログラムが共有されていることを検出できるからです。このことにより 1 つのコンテナを追加するためのコストをコンテナに特有のデータに割り当てられたディスク領域と、カーネルがスケジュールと管理に使ういくつかの追加的プロセスだけに減らすことが可能です。</para>

	  <para>もちろん、ここではすべての利用できるオプションを説明していません。このため、より広範囲におよぶ情報を入手するには、<citerefentry> <refentrytitle>lxc</refentrytitle> <manvolnum>7</manvolnum> </citerefentry> と <citerefentry> <refentrytitle>lxc.container.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> マニュアルページおよびこれらのマニュアルページから参照されている文書を参照してください。</para>
        </sidebar>
      </section>
    </section>
    <section>
      <title>KVM を使った仮想化</title>
      <indexterm><primary>KVM</primary></indexterm>

      <para>KVM は <emphasis>Kernel-based Virtual Machine</emphasis> を意味しており、仮想化システムの使うほとんどの基礎構造を提供する最初で最高のカーネルモジュールです。しかしながら、LVM 自身は仮想化システムではありません。仮想化の実際の制御を行うには QEMU に基づくアプリケーションを使います。この節で <command>qemu-*</command> コマンドがあっても心配しないでください。なぜならこのコマンドは KVM に関連するものだからです。</para>

      <para>他の仮想化システムと異なり、KVM は最初から Linux カーネルにマージされていました。KVM の開発者はプロセッサが備える仮想化専用命令セット (Intel-VT と AMD-V) を有効活用することを選びました。仮想化専用命令セットを活用することで、KVM は軽量で簡潔でリソースを大量に消費しないものになっています。もちろんその代償として KVM にも欠点があります。それはすべてのコンピュータが KVM を動かせるわけではなく、適切なプロセッサを備えたコンピュータでなければ KVM を動かせないという点です。x86 ベースのコンピュータで <filename>/proc/cpuinfo</filename> 内の CPU フラグに「vmx」または「svm」が含まれている場合、そのプロセッサは KVM を動かすことができることを意味します。</para>

      <para>Red Hat が KVM の開発を活発に支援したことで、KVM は事実上 Linux 仮想化の基準点になりました。</para>
      <section>
        <title>準備段階</title>
        <indexterm><primary><command>virt-install</command></primary></indexterm>

	<para>VirtualBox などのツールと異なり、KVM は仮想マシンを作成管理するためのユーザインターフェースを含みません。仮想マシンを開始することが可能な実行ファイルおよび適切なカーネルモジュールを読み込むための初期化スクリプトを含むパッケージが <emphasis role="pkg">qemu-kvm</emphasis> パッケージです。</para>
        <indexterm><primary>libvirt</primary></indexterm>
        <indexterm><primary><emphasis role="pkg">virt-manager</emphasis></primary></indexterm>

	<para>幸いなことに、Red Hat は <emphasis>libvirt</emphasis> ライブラリおよび関連する<emphasis>仮想マシンマネージャ</emphasis>ツールを開発することで、この問題に対処するためのツールを提供しています。libvirt により仮想マシンを管理する方法が統一され、仮想マシンの管理方法が裏で動く仮想システムに依存しなくなります (libvirt は現在 QEMU、KVM、Xen、LXC、OpenVZ、VirtualBox、VMWare、UML をサポートしています)。<command>virtual-manager</command> は仮想マシンを作成管理するために libvirt を使うグラフィルインターフェースです。</para>
        <indexterm><primary><emphasis role="pkg">virtinst</emphasis></primary></indexterm>

	<para>最初に <command>apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</command> を使って、必要なパッケージをインストールします。<emphasis role="pkg">libvirt-bin</emphasis> には、<command>libvirtd</command> デーモンが含まれます。<command>libvirtd</command> デーモンを使うことでホストで実行されている仮想マシンを (潜在的にリモートで) 管理したり、ホスト起動時に要求された VM を開始したりすることが可能です。加えて、<emphasis role="pkg">libvirt-bin</emphasis> パッケージは <command>virsh</command> コマンドラインツールを提供します。<command>virsh</command> を使うことで、<command>libvirtd</command> の管理するマシンを操作することが可能です。</para>

	<para><emphasis role="pkg">virtinst</emphasis> パッケージには <command>virt-install</command> コマンドが含まれます。<command>virt-install</command> を使うことで、コマンドラインから仮想マシンを作成することが可能になります。最後に、<emphasis role="pkg">virt-viewer</emphasis> を使うことで、仮想マシンのグラフィカルコンソールにアクセスすることが可能になります。</para>
      </section>
      <section>
        <title>ネットワークの設定</title>

	<para>Xen や LXC と同様に、最もよく使われるネットワーク設定は仮想マシンのネットワークインターフェースをグループ化するブリッジです (<xref linkend="sect.lxc.network" />を参照してください)。</para>

	<para>ネットワーク設定には別の方法もあります。KVM の提供するデフォルト設定の中では、仮想マシンに (192.168.122.0/24 の範囲内に) プライベートアドレスが割り当てられており、さらに NAT が設定されています。この設定により仮想マシンは外部ネットワークにアクセスすることが可能です。</para>

	<para>この節の残りでは、ホストが <literal>eth0</literal> 物理インターフェースと <literal>br0</literal> ブリッジを備え、<literal>eth0</literal> が <literal>br0</literal> に接続されていることを仮定します。</para>
      </section>
      <section>
        <title><command>virt-install</command> を使ったインストール</title>
        <indexterm><primary><command>virt-install</command></primary></indexterm>

	<para>仮想マシンの作成は普通のシステムをインストールするのとよく似ています。違いは、仮想マシンの性質をコマンドラインから非常に長々と指定する点です。</para>

	<para>具体的に言えば、これはホストシステムに保存された Debian DVD イメージを挿入された仮想 DVD-ROM ドライブから仮想マシンを起動することにより Debian インストーラを使うことを意味します。仮想マシンは VNC プロトコル (詳しくは<xref linkend="sect.remote-desktops" />を参照してください) を介してグラフィカルコンソールに表示されます。これによりインストール作業を操作することが可能になります。</para>

	<para>最初にディスクイメージの保存先を libvirtd に伝える必要があります。デフォルトの保存先 (<filename>/var/lib/libvirt/images/</filename>) でも構わないならばこれは必要ありません。</para>

        <screen><computeroutput>root@mirwiz:~# </computeroutput><userinput>mkdir /srv/kvm</userinput>
<computeroutput>root@mirwiz:~# </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>
<computeroutput>Pool srv-kvm created

root@mirwiz:~# </computeroutput></screen>

        <sidebar>
          <title><emphasis>TIP</emphasis> libvirt グループに対するユーザの追加</title>
          <para>この節で挙げたすべての例では、root がコマンドを実行しています。実際、あるユーザがローカルの libvirt デーモンを操作するには、root になるか <literal>libvirt</literal> グループのメンバーになって (デフォルト状態では <literal>libvirt</literal> グループはユーザの初期参加グループに設定されていません) コマンドを実行する必要があります。このユーザに root 権限を頻繁に使わせることを避けたい場合、そのユーザを <literal>libvirt</literal> グループに追加して、本人の権限でさまざまなコマンドを実行するように設定することが可能です。</para>
        </sidebar>

	<para>それでは仮想マシンのインストール作業を開始し、<command>virt-install</command> の最も重要なオプションを詳細に見て行きましょう。<command>virt-install</command> は仮想マシンとそのパラメータを libvirtd に登録し、インストールを進めるために仮想マシンを開始します。</para>

        <screen><computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id="virtinst.connect"></co>
               --virt-type kvm           <co id="virtinst.type"></co>
               --name testkvm            <co id="virtinst.name"></co>
               --ram 1024                <co id="virtinst.ram"></co>
               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <co id="virtinst.disk"></co>
               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <co id="virtinst.cdrom"></co>
               --network bridge=br0      <co id="virtinst.network"></co>
               --vnc                     <co id="virtinst.vnc"></co>
               --os-type linux           <co id="virtinst.os"></co>
               --os-variant debianwheezy
</userinput><computeroutput>
Starting install...
Allocating 'testkvm.qcow'             |  10 GB     00:00
Creating domain...                    |    0 B     00:00
Guest installation complete... restarting guest.
</computeroutput></screen>
        <calloutlist>
          <callout arearefs="virtinst.connect">
	    <para><literal>--connect</literal> オプションは使用する「ハイパーバイザ」を指定します。これは仮想システムを表す URL (<literal>xen://</literal>、<literal>qemu://</literal>、<literal>lxc://</literal>、<literal>openvz://</literal>、<literal>vbox://</literal> など) と VM をホストするマシン (ローカルホストの場合、空でも構いません) の形をしています。QEMU/KVM の場合、これに加えて各ユーザは制限されたパーミッションで稼働する仮想マシンを管理できます。この場合 URL パスは「システム」マシン (<literal>/system</literal>) かその他 (<literal>/session</literal>) かで識別されます。</para>
          </callout>
          <callout arearefs="virtinst.type">
	    <para><literal>--virt-type kvm</literal> を指定することで KVM を使うことが可能です。<literal>--connect</literal> で指定した URL を一見すると QEMU が使われるように見えますが、これは KVM は QEMU と同じ方法で管理されているためです。</para>
          </callout>
          <callout arearefs="virtinst.name">
	    <para><literal>--name</literal> オプションは仮想マシンの (一意的な) 名前を定義します。</para>
          </callout>
          <callout arearefs="virtinst.ram">
	    <para><literal>--ram</literal> オプションは仮想マシンに割り当てる RAM の量 (MB 単位) を指定します。</para>
          </callout>
          <callout arearefs="virtinst.disk">
	    <para><literal>--disk</literal> オプションは仮想マシンのハードディスクとして利用するイメージファイルの場所を指定します。このファイルが存在しなければ、<literal>size</literal> パラメータで指定されたサイズ (GB 単位) のイメージファイルが作成されます。<literal>format</literal> パラメータはイメージファイルを保存するさまざまな方法を選択します。デフォルトフォーマット (<literal>raw</literal>) はディスクサイズと内容が全く同じ単一ファイルです。ここではより先進的なフォーマット qcow2 を選びました。qcow2 は QEMU 専用のフォーマットです。qcow2 フォーマットのファイルは作成時のサイズは小さいのですが、仮想マシンが領域を実際に利用することになった時にサイズが増加します。</para>
          </callout>
          <callout arearefs="virtinst.cdrom">
	    <para><literal>--cdrom</literal> オプションはインストール時に利用する光学ディスクの場所を指定するために使われます。場所には ISO ファイルのローカルパス、ファイル取得先の URL、物理 CD-ROM ドライブのデバイスファイル (例 <literal>/dev/cdrom</literal>) のどれか 1 つを使うことが可能です。</para>
          </callout>
          <callout arearefs="virtinst.network">
	    <para><literal>--network</literal> オプションはホストネットワーク設定の中に仮想ネットワークを統合する方法を指定します。デフォルトは既存のネットワークブリッジに仮想ネットワークを統合する方法です (例では明示的にこの挙動を指定しています)。指定したブリッジが存在しない場合、仮想マシンが到達できるネットワークは NAT を介した物理ネットワークだけに限定されるので、仮想マシンはプライベートサブネット範囲 (192.168.122.0/24) に含まれるアドレスを割り当てられます。</para>
          </callout>
          <callout arearefs="virtinst.vnc">
	    <para><literal>--vnc</literal> は VNC を使ってグラフィカルコンソールを利用できるようにすることを意味します。VNC サーバに対するデフォルトの挙動を使った場合、ローカルインターフェースだけがリッスンされます。さらに仮想マシンを操作する VNC クライアントを別のホスト上で実行する場合、VNC 接続を確立するには SSH トンネルを設定する必要があります (<xref linkend="sect.ssh-port-forwarding" />を参照してください)。別の方法として、VNC サーバをすべてのインターフェースを介して利用できるようにするために、<literal>--vnclisten=0.0.0.0</literal> を使うことも可能です。しかしこの方針を取る場合、ファイアウォールを適切に設計するべきという点に注意してください。</para>
          </callout>
          <callout arearefs="virtinst.os">
	    <para><literal>--os-type</literal> と <literal>--os-variant</literal> オプションは、指定されたオペレーティングシステムの備える既知の機能に基づいて、仮想マシンのいくつかのパラメータを最適化するためのものです。</para>
          </callout>
        </calloutlist>

	<para><command>virt-install</command> を実行した時点で仮想マシンが実行されます。インストール作業に進むためには、グラフィカルコンソールに接続する必要があります。上の操作をグラフィカルデスクトップ環境から行った場合、自動的に接続が開始されます。そうでない場合、グラフィカルコンソールを開くために <command>virt-viewer</command> を任意のグラフィカル環境から実行します (この時にリモートホストの root パスワードが 2 回尋ねられる点に注意してください。なぜなら、この操作には 2 つの SSH 接続を必要とするからです)。</para>

        <screen><computeroutput>$ </computeroutput><userinput>virt-viewer --connect qemu+ssh://root@<replaceable>server</replaceable>/system testkvm
</userinput><computeroutput>root@server's password: 
root@server's password: </computeroutput></screen>

	<para>インストール作業が終了したら、仮想マシンが再起動されます。これで仮想マシンを利用する準備が整いました。</para>
      </section>
      <section>
        <title><command>virsh</command> を使ったマシンの管理</title>
        <indexterm><primary><command>virsh</command></primary></indexterm>

	<para>これでインストールが終了しました。利用できる仮想マシンを取り扱う方法に移りましょう。最初に <command>virsh</command> を使って <command>libvirtd</command> が管理している仮想マシンのリストを確認します。</para>

        <screen><computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all
</userinput><computeroutput> Id Name                 State
----------------------------------
  - testkvm              shut off
</computeroutput></screen>

	<para>それではテスト用仮想マシンを起動しましょう。</para>

        <screen><computeroutput># </computeroutput><userinput>virsh -c qemu:///system start testkvm
</userinput><computeroutput>Domain testkvm started</computeroutput></screen>

	<para>そして、グラフィカルコンソールへの接続命令を出します (接続する VNC 画面を <command>vncviewer</command> へのパラメータの形で指定することが可能です)。</para>

        <screen><computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm
</userinput><computeroutput>:0</computeroutput></screen>

	<para>その他の利用できる <command>virsh</command> サブコマンドには以下のものがあります。</para>
        <itemizedlist>
          <listitem>
	    <para><literal>reboot</literal>。仮想マシンを再起動します。</para>
          </listitem>
          <listitem>
	    <para><literal>shutdown</literal>。仮想マシンを正常にシャットダウンします。</para>
          </listitem>
          <listitem>
	    <para><literal>destroy</literal>。仮想マシンを無理やり停止します。</para>
          </listitem>
          <listitem>
	    <para><literal>suspend</literal>。仮想マシンを一時停止します。</para>
          </listitem>
          <listitem>
	    <para><literal>resume</literal>。一時停止された仮想マシンを再開します。</para>
          </listitem>
          <listitem>
	    <para><literal>autostart</literal>。ホスト起動時にこの仮想マシンを自動的に起動することを有効化します (または <literal>--disable</literal> オプションを付けて無効化します)。</para>
          </listitem>
          <listitem>
	    <para><literal>undefine</literal>。仮想マシンのすべての痕跡を <command>libvirtd</command> から削除します。</para>
          </listitem>
        </itemizedlist>

	<para>ここに挙げたすべてのサブコマンドは仮想マシン識別子をパラメータとして受け取ります。</para>
      </section>
      <section>
        <title>yum を使い RPM に基づくシステムを Debian の中にインストールする</title>

	<para>仮想マシンが Debian (または Debian 派生物) を実行することを意図している場合、上で述べた通り <command>debootstrap</command> を使ってシステムを初期化することが可能です。しかし、仮想マシンに RPM に基づくシステム (Fedora、CentOS、Scientific Linux など) をインストールする場合、<command>yum</command> ユーティリティ (同名のパッケージに含まれます) を使ってシステムをセットアップする必要があります。</para>
	
        <para>RPM に基づくシステムをインストールする際には、特に <command>yum</command> 設定ファイルなどのファイルの初期セットを展開するために <command>rpm</command> を使い、その後パッケージの残りのセットを展開するために <command>yum</command> を呼び出す必要があります。しかし、chroot の外から <command>yum</command> を呼び出しているため、一時的な修正が必要です。以下に載せた例では、対象の chroot 先は <filename>/srv/centos</filename> です。</para>

        <screen><computeroutput># </computeroutput><userinput>rootdir="/srv/centos"
</userinput><computeroutput># </computeroutput><userinput>mkdir -p "$rootdir" /etc/rpm
</userinput><computeroutput># </computeroutput><userinput>echo "%_dbpath /var/lib/rpm" &gt; /etc/rpm/macros.dbpath
</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root "$rootdir" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!
rpm: However assuming you know what you are doing...
warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY
# </computeroutput><userinput>sed -i -e "s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g" $rootdir/etc/yum.repos.d/*.repo
</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core
</userinput><computeroutput>[...]
# </computeroutput><userinput>sed -i -e "s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g" $rootdir/etc/yum.repos.d/*.repo
</userinput></screen>
      </section>
    </section>
  </section>
  <section id="sect.automated-installation">
    <title>自動インストール</title>
    <indexterm><primary>配備</primary></indexterm>
    <indexterm><primary>インストール</primary><secondary>自動インストール</secondary></indexterm>

    <para>巨大な IT サービスの管理者と同様に Falcot Corp の管理者もまた、新しいマシンへシステムを素早く (可能であれば自動的に) インストール (または再インストール) するツールを必要としています。</para>

    <para>自動インストールの要求に応えるためのさまざまな解決策が存在します。一方では、SystemImager などの一般的なツールが存在します。こちらの解決策ではテンプレートマシンに基づくイメージを事前に準備し、そのイメージを目標のシステムで展開します。他方では、標準的な Debian インストーラが存在します。こちらの解決策ではインストール作業中に尋ねられる質問の回答を含めた設定ファイルを事前に準備し、この設定ファイルに基づいて目標のシステムを設定します。両者の折衷案として、FAI (<emphasis>Fully Automatic Installer</emphasis>) などのハイブリッドツールが存在します。こちらの解決策ではパッケージングシステムでシステムをインストールし、自分自身の機能を使って大規模な配備に特有のタスク (起動、パーティショニング、設定など) をこなします。</para>

    <para>これらの解決策には、利点と欠点があります。たとえば SystemImager は特定のパッケージングシステムに依存しません。このことにより、複数の Linux ディストリビューションを使う数多くのマシン群を管理することが可能です。SystemImager には再インストール不要の更新システムが含まれていますが、この更新システムを信頼できるのは各マシンは個別に変更されないという仮定が満足される場合だけです。さらに言い換えれば、ユーザは自分のマシンにインストールされたソフトウェアを更新してはいけませんし、他のソフトウェアをインストールしてもいけません。同様に、セキュリティ更新も自動的に行ってはいけません。なぜなら、セキュリティ更新は SystemImager がメンテナンスする中央集権化された基準イメージによって提供されるからです。また SystemImager による解決策では、管理される側のマシンの種類が同じである必要があります。異種マシンを管理する場合、多くの異なるイメージをメンテナンスおよび管理する必要があります (i386 用のイメージを powerpc マシンに使うことはできません)。</para>

    <para>逆に、debian-installer を使ってインストールを自動化する場合、マシンごとの違いに適合したシステムをインストールすることが可能です。具体的に言えば、インストーラは対応するリポジトリから適切なカーネルとソフトウェアパッケージを取得し、利用できるハードウェアを検出し、利用できる領域全体を活かしてハードディスク全体をパーティショニングし、対応する Debian システムをインストールし、適切なブートローダを設定します。しかしながら、標準的なインストーラは基本システムと事前に選択された「tasks」を含む標準的な Debian バージョンをインストールするだけです。さらに、パッケージングされていないアプリケーションを備えた特製のシステムをインストールできません。この特別な必要性を満足させるにはインストーラのカスタマイズが必要です。幸いなことに、インストーラはとてもモジュール化されており、カスタマイズに必要なほとんどの作業を自動化するツールが存在します。最も重要なツールは simple-CDD です (CDD は <emphasis>Custom Debian Derivative</emphasis> の頭字語です)。しかしながら simple-CDD が取り扱うことが可能なのは最初のインストールだけです。しかしこれは通常問題になりません。なぜなら、APT ツールのおかげでインストール後の更新作業は効果的に行うことが可能だからです。</para>

    <para>ここでは FAI については大ざっぱな概要を説明し、(もはや Debian に含まれない) SystemImager については完全に省略し、Debian だけのシステムではより興味深い debian-installer と simple-CDD に特に注目します。</para>
    <section id="sect.fai">
      <title>Fully Automatic Installer (FAI)</title>
      <indexterm><primary>Fully Automatic Installer (FAI)</primary></indexterm>

      <para><foreignphrase>Fully Automatic Installer</foreignphrase> はおそらく最も古い Debian 用の自動配備システムで、基準としての地位を確立しています。しかしその一方で FAI の高い柔軟性は FAI の複雑性によって成し遂げられています。</para>

      <para>FAI には、配備情報を保存してネットワークから目標のマシンを起動することを可能にするために、サーバシステムが必要です。サーバシステムには、<emphasis role="pkg">fai-server</emphasis> パッケージ (または <emphasis role="pkg">fai-quickstart</emphasis>、これには、標準的な設定に必要な要素が含まれています) が必要です。</para>

      <para>FAI は特殊なやり方で、インストールできるさまざまなプロファイルを定義します。FAI は基準となるインストール状態を単純に複製するのではありません。FAI は本格的なインストーラで、サーバに保存されているファイルとスクリプトを通じて完全に設定することが可能です。しかし、これらのファイルを保存するデフォルトの場所 <filename>/srv/fai/config/</filename> は自動的に作成されません。このため管理者は対応するファイルと一緒にこれも作成する必要があります。ほとんどの場合、これらのファイルは例ファイルからカスタマイズされます。このファイルは <emphasis role="pkg">fai-doc</emphasis> パッケージの文書の中 (より具体的に言えば <filename>/usr/share/doc/fai-doc/examples/simple/</filename> ディレクトリの中) に含まれています。</para>

      <para>プロファイルを定義したら、<command>fai-setup</command> コマンドを使って FAI のインストールを開始するために必要な要素を生成します。そしてこれはインストール中に使われる最小限のシステム (NFS-root) の準備と更新を行うことを意味します。別の方法として、<command>fai-cd</command> を使って専用の CD を生成することも可能です。</para>

      <para>これらすべての設定ファイルを作成するには、FAI の動作方法を理解する必要があります。典型的なインストール作業は以下の順番で進みます。</para>
      <itemizedlist>
        <listitem>
	  <para>ネットワークからカーネルを取得して起動します。</para>
        </listitem>
        <listitem>
	  <para>NFS でルートファイルシステムをマウントします。</para>
        </listitem>
        <listitem>
	  <para>インストール作業を制御する <command>/usr/sbin/fai</command> を実行します (<command>/usr/sbin/fai</command> が以降の各段階を初期化します)。</para>
        </listitem>
        <listitem>
	  <para>サーバから <filename>/fai/</filename> に設定領域をコピーします。</para>
        </listitem>
        <listitem>
	  <para><command>fai-class</command> を実行します。<filename>/fai/class/[0-9][0-9]*</filename> スクリプトが順番に実行され、インストールされるマシンに適用する「クラス」の名前が返されます。この情報は以降の各段階の基礎としての機能を果たします。これを使うことで、インストールおよび設定されるサービスを定義する際に幾らかの柔軟性を持たせることが可能です。</para>
        </listitem>
        <listitem>
	  <para>対応するクラスに基づき、設定変数を取得します。</para>
        </listitem>
        <listitem>
	  <para><filename>/fai/disk_config/<replaceable>class</replaceable></filename> で定義された情報に基づいて、ディスクのパーティショニングとパーティションのフォーマットを行います。</para>
        </listitem>
        <listitem>
	  <para>指定されたパーティションをマウントします。</para>
        </listitem>
        <listitem>
	  <para>基本システムをインストールします。</para>
        </listitem>
        <listitem>
	  <para><command>fai-debconf</command> を使って Debconf データベースを事前配布します。</para>
        </listitem>
        <listitem>
	  <para>APT で利用できるパッケージのリストを取得します。</para>
        </listitem>
        <listitem>
	  <para><filename>/fai/package_config/<replaceable>class</replaceable></filename> にリストされたパッケージをインストールします。</para>
        </listitem>
        <listitem>
	  <para>設定後にスクリプト <filename>/fai/scripts/<replaceable>class</replaceable>/[0-9][0-9]*</filename> を実行します。</para>
        </listitem>
        <listitem>
	  <para>インストールログを記録し、パーティションをアンマウントし、再起動します。</para>
        </listitem>
      </itemizedlist>
    </section>
    <section id="sect.d-i-preseeding">
      <title>Debian-Installer の事前設定</title>
      <indexterm><primary>preseed</primary></indexterm>
      <indexterm><primary>事前設定</primary></indexterm>

      <para>結局のところ、Debian システムをインストールする最良のツールは必然的に公式の Debian インストーラということになるでしょう。このため、当初から debian-installer は <emphasis role="pkg">debconf</emphasis> の提供するインフラを活用して自動的に使えるように設計されています。<emphasis role="pkg">debconf</emphasis> を使うことで、尋ねられる質問の数を減らしたり (隠された質問に対する回答はデフォルトが使われます)、質問に対する回答を個別に事前設定したりすることが可能です。このことにより、インストールを非対話的に進めることが可能です。質問に対する回答の事前設定機能は <emphasis>preseed</emphasis> として知られています。</para>

      <sidebar>
        <title><emphasis>GOING FURTHER</emphasis> 中央集権型データベースを用いる Debconf</title>
        <indexterm><primary><command>debconf</command></primary></indexterm>

	<para>preseed を使うことで、インストール時に尋ねられる Debconf 質問に対する回答を事前に提供することが可能です。しかし、これらの回答は静的なもので、時間経過に従い変化しません。既にインストールされたマシンは更新が必要かもしれませんし、新しい回答が必要かもしれません。このため <filename>/etc/debconf.conf</filename> 設定ファイルから、Debconf が外部のデータソース (LDAP ディレクトリサーバ、NFS や Samba 経由でアクセスされるリモートファイル) を使うようにセットアップして、お互いに補完し合う複数の外部データソースを同時に定義することが可能です。とは言うものの、ローカルデータベースも使えます (読み書きアクセスできます)。これに対してリモートデータベースは読み込みのみに制限されていることが多いです。<citerefentry><refentrytitle>debconf.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> マニュアルページではすべての可能性が詳細に説明されています (このマニュアルは <emphasis role="pkg">debconf-doc</emphasis> パッケージに含まれます)。</para>
      </sidebar>
      <section>
        <title>preseed ファイルの利用</title>

	<para>インストーラが preseed ファイルを取得することが可能な場所にはいくつかあります。</para>
        <itemizedlist>
          <listitem>
	    <para>マシンを開始するために使われる initrd の中。この場合、インストールの最初から preseed を行い、すべての質問を回避することが可能です。preseed ファイルの名前は必ず <filename>preseed.cfg</filename> にして initrd のルートに保存しなければいけません。</para>
          </listitem>
          <listitem>
	    <para>起動メディア (CD や USB メモリ) の中。メディアがマウントされた直後から preseed が始まります。これは言語とキーボードレイアウトに関する質問の直後を意味します。<literal>preseed/file</literal> 起動パラメータを使って preseed ファイルの場所を指定することが可能です (たとえば、インストールが CD-ROM から開始された場合は <filename>/cdrom/preseed.cfg</filename>、USB メモリから開始された場合は <filename>/hd-media/preseed.cfg</filename> などです)。</para>
          </listitem>
          <listitem>
	    <para>ネットワーク上。ネットワークが (自動的に) 設定された直後から preseed が始まります。対応する起動パラメータは <literal>preseed/url=http://<replaceable>server</replaceable>/preseed.cfg</literal> です。</para>
          </listitem>
        </itemizedlist>

	<para>一見すると、preseed ファイルを initrd の中に含めることが最もうまい解決策のように見えます。しかし、実際のところこれはほとんど行われません。なぜなら、インストーラの initrd を生成することはかなり複雑だからです。その他の 2 種類の解決策はよく使われます。なぜなら起動パラメータを使うことで、インストール作業の最初の質問の回答を他のやり方で事前指定することが可能だからです。インストールごとに起動パラメータを手作業で打ち込む手間を省くためによく使われる方法は <command>isolinux</command> (CD-ROM の場合) や <command>syslinux</command> (USB メモリの場合) の設定ファイルに起動パラメータを保存することです。</para>
      </section>
      <section>
        <title>preseed ファイルの作成</title>

	<para>preseed ファイルはプレーンテキストファイルで、各行に 1 つの Debconf 質問に対する回答が含まれます。行は空白 (スペースかタブ) で区切られた 4 種類のフィールドに分割されます。たとえば <literal>d-i mirror/suite string stable</literal> のようになります。</para>
        <itemizedlist>
          <listitem>
	    <para>最初のフィールドはこの質問の「所有者」です。インストーラに関する質問の場合「d-i」を使い、Debian パッケージからの質問の場合パッケージ名を使います。</para>
          </listitem>
          <listitem>
	    <para>2 番目のフィールドは質問の識別子です。</para>
          </listitem>
          <listitem>
	    <para>3 番目のフィールドは質問の種類です。</para>
          </listitem>
          <listitem>
	    <para>4 番目以降のフィールドは質問に対する回答です。3 番目のフィールドの後ろに必ず 1 つの空白を含めなければいけない点に注意してください。さらに 1 つ以上の回答がある場合、続く空白文字は回答の一部として取り扱われます。</para>
          </listitem>
        </itemizedlist>

	<para>preseed ファイルを書く最も簡単な方法はシステムを手作業でインストールする方法です。インストール完了後に <command>debconf-get-selections --installer</command> を実行してインストーラに関連する回答を取得します。<command>debconf-get-selections</command> を使えば、他のパッケージに関連する回答を取得することが可能です。しかしながら、最も明確な解決策は記載例と基準文書を参考にしながら手作業で preseed ファイルを書くことです。なぜなら、このような取り扱い方をすることで、デフォルト回答に対して上書きが必要な質問だけに回答を事前指定することが可能だからです。さらに <literal>priority=critical</literal> 起動パラメータ使うことで、Debconf が重要な質問だけを尋ね、他の質問にはデフォルトの回答を使うようにすることが可能です。</para>

        <sidebar>
          <title><emphasis>DOCUMENTATION</emphasis> インストールガイドの付録</title>

	  <para>オンライン上で利用できるインストールガイドの付録には、preseed ファイルの使い方に関する詳細な文書が含まれます。また、コメントの形で詳細を説明された見本ファイルが含まれます。これは自分用カスタマイズのひな形として使えるように用意されています。<ulink type="block" url="https://www.debian.org/releases/jessie/amd64/apb.html" /><ulink type="block" url="https://www.debian.org/releases/jessie/example-preseed.txt" /></para>
        </sidebar>
      </section>
      <section>
        <title>カスタマイズされた起動メディアの作成</title>

	<para>preseed ファイルを保存する場所を知ることは誠に結構なことですが、場所がすべてではありません。なぜなら、起動パラメータを変更し preseed ファイルを追加するためには、いずれにせよインストール用の起動メディアを改造しなければいけないからです。</para>
        <section>
          <title>ネットワークからの起動</title>

	  <para>コンピュータをネットワークから起動する場合、初期化要素を送信するサーバを使って起動パラメータを定義することも可能です。この場合、初期化要素の定義は起動サーバの PXE 設定の中で行う必要があります。より具体的に言えば <filename>/tftpboot/pxelinux.cfg/default</filename> 設定ファイルの中で設定します。そして、事前にネットワーク起動をセットアップすることが必要です。詳しくはインストールガイドをご覧ください。<ulink type="block" url="https://www.debian.org/releases/jessie/amd64/ch04s05.html" /></para>
        </section>
        <section>
          <title>起動可能な USB メモリの準備</title>

	  <para>起動可能な USB メモリを用意した場合 (<xref linkend="sect.install-usb" />を参照してください)、以下に挙げるいくつかの追加的な操作が必要です。USB メモリの内容は <filename>/media/usbdisk/</filename> で利用できるとします。</para>
          <itemizedlist>
            <listitem>
	      <para><filename>/media/usbdisk/preseed.cfg</filename> に preseed ファイルをコピーします</para>
            </listitem>
            <listitem>
	      <para><filename>/media/usbdisk/syslinux.cfg</filename> を編集して、必要な起動パラメータを追加します (以下の例をご覧ください)。</para>
            </listitem>
          </itemizedlist>

          <example>
            <title>syslinux.cfg ファイルと preseed パラメータ</title>

            <programlisting>default vmlinuz
append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --</programlisting>
          </example>
        </section>
        <section>
          <title>CD-ROM イメージの作成</title>
          <indexterm><primary>debian-cd</primary></indexterm>

	  <para>USB メモリは読み書きメディアなので、ファイルを追加したりいくつかのパラメータを変更することが簡単に可能です。CD-ROM の場合、この操作はさらに複雑になります。なぜなら、完全な ISO イメージを再生成する必要があるからです。<emphasis role="pkg">debian-cd</emphasis> がこの作業を担当しますが、<emphasis role="pkg">debian-cd</emphasis> ツールは少し使いにくいです。すなわち、<emphasis role="pkg">debian-cd</emphasis> ツールを使うには、ローカルミラーと <filename>/usr/share/debian-cd/CONF.sh</filename> によって提供されるすべてのオプションについて理解する必要があります。そしてさらに、<command>make</command> を複数回実行する必要があります。このため、<filename>/usr/share/debian-cd/README</filename> を一読することを強く推奨します。</para>

	  <para>そうは言っても、debian-cd の挙動は大きく変わるものではありません。具体的に言えば CD-ROM の内容を展開した「image」ディレクトリが生成され、その後 <command>genisoimage</command>、<command>mkisofs</command>、<command>xorriso</command> などのツールを使って「image」ディレクトリを ISO ファイルに変換します。image ディレクトリは debian-cd の <command>make image-trees</command> 段階の後に仕上げられます。この時点で、preseed ファイルを適切なディレクトリ (通常 <filename>$TDIR/$CODENAME/CD1/</filename> です、ここで $TDIR と $CODENAME は <filename>CONF.sh</filename> 設定ファイルによって定義されるパラメータです) に追加します。CD-ROM は <command>isolinux</command> をブートローダとして使います。必要な起動パラメータを追加するためには、<command>isolinux</command> の設定ファイル (ファイルは <filename>$TDIR/$CODENAME/boot1/isolinux/isolinux.cfg</filename> にあります) を debian-cd が生成した内容に適合させなければいけません。この後、「通常の」プロセスを再開し、<command>make image CD=1</command> (または <command>make images</command> 複数の CD-ROM を生成する場合) を実行して ISO イメージを生成します。</para>
        </section>
      </section>
    </section>
    <section id="sect.simple-cdd">
      <title>Simple-CDD、一体型の解決策</title>
      <indexterm><primary>simple-cdd</primary></indexterm>

      <para>単純に preseed ファイルを使うだけでは、大規模な配備に要求されるすべてを満足させることはできません。preseed ファイルを使うことで、通常のインストール作業の最後にいくつかのスクリプトを実行することが可能とは言うものの、インストールするパッケージ群の選択にはまだ大きな制限があります (基本的に「tasks」を選択できるだけです)。それどころかより重要なこととして、preseed ファイルを使えば公式の Debian パッケージをインストールすることが可能ですが、自前で作成したパッケージをインストールすることは不可能です。</para>

      <para>逆に、debian-cd を使えば外部パッケージを組み込むことが可能で、debian-installer を使えばインストール作業に新しい段階を挿入するように拡張することが可能です。これらの機能を組み合わせることで、自分の要求を完全に満足するカスタマイズされたインストーラを作成することが可能です。さらにこの方針を取ることで、必要なパッケージを展開した後、いくつかのサービスを設定することが可能です。幸いなことに、この方針は単なる仮説というわけではありません。なぜなら、これこそが Simple-CDD (<emphasis role="pkg">simple-cdd</emphasis> パッケージに含まれます) のやっていることだからです。</para>

      <para>Simple-CDD の目的とは、利用できるパッケージの一部を選択したり、Debconf を使ってパッケージを事前設定したり、特定のソフトウェアを追加したり、インストール作業の最後にカスタムスクリプトを実行することで、誰でも簡単に Debian の派生ディストリビューションを作成することです。これは「ユニバーサルオペレーティングシステム」の原理に一致します。なぜなら、このことにより誰でも自分自身の要求にオペレーティングシステムを適合させることが可能だからです。</para>
      <section>
        <title>プロファイルの作成</title>

	<para>Simple-CDD は FAI における「クラス」の概念に対応する「プロファイル」を定義し、マシンは (インストール時に定義される) 複数のプロファイルを持つことが可能です。プロファイルは以下に挙げる <filename>profiles/<replaceable>profile</replaceable>.*</filename> ファイルを使って定義されます。</para>
        <itemizedlist>
          <listitem>
	    <para><filename>.description</filename> ファイル。プロファイルに関する 1 行の説明が含まれます。</para>
          </listitem>
          <listitem>
	    <para><filename>.packages</filename> ファイル。プロファイルが選択された場合に自動的にインストールするパッケージがリストされています。</para>
          </listitem>
          <listitem>
	    <para><filename>.downloads</filename> ファイル。インストールメディアに保存するがシステムにインストールしないパッケージがリストされています。</para>
          </listitem>
          <listitem>
	    <para><filename>.preseed</filename> ファイル。(インストーラおよびパッケージの) Debconf 質問に関する preseed 情報が含まれます。</para>
          </listitem>
          <listitem>
	    <para><filename>.postinst</filename> ファイル。インストール作業の最後に実行されるスクリプトが含まれます。</para>
          </listitem>
          <listitem>
	    <para><filename>.conf</filename> ファイル。イメージに保存されるプロファイルに基づいていくつかの Simple-CDD パラメータを修正することが可能です。</para>
          </listitem>
        </itemizedlist>

	<para><literal>default</literal> プロファイルは特別な役割を担います。なぜなら <literal>default</literal> プロファイルは常に選択されるからです。そして <literal>default</literal> プロファイルには、Simple-CDD を動作させるために最低限必要な要素が含まれています。通常 <literal>default</literal> プロファイルの中でカスタマイズが必要なのは <literal>simple-cdd/profiles</literal> preseed パラメータだけです。なぜなら、これを使うことで Simple-CDD によって追加されたインストールするプロファイルの選択に関する質問を避けることが可能だからです。</para>

	<para>コマンドは <filename>profiles</filename> ディレクトリの親ディレクトリから実行する必要がある点に注意してください。</para>
      </section>
      <section>
        <title><command>build-simple-cdd</command> の設定と利用</title>
        <indexterm><primary><command>build-simple-cdd</command></primary></indexterm>

        <sidebar>
          <title><emphasis>QUICK LOOK</emphasis> 詳細説明を含む設定ファイル</title>

	  <para>すべての利用できるパラメータを含む Simple-CDD 設定ファイルの例 (<filename>/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz</filename>) がパッケージに含まれています。カスタム設定ファイルを作成する際に、この例を足掛かりとして使うことが可能です。</para>
        </sidebar>

	<para>Simple-CDD を完全に動作させるには多くのパラメータが必要です。通常、パラメータは設定ファイルの中にまとめられ、この設定ファイルを <command>build-simple-cdd</command> の <literal>--conf</literal> オプションに指定します。しかし、パラメータは専用パラメータを <command>build-simple-cdd</command> に渡すことでも指定することも可能です。以下では、パラメータの使い方と <command>build-simple-cdd</command> の挙動を大ざっぱに説明しています。</para>
        <itemizedlist>
          <listitem>
	    <para><literal>profiles</literal> パラメータは生成する CD-ROM イメージに含めるプロファイルをリストします。</para>
          </listitem>
          <listitem>
	    <para>Simple-CDD は要求されるパッケージのリストに基づいて、<literal>server</literal> で指定されているサーバから適切なファイルをダウンロードし、(後に debian-cd に渡される) ローカルミラーにまとめます。</para>
          </listitem>
          <listitem>
	    <para>また <literal>local_packages</literal> で指定されたカスタムパッケージがこのローカルミラーの中に統合されます。</para>
          </listitem>
          <listitem>
	    <para>この後、組み込むパッケージのリストを使って debian-cd が実行されます (実行場所は <literal>debian_cd_dir</literal> 変数を使って設定されたデフォルト位置です)。</para>
          </listitem>
          <listitem>
	    <para>debian-cd が <literal>debian_cd_dir</literal> で指定したディレクトリを用意した後、Simple-CDD はいくつかの変更をこのディレクトリに加えます。</para>
            <itemizedlist>
              <listitem>
		<para>プロファイルを含むファイルが <filename>simple-cdd</filename> サブディレクトリに追加されます (CD-ROM に追加されます)。</para>
              </listitem>
              <listitem>
		<para><literal>all_extras</literal> パラメータで指定された他のファイルがディレクトリに追加されます。</para>
              </listitem>
              <listitem>
		<para>起動パラメータが調整され、preseed が有効化されます。言語と国に関する質問を避けるには、これらの情報を <literal>language</literal> と <literal>country</literal> 変数に保存します。</para>
              </listitem>
            </itemizedlist>
          </listitem>
          <listitem>
	    <para>この後、debian-cd が最終的な ISO イメージを生成します。</para>
          </listitem>
        </itemizedlist>
      </section>
      <section>
        <title>ISO イメージの生成</title>

	<para>設定ファイルの記述と自分のプロファイルの定義が完了したら、<command>build-simple-cdd --conf simple-cdd.conf</command> を実行します。数分後、要求されたイメージが <filename>images/debian-8.0-amd64-CD-1.iso</filename> に完成します。</para>
      </section>
    </section>
  </section>
  <section id="sect.monitoring">
    <title>監視</title>

    <para>監視は一般的な用語で、さまざまな目的で行われるさまざまな活動を意味します。すなわち一方では、マシンの提供するリソースが使い切られ、更新が必要になることを予測することが可能です。さらに他方では、サービスが利用できなくなったり適切に動作していないことを可能な限り早く管理者に警告することにより、発生した問題の早急な修正を可能にすることを意味します。</para>

    <para><emphasis>Munin</emphasis> は最初の範囲をカバーします。すなわち <emphasis>Munin</emphasis> はいくつかのパラメータの経時変化をグラフィカルに図示します (使用された RAM、専有されたディスク領域、プロセッサの負荷、ネットワークトラフィック、Apache/MySQL の負荷などを図示します)。<emphasis>Nagios</emphasis> は 2 番目の範囲をカバーします。すなわち <emphasis>Nagios</emphasis> はサービスの稼働状態と利用可能状態を定期的に確認し、適切な経路 (電子メール、テキストメッセージなど) を通じて警告を送信します。<emphasis>Munin</emphasis> と <emphasis>Nagios</emphasis> はモジュール式に設計されているので、特定のパラメータやサービスを監視する新しいプラグインを簡単に作成できます。</para>

    <sidebar>
      <title><emphasis>ALTERNATIVE</emphasis> Zabbix、統合監視ツール</title>
      <indexterm><primary>Zabbix</primary></indexterm>

      <para>Munin と Nagios はとてもよく使われていますが、監視分野における唯一の選択肢というわけではありませんし、両者が担当している範囲は監視タスクの半分 (片方がグラフ化、もう一方が警告) に留まっています。これに対して、Zabbix は監視タスクの両方を統合しています。さらに Zabbix は最もよく使われる側面を設定するためのウェブインターフェースを備えています。Zabbix は最近の数年間で急速に成長し続けており、今や Munin と Nagios の対抗馬と考えられています。監視サーバには <emphasis role="pkg">zabbix-server-pgsql</emphasis> (または <emphasis role="pkg">zabbix-server-mysql</emphasis>) をインストールし、ウェブインターフェースを使うには <emphasis role="pkg">zabbix-frontend-php</emphasis> もインストールします。監視対象ホストには <emphasis role="pkg">zabbix-agent</emphasis> をインストールして、監視サーバからのデータ要求に応答します。<ulink type="block" url="http://www.zabbix.com/" /></para>
    </sidebar>

    <sidebar>
      <title><emphasis>ALTERNATIVE</emphasis> Icinga、Nagios のフォーク</title>
      <indexterm><primary>Icinga</primary></indexterm>

      <para>Nagios の開発モデル (開発は企業によって管理されています) に関する意見の食い違いが引き金となり、多数の開発者が Nagios をフォークし、新しい名前として Icinga を使っています。現時点で Icinga はまだ Nagios の設定およびプラグインと互換性を持っています。しかし、Icinga にはいくつかの機能が追加されています。<ulink type="block" url="http://www.icinga.org/" /></para>
    </sidebar>
    <section id="sect.munin">
      <title>Munin のセットアップ</title>
      <indexterm><primary>Munin</primary></indexterm>

      <para>Munin の目的は多くのマシンを監視することです。そしてこのため、Munin は当然クライアント/サーバアーキテクチャを採用しています。グラフ化を担当している中央ホストがすべての監視されているホストからデータを収集し、データの履歴グラフを生成します。</para>
      <section>
        <title>監視対象ホストの設定</title>

	<para>最初に <emphasis role="pkg">munin-node</emphasis> パッケージをインストールします。<emphasis role="pkg">munin-node</emphasis> パッケージによってインストールされるデーモンはポート 4949 番をリッスンし、すべての動作しているプラグインによって収集されたデータを送り返します。それぞれのプラグインは収集されたデータの説明および最新の計測値を返す簡単なプログラムです。プラグインは <filename>/usr/share/munin/plugins/</filename> に保存されますが、実際に使われるのは <filename>/etc/munin/plugins/</filename> 内からシンボリックリンクを張られたプラグインだけです。</para>

	<para><emphasis role="pkg">munin-node</emphasis> パッケージのインストールが完了したら、利用できるソフトウェアと現在のホストの設定に基づいて有効なプラグイン群が決定されます。しかしながら、有効プラグインの自動決定は各プラグインの提供する機能に依存します。通常、手作業で結果を確認して微調整することを推奨します。すべてのプラグインに対して包括的な文書が用意されているわけではありませんが、<ulink url="http://gallery.munin-monitoring.org">プラグインギャラリー</ulink>を閲覧すると面白いかもしれません。さらに、すべてのプラグインはスクリプトで、その多くは単純かつ詳細に説明されています。このため、各プラグインの機能を理解して無効化するべきプラグインを決定するには <filename>/etc/munin/plugins/</filename> を閲覧すると良いでしょう。同様に、<filename>/usr/share/munin/plugins/</filename> の中にある興味深いプラグインを有効化するには、<command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command> を使ってシンボリックリンクを作成するだけです。プラグイン名がアンダースコア「_」で終わる場合、そのプラグインはパラメータが必要という点に注意してください。シンボリックリンクの名前を使って、このパラメータを指定します。従って、たとえば「if_」プラグインは必ず <filename>if_eth0</filename> シンボリックリンクを使って有効化しなければいけません。こうすることで、eth0 インターフェースのネットワークトラフィックを監視します。</para>

	<para>すべてのプラグインを正常に設定したら、収集されたデータへのアクセス制御に関するデーモン設定を更新します。これを行うには、<filename>/etc/munin/munin-node.conf</filename> ファイルの中で <literal>allow</literal> 指示文を使います。デフォルト設定は <literal>allow ^127\.0\.0\.1$</literal> で、ローカルホストへのアクセスのみを許可します。通常、管理者はグラフ化を担当しているホストの IP アドレスを含めた同様の行を追加します。その後、<command>service munin-node restart</command> を使ってデーモンを再起動します。</para>

        <sidebar>
          <title><emphasis>GOING FURTHER</emphasis> ローカルプラグインの作成</title>

	  <para>Munin にはプラグインの動作様式と新規プラグインの開発方法に関する詳細な文書が用意されています。<ulink type="block" url="http://munin-monitoring.org/wiki/plugins" /></para>

	  <para>プラグインを検査するには、プラグインが munin-node から実行されたのと同じ状況下で実行するのが最良の方法です。この状況を模倣するには root で <command>munin-run <replaceable>plugin</replaceable></command> を実行します。このコマンドに与えられた 2 番目のパラメータ (<literal>config</literal> など) はパラメータとしてプラグインに渡されます。</para>

	  <para>プラグインに <literal>config</literal> パラメータを付けて実行した場合、プラグイン自身を説明する一連のフィールドが返されます。</para>

          <screen><computeroutput>$ </computeroutput><userinput>sudo munin-run load config
</userinput><computeroutput>graph_title Load average
graph_args --base 1000 -l 0
graph_vlabel load
graph_scale no
graph_category system
load.label load
graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run "immediately").
load.info 5 minute load average
</computeroutput></screen>

	  <para>利用できるさまざまなフィールドは「Munin ガイド」の一部として提供されている「プラグインリファレンス」で説明されています。<ulink type="block" url="http://munin.readthedocs.org/en/latest/reference/plugin.html" /></para>

	  <para>パラメータを付けずにプラグインを実行した場合、プラグインは最新の計測値を返却します。従って、たとえば <command>sudo munin-run load</command> を実行すると <literal>load.value 0.12</literal> が返されます。</para>

	  <para>最後に、プラグインに <literal>autoconf</literal> パラメータを付けて実行した場合、プラグインは自分が対象のホストで有効化されているか否かに基づいて「yes」(終了ステータス 0) または「no」(終了ステータス 1) を返すべきです。</para>
        </sidebar>
      </section>
      <section>
        <title>グラフ化担当マシンの設定</title>

	<para>「グラフ化担当マシン」はデータを集計し対応するグラフを生成するだけのコンピュータです。「グラフ化担当マシン」に必要なソフトウェアは <emphasis role="pkg">munin</emphasis> パッケージに含まれます。標準的な設定は <command>munin-cron</command> を (5 分ごとに) 実行します。このコマンドは <filename>/etc/munin/munin.conf</filename> にリストされているすべてのホスト (デフォルトではローカルホストのみがリストされています) からデータを収集し、時系列データを <filename>/var/lib/munin/</filename> にある RRD ファイル (<emphasis>Round Robin Database</emphasis>、経時変化するデータを保存するために設計されたファイルフォーマット) に保存し、<filename>/var/cache/munin/www/</filename> に含まれるグラフを使って HTML ページを生成します。</para>

	<para>すべての監視対象のマシンは <filename>/etc/munin/munin.conf</filename> 設定ファイルにリストされていなければいけません。各マシンは完全なセクションの形でリストされています。セクションはマシンと同じ名前で、少なくとも対応する IP アドレスを指定する <literal>address</literal> エントリを持っていなければいけません。</para>

        <programlisting>[ftp.falcot.com]
    address 192.168.0.12
    use_node_name yes</programlisting>

	<para>セクションをさらに複雑にして、複数のマシンからのデータをまとめて作成されるグラフを追加することも可能です。設定ファイルの中で提供されている見本がカスタマイズの良い足掛かりとなります。</para>

	<para>最後の段階は生成されたページを公開することです。そしてこれは、ウェブサイトから <filename>/var/cache/munin/www/</filename> の内容を利用できるようにするようウェブサーバを設定することを意味しています。通常このウェブサイトへのアクセスは認証メカニズムか IP に基づくアクセス制御を使って制限されています。アクセス制御の詳細は<xref linkend="sect.http-web-server" />をご覧ください。</para>
      </section>
    </section>
    <section id="sect.nagios">
      <title>Nagios のセットアップ</title>
      <indexterm><primary>Nagios</primary></indexterm>

      <para>Munin と異なり、Nagios の場合、必ずしも監視対象のホストに何かをインストールする必要はありません。そしてほとんどの場合、Nagios はネットワークサービスの可用性を確認するために使われます。たとえば Nagios を使うことで、ウェブサーバに接続して特定のウェブページがある時間内に取得できるかを確認することが可能です。</para>
      <section>
        <title>インストール</title>

	<para>Nagios をセットアップするには、最初に <emphasis role="pkg">nagios3</emphasis>、<emphasis role="pkg">nagios-plugins</emphasis>、<emphasis role="pkg">nagios3-doc</emphasis> パッケージをインストールします。これらのパッケージをインストールするとウェブインターフェースが設定され、最初の <literal>nagiosadmin</literal> ユーザが作成されます (このユーザのパスワードが尋ねられます)。他のユーザを追加するには、Apache の <command>htpasswd</command> コマンドを使ってユーザを <filename>/etc/nagios3/htpasswd.users</filename> ファイルに追加するだけです。Debconf 質問がインストール中に表示されない場合、<command>dpkg-reconfigure nagios3-cgi</command> を使って <literal>nagiosadmin</literal> のパスワードを定義することも可能です。</para>

	<para>ブラウザで <literal>http://<replaceable>server</replaceable>/nagios3/</literal> にアクセスすると、ウェブインターフェースが表示されます。特に、Nagios は自分が実行されているマシンのいくつかのパラメータを既に監視している点に注意してください。しかしながら、たとえばホストに対するコメントを追加するなどの対話型機能は動作しません。Nagios のデフォルト設定はこれらの機能を無効化し、セキュリティの理由からとても厳しい制限を設けています。</para>

	<para><filename>/usr/share/doc/nagios3/README.Debian</filename> で説明されている通り、いくつかの機能を有効化するには <filename>/etc/nagios3/nagios.cfg</filename> を編集し、<literal>check_external_commands</literal> パラメータを「1」に設定します。また、以下のようにして、Nagios が使うディレクトリに書き込みパーミッションを設定する必要があります。</para>

        <screen><computeroutput># </computeroutput><userinput>service nagios3 stop</userinput>
<computeroutput>[...]
# </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios3/rw
</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios3
</userinput><computeroutput># </computeroutput><userinput>service nagios3 start</userinput>
<computeroutput>[...]</computeroutput></screen>
      </section>
      <section>
        <title>設定</title>

	<para>Nagios のウェブインターフェースはかなり良くできていますが、設定もできませんし、監視対象のホストやサービスの追加もできません。全体の設定は中央設定ファイル <filename>/etc/nagios3/nagios.cfg</filename> から参照されているオブジェクト設定ファイルを使って管理されます。</para>

	<para>Nagios の概念を理解していない場合、オブジェクト設定ファイルの内容に立ち入るべきではありません。オブジェクト設定ファイルから設定するオブジェクトには以下の種類があります。</para>
        <itemizedlist>
          <listitem>
	    <para><emphasis>host</emphasis> は監視対象のマシンです。</para>
          </listitem>
          <listitem>
	    <para><emphasis>hostgroup</emphasis> はグループ化して表示されたり、同じ設定要素を持つホスト群です。</para>
          </listitem>
          <listitem>
	    <para><emphasis>service</emphasis> はホストやホストグループへの検査項目を定義します。これは多くの場合、あるネットワークサービスに対する検査を定義するものですが、いくつかのパラメータ (たとえば空きディスク領域やプロセッサ負荷) が条件を満たす範囲内にあるかに対する検査を定義することも可能です。</para>
          </listitem>
          <listitem>
	    <para><emphasis>servicegroup</emphasis> はグループ化して表示されるサービス群です。</para>
          </listitem>
          <listitem>
	    <para><emphasis>contact</emphasis> は警告を受け取る人です。</para>
          </listitem>
          <listitem>
	    <para><emphasis>contactgroup</emphasis> は警告を受け取る人のグループです。</para>
          </listitem>
          <listitem>
	    <para><emphasis>timeperiod</emphasis> は時間範囲で、この範囲内にいくつかのサービスを確認します。</para>
          </listitem>
          <listitem>
	    <para><emphasis>command</emphasis> はサービスを確認するために実行するコマンドラインです。</para>
          </listitem>
        </itemizedlist>

	<para>オブジェクトの種類に応じて、各オブジェクトにはカスタマイズが可能な複数の属性が含まれます。完全なリストはここに挙げるには長すぎますが、最重要の属性はオブジェクト間の関係性を示す属性です。</para>

	<para><emphasis>service</emphasis> は <emphasis>command</emphasis> を使い、<emphasis>timeperiod</emphasis> で定めた時間内に <emphasis>host</emphasis> (または <emphasis>hostgroup</emphasis>) で稼働する特定の機能を確認します。問題が起きた場合、Nagios はそのサービスに関連付けられた <emphasis>contactgroup</emphasis> のメンバーに警告を送信します。各メンバーは対応する <emphasis>contact</emphasis> オブジェクトに書かれたチャンネルを介して警告を受け取ります。</para>

	<para>継承システムにより、情報を複製せずに多くのオブジェクト間の属性群を簡単に共有することが可能です。加えて、初期設定には数多くの標準的なオブジェクトが定義されています。このため多くの場合、初期設定の標準的なオブジェクトに加えて新たな <emphasis>host</emphasis>、<emphasis>service</emphasis>、<emphasis>contact</emphasis> を定義するだけで簡単に設定を完了させることが可能です。<filename>/etc/nagios3/conf.d/</filename> に含まれるファイルはオブジェクトの動作に関する良い情報源です。</para>

	<para>Falcot Corp の管理者は以下の設定を使います。</para>

        <example>
          <title><filename>/etc/nagios3/conf.d/falcot.cfg</filename> ファイル</title>

          <programlisting>define contact{
    name                            generic-contact
    service_notification_period     24x7
    host_notification_period        24x7
    service_notification_options    w,u,c,r
    host_notification_options       d,u,r
    service_notification_commands   notify-service-by-email
    host_notification_commands      notify-host-by-email
    register                        0 ; Template only
}
define contact{
    use             generic-contact
    contact_name    rhertzog
    alias           Raphael Hertzog
    email           hertzog@debian.org
}
define contact{
    use             generic-contact
    contact_name    rmas
    alias           Roland Mas
    email           lolando@debian.org
}

define contactgroup{
    contactgroup_name     falcot-admins
    alias                 Falcot Administrators
    members               rhertzog,rmas
}

define host{
    use                   generic-host ; Name of host template to use
    host_name             www-host
    alias                 www.falcot.com
    address               192.168.0.5
    contact_groups        falcot-admins
    hostgroups            debian-servers,ssh-servers
}
define host{
    use                   generic-host ; Name of host template to use
    host_name             ftp-host
    alias                 ftp.falcot.com
    address               192.168.0.6
    contact_groups        falcot-admins
    hostgroups            debian-servers,ssh-servers
}

# 'check_ftp' コマンドにカスタムパラメータを渡します
define command{
    command_name          check_ftp2
    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35
}

# Falcot の運用する一般サービスを定義します
define service{
    name                  falcot-service
    use                   generic-service
    contact_groups        falcot-admins
    register              0
}

# www-host 上の監視対象サービスを定義します
define service{
    use                   falcot-service
    host_name             www-host
    service_description   HTTP
    check_command         check_http
}
define service{
    use                   falcot-service
    host_name             www-host
    service_description   HTTPS
    check_command         check_https
}
define service{
    use                   falcot-service
    host_name             www-host
    service_description   SMTP
    check_command         check_smtp
}

# ftp-host 上の監視対象サービスを定義します
define service{
    use                   falcot-service
    host_name             ftp-host
    service_description   FTP
    check_command         check_ftp2
}</programlisting>
        </example>

	<para>この設定ファイルでは、2 種類の監視対象ホストが定義されています。1 番目のホストはウェブサーバです。Nagios はこのホストに対してウェブサーバが HTTP (80) とセキュア HTTP (443) ポートで稼働していること、SMTP サーバがポート 25 番で稼働していることを確認します。2 番目のホストは FTP サーバです。Nagios はこのホストに対して応答が 20 秒以内に返されることが保証されることを確認します。Nagios は FTP サーバからの応答にかかる時間が 20 秒より長い場合に<emphasis>警告</emphasis>を、30 秒より長い場合に危機的な警告を発します。Nagios のウェブインターフェースは SSH サービスが監視されていることを示しています。すなわちこれは <literal>ssh-servers</literal> ホストグループに所属するホストの情報です。標準的なサービスの稼動状態確認は <filename>/etc/nagios3/conf.d/services_nagios2.cfg</filename> で定義されています。</para>

	<para>継承の使い方に注意してください。具体的に言えば、オブジェクトを継承するには「use <replaceable>parent-name</replaceable>」の形で親オブジェクトの名前を指定します。親オブジェクトは識別可能でなければいけません。つまり、親オブジェクトに「name <replaceable>identifier</replaceable>」属性を与える必要があります。親オブジェクトが真のオブジェクトでなく、属性継承の機能を担うだけの場合、このオブジェクトに「register 0」属性を与えます。こうすることで Nagios はこのオブジェクトを考慮しなくなり、真のオブジェクトならば必須とされるいくつかのパラメータが欠けていてもその問題を無視するようになります。</para>

        <sidebar>
          <title><emphasis>DOCUMENTATION</emphasis> オブジェクト属性のリスト</title>

	  <para>Nagios を設定するさまざまな方法に関してより深い理解を得るには、<emphasis role="pkg">nagios3-doc</emphasis> パッケージに含まれる文書を読むと良いでしょう。この文書はウェブインターフェースの左上にある「Documentation」リンクから直接的に利用できます。この文書には、すべてのオブジェクト型のリストと各オブジェクトの取りうるすべての属性が説明されています。さらに、新しいプラグインの作り方も説明されています。</para>
        </sidebar>

        <sidebar>
          <title><emphasis>GOING FURTHER</emphasis> NRPE を使ったリモートマシンの状態検査</title>

	  <para>多くの Nagios プラグインでは、あるホストに固有のいくつかのパラメータを確認することが可能です。多くのマシンに対するパラメータの確認が必要にも関わらず Nagios サーバでパラメータを収集する場合、NRPE (<emphasis>Nagios Remote Plugin Executor</emphasis>) プラグインを配備する必要があります。Nagios サーバには <emphasis role="pkg">nagios-nrpe-plugin</emphasis> パッケージをインストールし、ローカルでテストを実行するホストに <emphasis role="pkg">nagios-nrpe-server</emphasis> をインストールする必要があります。<emphasis role="pkg">nagios-nrpe-server</emphasis> は <filename>/etc/nagios/nrpe.cfg</filename> から設定を取得します。このファイルには、リモートからの命令に従って実行されるテストとリモートからのテスト開始命令を受け入れるマシンの IP アドレスをリストするべきです。Nagios サーバ側でリモートテストを有効化するには、<emphasis>check_nrpe</emphasis> コマンドを使って一致するサービスを追加するだけで済みます。</para>
        </sidebar>
      </section>
    </section>
  </section>
</chapter>
