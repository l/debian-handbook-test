<?xml version='1.0' encoding='utf-8' ?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [
]>
<chapter id="advanced-administration" lang="nb-NO">
	<chapterinfo>
		 <keywordset>
			<keyword>RAID</keyword>
			 <keyword>LVM</keyword>
			 <keyword>FAI</keyword>
			 <keyword>Å komme foran</keyword>
			 <keyword>Overvåking</keyword>
			 <keyword>Visualisering</keyword>
			 <keyword>Xen</keyword>
			 <keyword>LXC</keyword>

		</keywordset>

	</chapterinfo>
	 <title>Avansert administrasjon</title>
	 <highlights> <para>
		Dette kapittelet besøker igjen noen aspekter vi allerede har beskrevet, med et annet perspektiv: I stedet for å installere en enkelt datamaskin, vil vi studere massedistribusjonssystemer; Istedet for å opprette RAID eller LVM ved nye installasjoner, vil vi lære å gjøre det for hånd, slik at vi senere kan endre våre første valg. Til slutt vil vi diskutere overvåkningsverktøy og virtualiseringsteknikker. Som en konsekvens, er dette kapittelet mer spesielt rettet mot profesjonelle administratorer, og fokuserer litt mindre på personer med ansvar for sine hjemmenettverk .
	</para>
	 </highlights> <section id="sect.raid-and-lvm">
		<title>RAID og LVM</title>
		 <para>
			<xref linkend="installation" /> presenteres disse teknologiene fra installeringssynspunkt, og hvordan de kan integreres til å gjøre utplasseringen lett fra start. Etter den første installasjonen, må en administrator kunne håndtere utvikling av lagringsplassbehov uten å måtte ty til en kostbar reinstallasjon. De må derfor forstå de nødvendige verktøy for å håndtere RAID- og LVM-volumer.
		</para>
		 <para>
			RAID og LVM er begge teknikker til trekke ut de monterte volumene fra sien fysiske motstykker (faktiske harddisker eller partisjoner); den første e sikrer data mot maskinvarefeil ved å innføre redundans, sistnevnte gjør volumadministrasjon mer fleksibel og uavhengig av den faktiske størrelsen på de underliggende disker. I begge tilfeller ender systemet opp med nye blokk-enheter, som kan brukes til å lage filsystemer eller vekselfiler, uten nødvendigvis å ha dem tilordnet til en fysisk disk. RAID og LVM kommer med helt forskjellig bakgrunn, men funksjonaliteten kan overlappe noe, de er derfor ofte nevnt sammen.
		</para>
		 <sidebar> <title><emphasis>PERSPECTIVE</emphasis> Btrfs kombinerer LVM og RAID</title>
		 <para>
			Mens LVM og RAID er to forskjellige kjerne-delsystemer som ligger mellom disk blokk-enheter og filsystemene deres,<emphasis>btrfs</emphasis> er et nytt filsystem, opprinnelig utviklet i Oracle, som skal kombinere kjennetegnsettene til LVM og RAID og mye mer. Det er for det meste funksjonelt, og selv om det fremdeles er merket "eksperimentell" fordi utviklingsstadiet er ufullstendig (noen funksjoner er ikke implementert ennå), har det allerede sett i bruk i produksjonsmiljøer. <ulink type="block" url="http://btrfs.wiki.kernel.org/" />
		</para>
		 <para>
			Blant funksjonene å legge merke til, er muligheten til på ethvert tidspunkt å ta et øyeblikksbilde av et filsystem-tre. Denne øyeblikksbilde-kopien vil ikke i utgangspunktet bruke diskplass, data blir bare duplisert når en av kopiene blir endret. Filsystemet håndterer også gjennomsiktig komprimering av filer, og kontrollsummer sikrer integriteten til alle lagrede data.
		</para>
		 </sidebar> <para>
			Både for RAID og LVM gir kjernen en blokk-spesialfil, som lik dem som svarer til en harddisk eller en partisjon. Når et program, eller en annen del av kjernen, krever tilgang til et blokk innrettet slik, ruter det aktuelle subsystem blokken til det aktuelle fysiske laget. Avhengig av konfigurasjonen, kan denne blokken lagres på en eller flere fysiske disker, og den fysiske plasseringen kan ikke være direkte korrelert til plassering av blokken i den logiske enheten.
		</para>
		 <section id="sect.raid-soft">
			<title>Programvare RAID</title>
			 <indexterm>
				<primary>RAID</primary>
			</indexterm>
			 <para>
				RAID betyr <emphasis>Redundant Array of Independent Disks</emphasis>. Målet med dette systemet er å hindre tap av data i tilfelle feil på harddisken . Det generelle prinsippet er ganske enkelt: Data er lagret på flere fysiske disker i stedet for bare en, med en konfigurerbar grad av redundans. Avhengig av denne redundansstørrelsen, og til og med i tilfelle av et uventet diskfeil, kan data uten tap bli rekonstruert fra de gjenværende disker.
			</para>
			 <sidebar> <title><emphasis>CULTURE</emphasis> <foreignphrase>Independent</foreignphrase> or <foreignphrase>inexpensive</foreignphrase>?</title>
			 <para>
				I-en i RAID sto opprinnelig for <emphasis>inexpensive</emphasis>, fordi RAID tillot en drastisk økning i datasikkerhet uten å kreve investering i dyre high-end disker. Sannsynligvis på grunn av bildehensyn, imidlertid, nå er det en mer vanlig vurdering å stå for <emphasis>independent</emphasis>, som ikke har den uønskede smaken av å være billig.
			</para>
			 </sidebar> <para>
				RAID kan implementeres enten ved øremerket maskinvare (RAID-moduler integrert i SCSI eller SATA-kontrolleren kort) eller ved bruk av programvare-sammendrag (kjernen). Enten maskinvare eller programvare, kan et RAID-system, med nok reserve, transparent fortsette operativt når en disk svikter; de øvre lag av stabelen (applikasjoner) kan også beholde tilgangen til data tross feilen. Selvfølgelig, denne "degradert modus" kan ha en innvirkning på ytelsen, og reservekapasiteten er redusert, slik at en ytterligere diskfeil kan føre til tap av data. I praksis vil derfor en bestrebe på å bli værende bare med denne redusert driften så lenge som det tar å erstatte den ødelagte disken. Så snart den nye disken er på plass, kan RAID-systemet rekonstruere de nødvendige data, og gå tilbake til en sikker modus. Programmene vil ikke merke noe, bortsett fra en potensielt redusert tilgangshastighet, mens området er i redusert drift eller under rekonstruksjonsfasen.
			</para>
			 <para>
				Når RAID implementeres av maskinvare, skjer oppsettet vanligvis innen BIOSs konfigurasjonsverktøy, og kjernen vil vurdere en RAID-tabell som en enkelt disk, som vil virke som en standard fysisk disk, selv om navnet på enheten kan være forskjellige (avhengig av driveren).
			</para>
			 <para>
				Vi fokuserer bare på programvare-RAID i denne boken.
			</para>
			 <section id="sect.raid-levels">
				<title>Ulike RAID-nivåer</title>
				 <para>
					RAID er faktisk ikke et enkelt system, men et spekter av systemer som identifiseres av sine nivåer. Nivåene skiller seg ved sin utforming og mengden av reserve de gir. Jo større overflødighet, jo sikrere mot feil, siden systemet vil være i stand til å fortsette arbeidet med flere disker som feiler. Motstykket er at plassen som kan brukes krymper for et gitt sett med disker. Sett den andre veien, vil flere disker være nødvendig for å lagre et gitt mengde av data.
				</para>
				 <variablelist>
					<varlistentry>
						<term>Lineær RAID</term>
						 <listitem>
							<para>
								Selv om kjernens RAID-delsystem kan lage "lineær RAID", er ikke dette RAID, siden dette oppsettet ikke gir overskudd. Kjernen samler bare flere disker etter hverandre og resulterer i et samlet volum som e n virtuell disk (en blokkenhet). Det er omtrent dens eneste funksjon. Dette oppsettet brukes sjelden i seg selv (se senere for unntak), spesielt siden mangelen på overskudd betyr at om en disk svikter, aggregerer det, og gjør alle data utilgjengelige.
							</para>

						</listitem>

					</varlistentry>
					 <varlistentry>
						<term>RAID-0</term>
						 <listitem>
							<para>
								Dette nivået gir intet overskudd heller, men diskene blir ikke ganske enkelt fastlåst etter hverandre: De nlir delt i <emphasis>stripes</emphasis>, og blokkene på den virtuelle enheten er lagret på striper på alternerende fysiske disker. I et to-disk RAID-0 oppsett, for eksempel, vil partall-blokker på den virtuelle enheten vil bli lagret på den første fysiske disken, mens oddetalls blokker vil komme på den andre fysiske disken.
							</para>
							 <para>
								Dette systemet har ikke som mål økt pålitelighet, siden (som i det lineære tilfellet) tilgjengeligheten til alle data er i fare så snart en disk svikter, men å øke ytelsen: Under sekvensiell tilgang til store mengder sammenhengende data, vil kjernen være i stand til å lese fra begge disker (eller skrive til dem) i parallell, noe som øker hastigheten på dataoverføringen. Imidlertid krymper RAID-0-bruken når nisjen dens fylles med LVM (se senere).
							</para>

						</listitem>

					</varlistentry>
					 <varlistentry>
						<term>RAID-1</term>
						 <listitem>
							<para>
								Dette nivået, også kjent som "RAID speiling", er både den enkleste og det mest brukte oppsettet. I standardformen, bruker den to fysiske disker av samme størrelse, og gir et logisk volum av samme størrelse på nytt. Data er lagret identisk på begge disker, derav kallenavnet "speile". Når en disk svikter, finnes dataene fremdeles på den andre. For virkelig kritiske data, kan RAID-1 selvsagt settes opp på mer enn to disker, med direkte konsekvenser for forholdet mellom maskinvarekostnader opp mot tilgjengelig plass for nyttelast.
							</para>
							 <sidebar> <title><emphasis>NOTE</emphasis> Disk- og klynge-størrelser</title>
							 <para>
								Hvis to disker av forskjellige størrelse er satt opp i et speil, vil ikke den største bli brukt fullt ut, siden det vil inneholde de samme dataene som den minste og ingenting mer. Denne nyttige tilgjengelige plassen levert av et RAID-1-volum passer derfor til størrelsen på den minste disken i rekken. Dette gjelder likevel for RAID-volumer med høyere RAID-nivå, selv om overskudd er lagret på en annen måte.
							</para>
							 <para>
								Det er derfor viktig, når du setter opp RAID-matriser (unntatt for RAID-0 og "lineær RAID"), å bare montere disker av identiske eller svært nære størrelser, for å unngå å sløse med ressurser.
							</para>
							 </sidebar> <sidebar> <title><emphasis>NOTE</emphasis> Reservedisker</title>
							 <para>
								RAID-nivåer som inkluderer overskudd tillater tilordning av flere disker enn det som kreves til en matrise. De ekstra diskene blir brukt som reservedeler når en av de viktigste diskene svikter. For eksempel, i et speilbilde av to-plater pluss en i reserve: Dersom en av de to første diskene svikter, vil kjernen automatisk (og umiddelbart) rekonstruere speilet ved hjelp av en ekstra disk, slik at overskuddet forblir sikret etter gjenoppbyggingstidspunktet. Dette kan brukes som en annen form for ekstra sikkerhet for kritiske data.
							</para>
							 <para>
								En ville bli tilgitt for å undre seg over hvordan dette er bedre enn bare speiling på tre disker til å begynne med. Fordelen med "ledig disk"-oppsettet er at en ekstra disk kan deles på tvers av flere RAID-volumer. For eksempel kan man ha tre speilende volumer, med overskudd sikret selv i tilfelle av disksvikt med bare syv disker (tre par, pluss en felles i reserve), - i stedet for de ni diskene som ville være nødvendig med tre trillinger.
							</para>
							 </sidebar> <para>
								Dette RAID-nivået, selv om dyrere (da bare halvparten av de fysiske lagringsplass, i beste fall, er i bruk), er mye brukt i praksis. Det er enkelt å forstå, og det gir svært enkle sikkerhetskopier: Siden begge diskene har identisk innhold, kan en av dem bli midlertidig ekstrahert uten noen innvirkning på systemet ellers. Leseytelsen er ofte økt siden kjernen kan lese halvparten av dataene på hver disk i parallell, mens skriveytelsen er ikke altfor alvorlig svekket. I tilfelle med en RAID-en matrise med N disker, forbilir data tilgjengelig selv med N-1 diskfeil.
							</para>

						</listitem>

					</varlistentry>
					 <varlistentry>
						<term>RAID-4</term>
						 <listitem>
							<para>
								Dette RAID-nivået, ikke mye brukt, bruker N-plater til å lagre nyttige data, og en ekstra disk til å lagre overskuddsinformasjon. Hvis den disken svikter, kan systemet rekonstruere innholdet fra den andre N-en. Hvis en av de N datadiskene svikter, inneholder den gjenværende N-1 kombinert med "paritets"-disken nok informasjon til å rekonstruere de nødvendige dataene.
							</para>
							 <para>
								RAID-4 er ikke for dyrt siden det bare omfatter en one-in-N økning i kostnader, og har ingen merkbar innvirkning på leseytelsen, men skriving går langsommere. Videre, siden et skript til hvilket som helst av N-platerene også omfatter et skript til paritetsdisken, ser den sistnevnte mange flere skrivinger enn den første, og dens levetid kan forkorte dramatisk som et konsekvens. Data på en RAID-4-matrise er bare trygg opp til en feilende disk (av de N + 1).
							</para>

						</listitem>

					</varlistentry>
					 <varlistentry>
						<term>RAID-5</term>
						 <listitem>
							<para>
								RAID-5 løser asymmetri-spørsmålet til RAID-4: Paritetsblokker er spredt over alle N+1-disker, uten at en enkeltdisk har en bestemt rolle.
							</para>
							 <para>
								Lese- og skrivehastighet er identiske til RAID-4. Her igjen, forblir systemet funksjonell med opp til en disk som feiler (av de N + 1), men ikke flere.
							</para>

						</listitem>

					</varlistentry>
					 <varlistentry>
						<term>RAID-6</term>
						 <listitem>
							<para>
								RAID-6 kan betraktes som en forlengelse av RAID-5, der hver serie med N-blokker involverer to reserveblokker, og hver slik serie med N+2 blokker er spredt over N+2 disker.
							</para>
							 <para>
								Dette RAID-nivået er litt dyrere enn de to foregående, men det bringer litt ekstra sikkerhet siden opptil to stasjoner (i N+2) kan svikte uten at det går ut over datatilgjengeligheten. Motstykket er at skriveoperasjoner nå innebærer å skrive ut på en datablokk og to reserveblokker, noe som gjør dem enda tregere.
							</para>

						</listitem>

					</varlistentry>
					 <varlistentry>
						<term>RAID-1+0</term>
						 <listitem>
							<para>
								Dette er ikke strengt tatt et RAID-nivå, men en samling av to RAID-grupperinger. Med start fra 2×N disker, setter man dem først opp i parene i N RAID-1-volumer; Disse N-volumene blir så samlet til ett, enten ved "lineær RAID" eller (i økende grad) av LVM. Dette siste tilfellet går lenger enn ren RAID, men det er ikke noe problem med det.
							</para>
							 <para>
								RAID-1+0 kan overleve flere diskfeil: opp til N i 2xN matrisen som er beskrevet ovenfor, forutsatt at minst en disk fortsetter å virke i hver av RAID-1-parene.
							</para>
							 <sidebar id="sidebar.raid-10"> <title><emphasis>GOING FURTHER</emphasis> RAID-10</title>
							 <para>
								RAID-10 er generelt ansett som synonym for RAID-1+0, men en Linux spesifisitet gjør det faktisk til en generalisering. Dette oppsettet gjør det mulig for et system, der hver blokk er lagret på to forskjellige disker, selv med et oddetall disker, at kopiene blir spredt ut i en konfigurerbar modell.
							</para>
							 <para>
								Yteevnen vil variere avhengig av valgt repartisjonsmodell og reservenivå, og av arbeidsmengden til det det logiske volumet.
							</para>
							 </sidebar>
						</listitem>

					</varlistentry>

				</variablelist>
				 <para>
					Selvfølgelig vil RAID-nivået velges ut fra begrensningene og kravene til hvert program. Merk at en enkelt datamaskin kan ha flere forskjellige RAID-matriser med forskjellige konfigurasjoner.
				</para>

			</section>
			 <section id="sect.raid-setup">
				<title>Å sette opp RAID</title>
				 <indexterm>
					<primary><emphasis role="pkg">mdadm</emphasis></primary>
				</indexterm>
				 <para>
					Å sette opp RAID-volumer krever <emphasis role="pkg">mdadm</emphasis>-pakken; den leverer <command>mdadm</command>-kommandoen, som gjør det mulig å lage og håndtere RAID-tabeller, samt prosedyrer og verktøy som integrerer den i resten av systemet, inkludert overvåkningssystemet.
				</para>
				 <para>
					Vårt eksempel vil være en tjener med en rekke disker, der noen er allerede brukt, og resten blir tilgjengelig til å sette opp RAID. Vi har i utgangspunktet følgende disker og partisjoner:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							<filename>sdb</filename>-disken, 4 GB, er fullt tilgjengelig;
						</para>

					</listitem>
					 <listitem>
						<para>
							<filename>sdc</filename>-disken, 4 GB, er også fullt ut tilgjengelig;
						</para>

					</listitem>
					 <listitem>
						<para>
							På <filename>sdd</filename>-disken, er bare partisjonen <filename>sdd2</filename> (rundt 4 GB) tilgjengelig;
						</para>

					</listitem>
					 <listitem>
						<para>
							til slutt er en <filename>sde</filename>-disk, også på 4 GB, fullt ut tilgjengelig.
						</para>

					</listitem>

				</itemizedlist>
				 <sidebar> <title><emphasis>NOTE</emphasis> Identifisere eksisterende RAID-volumer</title>
				 <para>
					<filename>/proc/mdstat</filename>-filen lister eksisterende volumer og tilstanden deres. Når du oppretter et nytt RAID-volum, bør man være forsiktig for å ikke gi det samme navnet som på et eksisterende volum.
				</para>
				 </sidebar> <para>
					Vi kommer til å bruke disse fysiske elementer for å bygge to volumer, en RAID-0 og ett speil (RAID-1). La oss starte med RAID-0-volumet:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</userinput>
<computeroutput>mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
# </computeroutput><userinput>mdadm --query /dev/md0</userinput>
<computeroutput>/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.
# </computeroutput><userinput>mdadm --detail /dev/md0</userinput>
<computeroutput>/dev/md0:
        Version : 1.2
  Creation Time : Wed May  6 09:24:34 2015
     Raid Level : raid0
     Array Size : 8387584 (8.00 GiB 8.59 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:24:34 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

     Chunk Size : 512K

           Name : mirwiz:0  (local to host mirwiz)
           UUID : bb085b35:28e821bd:20d697c9:650152bb
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       1       8       32        1      active sync   /dev/sdc
# </computeroutput><userinput>mkfs.ext4 /dev/md0</userinput>
<computeroutput>mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 2095104 4k blocks and 524288 inodes
Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </computeroutput><userinput>mkdir /srv/raid-0</userinput>
<computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0</userinput>
<computeroutput># </computeroutput><userinput>df -h /srv/raid-0</userinput>
<computeroutput>Filesystem      Size  Used Avail Use% Mounted on
/dev/md0        7.9G   18M  7.4G   1% /srv/raid-0
</computeroutput></screen>
				 <para>
					<command>mdadm --create</command>-kommandoen krever flere parametre: Navnet på volumet som skal lages (<filename>/dev/md*</filename>, der MD står for <foreignphrase>Multiple Device</foreignphrase>), RAID-nivået, antall disker (som er obligatorisk til tross for at det er mest meningsfullt bare med RAID-1 og over), og de fysiske enhetene som skal brukes. Når enheten er opprettet, kan vi bruke den som vi ville bruke en vanlig partisjon, opprette et filsystem på den, montere dette filsystemet, og så videre. Vær oppmerksom på at vår etablering av et RAID-0-volum på <filename>md0</filename> ikke er et sammentreff, og nummereringen av tabellen ikke trenger å være korrelert til det valgte størrelsen på reservekapasiteten. Det er også mulig å lage navngitte RAID arrays, ved å gi <command>mdadm</command>.parametre slik som <filename>/dev/md/linear</filename> instedet for <filename>/dev/md0</filename>.
				</para>
				 <para>
					Opprettelse av en RAID-1 følger op en lignende måte, forskjellene bare blir merkbare etter etableringen:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</userinput>
<computeroutput>mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%
Continue creating array? </computeroutput><userinput>y</userinput>
<computeroutput>mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md1 started.
# </computeroutput><userinput>mdadm --query /dev/md1</userinput>
<computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
        Version : 1.2
  Creation Time : Wed May  6 09:30:19 2015
     Raid Level : raid1
     Array Size : 4192192 (4.00 GiB 4.29 GB)
  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:30:40 2015
          State : clean, resyncing (PENDING) 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       1       8       64        1      active sync   /dev/sde
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
          State : clean
[...]
</computeroutput></screen>
				 <sidebar> <title><emphasis>TIP</emphasis> RAID, disker og partisjoner</title>
				 <para>
					Som illustrert ved vårt eksempel, kan RAID-enheter bygges fra ediskpartisjoner, og krever ikke hele disker.
				</para>
				 </sidebar> <para>
					Noen få merknader er på sin plass. Først, <command>mdadm</command> merker at de fysiske elementene har forskjellige størrelser; Siden dette innebærer at noe plass går tapt på de større elementene, kreves en bekreftelse.
				</para>
				 <para>
					Enda viktigere, merk tilstanden til speilet. Normal tilstand for et RAID speil er at begge diskene har nøyaktig samme innhold. Men ingenting garanterer dette er tilfelle når volumet blir opprettet. RAIDs subsystem vil derfor gi denne garantien selv, og det vil være en synkroniseringsfase så snart RAID-enheten er opprettet. Etter en tid (det nøyaktige tiden vil avhenge av den faktiske størrelsen på diskene ...), skifter RAID-tabellen til "aktiv" eller "ren" tilstand. Legg merke til at i løpet av denne gjenoppbyggingsfasen, er speilet i en degradert modus, og reservekapasitet er ikke sikret. En disk som svikter i løpet av dette risiko-vinduet kan føre til at alle data mistes. Store mengder av viktige data er imidlertid sjelden lagret på en nyopprettet RAID før den første synkroniseringen. Legg merke til at selv i degradert modus, vil <filename>/dev/md1</filename> kunne brukes, et filsystem kan opprettes på den, så vel som noe data kopieres på den.
				</para>
				 <sidebar> <title><emphasis>TIP</emphasis> Å starte et speil i degradert modus</title>
				 <para>
					Noen ganger er to disker ikke umiddelbart tilgjengelig når man ønsker å starte et RAID-1-speil, for eksempel fordi en av diskene en planlegger å inkludere allerede er brukt til å lagre dataene man ønsker å flytte til matrisen. I slike tilfeller er det mulig å bevisst skape en degraderte RAID-1-tabell ved å sende <filename>missing</filename> i stedet for et enhetsfil som ett av argumentene til <command>mdadm</command>. Når dataene er blitt kopiert til "speilet", kan den gamle disken legges til matrisen. Så vil det finne sted en synkronisering, noe som gir oss den reservekapasitet som var ønsket i første omgang.
				</para>
				 </sidebar> <sidebar> <title><emphasis>TIP</emphasis> Å sette opp et speil uten synkronisering</title>
				 <para>
					RAID-1-volumer er ofte laget for å bli brukt som en ny disk, ofte betraktet som blank. Det faktiske innledende innholdet på disken er dermed ikke så relevant, siden man trenger bare å vite at dataene som er skrevet etter etableringen av volumet, spesielt filsystemet, kan nås senere.
				</para>
				 <para>
					Man kan derfor lure på om poenget med å synkronisere begge diskene ved tidpunktet for opprettelsen. Hvorfor bry seg om innholdet er identisk på soner i volumet som vi vet kun vil leses etter at vi har skrevet til dem?
				</para>
				 <para>
					Heldigvis kan denne synkroniseringfasen unngås ved å gå forbi <literal>--assume-clean</literal>-valget til <command>mdadm</command>. Imidlertid kan dette alternativet føre til overraskelser i tilfeller hvor de første dataene vil bli lest (for eksempel hvis et filsystem er allerede der på de fysiske diskene), som er grunnen til at den ikke er aktivert som standard.
				</para>
				 </sidebar> <para>
					La oss nå se hva som skjer når en av elementene i RAID-1-tabellen svikter.<command>mdadm</command>, in particular its <literal>--fail</literal>-valget tillater å simulere en slik diskfeiling:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde</userinput>
<computeroutput>mdadm: set /dev/sde faulty in /dev/md1
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
    Update Time : Wed May  6 09:39:39 2015
          State : clean, degraded 
 Active Devices : 1
Working Devices : 1
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 19

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       0        0        2      removed

       1       8       64        -      faulty   /dev/sde</computeroutput></screen>
				 <para>
					Innholdet i volumet er fortsatt tilgjengelig (og, hvis det er montert, legger ikke programmene merke til en ting), men datasikkerheten er ikke trygg lenger: Skulle <filename>sdd</filename> disken i sin tur svikte, vil dataene gå tapt. Vi ønsker å unngå denne risikoen, så vi erstatter den ødelagte disken med et ny en, <filename>sdf</filename>:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>
<computeroutput>mdadm: added /dev/sdf
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
   Raid Devices : 2
  Total Devices : 3
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:48:49 2015
          State : clean, degraded, recovering 
 Active Devices : 1
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 1

 Rebuild Status : 28% complete

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 26

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      spare rebuilding   /dev/sdf

       1       8       64        -      faulty   /dev/sde
# </computeroutput><userinput>[...]</userinput>
<computeroutput>[...]
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
    Update Time : Wed May  6 09:49:08 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 41

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf

       1       8       64        -      faulty   /dev/sde</computeroutput></screen>
				 <para>
					Her igjen, utløser kjernen automatisk en rekonstruksjonsase der volumet, fortsatt tilgjengelig, men i en degradert modus. Når gjenoppbyggingen er over, er RAID-matrisen tilbake i normal tilstand. Man kan da si til systemet at <filename>sde</filename>-disken er i ferd med å bli fjernet fra matrisen, for å ende opp med et klassisk RAID-speil på to disker:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>
<computeroutput>mdadm: hot removed /dev/sde from /dev/md1
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf</computeroutput></screen>
				 <para>
					Fra da av kan driveren fysisk fjernes når tjeneren nærmest er slått av, eller til og med hot-fjernes når maskinvareoppsettet tillater det. Slike konfigurasjoner inkluderer noen SCSI-kontrollere, de fleste SATA disker, og eksterne harddisker som opererer via USB eller Firewire.
				</para>

			</section>
			 <section id="sect.backup-raid-config">
				<title>Sikkerhetskopi av konfigureringen</title>
				 <para>
					De fleste av meta-dataene som gjelder gjelder RAID-volumer lagres direkte på diskene til disse matrisene, slik at kjernen kan oppdage matriser og komponentene deres, og montere dem automatisk når systemet starter opp. Men det oppmuntres til sikkerhetskopiering av denne konfigurasjonen, fordi denne deteksjonen ikke er feilfri, og det er bare å forvente at den vil svikte akkurat under sensitive omstendigheter. I vårt eksempel, hvis en svikt i <filename>sde</filename>-disken hadde vært virkelig (i stedet for simulert) og at systemet har blitt startet på nytt uten å fjerne denne <filename>sde</filename>-disken, kunne denne disken begynne å jobbe igjen etter å ha blitt undersøkt under omstarten. Kjernen vil da ha tre fysiske elementer, som hver utgir seg for å inneholde halvparten av det samme RAID volumet. En annen kilde til forvirring kan komme når RAID-volumer fra to tjenere er konsolidert inn i bare en tjener. Hvis disse matrisene kjørte normalt før diskene ble flyttet, ville kjernen være i stand til å oppdage og montere parene riktig; men hvis de flyttede diskene hadde blitt samlet i en <filename>md1</filename> på den gamle tjeneren, og den nye tjeneren allerede har en <filename>md1</filename>, ville en av speilene få nytt navn.
				</para>
				 <para>
					Å sikkerhetskopiere konfigurasjonen er derfor viktig, om bare som referanse. Den vanlige måten å gjøre det på er å redigere <filename>/etc/mdadm/mdadm.conf</filename>-filen, et eksempel på det er listet her:
				</para>
				 <example id="example.mdadm-conf">
					<title><command>mdadm</command> konfigurasjonsfil</title>
					 
<programlisting># mdadm.conf
#
# Please refer to mdadm.conf(5) for information about this file.
#

# by default (built-in), scan all partitions (/proc/partitions) and all
# containers for MD superblocks. alternatively, specify devices to scan, using
# wildcards if desired.
DEVICE /dev/sd*

# auto-create devices with Debian standard permissions
CREATE owner=root group=disk mode=0660 auto=yes

# automatically tag new arrays as belonging to the local system
HOMEHOST &lt;system&gt;

# instruct the monitoring daemon where to send mail alerts
MAILADDR root

# definitions of existing MD arrays
ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464

# This configuration was auto-generated on Thu, 17 Jan 2013 16:21:01 +0100
# by mkconf 3.2.5-3
</programlisting>

				</example>
				 <para>
					En av de mest nyttige detaljer er <literal>DEVICE</literal>-valget, som viser enhetene som systemet automatisk vil se etter deler av RAID-volumer ved oppstartstidtidspunktet. I vårt eksempel, erstattet vi standardverdien, <literal>partitions containers</literal>,med en eksplisitt liste over enhetsfiler, siden vi valgte å bruke hele disker og ikke bare partisjoner for noen volumer.
				</para>
				 <para>
					De to siste linjene i vårt eksempel er de som tillater kjernen trygt velge hvilke volumnummer som skal tilordnes hvilken matrise. Metadataene som er lagret på selve diskene er nok til å sette volumene sammen igjen, men ikke for å bestemme volumnummeret (og det matchende <filename>/dev/md*</filename>-enhetsnavn).
				</para>
				 <para>
					Heldigvis kan disse lenjene generes automatisk:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>
<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</computeroutput></screen>
				 <para>
					Innholdet i disse to siste linjene ikke er avhengig av listen over disker som inngår i volumet. Det er derfor ikke nødvendig å regenerere disse linjene når du skifter ut en feilet disk med en ny. På den annen side må man sørge for å oppdatere filen når du oppretter eller slette et RAID-oppsett.
				</para>

			</section>

		</section>
		 <section id="sect.lvm">
			<title>LVM</title>
			 <indexterm>
				<primary>LVM</primary>
			</indexterm>
			 <indexterm>
				<primary>Logical Volume Manager</primary>
			</indexterm>
			 <para>
				LVM, <emphasis>Logical Volume Manager</emphasis>, er en annen tilnærming for å abstrahere logiske volumer fra sin fysiske forankring, som fokuserer på økt fleksibilitet i stedet for økt pålitelighet. LVM lar deg endre et logisk volum transparent så langt programmene angår; For eksempel, er det mulig å legge til nye disker, overføre dataene til dem, og fjerne gamle disker, uten at volumet demonteres.
			</para>
			 <section id="sect.lvm-concepts">
				<title>LVM konsepter</title>
				 <para>
					Denne fleksibilitet oppnås med et abstraksjonsnivå som involverer tre konsepter.
				</para>
				 <para>
					Først, PV (<emphasis>Physical Volume</emphasis>) er enheten nærmest maskinvaren: Det kan være partisjoner på en disk, en full disk, eller til og med en annen blokkenhet (inkludert, for eksempel, en RAID-matrise). Merk at når et fysisk element er satt opp til å bli en PV for LVM, skal den kun være tilgjengelige via LVM, ellers vil systemet bli forvirret.
				</para>
				 <para>
					Et antall PV-er kan samles i en VG (<emphasis>Volume Group</emphasis>), som kan sammenlignes med både virtuelle og utvidbare disker. VG er abstrakte, og vises ikke i en enhetsfil i <filename>/dev</filename>-hierarkiet, så det er ingen risk for å bruke dem direkte.
				</para>
				 <para>
					Den tredje typen objekt er LV (<emphasis>Logical Volume</emphasis>), som er en del av en VG; hvis vi holder på VG-as-disk analogien, LV kan sammenlignes med en partisjon. LV-en fremstår som en blokkenhet med en oppføring i <filename>/dev</filename>, og den kan brukes som en hvilken som helst annen fysisk partisjon (som oftest, for å være vert for et filsystem eller et vekselminne).
				</para>
				 <para>
					Det viktige er at splittingen av en VG til LVS-er er helt uavhengig av dens fysiske komponenter (PVS-ene). En VG med bare en enkelt fysisk komponent (en disk for eksempel) kan deles opp i et dusin logiske volumer; På samme måte kan en VG bruke flere fysiske disker og fremstå som et eneste stort logisk volum. Den eneste begrensningen, selvsagt, er at den totale størrelsen allokert til LV-er kan ikke være større enn den totale kapasiteten til PV-ene i volumgruppen.
				</para>
				 <para>
					Det er imildertid ofte fornuftig, å ha noen form for homogenitet blant de fysiske komponentene i en VG, og dele VG-en i logiske volumer som vil ha lignende brukermønstre. For eksempel, hvis tilgjengelig maskinvare inkluderer raske og tregere disker, de raske de kan bli gruppert i en VG og tregere seg i en annen; deler av den første kan deretter bli tildelt til applikasjoner som krever rask tilgang til data, mens den andre kan beholdes til mindre krevende oppgaver.
				</para>
				 <para>
					I alle fall, husk at en LV ikke er spesielt knyttet til en bestemt PV. Det er mulig å påvirke hvor data fra en LV fysisk er lagret, men å bruk denne muligheten på daglig basis er ikke nødvendig. Tvert imot: Ettersom settet med fysiske komponenter i en VG utvikler seg, kan de fysiske lagringsstedene som tilsvarer en bestemt LV, migreres over disker (mens den selvfølgelig blir værende innenfor PV-er tildelt VG-en).
				</para>

			</section>
			 <section id="sect.lvm-setup">
				<title>Å sette opp LVM</title>
				 <para>
					La oss nå følge, trinn for trinn, prosessen med å sette opp LVM for et typisk brukstilfelle: Vi ønsker å forenkle en kompleks lagringssituasjon. En slik situasjon skjer vanligvis etter en lang og innfløkt historie med akkumulerte midlertidige tiltak. For illustrasjonsformål, vil vi vurdere en tjener der lagringsbehovene har endret seg over tid, og endte opp i en labyrint av tilgjengelige partisjoner fordelt over flere delvis brukte disker. Mer konkret er følgende partisjoner tilgjengelige:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							på <filename>sdb</filename>-disken, en <filename>sdb2</filename>-partisjon, 4 GB;
						</para>

					</listitem>
					 <listitem>
						<para>
							på <filename>sdc</filename>-disken, en <filename>sdc3</filename>-partisjon, 3 GB;
						</para>

					</listitem>
					 <listitem>
						<para>
							<filename>sdd</filename>-disken, 4 GB, fullt tilgjengelig;
						</para>

					</listitem>
					 <listitem>
						<para>
							På <filename>sdf</filename>-disken, en <filename>sdf1</filename>-partisjon, 4 GB; og en <filename>sdf2</filename>-partisjon, 5 GB.
						</para>

					</listitem>

				</itemizedlist>
				 <para>
					I tillegg, la oss anta at diskene <filename>sdb</filename> og <filename>sdf</filename> er raskere enn de andre to.
				</para>
				 <para>
					Vårt mål er å sette opp tre logiske volumer for tre ulike programmer: En filtjener som krever 5 GB lagringsplass, en database (1 GB), og noe plass for sikkerhetskopiering (12 GB) De to første trenger god ytelse, men sikkerhetskopiering er mindre kritisk med tanke på tilgangshastighet. Alle disse begrensninger forhinder bruk av partisjoner på egen hånd; Å bruke LVM kan samle den fysiske størrelsen på enhetene, slik at den totale tilgjengelige plassen er den eneste begrensningen.
				</para>
				 <para>
					De verktøy som kreves er i <emphasis role="pkg">lvm2</emphasis>-pakken og det den krever. Når de er installert, skal det det tre trinn til for å sette opp LVM som svarer til de tre konsept-nivåene.
				</para>
				 <para>
					Først forbereder vi de fysiske volumene ved å bruke <command>pvcreate</command>:
				</para>
				 
<screen id="screen.pvcreate"><computeroutput># </computeroutput><userinput>pvdisplay</userinput>
<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2</userinput>
<computeroutput>  Physical volume "/dev/sdb2" successfully created
# </computeroutput><userinput>pvdisplay</userinput>
<computeroutput>  "/dev/sdb2" is a new physical volume of "4.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/sdb2
  VG Name               
  PV Size               4.00 GiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I

# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</userinput>
<computeroutput>  Physical volume "/dev/sdc3" successfully created
  Physical volume "/dev/sdd" successfully created
  Physical volume "/dev/sdf1" successfully created
  Physical volume "/dev/sdf2" successfully created
# </computeroutput><userinput>pvdisplay -C</userinput>
<computeroutput>  PV         VG   Fmt  Attr PSize PFree
  /dev/sdb2       lvm2 ---  4.00g 4.00g
  /dev/sdc3       lvm2 ---  3.09g 3.09g
  /dev/sdd        lvm2 ---  4.00g 4.00g
  /dev/sdf1       lvm2 ---  4.10g 4.10g
  /dev/sdf2       lvm2 ---  5.22g 5.22g
</computeroutput></screen>
				 <para>
					Så langt så bra. Vær oppmerksom på at en PV kan settes opp på en full disk, samt på individuelle partisjoner på den. Som vist ovenfor <command>pvdisplay</command>-kommandoen lister eksisterende PV-er, med to mulige utdata-resultater.
				</para>
				 <para>
					Nå la oss sette sammen disse fysiske elementer til VG-er ved bruke <command>vgcreate</command>. Vi vil samle bare PV-er fra de raske diskene inn i en <filename>vg_critical</filename>-VG. Den andre VG-en, <filename>vg_normal</filename>, vil også inkludere langsommere elementer.
				</para>
				 
<screen id="screen.vgcreate"><computeroutput># </computeroutput><userinput>vgdisplay</userinput>
<computeroutput>  No volume groups found
# </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1</userinput>
<computeroutput>  Volume group "vg_critical" successfully created
# </computeroutput><userinput>vgdisplay</userinput>
<computeroutput>  --- Volume group ---
  VG Name               vg_critical
  System ID             
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               8.09 GiB
  PE Size               4.00 MiB
  Total PE              2071
  Alloc PE / Size       0 / 0   
  Free  PE / Size       2071 / 8.09 GiB
  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp

# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</userinput>
<computeroutput>  Volume group "vg_normal" successfully created
# </computeroutput><userinput>vgdisplay -C</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize  VFree 
  vg_critical   2   0   0 wz--n-  8.09g  8.09g
  vg_normal     3   0   0 wz--n- 12.30g 12.30g
</computeroutput></screen>
				 <para>
					Her igjen, kommandoer er ganske greie (og <command>vgdisplay</command> foreslår to utdataformater). Merk at det er fullt mulig å bruke to partisjoner på samme fysiske disk i to forskjellige VG-er. Merk også at vi brukte en <filename>vg_</filename>-forstavelse til å navngi våre VG-er, men det er ikke noe mer enn en konvensjon.
				</para>
				 <para>
					Vi har nå to "virtuelle disker", med størrelse ca 8 GB og 12 GB, respektivt. La oss nå riste dem opp i "virtuelle partisjoner" (LV-er). Dette innbefatter <command>lvcreate</command>-kommandoen, og en litt mer komplisert syntaks:
				</para>
				 
<screen id="screen.lvcreate"><computeroutput># </computeroutput><userinput>lvdisplay</userinput>
<computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical</userinput>
<computeroutput>  Logical volume "lv_files" created
# </computeroutput><userinput>lvdisplay</userinput>
<computeroutput>  --- Logical volume ---
  LV Path                /dev/vg_critical/lv_files
  LV Name                lv_files
  VG Name                vg_critical
  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT
  LV Write Access        read/write
  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400
  LV Status              available
  # open                 0
  LV Size                5.00 GiB
  Current LE             1280
  Segments               2
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0

# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical</userinput>
<computeroutput>  Logical volume "lv_base" created
# </computeroutput><userinput>lvcreate -n lv_backups -L 12G vg_normal</userinput>
<computeroutput>  Logical volume "lv_backups" created
# </computeroutput><userinput>lvdisplay -C</userinput>
<computeroutput>  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_base    vg_critical -wi-a---  1.00g                                           
  lv_files   vg_critical -wi-a---  5.00g                                           
  lv_backups vg_normal   -wi-a--- 12.00g</computeroutput></screen>
				 <para>
					To parametere er nødvendig når du oppretter logiske volumer; de må sendes til <command>lvcreate</command> som valgmuligheter. Navnet på LV som skal opprettes er angitt med <literal>-n</literal>valget, og størrelsen dens er generelt gitt ved å bruke <literal>-L</literal>-alternativet. Vi trenger også, selvfølgelig, å fortelle kommandoen hvilken VG som skal brukes, derav den siste parameteren på kommandolinjen.
				</para>
				 <sidebar> <title><emphasis>GOING FURTHER</emphasis> <command>lvcreate</command>-valgene</title>
				 <para>
					<command>lvcreate</command>-kommandoen har flere alternativer for å tilpasse hvordan LV-en blir laget.
				</para>
				 <para>
					La oss først beskrive <literal>-l</literal>-valget, der LVs størrelse kan gis som et antall blokker (i motsetning til de "menneskelige" enheter vi brukte ovenfor). Disse blokkene (kalt PES, <emphasis>physical extents</emphasis>, i LVM-termer) er sammenhengende enheter med lagringsplass i PV-er, og de kan ikke deles på tvers over LV-er. Når man ønsker å definere lagringsplass for en LV med noe presisjon, for eksempel å bruke hele den tilgjengelige rommet, vil <literal>-l</literal>-valget trolig bli foretrukket fremfor <literal>-L</literal>.
				</para>
				 <para>
					Det er også mulig å antyde den fysiske plasseringen for en LV, slik at dens omfang lagres på en bestemt PV (mens du selvfølgelig er innenfor den som er tildelt til VG-en). Siden vi vet at <filename>sdb</filename> er raskere enn <filename>sdf</filename>, kan vi ønske å lagre <filename>lv_base</filename> der hvis vi ønsker å gi en fordel til databasetjeneren i forhold til filtjeneren. Kommandolinjen blir: <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command>. Merk at denne kommandoen kan mislykkes hvis PV-en ikke har nok ledig plass. I vårt eksempel, ville vi trolig må lage <filename>lv_base</filename> før <filename>lv_files</filename>for å unngå denne situasjonen - eller frigjøre litt plass på <filename>sdb2</filename> med <command>pvmove</command>-kommandoen.
				</para>
				 </sidebar> <para>
					Logiske volumer, en gang laget, ender opp som blokkenhetsfiler i <filename>/dev/mapper/</filename>:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>ls -l /dev/mapper</userinput>
<computeroutput>total 0
crw------- 1 root root 10, 236 Jun 10 16:52 control
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2
# </computeroutput><userinput>ls -l /dev/dm-*</userinput>
<computeroutput>brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0
brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1
brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2
</computeroutput></screen>
				 <sidebar> <title><emphasis>NOTE</emphasis> Å auto-oppdage LVM volumer</title>
				 <para>
					Ved datamaskin.oppstart kjører <filename>lvm2-activation</filename> systemd tjenesteenhet <command>vgchange -aay</command> for å "aktivisere" volumgrupper. Den skanner de tilgjengelige enhetene; de som har blitt initialisert som fysiske volumer for LVM er registrert i LVMs undersystem, de som tilhører volum-grupper monteres, og de aktuelle logiske volumer er startet og gjort tilgjengelige. Det er derfor ikke nødvendig å redigere konfigurasjonsfiler når du oppretter eller endrer LVM-volumer.
				</para>
				 <para>
					Merk imidlertid at utformingen av LVM-elementer (fysiske og logiske volumer, og volumgrupper) er sikkerhetskopiert i <filename>/etc/lvm/backup</filename>, som kan være nyttig i tilfelle av et problem (eller bare for å snike seg til en titt under panseret).
				</para>
				 </sidebar> <para>
					For å gjøre ting enklere, er praktiske og egnede symbolske lenker også opprettet i kataloger som samsvarer med VG-er:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical</userinput>
<computeroutput>total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0
# </computeroutput><userinput>ls -l /dev/vg_normal</userinput>
<computeroutput>total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</computeroutput></screen>
				 <para>
					LV-er kan deretter brukes akkurat som standard partisjoner:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups</userinput>
<computeroutput>mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 3145728 4k blocks and 786432 inodes
Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d
[...]
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </computeroutput><userinput>mkdir /srv/backups</userinput>
<computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups</userinput>
<computeroutput># </computeroutput><userinput>df -h /srv/backups</userinput>
<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_normal-lv_backups   12G   30M   12G   1% /srv/backups
# </computeroutput><userinput>[...]</userinput>
<computeroutput>[...]
# </computeroutput><userinput>cat /etc/fstab</userinput>
<computeroutput>[...]
/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2
/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2
/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</computeroutput></screen>
				 <para>
					Fra program-synspunkt, har de utallige små partisjonene nå blitt abstrahert til ett stort 12 GB volum, med en vennligere navn.
				</para>

			</section>
			 <section id="sect.lvm-over-time">
				<title>LMV over tid</title>
				 <para>
					Selv om muligheten til å aggregere partisjoner eller fysiske disker er praktisk, er dette ikke den viktigste fordelen LVM har brakt. Den fleksibiliteten den gir, er spesielt lagt merke til over tid, ettersom behovene utvikler seg. I vårt eksempel, la oss anta at nye store filer må lagres, og at LV øremerket til filtjeneren er for liten til å romme dem. Siden vi ikke har brukt hele plassen i <filename>vg_critical</filename>, kan vi vokse <filename>lv_files</filename>. For det formålet bruker vi <command>lvresize</command>-kommandoen, deretter <command>resize2fs</command> for å tilpasse filsystemet tilsvarende:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>
<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files
# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>
<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 5.00g
# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   2   2   0 wz--n- 8.09g 2.09g
# </computeroutput><userinput>lvresize -L 7G vg_critical/lv_files</userinput>
<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).
  Logical volume lv_files successfully resized
# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>
<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 7.00g
# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>
<computeroutput>resize2fs 1.42.12 (29-Aug-2014)
Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.

# </computeroutput><userinput>df -h /srv/files/</userinput>
<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</computeroutput></screen>
				 <sidebar> <title><emphasis>CAUTION</emphasis> Endre størrelse på filsystemer</title>
				 <para>
					Ikke alle filsystemer kan få størrelsen endret fra nettet; Å endre størrelsen på et volum kan derfor kreve at filsystemet demonteres først, og remontering i etterkant. Selvfølgelig, hvis man ønsker å krympe plassen avsatt til en LV, må filsystemet krympes først. Rekkefølgen reverseres når skaleringen går i motsatt retning: Det logiske volumet må utvides før det aktuelle filsystemet. Det er ganske enkelt, fordi filsystemet ikke på noe tidspunkt må være større enn blokkenheten den ligger på (enten enheten er en fysisk partisjon eller et logisk volum).
				</para>
				 <para>
					De ext3-, ext4- og xfs-filsystemer kan vokse på nettet, uten avmontering. Krymping krever avmontering. Reiserfs filsystem tillater online endring av størrelse i begge retninger. Den ærverdige ext2 gjør ingen av delene, og krever alltid demontering.
				</para>
				 </sidebar> <para>
					Vi kunne fortsette på en lignende måte å utvide volumet som er vertskap for databasen, bare til vi har nådd VG-ens grense for tilgjengelig plass:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>
<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_base 1008M  854M  104M  90% /srv/base
# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree 
  vg_critical   2   2   0 wz--n- 8.09g 92.00m</computeroutput></screen>
				 <para>
					Ikke noe problem ettersom LVM tillater å legge til fysiske volumer til eksisterende volumgrupper. For eksempel, kanskje vi har lagt merke til at <filename>sdb1</filename>-partisjonen, som så lagt ble brukt utenfor LVM, bare inneholdt arkiver som kan flyttes til <filename>lv_backups</filename>. Vi kan nå resirkulere den, og integrere den i volumgruppen, og dermed gjenvinne noen ledig plass. Dette er hensikten med <command>vgextend</command>-kommandoen. Selvfølgelig må partisjonen forberedes som et fysisk volum på forhånd. Når VG er utvidet, kan vi bruke lignende kommandoer som tidligere for å utvide det logiske volumet, og deretter filsystem:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>
<computeroutput>  Physical volume "/dev/sdb1" successfully created
# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>
<computeroutput>  Volume group "vg_critical" successfully extended
# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   3   2   0 wz--n- 9.09g 1.09g
# </computeroutput><userinput>[...]</userinput>
<computeroutput>[...]
# </computeroutput><userinput>df -h /srv/base/</userinput>
<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</computeroutput></screen>
				 <sidebar> <title><emphasis>GOING FURTHER</emphasis> Avansert LVM</title>
				 <para>
					LVM åpner også for mer avansert bruk, der mange detaljer kan spesifiseres for hånd. For eksempel kan en administrator justere størrelsen på blokkene som utgjør fysiske og logiske volumer, samt deres fysiske utforminger. Det er også mulig å flytte blokker mellom PV-er, for eksempel for å finjustere ytelsen, eller på en mer triviell måte, å frigjøre en PV når man trenger å trekke ut den tilsvarende fysiske disken fra VG-en (om det skal knytte den til en annen VG eller å fjerne den fra LVM helt). Manualsidene beskriver kommandoene er generelt klare og detaljerte. Et god inngangspunkt er <citerefentry><refentrytitle>lvm</refentrytitle>
					 <manvolnum>8</manvolnum></citerefentry>-manualside.
				</para>
				 </sidebar>
			</section>

		</section>
		 <section id="sect.raid-or-lvm">
			<title>RAID eller LVM?</title>
			 <para>
				RAID og LVM bringer både udiskutable fordeler så snart man forlater det enkle tilfelle med en stasjonær datamaskin med en enkelt harddisk der bruksmønster ikke endres over tid. Men RAID og LVM går i to forskjellige retninger, med divergerende mål, og det er legitimt å lure på hvilken som bør velges. Den mest hensiktsmessige svaret vil selvfølgelig avhenge av nåværende og forutsigbare krav.
			</para>
			 <para>
				Det finnes noen enkle tilfeller hvor spørsmålet ikke egentlig oppstår. Hvis kravet er å sikre data mot maskinvarefeil, så vil åpenbart at RAID bli satt opp med en romslig matrise med disker, ettersom LVM ikke løser dette problemet. Dersom, på den annen side er det behov for en fleksibel lagringsopplegg der volumene lagesw uavhengig av den fysiske utformingen av diskene, bidrar ikke RAID med mye, og LVM vil være det naturlige valget.
			</para>
			 <sidebar> <title><emphasis>NOTE</emphasis> Hvis ytelse betyr noe…</title>
			 <para>
				Hvis input / output hastighet er viktig, spesielt i form av aksesstid, å bruke LVM / eller RAID i en av de mange kombinasjonene kan ha noen innvirkning på ytelser, og dette kan påvirke beslutninger om hvilken som skal velges. Men disse forskjellene i ytelse er veldig små, og vil bare være målbare i noen brukstilfeller. Hvis ytelsen betyr noe, er det størst gevinst ved å bruke ikke-roterende lagringsmedier (<indexterm><primary>SSD</primary></indexterm><emphasis>solid-state drives</emphasis> eller SSDs). Kostnaden deres per megabyte er høyere enn for standard harddisker, kapasiteten deres er vanligvis mindre, men de gir utmerkede resultater for tilfeldige aksesser. Hvis bruksmønster inneholder mange input/output-operasjoner spredt rundt i filsystemet, for eksempel for databaser der komplekse spørringer blir kjørt rutinemessig, så oppveier fordelen av å kjøre dem på en SSD langt hva som kan oppnås ved å velge LVM over RAID eller omvendt. I slike situasjoner bør valget bestemmes av andre hensyn enn ren fart, siden ytelsesaspektet lettest håndteres ved å bruke SSD.
			</para>
			 </sidebar> <para>
				Den tredje bemerkelsesverdige brukstilfellet er når man bare ønsker å samle to disker i ett volum, enten av ytelseshensyn eller for å ha et enkelt filsystem som er større enn noen av de tilgjengelige diskene. Dette tilfellet kan adresseres både med en RAID-0 (eller til og med en lineær-RAID) og med et LVM-volum. Når du er i denne situasjonen, gir sperring ekstra begrensninger (for eksempel å måtte være på linje med resten av datamaskinene hvis de bare bruker RAID), vil konfigurasjonsvalget ofte være LVM. Første oppsett er snaut nok komplekst, og at svak økning i kompleksitet mer enn gjør opp for LVMs ekstra fleksibiliteten dersom kravene endres eller dersom nye disker må legges til.
			</para>
			 <para>
				Så selvfølgelig, er det det virkelig interessante brukereksempel, der lagringssystemet må gjøres både motstandsdyktig mot maskinvarefeil og gi en fleksibel volumtildeling. Verken RAID eller LVM kan imøtekomme begge kravene på egen hånd. Uansett, det er her vi bruker begge samtidig - eller rettere sagt, den ene oppå den andre. Ordningen som har alt, men blitt en standard siden RAID og LVM har nådd modenheten til å sikre datatallighet, først ved å gruppere disker i et lite antall store RAID matriser, og å bruke disse RAID-matrisene som LVM fysiske volumer. logiske partisjoner vil da bli meislet ut fra disse LV-ene for filsystemer. Salgspoenget med dette oppsettet er at når en disk svikter, vil bare et lite antall RAID-matriser trenge rekonstruering, og dermed begrense tiden administrator bruker for gjenoppretting.
			</para>
			 <para>
				La oss ta et konkret eksempel: PR-avdelingen på Falcot Corp trenger en arbeidsstasjon for videoredigering, men avdelingens budsjett tillater ikke investere i dyr maskinvare fra bunnen av. Det er avgjort å favorisere maskinvaren son spesifikk for den grafiske arbeidets art (skjerm og skjermkort), og å fortsette med felles maskinvare for lagring. Men som er viden kjent, har digital video noen spesielle krav til mengden av date for lagring, og gjennomstrømningshastighet for lesing og skriving er viktig for den generelle system-ytelsen (mer enn vanlig aksesstid, for eksempel). Disse begrensningene må være imøtekommet med felles maskinvare, i dette tilfellet med 300 GB SATA-harddisker. Systemdata må også gjøres motstandsdyktige mot maskinvarefeil, og også noen brukerdata. Redigerte videoklipp må faktisk være trygge, men for videoer som venter på redigering er det mindre kritisk, siden de er fortsatt på videobånd eller opptaksutstyret.
			</para>
			 <para>
				RAID-1 og LVM kombineres for å tilfredsstille disse begrensningene. Diskene er knyttet til to forskjellige SATA-kontrollere for å optimalisere parallell tilgang og redusere risikoen for samtidig svikt, og de synes derfor som <filename>sda</filename> og <filename>sdc</filename>. De er partisjonert likt langs det følgende skjemaet:
			</para>
			 
<screen><computeroutput># </computeroutput><userinput>fdisk -l /dev/sda</userinput>
<computeroutput>
Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x00039a9f

Device    Boot     Start       End   Sectors Size Id Type
/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect
/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris
/dev/sda3        4000185 586099395 582099210 298G 5  Extended
/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect
/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect
/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</computeroutput></screen>
			 <itemizedlist>
				<listitem>
					<para>
						De første partisjonene til begge disker (ca 1 GB) er satt sammen til ett RAID-1-volum <filename>md0</filename>. Dette speilet er direkte brukt til å lagre rotfilsystemet.
					</para>

				</listitem>
				 <listitem>
					<para>
						<filename>sda2</filename> og <filename>sdc2</filename>-partisjonene brukes som vekselminne-partisjoner, noe som gir en totalt 2 GB vekselminne. Med 1 GB RAM, har arbeidsstasjonen en komfortabel mengde tilgjengelig minne.
					</para>

				</listitem>
				 <listitem>
					<para>
						<filename>sda5</filename> og <filename>sdc5</filename>-partisjonene, så vel som <filename>sda6</filename> og <filename>sdc6</filename>, er samlet til to nye RAID-1 volumer på rundt 100 GB hver, <filename>md1</filename> og <filename>md2</filename>. Begge disse speilene er internalisert som fysiske volumer for LVM, og knyttet til volumgruppen <filename>vg_raid</filename>. Denne VG-en inneholder derfor et trygt rom på 200 GB.
					</para>

				</listitem>
				 <listitem>
					<para>
						De gjenstående partisjoner, <filename>sda7</filename> og <filename>sdc7</filename>, brukes direkte som fysiske volumer, og knyttet til en annen VG kallt <filename>vg_bulk</filename>, som da ender opp med omtrent 200 GB lagringsplass.
					</para>

				</listitem>

			</itemizedlist>
			 <para>
				Når VG-er er opprettet, kan de fordeles svært fleksibelt. Man må huske på at LV-er opprettet i <filename>vg_raid</filename> blir bevart selv om en av diskene svikter, noe som ikke vil være tilfelle for LV-er opprettet i <filename>vg_bulk</filename>. På den annen side, vil de sistnevnte fordeles i parallell på begge disker, som tillater høyere lese- eller skrive-hastigheter for store filer.
			</para>
			 <para>
				Vi vil derfor lage <filename>lv_usr</filename>, <filename>lv_var</filename> og <filename>lv_home</filename> LVs on <filename>vg_raid</filename> til å være vertskap for de matchende filsystemene. En annen stor LV, <filename>lv_movies</filename>, skal brukes som vert for endelige versjoner av filmer etter redigering. Den andre VG-en vil bli delt inn i et stort <filename>lv_rushes</filename>, for data rett fra det digitale videokameraet, og et <filename>lv_tmp</filename> for midlertidige filer. Plasseringen av arbeidsområdet er et mindre enkelt valg å ta. Mens god ytelse er nødvendig for det volumet, er det verdt å risikere å miste arbeid hvis en disk svikter under redigeringsøkten ? Avhengig av svaret på det spørsmålet, vil den aktuelle LV-en bli lagt til den ene VG-en eller på den andre.
			</para>
			 <para>
				Vi har nå både litt overskudd til viktige data og mye fleksibilitet i hvordan den tilgjengelige plassen er delt på tvers av programmene. Skal ny programvare installeres senere (for å redigere lydklipp, for eksempel), kan LV-vertskapet <filename>/usr/</filename> utvides smertefritt.
			</para>
			 <sidebar> <title><emphasis>NOTE</emphasis> Hvorfor tre RAID-1 volumer?</title>
			 <para>
				Vi kunne ha satt opp ett RAID-1-volum bare for å tjene som et fysisk volum for <filename>vg_raid</filename>. Hvorfor lage tre av dem da?
			</para>
			 <para>
				Grunnen til den første delingen (<filename>md0</filename> opp mot de andre) dreier seg om datasikkerhet. Data skrevet til begge elementer i et RAID-1-speil er nøyaktig de samme, og det er derfor mulig å omgå RAID-laget og montere en av diskene direkte. I tilfelle av, for eksempel en kjernefeil, eller hvis LVM-metadata blir ødelagt, er det fortsatt mulig å starte opp et minimalt system for å få tilgang til viktige data som for eksempel utformingen av diskene i RAID-en og LVM-en. Metadataene kan så rekonstrueres og filene kan igjen nås, slik at systemet kan bringes tilbake til sin nominelle tilstand.
			</para>
			 <para>
				Begrunnelsen for den andre delingen (<filename>md1</filename> mot <filename>md2</filename>) er mindre entydig, og mer knyttet til erkjennelsen av at fremtiden er usikker. Når arbeidsstasjonen er først montert, er de eksakte kravene til oppbevaring ikke nødvendigvis kjent med perfekt presisjon. De kan også utvikle seg over tid. I vårt tilfelle, kan vi ikke på forhånd vite det faktiske lagringsbehovet for video-opptak og komplette videoklipp. Hvis et bestemt klipp har en meget stor mengde uredigerte opptak, og VG-en øremerket til ledige data er mindre enn halvveis full, kan vi gjenbruke noe av den plassen som ikke trenges. Vi kan fjerne en av de fysiske volumene, la oss si <filename>md2</filename>, fra <filename>vg_raid</filename> og enten knytte det til <filename>vg_bulk</filename> direkte (hvis den forventede varigheten av operasjonen er kort nok til at vi kan leve med midlertidig fall i ytelsen), eller sette tilbake RAID-oppsettet på <filename>md2</filename> og integrere komponentene dens, <filename>sda6</filename> og <filename>sdc6</filename>, i den store VG-en (som ekspanderer til 200 GB i stedet for 100 GB). Det logiske volumet <filename>lv_rushes</filename> kan så ekspandere i tråd med det som kreves.
			</para>
			 </sidebar>
		</section>

	</section>
	 <section id="sect.virtualization">
		<title>Visualisering</title>
		 <indexterm>
			<primary>virtualization</primary>
		</indexterm>
		 <para>
			Virtualisering er en av de viktigste fremskritt i de seneste årenes datautvikling. Begrepet omfatter ulike abstraksjoner og teknikker som simulerer virtuelle datamaskiner med varierende grad av uavhengighet på selve maskinvaren. En fysisk tjener kan så være vert for flere systemer som arbeider samtidig og i isolasjon. Bruksområdene er mange, og ofte utledes fra denne isolasjon: For eksempel testmiljøer med varierende konfigurasjoner, eller separasjon av vertsbaserte tjenester mellom ulike virtuelle maskiner for sikkerheten.
		</para>
		 <para>
			There are multiple virtualization solutions, each with its own pros and cons. This book will focus on Xen, LXC, and KVM, but other noteworthy implementations include the following:
		</para>
		 <indexterm>
			<primary><emphasis>VMWare</emphasis></primary>
		</indexterm>
		 <indexterm>
			<primary><emphasis>Bochs</emphasis></primary>
		</indexterm>
		 <indexterm>
			<primary><emphasis>QEMU</emphasis></primary>
		</indexterm>
		 <indexterm>
			<primary><emphasis>VirtualBox</emphasis></primary>
		</indexterm>
		 <indexterm>
			<primary><emphasis>KVM</emphasis></primary>
		</indexterm>
		 <indexterm>
			<primary><emphasis>LXC</emphasis></primary>
		</indexterm>
		 <itemizedlist>
			<listitem>
				<para>
					QEMU is a software emulator for a full computer; performances are far from the speed one could achieve running natively, but this allows running unmodified or experimental operating systems on the emulated hardware. It also allows emulating a different hardware architecture: for instance, an <emphasis>amd64</emphasis> system can emulate an <emphasis>arm</emphasis> computer. QEMU is free software. <ulink type="block" url="http://www.qemu.org/" />
				</para>

			</listitem>
			 <listitem>
				<para>
					Bochs is another free virtual machine, but it only emulates the x86 architectures (i386 and amd64).
				</para>

			</listitem>
			 <listitem>
				<para>
					VMWare is a proprietary virtual machine; being one of the oldest out there, it is also one of the most widely-known. It works on principles similar to QEMU. VMWare proposes advanced features such as snapshotting a running virtual machine. <ulink type="block" url="http://www.vmware.com/" />
				</para>

			</listitem>
			 <listitem>
				<para>
					VirtualBox is a virtual machine that is mostly free software (some extra components are available under a proprietary license). Unfortunately it is in Debian's “contrib” section because it includes some precompiled files that cannot be rebuilt without a proprietary compiler. While younger than VMWare and restricted to the i386 and amd64 architectures, it still includes some snapshotting and other interesting features. <ulink type="block" url="http://www.virtualbox.org/" />
				</para>

			</listitem>

		</itemizedlist>
		 <section id="sect.xen">
			<title>Xen</title>
			 <para>
				Xen <indexterm><primary>Xen</primary></indexterm> is a “paravirtualization” solution. It introduces a thin abstraction layer, called a “hypervisor”, between the hardware and the upper systems; this acts as a referee that controls access to hardware from the virtual machines. However, it only handles a few of the instructions, the rest is directly executed by the hardware on behalf of the systems. The main advantage is that performances are not degraded, and systems run close to native speed; the drawback is that the kernels of the operating systems one wishes to use on a Xen hypervisor need to be adapted to run on Xen.
			</para>
			 <para>
				Let's spend some time on terms. The hypervisor is the lowest layer, that runs directly on the hardware, even below the kernel. This hypervisor can split the rest of the software across several <emphasis>domains</emphasis>, which can be seen as so many virtual machines. One of these domains (the first one that gets started) is known as <emphasis>dom0</emphasis>, and has a special role, since only this domain can control the hypervisor and the execution of other domains. These other domains are known as <emphasis>domU</emphasis>. In other words, and from a user point of view, the <emphasis>dom0</emphasis> matches the “host” of other virtualization systems, while a <emphasis>domU</emphasis> can be seen as a “guest”.
			</para>
			 <sidebar> <title><emphasis>CULTURE</emphasis> Xen and the various versions of Linux</title>
			 <para>
				Xen was initially developed as a set of patches that lived out of the official tree, and not integrated to the Linux kernel. At the same time, several upcoming virtualization systems (including KVM) required some generic virtualization-related functions to facilitate their integration, and the Linux kernel gained this set of functions (known as the <emphasis>paravirt_ops</emphasis> or <emphasis>pv_ops</emphasis> interface). Since the Xen patches were duplicating some of the functionality of this interface, they couldn't be accepted officially.
			</para>
			 <para>
				Xensource, the company behind Xen, therefore had to port Xen to this new framework, so that the Xen patches could be merged into the official Linux kernel. That meant a lot of code rewrite, and although Xensource soon had a working version based on the paravirt_ops interface, the patches were only progressively merged into the official kernel. The merge was completed in Linux 3.0. <ulink type="block" url="http://wiki.xenproject.org/wiki/XenParavirtOps" />
			</para>
			 <para>
				Since <emphasis role="distribution">Jessie</emphasis> is based on version 3.16 of the Linux kernel, the standard <emphasis role="pkg">linux-image-686-pae</emphasis> and <emphasis role="pkg">linux-image-amd64</emphasis> packages include the necessary code, and the distribution-specific patching that was required for <emphasis role="distribution">Squeeze</emphasis> and earlier versions of Debian is no more. <ulink type="block" url="http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix" />
			</para>
			 </sidebar> <sidebar> <title><emphasis>NOTE</emphasis> Architectures compatible with Xen</title>
			 <para>
				Xen is currently only available for the i386, amd64, arm64 and armhf architectures.
			</para>
			 </sidebar> <sidebar> <title><emphasis>CULTURE</emphasis> Xen and non-Linux kernels</title>
			 <para>
				Xen requires modifications to all the operating systems one wants to run on it; not all kernels have the same level of maturity in this regard. Many are fully-functional, both as dom0 and domU: Linux 3.0 and later, NetBSD 4.0 and later, and OpenSolaris. Others only work as a domU. You can check the status of each operating system in the Xen wiki: <ulink type="block" url="http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen" /> <ulink type="block" url="http://wiki.xenproject.org/wiki/DomU_Support_for_Xen" />
			</para>
			 <para>
				However, if Xen can rely on the hardware functions dedicated to virtualization (which are only present in more recent processors), even non-modified operating systems can run as domU (including Windows).
			</para>
			 </sidebar> <para>
				Using Xen under Debian requires three components:
			</para>
			 <itemizedlist>
				<listitem>
					<para>
						The hypervisor itself. According to the available hardware, the appropriate package will be either <emphasis role="pkg">xen-hypervisor-4.4-amd64</emphasis>, <emphasis role="pkg">xen-hypervisor-4.4-armhf</emphasis>, or <emphasis role="pkg">xen-hypervisor-4.4-arm64</emphasis>.
					</para>

				</listitem>
				 <listitem>
					<para>
						A kernel that runs on that hypervisor. Any kernel more recent than 3.0 will do, including the 3.16 version present in <emphasis role="distribution">Jessie</emphasis>.
					</para>

				</listitem>
				 <listitem>
					<para>
						The i386 architecture also requires a standard library with the appropriate patches taking advantage of Xen; this is in the <emphasis role="pkg">libc6-xen</emphasis> package.
					</para>

				</listitem>

			</itemizedlist>
			 <para>
				In order to avoid the hassle of selecting these components by hand, a few convenience packages (such as <emphasis role="pkg">xen-linux-system-amd64</emphasis>) have been made available; they all pull in a known-good combination of the appropriate hypervisor and kernel packages. The hypervisor also brings <emphasis role="pkg">xen-utils-4.4</emphasis>, which contains tools to control the hypervisor from the dom0. This in turn brings the appropriate standard library. During the installation of all that, configuration scripts also create a new entry in the Grub bootloader menu, so as to start the chosen kernel in a Xen dom0. Note however that this entry is not usually set to be the first one in the list, and will therefore not be selected by default. If that is not the desired behavior, the following commands will change it:
			</para>
			 
<screen><computeroutput># </computeroutput><userinput>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen
</userinput><computeroutput># </computeroutput><userinput>update-grub
</userinput></screen>
			 <para>
				Once these prerequisites are installed, the next step is to test the behavior of the dom0 by itself; this involves a reboot to the hypervisor and the Xen kernel. The system should boot in its standard fashion, with a few extra messages on the console during the early initialization steps.
			</para>
			 <para>
				Now is the time to actually install useful systems on the domU systems, using the tools from <emphasis role="pkg">xen-tools</emphasis>. This package provides the <command>xen-create-image</command> command, which largely automates the task. The only mandatory parameter is <literal>--hostname</literal>, giving a name to the domU; other options are important, but they can be stored in the <filename>/etc/xen-tools/xen-tools.conf</filename> configuration file, and their absence from the command line doesn't trigger an error. It is therefore important to either check the contents of this file before creating images, or to use extra parameters in the <command>xen-create-image</command> invocation. Important parameters of note include the following:
			</para>
			 <itemizedlist>
				<listitem>
					<para>
						<literal>--memory</literal>, to specify the amount of RAM dedicated to the newly created system;
					</para>

				</listitem>
				 <listitem>
					<para>
						<literal>--size</literal> and <literal>--swap</literal>, to define the size of the “virtual disks” available to the domU;
					</para>

				</listitem>
				 <listitem>
					<para>
						<literal>--debootstrap</literal>, to cause the new system to be installed with <command>debootstrap</command>; in that case, the <literal>--dist</literal> option will also most often be used (with a distribution name such as <emphasis role="distribution">jessie</emphasis>).
					</para>
					 <sidebar> <title><emphasis>GOING FURTHER</emphasis> Installing a non-Debian system in a domU</title>
					 <para>
						In case of a non-Linux system, care should be taken to define the kernel the domU must use, using the <literal>--kernel</literal> option.
					</para>
					 </sidebar>
				</listitem>
				 <listitem>
					<para>
						<literal>--dhcp</literal> states that the domU's network configuration should be obtained by DHCP while <literal>--ip</literal> allows defining a static IP address.
					</para>

				</listitem>
				 <listitem>
					<para>
						Lastly, a storage method must be chosen for the images to be created (those that will be seen as hard disk drives from the domU). The simplest method, corresponding to the <literal>--dir</literal> option, is to create one file on the dom0 for each device the domU should be provided. For systems using LVM, the alternative is to use the <literal>--lvm</literal> option, followed by the name of a volume group; <command>xen-create-image</command> will then create a new logical volume inside that group, and this logical volume will be made available to the domU as a hard disk drive.
					</para>
					 <sidebar> <title><emphasis>NOTE</emphasis> Storage in the domU</title>
					 <para>
						Entire hard disks can also be exported to the domU, as well as partitions, RAID arrays or pre-existing LVM logical volumes. These operations are not automated by <command>xen-create-image</command>, however, so editing the Xen image's configuration file is in order after its initial creation with <command>xen-create-image</command>.
					</para>
					 </sidebar>
				</listitem>

			</itemizedlist>
			 <para>
				Once these choices are made, we can create the image for our future Xen domU:
			</para>
			 
<screen><computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</userinput>
<computeroutput>
[...]
General Information
--------------------
Hostname       :  testxen
Distribution   :  jessie
Mirror         :  http://ftp.debian.org/debian/
Partitions     :  swap            128Mb (swap)
                  /               2G    (ext3)
Image type     :  sparse
Memory size    :  128Mb
Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64
Initrd path    :  /boot/initrd.img-3.16.0-4-amd64
[...]
Logfile produced at:
         /var/log/xen-tools/testxen.log

Installation Summary
---------------------
Hostname        :  testxen
Distribution    :  jessie
MAC Address     :  00:16:3E:8E:67:5C
IP-Address(es)  :  dynamic
RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b
Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ
</computeroutput></screen>
			 <para>
				We now have a virtual machine, but it is currently not running (and therefore only using space on the dom0's hard disk). Of course, we can create more images, possibly with different parameters.
			</para>
			 <para>
				Before turning these virtual machines on, we need to define how they'll be accessed. They can of course be considered as isolated machines, only accessed through their system console, but this rarely matches the usage pattern. Most of the time, a domU will be considered as a remote server, and accessed only through a network. However, it would be quite inconvenient to add a network card for each domU; which is why Xen allows creating virtual interfaces, that each domain can see and use in a standard way. Note that these cards, even though they're virtual, will only be useful once connected to a network, even a virtual one. Xen has several network models for that:
			</para>
			 <itemizedlist>
				<listitem>
					<para>
						The simplest model is the <emphasis>bridge</emphasis> model; all the eth0 network cards (both in the dom0 and the domU systems) behave as if they were directly plugged into an Ethernet switch.
					</para>

				</listitem>
				 <listitem>
					<para>
						Then comes the <emphasis>routing</emphasis> model, where the dom0 behaves as a router that stands between the domU systems and the (physical) external network.
					</para>

				</listitem>
				 <listitem>
					<para>
						Finally, in the <emphasis>NAT</emphasis> model, the dom0 is again between the domU systems and the rest of the network, but the domU systems are not directly accessible from outside, and traffic goes through some network address translation on the dom0.
					</para>

				</listitem>

			</itemizedlist>
			 <para>
				These three networking nodes involve a number of interfaces with unusual names, such as <filename>vif*</filename>, <filename>veth*</filename>, <filename>peth*</filename> and <filename>xenbr0</filename>. The Xen hypervisor arranges them in whichever layout has been defined, under the control of the user-space tools. Since the NAT and routing models are only adapted to particular cases, we will only address the bridging model.
			</para>
			 <para>
				The standard configuration of the Xen packages does not change the system-wide network configuration. However, the <command>xend</command> daemon is configured to integrate virtual network interfaces into any pre-existing network bridge (with <filename>xenbr0</filename> taking precedence if several such bridges exist). We must therefore set up a bridge in <filename>/etc/network/interfaces</filename> (which requires installing the <emphasis role="pkg">bridge-utils</emphasis> package, which is why the <emphasis role="pkg">xen-utils-4.4</emphasis> package recommends it) to replace the existing eth0 entry:
			</para>
			 
<programlisting>auto xenbr0
iface xenbr0 inet dhcp
    bridge_ports eth0
    bridge_maxwait 0
</programlisting>
			 <para>
				After rebooting to make sure the bridge is automatically created, we can now start the domU with the Xen control tools, in particular the <command>xl</command> command. This command allows different manipulations on the domains, including listing them and, starting/stopping them.
			</para>
			 
<screen><computeroutput># </computeroutput><userinput>xl list</userinput>
<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   463     1     r-----      9.8
# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>
<computeroutput>Parsing config from /etc/xen/testxen.cfg
# </computeroutput><userinput>xl list</userinput>
<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   366     1     r-----     11.4
testxen                                      1   128     1     -b----      1.1</computeroutput></screen>
			 <sidebar> <title><emphasis>TOOL</emphasis> Choice of toolstacks to manage Xen VM</title>
			 <indexterm>
				<primary><command>xm</command></primary>
			</indexterm>
			 <indexterm>
				<primary><command>xe</command></primary>
			</indexterm>
			 <para>
				In Debian 7 and older releases, <command>xm</command> was the reference command line tool to use to manage Xen virtual machines. It has now been replaced by <command>xl</command> which is mostly backwards compatible. But those are not the only available tools: <command>virsh</command> of libvirt and <command>xe</command> of XenServer's XAPI (commercial offering of Xen) are alternative tools.
			</para>
			 </sidebar> <sidebar> <title><emphasis>CAUTION</emphasis> Only one domU per image!</title>
			 <para>
				While it is of course possible to have several domU systems running in parallel, they will all need to use their own image, since each domU is made to believe it runs on its own hardware (apart from the small slice of the kernel that talks to the hypervisor). In particular, it isn't possible for two domU systems running simultaneously to share storage space. If the domU systems are not run at the same time, it is however quite possible to reuse a single swap partition, or the partition hosting the <filename>/home</filename> filesystem.
			</para>
			 </sidebar> <para>
				Note that the <filename>testxen</filename> domU uses real memory taken from the RAM that would otherwise be available to the dom0, not simulated memory. Care should therefore be taken, when building a server meant to host Xen instances, to provision the physical RAM accordingly.
			</para>
			 <para>
				Voilà! Our virtual machine is starting up. We can access it in one of two modes. The usual way is to connect to it “remotely” through the network, as we would connect to a real machine; this will usually require setting up either a DHCP server or some DNS configuration. The other way, which may be the only way if the network configuration was incorrect, is to use the <filename>hvc0</filename> console, with the <command>xl console</command> command:
			</para>
			 
<screen><computeroutput># </computeroutput><userinput>xl console testxen</userinput>
<computeroutput>[...]

Debian GNU/Linux 8 testxen hvc0

testxen login: </computeroutput></screen>
			 <para>
				One can then open a session, just like one would do if sitting at the virtual machine's keyboard. Detaching from this console is achieved through the <keycombo action="simul"><keycap>Control</keycap> <keycap>]</keycap></keycombo> key combination.
			</para>
			 <sidebar> <title><emphasis>TIP</emphasis> Getting the console straight away</title>
			 <para>
				Sometimes one wishes to start a domU system and get to its console straight away; this is why the <command>xl create</command> command takes a <literal>-c</literal> switch. Starting a domU with this switch will display all the messages as the system boots.
			</para>
			 </sidebar> <sidebar> <title><emphasis>TOOL</emphasis> OpenXenManager</title>
			 <para>
				OpenXenManager (in the <emphasis role="pkg">openxenmanager</emphasis> package) is a graphical interface allowing remote management of Xen domains via Xen's API. It can thus control Xen domains remotely. It provides most of the features of the <command>xl</command> command.
			</para>
			 </sidebar> <para>
				Once the domU is up, it can be used just like any other server (since it is a GNU/Linux system after all). However, its virtual machine status allows some extra features. For instance, a domU can be temporarily paused then resumed, with the <command>xl pause</command> and <command>xl unpause</command> commands. Note that even though a paused domU does not use any processor power, its allocated memory is still in use. It may be interesting to consider the <command>xl save</command> and <command>xl restore</command> commands: saving a domU frees the resources that were previously used by this domU, including RAM. When restored (or unpaused, for that matter), a domU doesn't even notice anything beyond the passage of time. If a domU was running when the dom0 is shut down, the packaged scripts automatically save the domU, and restore it on the next boot. This will of course involve the standard inconvenience incurred when hibernating a laptop computer, for instance; in particular, if the domU is suspended for too long, network connections may expire. Note also that Xen is so far incompatible with a large part of ACPI power management, which precludes suspending the host (dom0) system.
			</para>
			 <sidebar> <title><emphasis>DOCUMENTATION</emphasis> <command>xl</command> options</title>
			 <para>
				Most of the <command>xl</command> subcommands expect one or more arguments, often a domU name. These arguments are well described in the <citerefentry><refentrytitle>xl</refentrytitle>
				 <manvolnum>1</manvolnum></citerefentry> manual page.
			</para>
			 </sidebar> <para>
				Halting or rebooting a domU can be done either from within the domU (with the <command>shutdown</command> command) or from the dom0, with <command>xl shutdown</command> or <command>xl reboot</command>.
			</para>
			 <sidebar> <title><emphasis>GOING FURTHER</emphasis> Advanced Xen</title>
			 <para>
				Xen has many more features than we can describe in these few paragraphs. In particular, the system is very dynamic, and many parameters for one domain (such as the amount of allocated memory, the visible hard drives, the behavior of the task scheduler, and so on) can be adjusted even when that domain is running. A domU can even be migrated across servers without being shut down, and without losing its network connections! For all these advanced aspects, the primary source of information is the official Xen documentation. <ulink type="block" url="http://www.xen.org/support/documentation.html" />
			</para>
			 </sidebar>
		</section>
		 <section id="sect.lxc">
			<title>LXC</title>
			 <indexterm>
				<primary>LXC</primary>
			</indexterm>
			 <para>
				Even though it is used to build “virtual machines”, LXC is not, strictly speaking, a virtualization system, but a system to isolate groups of processes from each other even though they all run on the same host. It takes advantage of a set of recent evolutions in the Linux kernel, collectively known as <emphasis>control groups</emphasis>, by which different sets of processes called “groups” have different views of certain aspects of the overall system. Most notable among these aspects are the process identifiers, the network configuration, and the mount points. Such a group of isolated processes will not have any access to the other processes in the system, and its accesses to the filesystem can be restricted to a specific subset. It can also have its own network interface and routing table, and it may be configured to only see a subset of the available devices present on the system.
			</para>
			 <para>
				These features can be combined to isolate a whole process family starting from the <command>init</command> process, and the resulting set looks very much like a virtual machine. The official name for such a setup is a “container” (hence the LXC moniker: <emphasis>LinuX Containers</emphasis>), but a rather important difference with “real” virtual machines such as provided by Xen or KVM is that there's no second kernel; the container uses the very same kernel as the host system. This has both pros and cons: advantages include excellent performance due to the total lack of overhead, and the fact that the kernel has a global vision of all the processes running on the system, so the scheduling can be more efficient than it would be if two independent kernels were to schedule different task sets. Chief among the inconveniences is the impossibility to run a different kernel in a container (whether a different Linux version or a different operating system altogether).
			</para>
			 <sidebar> <title><emphasis>NOTE</emphasis> LXC isolation limits</title>
			 <para>
				LXC containers do not provide the level of isolation achieved by heavier emulators or virtualizers. In particular:
			</para>
			 <itemizedlist>
				<listitem>
					<para>
						since the kernel is shared among the host system and the containers, processes constrained to containers can still access the kernel messages, which can lead to information leaks if messages are emitted by a container;
					</para>

				</listitem>
				 <listitem>
					<para>
						for similar reasons, if a container is compromised and a kernel vulnerability is exploited, the other containers may be affected too;
					</para>

				</listitem>
				 <listitem>
					<para>
						on the filesystem, the kernel checks permissions according to the numerical identifiers for users and groups; these identifiers may designate different users and groups depending on the container, which should be kept in mind if writable parts of the filesystem are shared among containers.
					</para>

				</listitem>

			</itemizedlist>
			 </sidebar> <para>
				Since we are dealing with isolation and not plain virtualization, setting up LXC containers is more complex than just running debian-installer on a virtual machine. We will describe a few prerequisites, then go on to the network configuration; we will then be able to actually create the system to be run in the container.
			</para>
			 <section>
				<title>Preliminary Steps</title>
				 <para>
					The <emphasis role="pkg">lxc</emphasis> package contains the tools required to run LXC, and must therefore be installed.
				</para>
				 <para>
					LXC also requires the <emphasis>control groups</emphasis> configuration system, which is a virtual filesystem to be mounted on <filename>/sys/fs/cgroup</filename>. Since Debian 8 switched to systemd, which also relies on control groups, this is now done automatically at boot time without further configuration.
				</para>

			</section>
			 <section id="sect.lxc.network">
				<title>Network Configuration</title>
				 <para>
					The goal of installing LXC is to set up virtual machines; while we could of course keep them isolated from the network, and only communicate with them via the filesystem, most use cases involve giving at least minimal network access to the containers. In the typical case, each container will get a virtual network interface, connected to the real network through a bridge. This virtual interface can be plugged either directly onto the host's physical network interface (in which case the container is directly on the network), or onto another virtual interface defined on the host (and the host can then filter or route traffic). In both cases, the <emphasis role="pkg">bridge-utils</emphasis> package will be required.
				</para>
				 <para>
					The simple case is just a matter of editing <filename>/etc/network/interfaces</filename>, moving the configuration for the physical interface (for instance <literal>eth0</literal>) to a bridge interface (usually <literal>br0</literal>), and configuring the link between them. For instance, if the network interface configuration file initially contains entries such as the following:
				</para>
				 
<programlisting>auto eth0
iface eth0 inet dhcp</programlisting>
				 <para>
					They should be disabled and replaced with the following:
				</para>
				 
<programlisting>#auto eth0
#iface eth0 inet dhcp

auto br0
iface br0 inet dhcp
  bridge-ports eth0</programlisting>
				 <para>
					The effect of this configuration will be similar to what would be obtained if the containers were machines plugged into the same physical network as the host. The “bridge” configuration manages the transit of Ethernet frames between all the bridged interfaces, which includes the physical <literal>eth0</literal> as well as the interfaces defined for the containers.
				</para>
				 <para>
					In cases where this configuration cannot be used (for instance if no public IP addresses can be assigned to the containers), a virtual <emphasis>tap</emphasis> interface will be created and connected to the bridge. The equivalent network topology then becomes that of a host with a second network card plugged into a separate switch, with the containers also plugged into that switch. The host must then act as a gateway for the containers if they are meant to communicate with the outside world.
				</para>
				 <para>
					In addition to <emphasis role="pkg">bridge-utils</emphasis>, this “rich” configuration requires the <emphasis role="pkg">vde2</emphasis> package; the <filename>/etc/network/interfaces</filename> file then becomes:
				</para>
				 
<programlisting># Interface eth0 is unchanged
auto eth0
iface eth0 inet dhcp

# Virtual interface 
auto tap0
iface tap0 inet manual
  vde2-switch -t tap0

# Bridge for containers
auto br0
iface br0 inet static
  bridge-ports tap0
  address 10.0.0.1
  netmask 255.255.255.0
</programlisting>
				 <para>
					The network can then be set up either statically in the containers, or dynamically with DHCP server running on the host. Such a DHCP server will need to be configured to answer queries on the <literal>br0</literal> interface.
				</para>

			</section>
			 <section>
				<title>Setting Up the System</title>
				 <para>
					Let us now set up the filesystem to be used by the container. Since this “virtual machine” will not run directly on the hardware, some tweaks are required when compared to a standard filesystem, especially as far as the kernel, devices and consoles are concerned. Fortunately, the <emphasis role="pkg">lxc</emphasis> includes scripts that mostly automate this configuration. For instance, the following commands (which require the <emphasis role="pkg">debootstrap</emphasis> and <emphasis role="pkg">rsync</emphasis> packages) will install a Debian container:
				</para>
				 
<screen><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-create -n testlxc -t debian
</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap
Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... 
Downloading debian minimal ...
I: Retrieving Release 
I: Retrieving Release.gpg 
[...]
Download complete.
Copying rootfs to /var/lib/lxc/testlxc/rootfs...
[...]
Root password is 'sSiKhMzI', please change !
root@mirwiz:~# </computeroutput>
</screen>
				 <para>
					Note that the filesystem is initially created in <filename>/var/cache/lxc</filename>, then moved to its destination directory. This allows creating identical containers much more quickly, since only copying is then required.
				</para>
				 <para>
					Note that the debian template creation script accepts an <option>--arch</option> option to specify the architecture of the system to be installed and a <option>--release</option> option if you want to install something else than the current stable release of Debian. You can also set the <literal>MIRROR</literal> environment variable to point to a local Debian mirror.
				</para>
				 <para>
					The newly-created filesystem now contains a minimal Debian system, and by default the container has no network interface (besides the loopback one). Since this is not really wanted, we will edit the container's configuration file (<filename>/var/lib/lxc/testlxc/config</filename>) and add a few <literal>lxc.network.*</literal> entries:
				</para>
				 
<programlisting>lxc.network.type = veth
lxc.network.flags = up
lxc.network.link = br0
lxc.network.hwaddr = 4a:49:43:49:79:20
</programlisting>
				 <para>
					These entries mean, respectively, that a virtual interface will be created in the container; that it will automatically be brought up when said container is started; that it will automatically be connected to the <literal>br0</literal> bridge on the host; and that its MAC address will be as specified. Should this last entry be missing or disabled, a random MAC address will be generated.
				</para>
				 <para>
					Another useful entry in that file is the setting of the hostname:
				</para>
				 
<programlisting>lxc.utsname = testlxc
</programlisting>

			</section>
			 <section>
				<title>Starting the Container</title>
				 <para>
					Now that our virtual machine image is ready, let's start the container:
				</para>
				 
<screen role="scale" width="94"><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-start --daemon --name=testlxc
</userinput><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-console -n testlxc
</userinput><computeroutput>Debian GNU/Linux 8 testlxc tty1

testlxc login: </computeroutput><userinput>root</userinput><computeroutput>
Password: 
Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
root@testlxc:~# </computeroutput><userinput>ps auxwf</userinput>
<computeroutput>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init
root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald
root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D
root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux
root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux
root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux
root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     
root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \_ -bash
root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \_ ps auxfw
root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102
root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e
root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux
root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux
root@testlxc:~# </computeroutput></screen>
				 <para>
					We are now in the container; our access to the processes is restricted to only those started from the container itself, and our access to the filesystem is similarly restricted to the dedicated subset of the full filesystem (<filename>/var/lib/lxc/testlxc/rootfs</filename>). We can exit the console with <keycombo action="simul"><keycap>Control</keycap> <keycap>a</keycap></keycombo> <keycombo><keycap>q</keycap></keycombo>.
				</para>
				 <para>
					Note that we ran the container as a background process, thanks to the <option>--daemon</option> option of <command>lxc-start</command>. We can interrupt the container with a command such as <command>lxc-stop --name=testlxc</command>.
				</para>
				 <para>
					The <emphasis role="pkg">lxc</emphasis> package contains an initialization script that can automatically start one or several containers when the host boots (it relies on <command>lxc-autostart</command> which starts containers whose <literal>lxc.start.auto</literal> option is set to 1). Finer-grained control of the startup order is possible with <literal>lxc.start.order</literal> and <literal>lxc.group</literal>: by default, the initialization script first starts containers which are part of the <literal>onboot</literal> group and then the containers which are not part of any group. In both cases, the order within a group is defined by the <literal>lxc.start.order</literal> option.
				</para>
				 <sidebar> <title><emphasis>GOING FURTHER</emphasis> Mass virtualization</title>
				 <para>
					Since LXC is a very lightweight isolation system, it can be particularly adapted to massive hosting of virtual servers. The network configuration will probably be a bit more advanced than what we described above, but the “rich” configuration using <literal>tap</literal> and <literal>veth</literal> interfaces should be enough in many cases.
				</para>
				 <para>
					It may also make sense to share part of the filesystem, such as the <filename>/usr</filename> and <filename>/lib</filename> subtrees, so as to avoid duplicating the software that may need to be common to several containers. This will usually be achieved with <literal>lxc.mount.entry</literal> entries in the containers configuration file. An interesting side-effect is that the processes will then use less physical memory, since the kernel is able to detect that the programs are shared. The marginal cost of one extra container can then be reduced to the disk space dedicated to its specific data, and a few extra processes that the kernel must schedule and manage.
				</para>
				 <para>
					We haven't described all the available options, of course; more comprehensive information can be obtained from the <citerefentry> <refentrytitle>lxc</refentrytitle>
					 <manvolnum>7</manvolnum> </citerefentry> and <citerefentry> <refentrytitle>lxc.container.conf</refentrytitle>
					 <manvolnum>5</manvolnum></citerefentry> manual pages and the ones they reference.
				</para>
				 </sidebar>
			</section>

		</section>
		 <section>
			<title>Virtualization with KVM</title>
			 <indexterm>
				<primary>KVM</primary>
			</indexterm>
			 <para>
				KVM, which stands for <emphasis>Kernel-based Virtual Machine</emphasis>, is first and foremost a kernel module providing most of the infrastructure that can be used by a virtualizer, but it is not a virtualizer by itself. Actual control for the virtualization is handled by a QEMU-based application. Don't worry if this section mentions <command>qemu-*</command> commands: it is still about KVM.
			</para>
			 <para>
				Unlike other virtualization systems, KVM was merged into the Linux kernel right from the start. Its developers chose to take advantage of the processor instruction sets dedicated to virtualization (Intel-VT and AMD-V), which keeps KVM lightweight, elegant and not resource-hungry. The counterpart, of course, is that KVM doesn't work on any computer but only on those with appropriate processors. For x86-based computers, you can verify that you have such a processor by looking for “vmx” or “svm” in the CPU flags listed in <filename>/proc/cpuinfo</filename>.
			</para>
			 <para>
				With Red Hat actively supporting its development, KVM has more or less become the reference for Linux virtualization.
			</para>
			 <section>
				<title>Preliminary Steps</title>
				 <indexterm>
					<primary><command>virt-install</command></primary>
				</indexterm>
				 <para>
					Unlike such tools as VirtualBox, KVM itself doesn't include any user-interface for creating and managing virtual machines. The <emphasis role="pkg">qemu-kvm</emphasis> package only provides an executable able to start a virtual machine, as well as an initialization script that loads the appropriate kernel modules.
				</para>
				 <indexterm>
					<primary>libvirt</primary>
				</indexterm>
				 <indexterm>
					<primary><emphasis role="pkg">virt-manager</emphasis></primary>
				</indexterm>
				 <para>
					Fortunately, Red Hat also provides another set of tools to address that problem, by developing the <emphasis>libvirt</emphasis> library and the associated <emphasis>virtual machine manager</emphasis> tools. libvirt allows managing virtual machines in a uniform way, independently of the virtualization system involved behind the scenes (it currently supports QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare and UML). <command>virtual-manager</command> is a graphical interface that uses libvirt to create and manage virtual machines.
				</para>
				 <indexterm>
					<primary><emphasis role="pkg">virtinst</emphasis></primary>
				</indexterm>
				 <para>
					We first install the required packages, with <command>apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</command>. <emphasis role="pkg">libvirt-bin</emphasis> provides the <command>libvirtd</command> daemon, which allows (potentially remote) management of the virtual machines running of the host, and starts the required VMs when the host boots. In addition, this package provides the <command>virsh</command> command-line tool, which allows controlling the <command>libvirtd</command>-managed machines.
				</para>
				 <para>
					The <emphasis role="pkg">virtinst</emphasis> package provides <command>virt-install</command>, which allows creating virtual machines from the command line. Finally, <emphasis role="pkg">virt-viewer</emphasis> allows accessing a VM's graphical console.
				</para>

			</section>
			 <section>
				<title>Network Configuration</title>
				 <para>
					Just as in Xen and LXC, the most frequent network configuration involves a bridge grouping the network interfaces of the virtual machines (see <xref linkend="sect.lxc.network" />).
				</para>
				 <para>
					Alternatively, and in the default configuration provided by KVM, the virtual machine is assigned a private address (in the 192.168.122.0/24 range), and NAT is set up so that the VM can access the outside network.
				</para>
				 <para>
					The rest of this section assumes that the host has an <literal>eth0</literal> physical interface and a <literal>br0</literal> bridge, and that the former is connected to the latter.
				</para>

			</section>
			 <section>
				<title>Installation with <command>virt-install</command></title>
				 <indexterm>
					<primary><command>virt-install</command></primary>
				</indexterm>
				 <para>
					Creating a virtual machine is very similar to installing a normal system, except that the virtual machine's characteristics are described in a seemingly endless command line.
				</para>
				 <para>
					Practically speaking, this means we will use the Debian installer, by booting the virtual machine on a virtual DVD-ROM drive that maps to a Debian DVD image stored on the host system. The VM will export its graphical console over the VNC protocol (see <xref linkend="sect.remote-desktops" /> for details), which will allow us to control the installation process.
				</para>
				 <para>
					We first need to tell libvirtd where to store the disk images, unless the default location (<filename>/var/lib/libvirt/images/</filename>) is fine.
				</para>
				 
<screen><computeroutput>root@mirwiz:~# </computeroutput><userinput>mkdir /srv/kvm</userinput>
<computeroutput>root@mirwiz:~# </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>
<computeroutput>Pool srv-kvm created

root@mirwiz:~# </computeroutput></screen>
				 <sidebar> <title><emphasis>TIP</emphasis> Add your user to the libvirt group</title>
				 <para>
					All samples in this section assume that you are running commands as root. Effectively, if you want to control a local libvirt daemon, you need either to be root or to be a member of the <literal>libvirt</literal> group (which is not the case by default). Thus if you want to avoid using root rights too often, you can add yoursel to the <literal>libvirt</literal> group and run the various commands under your user identity.
				</para>
				 </sidebar> <para>
					Let us now start the installation process for the virtual machine, and have a closer look at <command>virt-install</command>'s most important options. This command registers the virtual machine and its parameters in libvirtd, then starts it so that its installation can proceed.
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id="virtinst.connect"></co>
               --virt-type kvm           <co id="virtinst.type"></co>
               --name testkvm            <co id="virtinst.name"></co>
               --ram 1024                <co id="virtinst.ram"></co>
               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <co id="virtinst.disk"></co>
               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <co id="virtinst.cdrom"></co>
               --network bridge=br0      <co id="virtinst.network"></co>
               --vnc                     <co id="virtinst.vnc"></co>
               --os-type linux           <co id="virtinst.os"></co>
               --os-variant debianwheezy
</userinput><computeroutput>
Starting install...
Allocating 'testkvm.qcow'             |  10 GB     00:00
Creating domain...                    |    0 B     00:00
Guest installation complete... restarting guest.
</computeroutput></screen>
				 <calloutlist>
					<callout arearefs="virtinst.connect">
						<para>
							The <literal>--connect</literal> option specifies the “hypervisor” to use. Its form is that of an URL containing a virtualization system (<literal>xen://</literal>, <literal>qemu://</literal>, <literal>lxc://</literal>, <literal>openvz://</literal>, <literal>vbox://</literal>, and so on) and the machine that should host the VM (this can be left empty in the case of the local host). In addition to that, and in the QEMU/KVM case, each user can manage virtual machines working with restricted permissions, and the URL path allows differentiating “system” machines (<literal>/system</literal>) from others (<literal>/session</literal>).
						</para>

					</callout>
					 <callout arearefs="virtinst.type">
						<para>
							Since KVM is managed the same way as QEMU, the <literal>--virt-type kvm</literal> allows specifying the use of KVM even though the URL looks like QEMU.
						</para>

					</callout>
					 <callout arearefs="virtinst.name">
						<para>
							The <literal>--name</literal> option defines a (unique) name for the virtual machine.
						</para>

					</callout>
					 <callout arearefs="virtinst.ram">
						<para>
							The <literal>--ram</literal> option allows specifying the amount of RAM (in MB) to allocate for the virtual machine.
						</para>

					</callout>
					 <callout arearefs="virtinst.disk">
						<para>
							The <literal>--disk</literal> specifies the location of the image file that is to represent our virtual machine's hard disk; that file is created, unless present, with a size (in GB) specified by the <literal>size</literal> parameter. The <literal>format</literal> parameter allows choosing among several ways of storing the image file. The default format (<literal>raw</literal>) is a single file exactly matching the disk's size and contents. We picked a more advanced format here, that is specific to QEMU and allows starting with a small file that only grows when the virtual machine starts actually using space.
						</para>

					</callout>
					 <callout arearefs="virtinst.cdrom">
						<para>
							The <literal>--cdrom</literal> option is used to indicate where to find the optical disk to use for installation. The path can be either a local path for an ISO file, an URL where the file can be obtained, or the device file of a physical CD-ROM drive (i.e. <literal>/dev/cdrom</literal>).
						</para>

					</callout>
					 <callout arearefs="virtinst.network">
						<para>
							The <literal>--network</literal> specifies how the virtual network card integrates in the host's network configuration. The default behavior (which we explicitly forced in our example) is to integrate it into any pre-existing network bridge. If no such bridge exists, the virtual machine will only reach the physical network through NAT, so it gets an address in a private subnet range (192.168.122.0/24).
						</para>

					</callout>
					 <callout arearefs="virtinst.vnc">
						<para>
							<literal>--vnc</literal> states that the graphical console should be made available using VNC. The default behavior for the associated VNC server is to only listen on the local interface; if the VNC client is to be run on a different host, establishing the connection will require setting up an SSH tunnel (see <xref linkend="sect.ssh-port-forwarding" />). Alternatively, the <literal>--vnclisten=0.0.0.0</literal> can be used so that the VNC server is accessible from all interfaces; note that if you do that, you really should design your firewall accordingly.
						</para>

					</callout>
					 <callout arearefs="virtinst.os">
						<para>
							The <literal>--os-type</literal> and <literal>--os-variant</literal> options allow optimizing a few parameters of the virtual machine, based on some of the known features of the operating system mentioned there.
						</para>

					</callout>

				</calloutlist>
				 <para>
					At this point, the virtual machine is running, and we need to connect to the graphical console to proceed with the installation process. If the previous operation was run from a graphical desktop environment, this connection should be automatically started. If not, or if we operate remotely, <command>virt-viewer</command> can be run from any graphical environment to open the graphical console (note that the root password of the remote host is asked twice because the operation requires 2 SSH connections):
				</para>
				 
<screen><computeroutput>$ </computeroutput><userinput>virt-viewer --connect qemu+ssh://root@<replaceable>server</replaceable>/system testkvm
</userinput><computeroutput>root@server's password: 
root@server's password: </computeroutput></screen>
				 <para>
					When the installation process ends, the virtual machine is restarted, now ready for use.
				</para>

			</section>
			 <section>
				<title>Managing Machines with <command>virsh</command></title>
				 <indexterm>
					<primary><command>virsh</command></primary>
				</indexterm>
				 <para>
					Now that the installation is done, let us see how to handle the available virtual machines. The first thing to try is to ask <command>libvirtd</command> for the list of the virtual machines it manages:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all
 Id Name                 State
----------------------------------
  - testkvm              shut off
</userinput></screen>
				 <para>
					Let's start our test virtual machine:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>virsh -c qemu:///system start testkvm
</userinput><computeroutput>Domain testkvm started</computeroutput></screen>
				 <para>
					We can now get the connection instructions for the graphical console (the returned VNC display can be given as parameter to <command>vncviewer</command>):
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm
</userinput><computeroutput>:0</computeroutput></screen>
				 <para>
					Other available <command>virsh</command> subcommands include:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							<literal>reboot</literal> to restart a virtual machine;
						</para>

					</listitem>
					 <listitem>
						<para>
							<literal>shutdown</literal> to trigger a clean shutdown;
						</para>

					</listitem>
					 <listitem>
						<para>
							<literal>destroy</literal>, to stop it brutally;
						</para>

					</listitem>
					 <listitem>
						<para>
							<literal>suspend</literal> to pause it;
						</para>

					</listitem>
					 <listitem>
						<para>
							<literal>resume</literal> to unpause it;
						</para>

					</listitem>
					 <listitem>
						<para>
							<literal>autostart</literal> to enable (or disable, with the <literal>--disable</literal> option) starting the virtual machine automatically when the host starts;
						</para>

					</listitem>
					 <listitem>
						<para>
							<literal>undefine</literal> to remove all traces of the virtual machine from <command>libvirtd</command>.
						</para>

					</listitem>

				</itemizedlist>
				 <para>
					All these subcommands take a virtual machine identifier as a parameter.
				</para>

			</section>
			 <section>
				<title>Installing an RPM based system in Debian with yum</title>
				 <para>
					If the virtual machine is meant to run a Debian (or one of its derivatives), the system can be initialized with <command>debootstrap</command>, as described above. But if the virtual machine is to be installed with an RPM-based system (such as Fedora, CentOS or Scientific Linux), the setup will need to be done using the <command>yum</command> utility (available in the package of the same name).
				</para>
				 <para>
					The procedure requires using <command>rpm</command> to extract an initial set of files, including notably <command>yum</command> configuration files, and then calling <command>yum</command> to extract the remaining set of packages. But since we call <command>yum</command> from outside the chroot, we need to make some temporary changes. In the sample below, the target chroot is <filename>/srv/centos</filename>.
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>rootdir="/srv/centos"
</userinput><computeroutput># </computeroutput><userinput>mkdir -p "$rootdir" /etc/rpm
</userinput><computeroutput># </computeroutput><userinput>echo "%_dbpath /var/lib/rpm" &gt; /etc/rpm/macros.dbpath
</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root "$rootdir" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!
rpm: However assuming you know what you are doing...
warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY
# </computeroutput><userinput>sed -i -e "s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g" $rootdir/etc/yum.repos.d/*.repo
</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core
</userinput><computeroutput>[...]
# </computeroutput><userinput>sed -i -e "s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g" $rootdir/etc/yum.repos.d/*.repo
</userinput></screen>

			</section>

		</section>

	</section>
	 <section id="sect.automated-installation">
		<title>Automated Installation</title>
		 <indexterm>
			<primary>deployment</primary>
		</indexterm>
		 <indexterm>
			<primary>installation</primary>
			<secondary>automated installation</secondary>
		</indexterm>
		 <para>
			The Falcot Corp administrators, like many administrators of large IT services, need tools to install (or reinstall) quickly, and automatically if possible, their new machines.
		</para>
		 <para>
			These requirements can be met by a wide range of solutions. On the one hand, generic tools such as SystemImager handle this by creating an image based on a template machine, then deploy that image to the target systems; at the other end of the spectrum, the standard Debian installer can be preseeded with a configuration file giving the answers to the questions asked during the installation process. As a sort of middle ground, a hybrid tool such as FAI (<emphasis>Fully Automatic Installer</emphasis>) installs machines using the packaging system, but it also uses its own infrastructure for tasks that are more specific to massive deployments (such as starting, partitioning, configuration and so on).
		</para>
		 <para>
			Each of these solutions has its pros and cons: SystemImager works independently from any particular packaging system, which allows it to manage large sets of machines using several distinct Linux distributions. It also includes an update system that doesn't require a reinstallation, but this update system can only be reliable if the machines are not modified independently; in other words, the user must not update any software on their own, or install any other software. Similarly, security updates must not be automated, because they have to go through the centralized reference image maintained by SystemImager. This solution also requires the target machines to be homogeneous, otherwise many different images would have to be kept and managed (an i386 image won't fit on a powerpc machine, and so on).
		</para>
		 <para>
			On the other hand, an automated installation using debian-installer can adapt to the specifics of each machine: the installer will fetch the appropriate kernel and software packages from the relevant repositories, detect available hardware, partition the whole hard disk to take advantage of all the available space, install the corresponding Debian system, and set up an appropriate bootloader. However, the standard installer will only install standard Debian versions, with the base system and a set of pre-selected “tasks”; this precludes installing a particular system with non-packaged applications. Fulfilling this particular need requires customizing the installer… Fortunately, the installer is very modular, and there are tools to automate most of the work required for this customization, most importantly simple-CDD (CDD being an acronym for <emphasis>Custom Debian Derivative</emphasis>). Even the simple-CDD solution, however, only handles initial installations; this is usually not a problem since the APT tools allow efficient deployment of updates later on.
		</para>
		 <para>
			We will only give a rough overview of FAI, and skip SystemImager altogether (which is no longer in Debian), in order to focus more intently on debian-installer and simple-CDD, which are more interesting in a Debian-only context.
		</para>
		 <section id="sect.fai">
			<title>Fully Automatic Installer (FAI)</title>
			 <indexterm>
				<primary>Fully Automatic Installer (FAI)</primary>
			</indexterm>
			 <para>
				<foreignphrase>Fully Automatic Installer</foreignphrase> is probably the oldest automated deployment system for Debian, which explains its status as a reference; but its very flexible nature only just compensates for the complexity it involves.
			</para>
			 <para>
				FAI requires a server system to store deployment information and allow target machines to boot from the network. This server requires the <emphasis role="pkg">fai-server</emphasis> package (or <emphasis role="pkg">fai-quickstart</emphasis>, which also brings the required elements for a standard configuration).
			</para>
			 <para>
				FAI uses a specific approach for defining the various installable profiles. Instead of simply duplicating a reference installation, FAI is a full-fledged installer, fully configurable via a set of files and scripts stored on the server; the default location <filename>/srv/fai/config/</filename> is not automatically created, so the administrator needs to create it along with the relevant files. Most of the times, these files will be customized from the example files available in the documentation for the <emphasis role="pkg">fai-doc</emphasis> package, more particularly the <filename>/usr/share/doc/fai-doc/examples/simple/</filename> directory.
			</para>
			 <para>
				Once the profiles are defined, the <command>fai-setup</command> command generates the elements required to start an FAI installation; this mostly means preparing or updating a minimal system (NFS-root) used during installation. An alternative is to generate a dedicated boot CD with <command>fai-cd</command>.
			</para>
			 <para>
				Creating all these configuration files requires some understanding of the way FAI works. A typical installation process is made of the following steps:
			</para>
			 <itemizedlist>
				<listitem>
					<para>
						fetching a kernel from the network, and booting it;
					</para>

				</listitem>
				 <listitem>
					<para>
						mounting the root filesystem from NFS;
					</para>

				</listitem>
				 <listitem>
					<para>
						executing <command>/usr/sbin/fai</command>, which controls the rest of the process (the next steps are therefore initiated by this script);
					</para>

				</listitem>
				 <listitem>
					<para>
						copying the configuration space from the server into <filename>/fai/</filename>;
					</para>

				</listitem>
				 <listitem>
					<para>
						running <command>fai-class</command>. The <filename>/fai/class/[0-9][0-9]*</filename> scripts are executed in turn, and return names of “classes” that apply to the machine being installed; this information will serve as a base for the following steps. This allows for some flexibility in defining the services to be installed and configured.
					</para>

				</listitem>
				 <listitem>
					<para>
						fetching a number of configuration variables, depending on the relevant classes;
					</para>

				</listitem>
				 <listitem>
					<para>
						partitioning the disks and formatting the partitions, based on information provided in <filename>/fai/disk_config/<replaceable>class</replaceable></filename>;
					</para>

				</listitem>
				 <listitem>
					<para>
						mounting said partitions;
					</para>

				</listitem>
				 <listitem>
					<para>
						installing the base system;
					</para>

				</listitem>
				 <listitem>
					<para>
						preseeding the Debconf database with <command>fai-debconf</command>;
					</para>

				</listitem>
				 <listitem>
					<para>
						fetching the list of available packages for APT;
					</para>

				</listitem>
				 <listitem>
					<para>
						installing the packages listed in <filename>/fai/package_config/<replaceable>class</replaceable></filename>;
					</para>

				</listitem>
				 <listitem>
					<para>
						executing the post-configuration scripts, <filename>/fai/scripts/<replaceable>class</replaceable>/[0-9][0-9]*</filename>;
					</para>

				</listitem>
				 <listitem>
					<para>
						recording the installation logs, unmounting the partitions, and rebooting.
					</para>

				</listitem>

			</itemizedlist>

		</section>
		 <section id="sect.d-i-preseeding">
			<title>Preseeding Debian-Installer</title>
			 <indexterm>
				<primary>preseed</primary>
			</indexterm>
			 <indexterm>
				<primary>preconfiguration</primary>
			</indexterm>
			 <para>
				At the end of the day, the best tool to install Debian systems should logically be the official Debian installer. This is why, right from its inception, debian-installer has been designed for automated use, taking advantage of the infrastructure provided by <emphasis role="pkg">debconf</emphasis>. The latter allows, on the one hand, to reduce the number of questions asked (hidden questions will use the provided default answer), and on the other hand, to provide the default answers separately, so that installation can be non-interactive. This last feature is known as <emphasis>preseeding</emphasis>.
			</para>
			 <sidebar> <title><emphasis>GOING FURTHER</emphasis> Debconf with a centralized database</title>
			 <indexterm>
				<primary><command>debconf</command></primary>
			</indexterm>
			 <para>
				Preseeding allows to provide a set of answers to Debconf questions at installation time, but these answers are static and do not evolve as time passes. Since already-installed machines may need upgrading, and new answers may become required, the <filename>/etc/debconf.conf</filename> configuration file can be set up so that Debconf uses external data sources (such as an LDAP directory server, or a remote file accessed via NFS or Samba). Several external data sources can be defined at the same time, and they complement one another. The local database is still used (for read-write access), but the remote databases are usually restricted to reading. The <citerefentry><refentrytitle>debconf.conf</refentrytitle>
				 <manvolnum>5</manvolnum></citerefentry> manual page describes all the possibilities in detail (you need the <emphasis role="pkg">debconf-doc</emphasis> package).
			</para>
			 </sidebar> <section>
				<title>Using a Preseed File</title>
				 <para>
					There are several places where the installer can get a preseeding file:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							in the initrd used to start the machine; in this case, preseeding happens at the very beginning of the installation, and all questions can be avoided. The file just needs to be called <filename>preseed.cfg</filename> and stored in the initrd root.
						</para>

					</listitem>
					 <listitem>
						<para>
							on the boot media (CD or USB key); preseeding then happens as soon as the media is mounted, which means right after the questions about language and keyboard layout. The <literal>preseed/file</literal> boot parameter can be used to indicate the location of the preseeding file (for instance, <filename>/cdrom/preseed.cfg</filename> when the installation is done off a CD-ROM, or <filename>/hd-media/preseed.cfg</filename> in the USB-key case).
						</para>

					</listitem>
					 <listitem>
						<para>
							from the network; preseeding then only happens after the network is (automatically) configured; the relevant boot parameter is then <literal>preseed/url=http://<replaceable>server</replaceable>/preseed.cfg</literal>.
						</para>

					</listitem>

				</itemizedlist>
				 <para>
					At a glance, including the preseeding file in the initrd looks like the most interesting solution; however, it is rarely used in practice, because generating an installer initrd is rather complex. The other two solutions are much more common, especially since boot parameters provide another way to preseed the answers to the first questions of the installation process. The usual way to save the bother of typing these boot parameters by hand at each installation is to save them into the configuration for <command>isolinux</command> (in the CD-ROM case) or <command>syslinux</command> (USB key).
				</para>

			</section>
			 <section>
				<title>Creating a Preseed File</title>
				 <para>
					A preseed file is a plain text file, where each line contains the answer to one Debconf question. A line is split across four fields separated by whitespace (spaces or tabs), as in, for instance, <literal>d-i mirror/suite string stable</literal>:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							the first field is the “owner” of the question; “d-i” is used for questions relevant to the installer, but it can also be a package name for questions coming from Debian packages;
						</para>

					</listitem>
					 <listitem>
						<para>
							the second field is an identifier for the question;
						</para>

					</listitem>
					 <listitem>
						<para>
							third, the type of question;
						</para>

					</listitem>
					 <listitem>
						<para>
							the fourth and last field contains the value for the answer. Note that it must be separated from the third field with a single space; if there are more than one, the following space characters are considered part of the value.
						</para>

					</listitem>

				</itemizedlist>
				 <para>
					The simplest way to write a preseed file is to install a system by hand. Then <command>debconf-get-selections --installer</command> will provide the answers concerning the installer. Answers about other packages can be obtained with <command>debconf-get-selections</command>. However, a cleaner solution is to write the preseed file by hand, starting from an example and the reference documentation: with such an approach, only questions where the default answer needs to be overridden can be preseeded; using the <literal>priority=critical</literal> boot parameter will instruct Debconf to only ask critical questions, and use the default answer for others.
				</para>
				 <sidebar> <title><emphasis>DOCUMENTATION</emphasis> Installation guide appendix</title>
				 <para>
					The installation guide, available online, includes detailed documentation on the use of a preseed file in an appendix. It also includes a detailed and commented sample file, which can serve as a base for local customizations. <ulink type="block" url="https://www.debian.org/releases/jessie/amd64/apb.html" /> <ulink type="block" url="https://www.debian.org/releases/jessie/example-preseed.txt" />
				</para>
				 </sidebar>
			</section>
			 <section>
				<title>Creating a Customized Boot Media</title>
				 <para>
					Knowing where to store the preseed file is all very well, but the location isn't everything: one must, one way or another, alter the installation boot media to change the boot parameters and add the preseed file.
				</para>
				 <section>
					<title>Booting From the Network</title>
					 <para>
						When a computer is booted from the network, the server sending the initialization elements also defines the boot parameters. Thus, the change needs to be made in the PXE configuration for the boot server; more specifically, in its <filename>/tftpboot/pxelinux.cfg/default</filename> configuration file. Setting up network boot is a prerequisite; see the Installation Guide for details. <ulink type="block" url="https://www.debian.org/releases/jessie/amd64/ch04s05.html" />
					</para>

				</section>
				 <section>
					<title>Preparing a Bootable USB Key</title>
					 <para>
						Once a bootable key has been prepared (see <xref linkend="sect.install-usb" />), a few extra operations are needed. Assuming the key contents are available under <filename>/media/usbdisk/</filename>:
					</para>
					 <itemizedlist>
						<listitem>
							<para>
								copy the preseed file to <filename>/media/usbdisk/preseed.cfg</filename>
							</para>

						</listitem>
						 <listitem>
							<para>
								edit <filename>/media/usbdisk/syslinux.cfg</filename> and add required boot parameters (see example below).
							</para>

						</listitem>

					</itemizedlist>
					 <example>
						<title>syslinux.cfg file and preseeding parameters</title>
						 
<programlisting>default vmlinuz
append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --
</programlisting>

					</example>

				</section>
				 <section>
					<title>Creating a CD-ROM Image</title>
					 <indexterm>
						<primary>debian-cd</primary>
					</indexterm>
					 <para>
						A USB key is a read-write media, so it was easy for us to add a file there and change a few parameters. In the CD-ROM case, the operation is more complex, since we need to regenerate a full ISO image. This task is handled by <emphasis role="pkg">debian-cd</emphasis>, but this tool is rather awkward to use: it needs a local mirror, and it requires an understanding of all the options provided by <filename>/usr/share/debian-cd/CONF.sh</filename>; even then, <command>make</command> must be invoked several times. <filename>/usr/share/debian-cd/README</filename> is therefore a very recommended read.
					</para>
					 <para>
						Having said that, debian-cd always operates in a similar way: an “image” directory with the exact contents of the CD-ROM is generated, then converted to an ISO file with a tool such as <command>genisoimage</command>, <command>mkisofs</command> or <command>xorriso</command>. The image directory is finalized after debian-cd's <command>make image-trees</command> step. At that point, we insert the preseed file into the appropriate directory (usually <filename>$TDIR/$CODENAME/CD1/</filename>, $TDIR and $CODENAME being parameters defined by the <filename>CONF.sh</filename> configuration file). The CD-ROM uses <command>isolinux</command> as its bootloader, and its configuration file must be adapted from what debian-cd generated, in order to insert the required boot parameters (the specific file is <filename>$TDIR/$CODENAME/boot1/isolinux/isolinux.cfg</filename>). Then the “normal” process can be resumed, and we can go on to generating the ISO image with <command>make image CD=1</command> (or <command>make images</command> if several CD-ROMs are generated).
					</para>

				</section>

			</section>

		</section>
		 <section id="sect.simple-cdd">
			<title>Simple-CDD: The All-In-One Solution</title>
			 <indexterm>
				<primary>simple-cdd</primary>
			</indexterm>
			 <para>
				Simply using a preseed file is not enough to fulfill all the requirements that may appear for large deployments. Even though it is possible to execute a few scripts at the end of the normal installation process, the selection of the set of packages to install is still not quite flexible (basically, only “tasks” can be selected); more important, this only allows installing official Debian packages, and precludes locally-generated ones.
			</para>
			 <para>
				On the other hand, debian-cd is able to integrate external packages, and debian-installer can be extended by inserting new steps in the installation process. By combining these capabilities, it should be possible to create a customized installer that fulfills our needs; it should even be able to configure some services after unpacking the required packages. Fortunately, this is not a mere hypothesis, since this is exactly what Simple-CDD (in the <emphasis role="pkg">simple-cdd</emphasis> package) does.
			</para>
			 <para>
				The purpose of Simple-CDD is to allow anyone to easily create a distribution derived from Debian, by selecting a subset of the available packages, preconfiguring them with Debconf, adding specific software, and executing custom scripts at the end of the installation process. This matches the “universal operating system” philosophy, since anyone can adapt it to their own needs.
			</para>
			 <section>
				<title>Creating Profiles</title>
				 <para>
					Simple-CDD defines “profiles” that match the FAI “classes” concept, and a machine can have several profiles (determined at installation time). A profile is defined by a set of <filename>profiles/<replaceable>profile</replaceable>.*</filename> files:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							the <filename>.description</filename> file contains a one-line description for the profile;
						</para>

					</listitem>
					 <listitem>
						<para>
							the <filename>.packages</filename> file lists packages that will automatically be installed if the profile is selected;
						</para>

					</listitem>
					 <listitem>
						<para>
							the <filename>.downloads</filename> file lists packages that will be stored onto the installation media, but not necessarily installed;
						</para>

					</listitem>
					 <listitem>
						<para>
							the <filename>.preseed</filename> file contains preseeding information for Debconf questions (for the installer and/or for packages);
						</para>

					</listitem>
					 <listitem>
						<para>
							the <filename>.postinst</filename> file contains a script that will be run at the end of the installation process;
						</para>

					</listitem>
					 <listitem>
						<para>
							lastly, the <filename>.conf</filename> file allows changing some Simple-CDD parameters based on the profiles to be included in an image.
						</para>

					</listitem>

				</itemizedlist>
				 <para>
					The <literal>default</literal> profile has a particular role, since it is always selected; it contains the bare minimum required for Simple-CDD to work. The only thing that is usually customized in this profile is the <literal>simple-cdd/profiles</literal> preseed parameter: this allows avoiding the question, introduced by Simple-CDD, about what profiles to install.
				</para>
				 <para>
					Note also that the commands will need to be invoked from the parent directory of the <filename>profiles</filename> directory.
				</para>

			</section>
			 <section>
				<title>Configuring and Using <command>build-simple-cdd</command></title>
				 <indexterm>
					<primary><command>build-simple-cdd</command></primary>
				</indexterm>
				 <sidebar> <title><emphasis>QUICK LOOK</emphasis> Detailed configuration file</title>
				 <para>
					An example of a Simple-CDD configuration file, with all possible parameters, is included in the package (<filename>/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz</filename>). This can be used as a starting point when creating a custom configuration file.
				</para>
				 </sidebar> <para>
					Simple-CDD requires many parameters to operate fully. They will most often be gathered in a configuration file, which <command>build-simple-cdd</command> can be pointed at with the <literal>--conf</literal> option, but they can also be specified via dedicated parameters given to <command>build-simple-cdd</command>. Here is an overview of how this command behaves, and how its parameters are used:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							the <literal>profiles</literal> parameter lists the profiles that will be included on the generated CD-ROM image;
						</para>

					</listitem>
					 <listitem>
						<para>
							based on the list of required packages, Simple-CDD downloads the appropriate files from the server mentioned in <literal>server</literal>, and gathers them into a partial mirror (which will later be given to debian-cd);
						</para>

					</listitem>
					 <listitem>
						<para>
							the custom packages mentioned in <literal>local_packages</literal> are also integrated into this local mirror;
						</para>

					</listitem>
					 <listitem>
						<para>
							debian-cd is then executed (within a default location that can be configured with the <literal>debian_cd_dir</literal> variable), with the list of packages to integrate;
						</para>

					</listitem>
					 <listitem>
						<para>
							once debian-cd has prepared its directory, Simple-CDD applies some changes to this directory:
						</para>
						 <itemizedlist>
							<listitem>
								<para>
									files containing the profiles are added in a <filename>simple-cdd</filename> subdirectory (that will end up on the CD-ROM);
								</para>

							</listitem>
							 <listitem>
								<para>
									other files listed in the <literal>all_extras</literal> parameter are also added;
								</para>

							</listitem>
							 <listitem>
								<para>
									the boot parameters are adjusted so as to enable the preseeding. Questions concerning language and country can be avoided if the required information is stored in the <literal>language</literal> and <literal>country</literal> variables.
								</para>

							</listitem>

						</itemizedlist>

					</listitem>
					 <listitem>
						<para>
							debian-cd then generates the final ISO image.
						</para>

					</listitem>

				</itemizedlist>

			</section>
			 <section>
				<title>Generating an ISO Image</title>
				 <para>
					Once we have written a configuration file and defined our profiles, the remaining step is to invoke <command>build-simple-cdd --conf simple-cdd.conf</command>. After a few minutes, we get the required image in <filename>images/debian-8.0-amd64-CD-1.iso</filename>.
				</para>

			</section>

		</section>

	</section>
	 <section id="sect.monitoring">
		<title>Overvåking</title>
		 <para>
			Monitoring is a generic term, and the various involved activities have several goals: on the one hand, following usage of the resources provided by a machine allows anticipating saturation and the subsequent required upgrades; on the other hand, alerting the administrator as soon as a service is unavailable or not working properly means that the problems that do happen can be fixed sooner.
		</para>
		 <para>
			<emphasis>Munin</emphasis> covers the first area, by displaying graphical charts for historical values of a number of parameters (used RAM, occupied disk space, processor load, network traffic, Apache/MySQL load, and so on). <emphasis>Nagios</emphasis> covers the second area, by regularly checking that the services are working and available, and sending alerts through the appropriate channels (e-mails, text messages, and so on). Both have a modular design, which makes it easy to create new plug-ins to monitor specific parameters or services.
		</para>
		 <sidebar> <title><emphasis>ALTERNATIVE</emphasis> Zabbix, an integrated monitoring tool</title>
		 <indexterm>
			<primary>Zabbix</primary>
		</indexterm>
		 <para>
			Although Munin and Nagios are in very common use, they are not the only players in the monitoring field, and each of them only handles half of the task (graphing on one side, alerting on the other). Zabbix, on the other hand, integrates both parts of monitoring; it also has a web interface for configuring the most common aspects. It has grown by leaps and bounds during the last few years, and can now be considered a viable contender. On the monitoring server, you would install <emphasis role="pkg">zabbix-server-pgsql</emphasis> (or <emphasis role="pkg">zabbix-server-mysql</emphasis>), possibly together with <emphasis role="pkg">zabbix-frontend-php</emphasis> to have a web interface. On the hosts to monitor you would install <emphasis role="pkg">zabbix-agent</emphasis> feeding data back to the server. <ulink type="block" url="http://www.zabbix.com/" />
		</para>
		 </sidebar> <sidebar> <title><emphasis>ALTERNATIVE</emphasis> Icinga, a Nagios fork</title>
		 <indexterm>
			<primary>Icinga</primary>
		</indexterm>
		 <para>
			Spurred by divergences in opinions concerning the development model for Nagios (which is controlled by a company), a number of developers forked Nagios and use Icinga as their new name. Icinga is still compatible — so far — with Nagios configurations and plugins, but it also adds extra features. <ulink type="block" url="http://www.icinga.org/" />
		</para>
		 </sidebar> <section id="sect.munin">
			<title>Setting Up Munin</title>
			 <indexterm>
				<primary>Munin</primary>
			</indexterm>
			 <para>
				The purpose of Munin is to monitor many machines; therefore, it quite naturally uses a client/server architecture. The central host — the grapher — collects data from all the monitored hosts, and generates historical graphs.
			</para>
			 <section>
				<title>Configuring Hosts To Monitor</title>
				 <para>
					The first step is to install the <emphasis role="pkg">munin-node</emphasis> package. The daemon installed by this package listens on port 4949 and sends back the data collected by all the active plugins. Each plugin is a simple program returning a description of the collected data as well as the latest measured value. Plugins are stored in <filename>/usr/share/munin/plugins/</filename>, but only those with a symbolic link in <filename>/etc/munin/plugins/</filename> are really used.
				</para>
				 <para>
					When the package is installed, a set of active plugins is determined based on the available software and the current configuration of the host. However, this autoconfiguration depends on a feature that each plugin must provide, and it is usually a good idea to review and tweak the results by hand. Browsing the <ulink url="http://gallery.munin-monitoring.org">Plugin Gallery</ulink> can be interesting even though not all plugins have comprehensive documentation. However, all plugins are scripts and most are rather simple and well-commented. Browsing <filename>/etc/munin/plugins/</filename> is therefore a good way of getting an idea of what each plugin is about and determining which should be removed. Similarly, enabling an interesting plugin found in <filename>/usr/share/munin/plugins/</filename> is a simple matter of setting up a symbolic link with <command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command>. Note that when a plugin name ends with an underscore “_”, the plugin requires a parameter. This parameter must be stored in the name of the symbolic link; for instance, the “if_” plugin must be enabled with a <filename>if_eth0</filename> symbolic link, and it will monitor network traffic on the eth0 interface.
				</para>
				 <para>
					Once all plugins are correctly set up, the daemon configuration must be updated to describe access control for the collected data. This involves <literal>allow</literal> directives in the <filename>/etc/munin/munin-node.conf</filename> file. The default configuration is <literal>allow ^127\.0\.0\.1$</literal>, and only allows access to the local host. An administrator will usually add a similar line containing the IP address of the grapher host, then restart the daemon with <command>service munin-node restart</command>.
				</para>
				 <sidebar> <title><emphasis>GOING FURTHER</emphasis> Creating local plugins</title>
				 <para>
					Munin does include detailed documentation on how plugins should behave, and how to develop new plugins. <ulink type="block" url="http://munin-monitoring.org/wiki/plugins" />
				</para>
				 <para>
					A plugin is best tested when run in the same conditions as it would be when triggered by munin-node; this can be simulated by running <command>munin-run <replaceable>plugin</replaceable></command> as root. A potential second parameter given to this command (such as <literal>config</literal>) is passed to the plugin as a parameter.
				</para>
				 <para>
					When a plugin is invoked with the <literal>config</literal> parameter, it must describe itself by returning a set of fields:
				</para>
				 
<screen><computeroutput>$ </computeroutput><userinput>sudo munin-run load config
</userinput><computeroutput>graph_title Load average
graph_args --base 1000 -l 0
graph_vlabel load
graph_scale no
graph_category system
load.label load
graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run "immediately").
load.info 5 minute load average
</computeroutput></screen>
				 <para>
					The various available fields are described by the “Plugin reference” available as part of the “Munin guide”. <ulink type="block" url="http://munin.readthedocs.org/en/latest/reference/plugin.html" />
				</para>
				 <para>
					When invoked without a parameter, the plugin simply returns the last measured values; for instance, executing <command>sudo munin-run load</command> could return <literal>load.value 0.12</literal>.
				</para>
				 <para>
					Finally, when a plugin is invoked with the <literal>autoconf</literal> parameter, it should return “yes” (and a 0 exit status) or “no” (with a 1 exit status) according to whether the plugin should be enabled on this host.
				</para>
				 </sidebar>
			</section>
			 <section>
				<title>Configuring the Grapher</title>
				 <para>
					The “grapher” is simply the computer that aggregates the data and generates the corresponding graphs. The required software is in the <emphasis role="pkg">munin</emphasis> package. The standard configuration runs <command>munin-cron</command> (once every 5 minutes), which gathers data from all the hosts listed in <filename>/etc/munin/munin.conf</filename> (only the local host is listed by default), saves the historical data in RRD files (<emphasis>Round Robin Database</emphasis>, a file format designed to store data varying in time) stored under <filename>/var/lib/munin/</filename> and generates an HTML page with the graphs in <filename>/var/cache/munin/www/</filename>.
				</para>
				 <para>
					All monitored machines must therefore be listed in the <filename>/etc/munin/munin.conf</filename> configuration file. Each machine is listed as a full section with a name matching the machine and at least an <literal>address</literal> entry giving the corresponding IP address.
				</para>
				 
<programlisting>[ftp.falcot.com]
    address 192.168.0.12
    use_node_name yes
</programlisting>
				 <para>
					Sections can be more complex, and describe extra graphs that could be created by combining data coming from several machines. The samples provided in the configuration file are good starting points for customization.
				</para>
				 <para>
					The last step is to publish the generated pages; this involves configuring a web server so that the contents of <filename>/var/cache/munin/www/</filename> are made available on a website. Access to this website will often be restricted, using either an authentication mechanism or IP-based access control. See <xref linkend="sect.http-web-server" /> for the relevant details.
				</para>

			</section>

		</section>
		 <section id="sect.nagios">
			<title>Setting Up Nagios</title>
			 <indexterm>
				<primary>Nagios</primary>
			</indexterm>
			 <para>
				Unlike Munin, Nagios does not necessarily require installing anything on the monitored hosts; most of the time, Nagios is used to check the availability of network services. For instance, Nagios can connect to a web server and check that a given web page can be obtained within a given time.
			</para>
			 <section>
				<title>Å installere</title>
				 <para>
					The first step in setting up Nagios is to install the <emphasis role="pkg">nagios3</emphasis>, <emphasis role="pkg">nagios-plugins</emphasis> and <emphasis role="pkg">nagios3-doc</emphasis> packages. Installing the packages configures the web interface and creates a first <literal>nagiosadmin</literal> user (for which it asks for a password). Adding other users is a simple matter of inserting them in the <filename>/etc/nagios3/htpasswd.users</filename> file with Apache's <command>htpasswd</command> command. If no Debconf question was displayed during installation, <command>dpkg-reconfigure nagios3-cgi</command> can be used to define the <literal>nagiosadmin</literal> password.
				</para>
				 <para>
					Pointing a browser at <literal>http://<replaceable>server</replaceable>/nagios3/</literal> displays the web interface; in particular, note that Nagios already monitors some parameters of the machine where it runs. However, some interactive features such as adding comments to a host do not work. These features are disabled in the default configuration for Nagios, which is very restrictive for security reasons.
				</para>
				 <para>
					As documented in <filename>/usr/share/doc/nagios3/README.Debian</filename>, enabling some features involves editing <filename>/etc/nagios3/nagios.cfg</filename> and setting its <literal>check_external_commands</literal> parameter to “1”. We also need to set up write permissions for the directory used by Nagios, with commands such as the following:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>service nagios3 stop</userinput>
<computeroutput>[...]
# </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios3/rw
</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios3
</userinput><computeroutput># </computeroutput><userinput>service nagios3 start</userinput>
<computeroutput>[...]</computeroutput></screen>

			</section>
			 <section>
				<title>Konfigurering</title>
				 <para>
					The Nagios web interface is rather nice, but it does not allow configuration, nor can it be used to add monitored hosts and services. The whole configuration is managed via files referenced in the central configuration file, <filename>/etc/nagios3/nagios.cfg</filename>.
				</para>
				 <para>
					These files should not be dived into without some understanding of the Nagios concepts. The configuration lists objects of the following types:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							a <emphasis>host</emphasis> is a machine to be monitored;
						</para>

					</listitem>
					 <listitem>
						<para>
							a <emphasis>hostgroup</emphasis> is a set of hosts that should be grouped together for display, or to factor some common configuration elements;
						</para>

					</listitem>
					 <listitem>
						<para>
							a <emphasis>service</emphasis> is a testable element related to a host or a host group. It will most often be a check for a network service, but it can also involve checking that some parameters are within an acceptable range (for instance, free disk space or processor load);
						</para>

					</listitem>
					 <listitem>
						<para>
							a <emphasis>servicegroup</emphasis> is a set of services that should be grouped together for display;
						</para>

					</listitem>
					 <listitem>
						<para>
							a <emphasis>contact</emphasis> is a person who can receive alerts;
						</para>

					</listitem>
					 <listitem>
						<para>
							a <emphasis>contactgroup</emphasis> is a set of such contacts;
						</para>

					</listitem>
					 <listitem>
						<para>
							a <emphasis>timeperiod</emphasis> is a range of time during which some services have to be checked;
						</para>

					</listitem>
					 <listitem>
						<para>
							a <emphasis>command</emphasis> is the command line invoked to check a given service.
						</para>

					</listitem>

				</itemizedlist>
				 <para>
					According to its type, each object has a number of properties that can be customized. A full list would be too long to include, but the most important properties are the relations between the objects.
				</para>
				 <para>
					A <emphasis>service</emphasis> uses a <emphasis>command</emphasis> to check the state of a feature on a <emphasis>host</emphasis> (or a <emphasis>hostgroup</emphasis>) within a <emphasis>timeperiod</emphasis>. In case of a problem, Nagios sends an alert to all members of the <emphasis>contactgroup</emphasis> linked to the service. Each member is sent the alert according to the channel described in the matching <emphasis>contact</emphasis> object.
				</para>
				 <para>
					An inheritance system allows easy sharing of a set of properties across many objects without duplicating information. Moreover, the initial configuration includes a number of standard objects; in many cases, defining new hosts, services and contacts is a simple matter of deriving from the provided generic objects. The files in <filename>/etc/nagios3/conf.d/</filename> are a good source of information on how they work.
				</para>
				 <para>
					The Falcot Corp administrators use the following configuration:
				</para>
				 <example>
					<title><filename>/etc/nagios3/conf.d/falcot.cfg</filename> file</title>
					 
<programlisting>define contact{
    name                            generic-contact
    service_notification_period     24x7
    host_notification_period        24x7
    service_notification_options    w,u,c,r
    host_notification_options       d,u,r
    service_notification_commands   notify-service-by-email
    host_notification_commands      notify-host-by-email
    register                        0 ; Template only
}
define contact{
    use             generic-contact
    contact_name    rhertzog
    alias           Raphael Hertzog
    email           hertzog@debian.org
}
define contact{
    use             generic-contact
    contact_name    rmas
    alias           Roland Mas
    email           lolando@debian.org
}

define contactgroup{
    contactgroup_name     falcot-admins
    alias                 Falcot Administrators
    members               rhertzog,rmas
}

define host{
    use                   generic-host ; Name of host template to use
    host_name             www-host
    alias                 www.falcot.com
    address               192.168.0.5
    contact_groups        falcot-admins
    hostgroups            debian-servers,ssh-servers
}
define host{
    use                   generic-host ; Name of host template to use
    host_name             ftp-host
    alias                 ftp.falcot.com
    address               192.168.0.6
    contact_groups        falcot-admins
    hostgroups            debian-servers,ssh-servers
}

# 'check_ftp' command with custom parameters
define command{
    command_name          check_ftp2
    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35
}

# Generic Falcot service
define service{
    name                  falcot-service
    use                   generic-service
    contact_groups        falcot-admins
    register              0
}

# Services to check on www-host
define service{
    use                   falcot-service
    host_name             www-host
    service_description   HTTP
    check_command         check_http
}
define service{
    use                   falcot-service
    host_name             www-host
    service_description   HTTPS
    check_command         check_https
}
define service{
    use                   falcot-service
    host_name             www-host
    service_description   SMTP
    check_command         check_smtp
}

# Services to check on ftp-host
define service{
    use                   falcot-service
    host_name             ftp-host
    service_description   FTP
    check_command         check_ftp2
}
</programlisting>

				</example>
				 <para>
					This configuration file describes two monitored hosts. The first one is the web server, and the checks are made on the HTTP (80) and secure-HTTP (443) ports. Nagios also checks that an SMTP server runs on port 25. The second host is the FTP server, and the check includes making sure that a reply comes within 20 seconds. Beyond this delay, a <emphasis>warning</emphasis> is emitted; beyond 30 seconds, the alert is deemed critical. The Nagios web interface also shows that the SSH service is monitored: this comes from the hosts belonging to the <literal>ssh-servers</literal> hostgroup. The matching standard service is defined in <filename>/etc/nagios3/conf.d/services_nagios2.cfg</filename>.
				</para>
				 <para>
					Note the use of inheritance: an object is made to inherit from another object with the “use <replaceable>parent-name</replaceable>”. The parent object must be identifiable, which requires giving it a “name <replaceable>identifier</replaceable>” property. If the parent object is not meant to be a real object, but only to serve as a parent, giving it a “register 0” property tells Nagios not to consider it, and therefore to ignore the lack of some parameters that would otherwise be required.
				</para>
				 <sidebar> <title><emphasis>DOCUMENTATION</emphasis> List of object properties</title>
				 <para>
					A more in-depth understanding of the various ways in which Nagios can be configured can be obtained from the documentation provided by the <emphasis role="pkg">nagios3-doc</emphasis> package. This documentation is directly accessible from the web interface, with the “Documentation” link in the top left corner. It includes a list of all object types, with all the properties they can have. It also explains how to create new plugins.
				</para>
				 </sidebar> <sidebar> <title><emphasis>GOING FURTHER</emphasis> Remote tests with NRPE</title>
				 <para>
					Many Nagios plugins allow checking some parameters local to a host; if many machines need these checks while a central installation gathers them, the NRPE (<emphasis>Nagios Remote Plugin Executor</emphasis>) plugin needs to be deployed. The <emphasis role="pkg">nagios-nrpe-plugin</emphasis> package needs to be installed on the Nagios server, and <emphasis role="pkg">nagios-nrpe-server</emphasis> on the hosts where local tests need to run. The latter gets its configuration from <filename>/etc/nagios/nrpe.cfg</filename>. This file should list the tests that can be started remotely, and the IP addresses of the machines allowed to trigger them. On the Nagios side, enabling these remote tests is a simple matter of adding matching services using the new <emphasis>check_nrpe</emphasis> command.
				</para>
				 </sidebar>
			</section>

		</section>

	</section>
</chapter>

