<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html
    xmlns="http://www.w3.org/1999/xhtml"><head><meta
        http-equiv="Content-Type"
        content="text/html; charset=UTF-8" /><title
        xmlns:d="http://docbook.org/ns/docbook">12.2. Visualisering</title><link
        rel="stylesheet"
        type="text/css"
        href="Common_Content/css/default.css" /><link
        rel="stylesheet"
        media="print"
        href="Common_Content/css/print.css"
        type="text/css" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="generator"
        content="publican v4.3.2" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="package"
        content="Debian-debian-handbook-8-nb-NO-1.0-1" /><meta
        name="keywords"
        content="RAID, LVM, FAI, Å komme foran, Overvåking, Visualisering, Xen, LXC" /><link
        rel="home"
        href="index.html"
        title="Håndbok for Debian-administratoren" /><link
        rel="up"
        href="advanced-administration.html"
        title="Kapittel 12. Avansert administrasjon" /><link
        rel="prev"
        href="advanced-administration.html"
        title="Kapittel 12. Avansert administrasjon" /><link
        rel="next"
        href="sect.automated-installation.html"
        title="12.3. Automated Installation" /><link
        xmlns=""
        rel="canonical"
        href="https://debian-handbook.info/browse/nb-NO/stable/sect.virtualization.html" /></head><body><div
        id="banner"><a
          href="http://debian-handbook.info/get/"><span
            class="text">Download the ebook</span></a></div><p
        id="title"><a
          class="left"
          href="http://www.debian.org"><img
            alt="Product Site"
            src="Common_Content/images//image_left.png" /></a><a
          class="right"
          href="index.html"><img
            alt="Documentation Site"
            src="Common_Content/images//image_right.png" /></a></p><ul
        class="docnav top"><li
          class="previous"><a
            accesskey="p"
            href="advanced-administration.html"><strong>Forrige</strong></a></li><li
          class="home">Håndbok for Debian-administratoren</li><li
          class="next"><a
            accesskey="n"
            href="sect.automated-installation.html"><strong>Neste</strong></a></li></ul><div
        class="section"><div
          class="titlepage"><div><div><h2
                class="title"><a
                  xmlns=""
                  id="sect.virtualization"></a>12.2. Visualisering</h2></div></div></div><a
          id="id-1.15.5.2"
          class="indexterm"></a><div
          class="para">
			Virtualisering er en av de viktigste fremskritt i de seneste årenes datautvikling. Begrepet omfatter ulike abstraksjoner og teknikker som simulerer virtuelle datamaskiner med varierende grad av uavhengighet på selve maskinvaren. En fysisk tjener kan så være vert for flere systemer som arbeider samtidig og i isolasjon. Bruksområdene er mange, og ofte utledes fra denne isolasjon: For eksempel testmiljøer med varierende konfigurasjoner, eller separasjon av vertsbaserte tjenester mellom ulike virtuelle maskiner for sikkerheten.
		</div><div
          class="para">
			Det er flere virtualiseringsløsninger, hver med sine egne fordeler og ulemper. Denne boken vil fokusere på Xen, LXC, og KVM, mens andre viktige implementeringer omfatter de følgende:
		</div><a
          id="id-1.15.5.5"
          class="indexterm"></a><a
          id="id-1.15.5.6"
          class="indexterm"></a><a
          id="id-1.15.5.7"
          class="indexterm"></a><a
          id="id-1.15.5.8"
          class="indexterm"></a><a
          id="id-1.15.5.9"
          class="indexterm"></a><a
          id="id-1.15.5.10"
          class="indexterm"></a><div
          xmlns:d="http://docbook.org/ns/docbook"
          class="itemizedlist"><ul><li
              class="listitem"><div
                class="para">
					QEMU er en programvare-emulator for en fullverdig datamaskin. Prestasjonen er langt fra den hastigheten man kunne oppnå ved å kjøre den opprinnelige, men den tillater å kjøre umodifiserte eller eksperimentelle operativsystemer på den emulerte maskinvaren. Den tillater også å emulere en annen maskinvare-arkitektur: For eksempel, kan et <span
                  class="emphasis"><em>amd64</em></span>-system emulere en <span
                  class="emphasis"><em>arm</em></span>-datamaskin. QEMU er fri programvare. <div
                  xmlns=""
                  class="url">→ <a
                    xmlns="http://www.w3.org/1999/xhtml"
                    href="http://www.qemu.org/">http://www.qemu.org/</a></div>
				</div></li><li
              class="listitem"><div
                class="para">
					Bochs er en annen gratis virtuell maskin, men den emulerer bare x86-arkitekturene (i386 eller amd64).
				</div></li><li
              class="listitem"><div
                class="para">
					VMWare er en proprietær virtuell maskin; som er en av de eldste der ute, er det også en av de mest kjente. Det fungerer på prinsipper som ligner på QEMU. VMWare foreslår avanserte funksjoner som å ta øyeblikksbilder av en kjørende virtuell maskin. <div
                  xmlns=""
                  class="url">→ <a
                    xmlns="http://www.w3.org/1999/xhtml"
                    href="http://www.vmware.com/">http://www.vmware.com/</a></div>
				</div></li><li
              class="listitem"><div
                class="para">
					VirtualBox er en virtuell maskin som stort sett er fri programvare (noen ekstra komponenter er tilgjengelige under en proprietær lisens). Dessverre er det i Debians "contrib"-del fordi den inneholder noen ferdigbygde filer som ikke kan bygges opp igjen uten en proprietær kompilator. Mens VirtualBox er yngre enn VMWare og begrenset til i386 eller amd64 arkitekturer, inneholder den fortsatt muligheten til å ta noen øyeblikksbilder og andre interessante funksjoner. <div
                  xmlns=""
                  class="url">→ <a
                    xmlns="http://www.w3.org/1999/xhtml"
                    href="http://www.virtualbox.org/">http://www.virtualbox.org/</a></div>
				</div></li></ul></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    xmlns=""
                    id="sect.xen"></a>12.2.1. Xen</h3></div></div></div><div
            class="para">
				Xen <a
              id="id-1.15.5.12.2.1"
              class="indexterm"></a> er en "paravirtualiserings"-løsning. Den introduserer et tynt abstraksjonslag, kalt en "hypervisor", mellom maskinvaren og de øvre systemer; Dette fungerer som en dommer som kontrollerer tilgangen til maskinvaren fra de virtuelle maskinene. Men den håndterer bare noen av instruksjonene, resten kjøres direkte av maskinvaren på vegne av systemene. Den største fordelen er at prestasjonen blir ikke dårligere, og systemer kjører med nær sin opprinnelige hastighet; ulempen er at operativsystem-kjernene man ønsker å bruke på en Xen hypervisor, trenger tilpasning for å kjøre på Xen.
			</div><div
            class="para">
				La oss bruke litt tid på vilkår. Hypervisoren er det nederste laget, som kjører går er direkte på maskinvaren, selv under kjernen. Dette hypervisor kan dele resten av programvaren over flere <span
              class="emphasis"><em>domains</em></span>, som kan sees på så mange virtuelle maskiner. En av disse domenene (den første som blir startet) er kjent som <span
              class="emphasis"><em>dom0</em></span>, og har en spesiell rolle, siden bare dette domenet kan kontrollere hypervisor og kjøring av andre domener. Disse andre domener er kjent som<span
              class="emphasis"><em>domU</em></span>. Med andre ord, og fra et brukersynspunkt <span
              class="emphasis"><em>dom0</em></span>-et samsvarer med "verten" i andre visualiseringssystmer, mens en <span
              class="emphasis"><em>domU</em></span> kan bli sett på som en "gjest".
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>CULTURE</em></span> Xen og de ulike versjonene av Linux</strong></p></div></div></div><div
              class="para">
				Xen ble opprinnelig utviklet som et sett av oppdateringer ut fra det offisielle treet, men uten å bli integrert i Linux-kjernen. Samtidig krevde flere kommende virtualiseringssystemer (inkludert KVM) noen generiske virtualisering-relaterte funksjoner for å lette integrering sin, og Linux-kjernen fikk dette settet av funksjoner (kjent som <span
                class="emphasis"><em>paravirt_ops</em></span> eller <span
                class="emphasis"><em>pv_ops</em></span>-grensesnittet). Ettersom Xen dupliserte noen av funksjonaliteten til dette grensesnittet, kunde de ikke bli akseptert offentlig .
			</div><div
              class="para">
				XenSource, selskapet bak Xen, måttr derfor legge til Xen i dette nye rammeverket, slik at Xens rettelser kunne flettes inn i den offisielle Linux-kjernen. Det betydde mye omskriving av kode, og selv om XenSource snart hadde en fungerende versjon basert på paravirt_ops-grensesnittet, ble rettelsene bare gradvis fusjonert inn den offisielle kjernen. Flettingen ble ferdigstilt i Linux 3.0. <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://wiki.xenproject.org/wiki/XenParavirtOps">http://wiki.xenproject.org/wiki/XenParavirtOps</a></div>
			</div><div
              class="para">
				Ettersom <span
                class="distribution distribution">Jessie</span> er basert på Linux-kjernes versjon 3.16, inkluderer standardpakkene <span
                class="pkg pkg">linux-image-686-pae</span> og <span
                class="pkg pkg">linux-image-amd64</span> den nødvendige koden, og distribusjons-spesifikke rettelser som trengs til <span
                class="distribution distribution">Squeeze</span> og tidligere versjoner av Debian er ikke nødvendig lenger. <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix">http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix</a></div>
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>NOTE</em></span> Arkitekturer som er kompatible med Xen</strong></p></div></div></div><div
              class="para">
				Xen er foreløpig kun tilgjengelig for i386-, amd64-, arm64- og armh-arkitekturer.
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>CULTURE</em></span> Xen og ikke-Linux kjerner</strong></p></div></div></div><div
              class="para">
				Xen krever endringer i alle operativsystemer man ønsker å kjøre den på. Her har ikke alle kjerner har samme nivå av modenhet. Mange er fullt funksjonelle, både som dom0 og DOMU: Linux 3.0 og senere, NetBSD 4.0 og senere, og OpenSolaris. Andre fungere bare som en domU. Du kan sjekke status for hvert operativsystem i Xen wikien: <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen">http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen</a></div> <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://wiki.xenproject.org/wiki/DomU_Support_for_Xen">http://wiki.xenproject.org/wiki/DomU_Support_for_Xen</a></div>
			</div><div
              class="para">
				Men hvis Xen kan stole på maskinvarefunksjonene øremerket til virtualisering (som bare er til stede i nyere prosessorer), kan til og med ikke-modifiserte operativsystemer kjøres som domU (inkludert Windows).
			</div></div><div
            class="para">
				Å bruke Xen under Debians krever tre komponenter:
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						Hypervisoren selv. Etter tilgjengelig maskinvare, vil den aktuelle pakken være enten <span
                    class="pkg pkg">xen-hypervisor-4.4-amd64</span>, <span
                    class="pkg pkg">xen-hypervisor-4.4-armhf</span>, eller <span
                    class="pkg pkg">xen-hypervisor-4.4-arm64</span>.
					</div></li><li
                class="listitem"><div
                  class="para">
						En kjerne som kjører på den aktuelle hypervisoren. Enhver kjerne nyere enn 3.0 vil gjøre det, inkludert 3.16 versjon i <span
                    class="distribution distribution">Jessie</span>.
					</div></li><li
                class="listitem"><div
                  class="para">
						i386 arkitekturen krever også et standard bibliotek med de riktige oppdateringer som drar nytte av Xen; Dette er i <span
                    class="pkg pkg">libc6-xen</span>-pakken.
					</div></li></ul></div><div
            class="para">
				For å unngå å måtte velge disse komponentene for hånd, er noen hjelpepakker tilgjengelige (for eksempel <span
              class="pkg pkg">xen-linux-system-amd64</span>). De trekker alle inn en kjent, god kombinasjon med de aktuelle hypervisor- og kjerne-pakkene. Hypervisoren har også med <span
              class="pkg pkg">xen-utils-4.4</span>, som inneholder verktøy for å kontrollere hypervisoren fra dom0. Dette bringer i sin tur det aktuelle standard biblioteket. Under installasjonen av alt dette, lager også konfigurasjonsskriptene en ny oppføring i Grub oppstart-menyen, slik som å starte den valgt kjernen i en Xen dom0. Merk imidlertid at denne inngangen vanligvis er satt som den første på listen, og vil derfor bli valgt som standard. Hvis det ikke er ønsket, vil følgende kommandoer endre det:
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen
</code></strong><code
              class="computeroutput"># </code><strong
              class="userinput"><code>update-grub
</code></strong></pre><div
            class="para">
				Når disse nødvendigheter er installert, er neste skritt å teste hvordan l dom0 selv virker. Dette innebærer omstart for hypervisoren og Xen-kjernen. Systemet skal starte på vanlig måte, med noen ekstra meldinger på konsollen under de tidlige initialiseringstrinnene.
			</div><div
            class="para">
				Nå er det faktisk på tide å installere nyttige systemer på domU-systemene med verktøy fra <span
              class="pkg pkg">xen-tools</span>. Denne pakken leverer <code
              class="command">xen-create-image</code>-kommandoen, som i stor grad automatiserer oppgaven. Den eneste nødvendige parameteren er <code
              class="literal">--hostname</code>, som gir navn til domU-en. Andre valg er viktige, men de kan lagres i <code
              class="filename">/etc/xen-tools/xen-tools.conf</code>-konfigurasjonsfilen, og fraværet deres fra kommandolinjen utløser ikke en feil. Det er derfor viktig å enten sjekke innholdet i denne filen før du oppretter bilder, eller å bruke ekstra parametre i bruken av <code
              class="command">xen-create-image</code>. Viktige parametre omfatter de følgende:
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--memory</code>, for å spesifisere hvor mye RAM som er øremerket til det systemet som nettopp er laget;
					</div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--size</code> og <code
                    class="literal">--swap</code>, for å definere størrelsen på den “virtual disks” som er tilgjengelig for domU-en;
					</div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--debootstrap</code>, to cause the new system to be installed with <code
                    class="command">debootstrap</code>; in that case, the <code
                    class="literal">--dist</code> option will also most often be used (with a distribution name such as <span
                    class="distribution distribution">jessie</span>).
					</div><div
                  class="sidebar"><div
                    class="titlepage"><div><div><p
                          class="title"><strong><span
                              class="emphasis"><em>GOING FURTHER</em></span> Installing a non-Debian system in a domU</strong></p></div></div></div><div
                    class="para">
						In case of a non-Linux system, care should be taken to define the kernel the domU must use, using the <code
                      class="literal">--kernel</code> option.
					</div></div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--dhcp</code> states that the domU's network configuration should be obtained by DHCP while <code
                    class="literal">--ip</code> allows defining a static IP address.
					</div></li><li
                class="listitem"><div
                  class="para">
						Lastly, a storage method must be chosen for the images to be created (those that will be seen as hard disk drives from the domU). The simplest method, corresponding to the <code
                    class="literal">--dir</code> option, is to create one file on the dom0 for each device the domU should be provided. For systems using LVM, the alternative is to use the <code
                    class="literal">--lvm</code> option, followed by the name of a volume group; <code
                    class="command">xen-create-image</code> will then create a new logical volume inside that group, and this logical volume will be made available to the domU as a hard disk drive.
					</div><div
                  class="sidebar"><div
                    class="titlepage"><div><div><p
                          class="title"><strong><span
                              class="emphasis"><em>NOTE</em></span> Storage in the domU</strong></p></div></div></div><div
                    class="para">
						Entire hard disks can also be exported to the domU, as well as partitions, RAID arrays or pre-existing LVM logical volumes. These operations are not automated by <code
                      class="command">xen-create-image</code>, however, so editing the Xen image's configuration file is in order after its initial creation with <code
                      class="command">xen-create-image</code>.
					</div></div></li></ul></div><div
            class="para">
				Once these choices are made, we can create the image for our future Xen domU:
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</code></strong>
<code
              class="computeroutput">
[...]
General Information
--------------------
Hostname       :  testxen
Distribution   :  jessie
Mirror         :  http://ftp.debian.org/debian/
Partitions     :  swap            128Mb (swap)
                  /               2G    (ext3)
Image type     :  sparse
Memory size    :  128Mb
Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64
Initrd path    :  /boot/initrd.img-3.16.0-4-amd64
[...]
Logfile produced at:
         /var/log/xen-tools/testxen.log

Installation Summary
---------------------
Hostname        :  testxen
Distribution    :  jessie
MAC Address     :  00:16:3E:8E:67:5C
IP-Address(es)  :  dynamic
RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b
Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ
</code></pre><div
            class="para">
				We now have a virtual machine, but it is currently not running (and therefore only using space on the dom0's hard disk). Of course, we can create more images, possibly with different parameters.
			</div><div
            class="para">
				Before turning these virtual machines on, we need to define how they'll be accessed. They can of course be considered as isolated machines, only accessed through their system console, but this rarely matches the usage pattern. Most of the time, a domU will be considered as a remote server, and accessed only through a network. However, it would be quite inconvenient to add a network card for each domU; which is why Xen allows creating virtual interfaces, that each domain can see and use in a standard way. Note that these cards, even though they're virtual, will only be useful once connected to a network, even a virtual one. Xen has several network models for that:
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						The simplest model is the <span
                    class="emphasis"><em>bridge</em></span> model; all the eth0 network cards (both in the dom0 and the domU systems) behave as if they were directly plugged into an Ethernet switch.
					</div></li><li
                class="listitem"><div
                  class="para">
						Then comes the <span
                    class="emphasis"><em>routing</em></span> model, where the dom0 behaves as a router that stands between the domU systems and the (physical) external network.
					</div></li><li
                class="listitem"><div
                  class="para">
						Finally, in the <span
                    class="emphasis"><em>NAT</em></span> model, the dom0 is again between the domU systems and the rest of the network, but the domU systems are not directly accessible from outside, and traffic goes through some network address translation on the dom0.
					</div></li></ul></div><div
            class="para">
				These three networking nodes involve a number of interfaces with unusual names, such as <code
              class="filename">vif*</code>, <code
              class="filename">veth*</code>, <code
              class="filename">peth*</code> and <code
              class="filename">xenbr0</code>. The Xen hypervisor arranges them in whichever layout has been defined, under the control of the user-space tools. Since the NAT and routing models are only adapted to particular cases, we will only address the bridging model.
			</div><div
            class="para">
				The standard configuration of the Xen packages does not change the system-wide network configuration. However, the <code
              class="command">xend</code> daemon is configured to integrate virtual network interfaces into any pre-existing network bridge (with <code
              class="filename">xenbr0</code> taking precedence if several such bridges exist). We must therefore set up a bridge in <code
              class="filename">/etc/network/interfaces</code> (which requires installing the <span
              class="pkg pkg">bridge-utils</span> package, which is why the <span
              class="pkg pkg">xen-utils-4.4</span> package recommends it) to replace the existing eth0 entry:
			</div><pre
            class="programlisting">auto xenbr0
iface xenbr0 inet dhcp
    bridge_ports eth0
    bridge_maxwait 0
</pre><div
            class="para">
				After rebooting to make sure the bridge is automatically created, we can now start the domU with the Xen control tools, in particular the <code
              class="command">xl</code> command. This command allows different manipulations on the domains, including listing them and, starting/stopping them.
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xl list</code></strong>
<code
              class="computeroutput">Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   463     1     r-----      9.8
# </code><strong
              class="userinput"><code>xl create /etc/xen/testxen.cfg</code></strong>
<code
              class="computeroutput">Parsing config from /etc/xen/testxen.cfg
# </code><strong
              class="userinput"><code>xl list</code></strong>
<code
              class="computeroutput">Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   366     1     r-----     11.4
testxen                                      1   128     1     -b----      1.1</code></pre><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>TOOL</em></span> Choice of toolstacks to manage Xen VM</strong></p></div></div></div><a
              id="id-1.15.5.12.24.2"
              class="indexterm"></a><a
              id="id-1.15.5.12.24.3"
              class="indexterm"></a><div
              class="para">
				In Debian 7 and older releases, <code
                class="command">xm</code> was the reference command line tool to use to manage Xen virtual machines. It has now been replaced by <code
                class="command">xl</code> which is mostly backwards compatible. But those are not the only available tools: <code
                class="command">virsh</code> of libvirt and <code
                class="command">xe</code> of XenServer's XAPI (commercial offering of Xen) are alternative tools.
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>CAUTION</em></span> Only one domU per image!</strong></p></div></div></div><div
              class="para">
				While it is of course possible to have several domU systems running in parallel, they will all need to use their own image, since each domU is made to believe it runs on its own hardware (apart from the small slice of the kernel that talks to the hypervisor). In particular, it isn't possible for two domU systems running simultaneously to share storage space. If the domU systems are not run at the same time, it is however quite possible to reuse a single swap partition, or the partition hosting the <code
                class="filename">/home</code> filesystem.
			</div></div><div
            class="para">
				Note that the <code
              class="filename">testxen</code> domU uses real memory taken from the RAM that would otherwise be available to the dom0, not simulated memory. Care should therefore be taken, when building a server meant to host Xen instances, to provision the physical RAM accordingly.
			</div><div
            class="para">
				Voilà! Our virtual machine is starting up. We can access it in one of two modes. The usual way is to connect to it “remotely” through the network, as we would connect to a real machine; this will usually require setting up either a DHCP server or some DNS configuration. The other way, which may be the only way if the network configuration was incorrect, is to use the <code
              class="filename">hvc0</code> console, with the <code
              class="command">xl console</code> command:
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xl console testxen</code></strong>
<code
              class="computeroutput">[...]

Debian GNU/Linux 8 testxen hvc0

testxen login: </code></pre><div
            class="para">
				One can then open a session, just like one would do if sitting at the virtual machine's keyboard. Detaching from this console is achieved through the <span
              class="keycap"><strong>Control</strong></span>+<span
              class="keycap"><strong>]</strong></span> key combination.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>TIP</em></span> Getting the console straight away</strong></p></div></div></div><div
              class="para">
				Sometimes one wishes to start a domU system and get to its console straight away; this is why the <code
                class="command">xl create</code> command takes a <code
                class="literal">-c</code> switch. Starting a domU with this switch will display all the messages as the system boots.
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>TOOL</em></span> OpenXenManager</strong></p></div></div></div><div
              class="para">
				OpenXenManager (in the <span
                class="pkg pkg">openxenmanager</span> package) is a graphical interface allowing remote management of Xen domains via Xen's API. It can thus control Xen domains remotely. It provides most of the features of the <code
                class="command">xl</code> command.
			</div></div><div
            class="para">
				Once the domU is up, it can be used just like any other server (since it is a GNU/Linux system after all). However, its virtual machine status allows some extra features. For instance, a domU can be temporarily paused then resumed, with the <code
              class="command">xl pause</code> and <code
              class="command">xl unpause</code> commands. Note that even though a paused domU does not use any processor power, its allocated memory is still in use. It may be interesting to consider the <code
              class="command">xl save</code> and <code
              class="command">xl restore</code> commands: saving a domU frees the resources that were previously used by this domU, including RAM. When restored (or unpaused, for that matter), a domU doesn't even notice anything beyond the passage of time. If a domU was running when the dom0 is shut down, the packaged scripts automatically save the domU, and restore it on the next boot. This will of course involve the standard inconvenience incurred when hibernating a laptop computer, for instance; in particular, if the domU is suspended for too long, network connections may expire. Note also that Xen is so far incompatible with a large part of ACPI power management, which precludes suspending the host (dom0) system.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>DOCUMENTATION</em></span> <code
                        class="command">xl</code> options</strong></p></div></div></div><div
              class="para">
				Most of the <code
                class="command">xl</code> subcommands expect one or more arguments, often a domU name. These arguments are well described in the <span
                class="citerefentry"><span
                  class="refentrytitle">xl</span>(1)</span> manual page.
			</div></div><div
            class="para">
				Halting or rebooting a domU can be done either from within the domU (with the <code
              class="command">shutdown</code> command) or from the dom0, with <code
              class="command">xl shutdown</code> or <code
              class="command">xl reboot</code>.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>GOING FURTHER</em></span> Advanced Xen</strong></p></div></div></div><div
              class="para">
				Xen has many more features than we can describe in these few paragraphs. In particular, the system is very dynamic, and many parameters for one domain (such as the amount of allocated memory, the visible hard drives, the behavior of the task scheduler, and so on) can be adjusted even when that domain is running. A domU can even be migrated across servers without being shut down, and without losing its network connections! For all these advanced aspects, the primary source of information is the official Xen documentation. <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://www.xen.org/support/documentation.html">http://www.xen.org/support/documentation.html</a></div>
			</div></div></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    xmlns=""
                    id="sect.lxc"></a>12.2.2. LXC</h3></div></div></div><a
            id="id-1.15.5.13.2"
            class="indexterm"></a><div
            class="para">
				Even though it is used to build “virtual machines”, LXC is not, strictly speaking, a virtualization system, but a system to isolate groups of processes from each other even though they all run on the same host. It takes advantage of a set of recent evolutions in the Linux kernel, collectively known as <span
              class="emphasis"><em>control groups</em></span>, by which different sets of processes called “groups” have different views of certain aspects of the overall system. Most notable among these aspects are the process identifiers, the network configuration, and the mount points. Such a group of isolated processes will not have any access to the other processes in the system, and its accesses to the filesystem can be restricted to a specific subset. It can also have its own network interface and routing table, and it may be configured to only see a subset of the available devices present on the system.
			</div><div
            class="para">
				These features can be combined to isolate a whole process family starting from the <code
              class="command">init</code> process, and the resulting set looks very much like a virtual machine. The official name for such a setup is a “container” (hence the LXC moniker: <span
              class="emphasis"><em>LinuX Containers</em></span>), but a rather important difference with “real” virtual machines such as provided by Xen or KVM is that there's no second kernel; the container uses the very same kernel as the host system. This has both pros and cons: advantages include excellent performance due to the total lack of overhead, and the fact that the kernel has a global vision of all the processes running on the system, so the scheduling can be more efficient than it would be if two independent kernels were to schedule different task sets. Chief among the inconveniences is the impossibility to run a different kernel in a container (whether a different Linux version or a different operating system altogether).
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>NOTE</em></span> LXC isolation limits</strong></p></div></div></div><div
              class="para">
				LXC containers do not provide the level of isolation achieved by heavier emulators or virtualizers. In particular:
			</div><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
						since the kernel is shared among the host system and the containers, processes constrained to containers can still access the kernel messages, which can lead to information leaks if messages are emitted by a container;
					</div></li><li
                  class="listitem"><div
                    class="para">
						for similar reasons, if a container is compromised and a kernel vulnerability is exploited, the other containers may be affected too;
					</div></li><li
                  class="listitem"><div
                    class="para">
						on the filesystem, the kernel checks permissions according to the numerical identifiers for users and groups; these identifiers may designate different users and groups depending on the container, which should be kept in mind if writable parts of the filesystem are shared among containers.
					</div></li></ul></div></div><div
            class="para">
				Since we are dealing with isolation and not plain virtualization, setting up LXC containers is more complex than just running debian-installer on a virtual machine. We will describe a few prerequisites, then go on to the network configuration; we will then be able to actually create the system to be run in the container.
			</div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.13.7"></a>12.2.2.1. Preliminary Steps</h4></div></div></div><div
              class="para">
					The <span
                class="pkg pkg">lxc</span> package contains the tools required to run LXC, and must therefore be installed.
				</div><div
              class="para">
					LXC also requires the <span
                class="emphasis"><em>control groups</em></span> configuration system, which is a virtual filesystem to be mounted on <code
                class="filename">/sys/fs/cgroup</code>. Since Debian 8 switched to systemd, which also relies on control groups, this is now done automatically at boot time without further configuration.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="sect.lxc.network"></a>12.2.2.2. Network Configuration</h4></div></div></div><div
              class="para">
					The goal of installing LXC is to set up virtual machines; while we could of course keep them isolated from the network, and only communicate with them via the filesystem, most use cases involve giving at least minimal network access to the containers. In the typical case, each container will get a virtual network interface, connected to the real network through a bridge. This virtual interface can be plugged either directly onto the host's physical network interface (in which case the container is directly on the network), or onto another virtual interface defined on the host (and the host can then filter or route traffic). In both cases, the <span
                class="pkg pkg">bridge-utils</span> package will be required.
				</div><div
              class="para">
					The simple case is just a matter of editing <code
                class="filename">/etc/network/interfaces</code>, moving the configuration for the physical interface (for instance <code
                class="literal">eth0</code>) to a bridge interface (usually <code
                class="literal">br0</code>), and configuring the link between them. For instance, if the network interface configuration file initially contains entries such as the following:
				</div><pre
              class="programlisting">auto eth0
iface eth0 inet dhcp</pre><div
              class="para">
					They should be disabled and replaced with the following:
				</div><pre
              class="programlisting">#auto eth0
#iface eth0 inet dhcp

auto br0
iface br0 inet dhcp
  bridge-ports eth0</pre><div
              class="para">
					The effect of this configuration will be similar to what would be obtained if the containers were machines plugged into the same physical network as the host. The “bridge” configuration manages the transit of Ethernet frames between all the bridged interfaces, which includes the physical <code
                class="literal">eth0</code> as well as the interfaces defined for the containers.
				</div><div
              class="para">
					In cases where this configuration cannot be used (for instance if no public IP addresses can be assigned to the containers), a virtual <span
                class="emphasis"><em>tap</em></span> interface will be created and connected to the bridge. The equivalent network topology then becomes that of a host with a second network card plugged into a separate switch, with the containers also plugged into that switch. The host must then act as a gateway for the containers if they are meant to communicate with the outside world.
				</div><div
              class="para">
					In addition to <span
                class="pkg pkg">bridge-utils</span>, this “rich” configuration requires the <span
                class="pkg pkg">vde2</span> package; the <code
                class="filename">/etc/network/interfaces</code> file then becomes:
				</div><pre
              class="programlisting"># Interface eth0 is unchanged
auto eth0
iface eth0 inet dhcp

# Virtual interface 
auto tap0
iface tap0 inet manual
  vde2-switch -t tap0

# Bridge for containers
auto br0
iface br0 inet static
  bridge-ports tap0
  address 10.0.0.1
  netmask 255.255.255.0
</pre><div
              class="para">
					The network can then be set up either statically in the containers, or dynamically with DHCP server running on the host. Such a DHCP server will need to be configured to answer queries on the <code
                class="literal">br0</code> interface.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.13.9"></a>12.2.2.3. Setting Up the System</h4></div></div></div><div
              class="para">
					Let us now set up the filesystem to be used by the container. Since this “virtual machine” will not run directly on the hardware, some tweaks are required when compared to a standard filesystem, especially as far as the kernel, devices and consoles are concerned. Fortunately, the <span
                class="pkg pkg">lxc</span> includes scripts that mostly automate this configuration. For instance, the following commands (which require the <span
                class="pkg pkg">debootstrap</span> and <span
                class="pkg pkg">rsync</span> packages) will install a Debian container:
				</div><pre
              class="screen"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-create -n testlxc -t debian
</code></strong><code
                class="computeroutput">debootstrap is /usr/sbin/debootstrap
Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... 
Downloading debian minimal ...
I: Retrieving Release 
I: Retrieving Release.gpg 
[...]
Download complete.
Copying rootfs to /var/lib/lxc/testlxc/rootfs...
[...]
Root password is 'sSiKhMzI', please change !
root@mirwiz:~# </code>
</pre><div
              class="para">
					Note that the filesystem is initially created in <code
                class="filename">/var/cache/lxc</code>, then moved to its destination directory. This allows creating identical containers much more quickly, since only copying is then required.
				</div><div
              class="para">
					Note that the debian template creation script accepts an <code
                class="option">--arch</code> option to specify the architecture of the system to be installed and a <code
                class="option">--release</code> option if you want to install something else than the current stable release of Debian. You can also set the <code
                class="literal">MIRROR</code> environment variable to point to a local Debian mirror.
				</div><div
              class="para">
					The newly-created filesystem now contains a minimal Debian system, and by default the container has no network interface (besides the loopback one). Since this is not really wanted, we will edit the container's configuration file (<code
                class="filename">/var/lib/lxc/testlxc/config</code>) and add a few <code
                class="literal">lxc.network.*</code> entries:
				</div><pre
              class="programlisting">lxc.network.type = veth
lxc.network.flags = up
lxc.network.link = br0
lxc.network.hwaddr = 4a:49:43:49:79:20
</pre><div
              class="para">
					These entries mean, respectively, that a virtual interface will be created in the container; that it will automatically be brought up when said container is started; that it will automatically be connected to the <code
                class="literal">br0</code> bridge on the host; and that its MAC address will be as specified. Should this last entry be missing or disabled, a random MAC address will be generated.
				</div><div
              class="para">
					Another useful entry in that file is the setting of the hostname:
				</div><pre
              class="programlisting">lxc.utsname = testlxc
</pre></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.13.10"></a>12.2.2.4. Starting the Container</h4></div></div></div><div
              class="para">
					Now that our virtual machine image is ready, let's start the container:
				</div><pre
              class="screen scale"
              width="94"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-start --daemon --name=testlxc
</code></strong><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-console -n testlxc
</code></strong><code
                class="computeroutput">Debian GNU/Linux 8 testlxc tty1

testlxc login: </code><strong
                class="userinput"><code>root</code></strong><code
                class="computeroutput">
Password: 
Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
root@testlxc:~# </code><strong
                class="userinput"><code>ps auxwf</code></strong>
<code
                class="computeroutput">USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init
root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald
root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D
root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux
root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux
root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux
root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     
root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \_ -bash
root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \_ ps auxfw
root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102
root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e
root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux
root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux
root@testlxc:~# </code></pre><div
              class="para">
					We are now in the container; our access to the processes is restricted to only those started from the container itself, and our access to the filesystem is similarly restricted to the dedicated subset of the full filesystem (<code
                class="filename">/var/lib/lxc/testlxc/rootfs</code>). We can exit the console with <span
                class="keycap"><strong>Control</strong></span>+<span
                class="keycap"><strong>a</strong></span> <span
                class="keycap"><strong>q</strong></span>.
				</div><div
              class="para">
					Note that we ran the container as a background process, thanks to the <code
                class="option">--daemon</code> option of <code
                class="command">lxc-start</code>. We can interrupt the container with a command such as <code
                class="command">lxc-stop --name=testlxc</code>.
				</div><div
              class="para">
					The <span
                class="pkg pkg">lxc</span> package contains an initialization script that can automatically start one or several containers when the host boots (it relies on <code
                class="command">lxc-autostart</code> which starts containers whose <code
                class="literal">lxc.start.auto</code> option is set to 1). Finer-grained control of the startup order is possible with <code
                class="literal">lxc.start.order</code> and <code
                class="literal">lxc.group</code>: by default, the initialization script first starts containers which are part of the <code
                class="literal">onboot</code> group and then the containers which are not part of any group. In both cases, the order within a group is defined by the <code
                class="literal">lxc.start.order</code> option.
				</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>GOING FURTHER</em></span> Mass virtualization</strong></p></div></div></div><div
                class="para">
					Since LXC is a very lightweight isolation system, it can be particularly adapted to massive hosting of virtual servers. The network configuration will probably be a bit more advanced than what we described above, but the “rich” configuration using <code
                  class="literal">tap</code> and <code
                  class="literal">veth</code> interfaces should be enough in many cases.
				</div><div
                class="para">
					It may also make sense to share part of the filesystem, such as the <code
                  class="filename">/usr</code> and <code
                  class="filename">/lib</code> subtrees, so as to avoid duplicating the software that may need to be common to several containers. This will usually be achieved with <code
                  class="literal">lxc.mount.entry</code> entries in the containers configuration file. An interesting side-effect is that the processes will then use less physical memory, since the kernel is able to detect that the programs are shared. The marginal cost of one extra container can then be reduced to the disk space dedicated to its specific data, and a few extra processes that the kernel must schedule and manage.
				</div><div
                class="para">
					We haven't described all the available options, of course; more comprehensive information can be obtained from the <span
                  class="citerefentry"><span
                    class="refentrytitle">lxc</span>(7)</span> and <span
                  class="citerefentry"><span
                    class="refentrytitle">lxc.container.conf</span>(5)</span> manual pages and the ones they reference.
				</div></div></div></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    xmlns=""
                    id="id-1.15.5.14"></a>12.2.3. Virtualization with KVM</h3></div></div></div><a
            id="id-1.15.5.14.2"
            class="indexterm"></a><div
            class="para">
				KVM, which stands for <span
              class="emphasis"><em>Kernel-based Virtual Machine</em></span>, is first and foremost a kernel module providing most of the infrastructure that can be used by a virtualizer, but it is not a virtualizer by itself. Actual control for the virtualization is handled by a QEMU-based application. Don't worry if this section mentions <code
              class="command">qemu-*</code> commands: it is still about KVM.
			</div><div
            class="para">
				Unlike other virtualization systems, KVM was merged into the Linux kernel right from the start. Its developers chose to take advantage of the processor instruction sets dedicated to virtualization (Intel-VT and AMD-V), which keeps KVM lightweight, elegant and not resource-hungry. The counterpart, of course, is that KVM doesn't work on any computer but only on those with appropriate processors. For x86-based computers, you can verify that you have such a processor by looking for “vmx” or “svm” in the CPU flags listed in <code
              class="filename">/proc/cpuinfo</code>.
			</div><div
            class="para">
				With Red Hat actively supporting its development, KVM has more or less become the reference for Linux virtualization.
			</div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.6"></a>12.2.3.1. Preliminary Steps</h4></div></div></div><a
              id="id-1.15.5.14.6.2"
              class="indexterm"></a><div
              class="para">
					Unlike such tools as VirtualBox, KVM itself doesn't include any user-interface for creating and managing virtual machines. The <span
                class="pkg pkg">qemu-kvm</span> package only provides an executable able to start a virtual machine, as well as an initialization script that loads the appropriate kernel modules.
				</div><a
              id="id-1.15.5.14.6.4"
              class="indexterm"></a><a
              id="id-1.15.5.14.6.5"
              class="indexterm"></a><div
              class="para">
					Fortunately, Red Hat also provides another set of tools to address that problem, by developing the <span
                class="emphasis"><em>libvirt</em></span> library and the associated <span
                class="emphasis"><em>virtual machine manager</em></span> tools. libvirt allows managing virtual machines in a uniform way, independently of the virtualization system involved behind the scenes (it currently supports QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare and UML). <code
                class="command">virtual-manager</code> is a graphical interface that uses libvirt to create and manage virtual machines.
				</div><a
              id="id-1.15.5.14.6.7"
              class="indexterm"></a><div
              class="para">
					We first install the required packages, with <code
                class="command">apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</code>. <span
                class="pkg pkg">libvirt-bin</span> provides the <code
                class="command">libvirtd</code> daemon, which allows (potentially remote) management of the virtual machines running of the host, and starts the required VMs when the host boots. In addition, this package provides the <code
                class="command">virsh</code> command-line tool, which allows controlling the <code
                class="command">libvirtd</code>-managed machines.
				</div><div
              class="para">
					The <span
                class="pkg pkg">virtinst</span> package provides <code
                class="command">virt-install</code>, which allows creating virtual machines from the command line. Finally, <span
                class="pkg pkg">virt-viewer</span> allows accessing a VM's graphical console.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.7"></a>12.2.3.2. Network Configuration</h4></div></div></div><div
              class="para">
					Just as in Xen and LXC, the most frequent network configuration involves a bridge grouping the network interfaces of the virtual machines (see <a
                class="xref"
                href="sect.virtualization.html#sect.lxc.network">Seksjon 12.2.2.2, «Network Configuration»</a>).
				</div><div
              class="para">
					Alternatively, and in the default configuration provided by KVM, the virtual machine is assigned a private address (in the 192.168.122.0/24 range), and NAT is set up so that the VM can access the outside network.
				</div><div
              class="para">
					The rest of this section assumes that the host has an <code
                class="literal">eth0</code> physical interface and a <code
                class="literal">br0</code> bridge, and that the former is connected to the latter.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.8"></a>12.2.3.3. Installation with <code
                      class="command">virt-install</code></h4></div></div></div><a
              id="id-1.15.5.14.8.2"
              class="indexterm"></a><div
              class="para">
					Creating a virtual machine is very similar to installing a normal system, except that the virtual machine's characteristics are described in a seemingly endless command line.
				</div><div
              class="para">
					Practically speaking, this means we will use the Debian installer, by booting the virtual machine on a virtual DVD-ROM drive that maps to a Debian DVD image stored on the host system. The VM will export its graphical console over the VNC protocol (see <a
                class="xref"
                href="sect.remote-login.html#sect.remote-desktops">Seksjon 9.2.2, «Å bruke eksterne grafiske skrivebord»</a> for details), which will allow us to control the installation process.
				</div><div
              class="para">
					We first need to tell libvirtd where to store the disk images, unless the default location (<code
                class="filename">/var/lib/libvirt/images/</code>) is fine.
				</div><pre
              class="screen"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>mkdir /srv/kvm</code></strong>
<code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>virsh pool-create-as srv-kvm dir --target /srv/kvm</code></strong>
<code
                class="computeroutput">Pool srv-kvm created

root@mirwiz:~# </code></pre><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>TIP</em></span> Add your user to the libvirt group</strong></p></div></div></div><div
                class="para">
					All samples in this section assume that you are running commands as root. Effectively, if you want to control a local libvirt daemon, you need either to be root or to be a member of the <code
                  class="literal">libvirt</code> group (which is not the case by default). Thus if you want to avoid using root rights too often, you can add yoursel to the <code
                  class="literal">libvirt</code> group and run the various commands under your user identity.
				</div></div><div
              class="para">
					Let us now start the installation process for the virtual machine, and have a closer look at <code
                class="command">virt-install</code>'s most important options. This command registers the virtual machine and its parameters in libvirtd, then starts it so that its installation can proceed.
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virt-install --connect qemu:///system  <span
                    id="virtinst.connect"><img
                      class="callout"
                      src="Common_Content/images/1.png"
                      alt="1" /></span>
               --virt-type kvm           <span
                    id="virtinst.type"><img
                      class="callout"
                      src="Common_Content/images/2.png"
                      alt="2" /></span>
               --name testkvm            <span
                    id="virtinst.name"><img
                      class="callout"
                      src="Common_Content/images/3.png"
                      alt="3" /></span>
               --ram 1024                <span
                    id="virtinst.ram"><img
                      class="callout"
                      src="Common_Content/images/4.png"
                      alt="4" /></span>
               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <span
                    id="virtinst.disk"><img
                      class="callout"
                      src="Common_Content/images/5.png"
                      alt="5" /></span>
               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <span
                    id="virtinst.cdrom"><img
                      class="callout"
                      src="Common_Content/images/6.png"
                      alt="6" /></span>
               --network bridge=br0      <span
                    id="virtinst.network"><img
                      class="callout"
                      src="Common_Content/images/7.png"
                      alt="7" /></span>
               --vnc                     <span
                    id="virtinst.vnc"><img
                      class="callout"
                      src="Common_Content/images/8.png"
                      alt="8" /></span>
               --os-type linux           <span
                    id="virtinst.os"><img
                      class="callout"
                      src="Common_Content/images/9.png"
                      alt="9" /></span>
               --os-variant debianwheezy
</code></strong><code
                class="computeroutput">
Starting install...
Allocating 'testkvm.qcow'             |  10 GB     00:00
Creating domain...                    |    0 B     00:00
Guest installation complete... restarting guest.
</code></pre><div
              class="calloutlist"><table
                border="0"
                summary="Callout list"><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.connect"><img
                          class="callout"
                          src="Common_Content/images/1.png"
                          alt="1" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							The <code
                        class="literal">--connect</code> option specifies the “hypervisor” to use. Its form is that of an URL containing a virtualization system (<code
                        class="literal">xen://</code>, <code
                        class="literal">qemu://</code>, <code
                        class="literal">lxc://</code>, <code
                        class="literal">openvz://</code>, <code
                        class="literal">vbox://</code>, and so on) and the machine that should host the VM (this can be left empty in the case of the local host). In addition to that, and in the QEMU/KVM case, each user can manage virtual machines working with restricted permissions, and the URL path allows differentiating “system” machines (<code
                        class="literal">/system</code>) from others (<code
                        class="literal">/session</code>).
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.type"><img
                          class="callout"
                          src="Common_Content/images/2.png"
                          alt="2" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							Since KVM is managed the same way as QEMU, the <code
                        class="literal">--virt-type kvm</code> allows specifying the use of KVM even though the URL looks like QEMU.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.name"><img
                          class="callout"
                          src="Common_Content/images/3.png"
                          alt="3" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							The <code
                        class="literal">--name</code> option defines a (unique) name for the virtual machine.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.ram"><img
                          class="callout"
                          src="Common_Content/images/4.png"
                          alt="4" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							The <code
                        class="literal">--ram</code> option allows specifying the amount of RAM (in MB) to allocate for the virtual machine.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.disk"><img
                          class="callout"
                          src="Common_Content/images/5.png"
                          alt="5" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							The <code
                        class="literal">--disk</code> specifies the location of the image file that is to represent our virtual machine's hard disk; that file is created, unless present, with a size (in GB) specified by the <code
                        class="literal">size</code> parameter. The <code
                        class="literal">format</code> parameter allows choosing among several ways of storing the image file. The default format (<code
                        class="literal">raw</code>) is a single file exactly matching the disk's size and contents. We picked a more advanced format here, that is specific to QEMU and allows starting with a small file that only grows when the virtual machine starts actually using space.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.cdrom"><img
                          class="callout"
                          src="Common_Content/images/6.png"
                          alt="6" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							The <code
                        class="literal">--cdrom</code> option is used to indicate where to find the optical disk to use for installation. The path can be either a local path for an ISO file, an URL where the file can be obtained, or the device file of a physical CD-ROM drive (i.e. <code
                        class="literal">/dev/cdrom</code>).
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.network"><img
                          class="callout"
                          src="Common_Content/images/7.png"
                          alt="7" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							The <code
                        class="literal">--network</code> specifies how the virtual network card integrates in the host's network configuration. The default behavior (which we explicitly forced in our example) is to integrate it into any pre-existing network bridge. If no such bridge exists, the virtual machine will only reach the physical network through NAT, so it gets an address in a private subnet range (192.168.122.0/24).
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.vnc"><img
                          class="callout"
                          src="Common_Content/images/8.png"
                          alt="8" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--vnc</code> states that the graphical console should be made available using VNC. The default behavior for the associated VNC server is to only listen on the local interface; if the VNC client is to be run on a different host, establishing the connection will require setting up an SSH tunnel (see <a
                        class="xref"
                        href="sect.remote-login.html#sect.ssh-port-forwarding">Seksjon 9.2.1.3, «Å lage krypterte tunneler med portvideresending (Port Forwarding)»</a>). Alternatively, the <code
                        class="literal">--vnclisten=0.0.0.0</code> can be used so that the VNC server is accessible from all interfaces; note that if you do that, you really should design your firewall accordingly.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.os"><img
                          class="callout"
                          src="Common_Content/images/9.png"
                          alt="9" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							The <code
                        class="literal">--os-type</code> and <code
                        class="literal">--os-variant</code> options allow optimizing a few parameters of the virtual machine, based on some of the known features of the operating system mentioned there.
						</div></td></tr></table></div><div
              class="para">
					At this point, the virtual machine is running, and we need to connect to the graphical console to proceed with the installation process. If the previous operation was run from a graphical desktop environment, this connection should be automatically started. If not, or if we operate remotely, <code
                class="command">virt-viewer</code> can be run from any graphical environment to open the graphical console (note that the root password of the remote host is asked twice because the operation requires 2 SSH connections):
				</div><pre
              class="screen"><code
                class="computeroutput">$ </code><strong
                class="userinput"><code>virt-viewer --connect qemu+ssh://root@<em
                    class="replaceable">server</em>/system testkvm
</code></strong><code
                class="computeroutput">root@server's password: 
root@server's password: </code></pre><div
              class="para">
					When the installation process ends, the virtual machine is restarted, now ready for use.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.9"></a>12.2.3.4. Managing Machines with <code
                      class="command">virsh</code></h4></div></div></div><a
              id="id-1.15.5.14.9.2"
              class="indexterm"></a><div
              class="para">
					Now that the installation is done, let us see how to handle the available virtual machines. The first thing to try is to ask <code
                class="command">libvirtd</code> for the list of the virtual machines it manages:
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system list --all
 Id Name                 State
----------------------------------
  - testkvm              shut off
</code></strong></pre><div
              class="para">
					Let's start our test virtual machine:
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system start testkvm
</code></strong><code
                class="computeroutput">Domain testkvm started</code></pre><div
              class="para">
					We can now get the connection instructions for the graphical console (the returned VNC display can be given as parameter to <code
                class="command">vncviewer</code>):
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system vncdisplay testkvm
</code></strong><code
                class="computeroutput">:0</code></pre><div
              class="para">
					Other available <code
                class="command">virsh</code> subcommands include:
				</div><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">reboot</code> to restart a virtual machine;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">shutdown</code> to trigger a clean shutdown;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">destroy</code>, to stop it brutally;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">suspend</code> to pause it;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">resume</code> to unpause it;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">autostart</code> to enable (or disable, with the <code
                      class="literal">--disable</code> option) starting the virtual machine automatically when the host starts;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">undefine</code> to remove all traces of the virtual machine from <code
                      class="command">libvirtd</code>.
						</div></li></ul></div><div
              class="para">
					All these subcommands take a virtual machine identifier as a parameter.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.10"></a>12.2.3.5. Installing an RPM based system in Debian with yum</h4></div></div></div><div
              class="para">
					If the virtual machine is meant to run a Debian (or one of its derivatives), the system can be initialized with <code
                class="command">debootstrap</code>, as described above. But if the virtual machine is to be installed with an RPM-based system (such as Fedora, CentOS or Scientific Linux), the setup will need to be done using the <code
                class="command">yum</code> utility (available in the package of the same name).
				</div><div
              class="para">
					The procedure requires using <code
                class="command">rpm</code> to extract an initial set of files, including notably <code
                class="command">yum</code> configuration files, and then calling <code
                class="command">yum</code> to extract the remaining set of packages. But since we call <code
                class="command">yum</code> from outside the chroot, we need to make some temporary changes. In the sample below, the target chroot is <code
                class="filename">/srv/centos</code>.
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>rootdir="/srv/centos"
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>mkdir -p "$rootdir" /etc/rpm
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>echo "%_dbpath /var/lib/rpm" &gt; /etc/rpm/macros.dbpath
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>rpm --nodeps --root "$rootdir" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</code></strong><code
                class="computeroutput">rpm: RPM should not be used directly install RPM packages, use Alien instead!
rpm: However assuming you know what you are doing...
warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY
# </code><strong
                class="userinput"><code>sed -i -e "s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g" $rootdir/etc/yum.repos.d/*.repo
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>yum --assumeyes --installroot $rootdir groupinstall core
</code></strong><code
                class="computeroutput">[...]
# </code><strong
                class="userinput"><code>sed -i -e "s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g" $rootdir/etc/yum.repos.d/*.repo
</code></strong></pre></div></div></div><ul
        class="docnav"><li
          class="previous"><a
            accesskey="p"
            href="advanced-administration.html"><strong>Forrige</strong>Kapittel 12. Avansert administrasjon</a></li><li
          class="up"><a
            accesskey="u"
            href="#"><strong>Opp</strong></a></li><li
          class="home"><a
            accesskey="h"
            href="index.html"><strong>Hjem</strong></a></li><li
          class="next"><a
            accesskey="n"
            href="sect.automated-installation.html"><strong>Neste</strong>12.3. Automated Installation</a></li></ul></body></html>
