<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html
    xmlns="http://www.w3.org/1999/xhtml"><head><meta
        http-equiv="Content-Type"
        content="text/html; charset=UTF-8" /><title
        xmlns:d="http://docbook.org/ns/docbook">12.2. Virtualisasi</title><link
        rel="stylesheet"
        type="text/css"
        href="Common_Content/css/default.css" /><link
        rel="stylesheet"
        media="print"
        href="Common_Content/css/print.css"
        type="text/css" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="generator"
        content="publican v4.3.2" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="package"
        content="Debian-debian-handbook-9-id-ID-1.0-1" /><meta
        name="keywords"
        content="RAID, LVM, FAI, Preseeding, Pemantauan, Virtualisasi, Xen, LXC" /><link
        rel="home"
        href="index.html"
        title="Buku Panduan Administrator Debian" /><link
        rel="up"
        href="advanced-administration.html"
        title="Bab 12. Administrasi Tingkat Lanjut" /><link
        rel="prev"
        href="advanced-administration.html"
        title="Bab 12. Administrasi Tingkat Lanjut" /><link
        rel="next"
        href="sect.automated-installation.html"
        title="12.3. Pemasangan Otomatis" /><meta
        xmlns=""
        name="flattr:id"
        content="4pz9jq" /><link
        xmlns=""
        rel="canonical"
        href="https://debian-handbook.info/browse/id-ID/stable/sect.virtualization.html" /></head><body><div
        id="banner"><a
          href="http://debian-handbook.info/get/"><span
            class="text">Download the ebook</span></a></div><p
        id="title"><a
          class="left"
          href="http://www.debian.org"><img
            alt="Product Site"
            src="Common_Content/images//image_left.png" /></a><a
          class="right"
          href="index.html"><img
            alt="Documentation Site"
            src="Common_Content/images//image_right.png" /></a></p><ul
        class="docnav top"><li
          class="previous"><a
            accesskey="p"
            href="advanced-administration.html"><strong>Sebelumnya</strong></a></li><li
          class="home">Buku Panduan Administrator Debian</li><li
          class="next"><a
            accesskey="n"
            href="sect.automated-installation.html"><strong>Lanjut</strong></a></li></ul><div
        class="section"><div
          class="titlepage"><div><div><h2
                class="title"><a
                  xmlns=""
                  id="sect.virtualization"></a>12.2. Virtualisasi</h2></div></div></div><a
          id="id-1.15.5.2"
          class="indexterm"></a><div
          class="para">
			Virtualisasi adalah salah satu kemajuan yang paling besar dalam beberapa tahun terakhir komputasi. Istilah ini mencakup berbagai abstraksi dan teknik simulasi komputer virtual dengan tingkat kebebasan yang variabel pada perangkat keras sebenarnya. Satu server fisik kemudian dapat mewadahi beberapa sistem yang bekerja pada waktu yang sama dan dalam isolasi. Ada banyak aplikasi, dan sering diturunkan dari isolasi ini: lingkungan uji dengan berbagai konfigurasi misalnya, atau pemisahan layanan kebeberapa mesin virtual untuk keamanan.
		</div><div
          class="para">
			Ada beberapa solusi virtualisasi, masing-masing dengan pro dan kontra. Buku ini akan fokus pada Xen, LXC, dan KVM, tetapi implementasi penting lain adalah sebagai berikut:
		</div><a
          id="id-1.15.5.5"
          class="indexterm"></a><a
          id="id-1.15.5.6"
          class="indexterm"></a><a
          id="id-1.15.5.7"
          class="indexterm"></a><a
          id="id-1.15.5.8"
          class="indexterm"></a><a
          id="id-1.15.5.9"
          class="indexterm"></a><a
          id="id-1.15.5.10"
          class="indexterm"></a><div
          xmlns:d="http://docbook.org/ns/docbook"
          class="itemizedlist"><ul><li
              class="listitem"><div
                class="para">
					QEMU adalah emulator perangkat lunak untuk sebuah komputer lengkap; kinerja jauh dari kecepatan yang bisa dicapai ketika berjalan secara native, tetapi ini memungkinkan menjalankan sistem operasi tanpa perubahan atau eksperimental pada perangkat keras yang diemulasi. Hal ini juga memungkinkan mengemulasi arsitektur perangkat keras yang berbeda: sebagai contoh, sistem <span
                  class="emphasis"><em>amd64</em></span> bisa mengemulasi komputer <span
                  class="emphasis"><em>arm</em></span>. QEMU adalah perangkat lunak bebas. <div
                  xmlns=""
                  class="url">→ <a
                    xmlns="http://www.w3.org/1999/xhtml"
                    href="http://www.qemu.org/">http://www.qemu.org/</a></div>
				</div></li><li
              class="listitem"><div
                class="para">
					Bochs adalah mesin virtual lain yang bebas, tapi itu hanya mengemulasi arsitektur x86 (i386 dan amd64).
				</div></li><li
              class="listitem"><div
                class="para">
					VMWare adalah sebuah mesin virtual yang proprietari; salah satu yang tertua di luar sana, itu juga satu dari yang paling dikenal. Itu bekerja dengan prinsip yang serupa dengan QEMU. VMWare menyediakan fitur-fitur tingkat lanjut seperti membuat snapshot dari sebuah mesin virtual yang sedang berjalan. <div
                  xmlns=""
                  class="url">→ <a
                    xmlns="http://www.w3.org/1999/xhtml"
                    href="http://www.vmware.com/">http://www.vmware.com/</a></div>
				</div></li><li
              class="listitem"><div
                class="para">
					VirtualBox is a virtual machine that is mostly free software (some extra components are available under a proprietary license). Unfortunately it is in Debian's “contrib” section because it includes some precompiled files that cannot be rebuilt without a proprietary compiler and it currently only resides in Debian Unstable as Oracle's policies make it impossible to keep it secure in a Debian stable release (see <a
                  href="https://bugs.debian.org/794466">#794466</a>). While younger than VMWare and restricted to the i386 and amd64 architectures, it still includes some snapshotting and other interesting features. <div
                  xmlns=""
                  class="url">→ <a
                    xmlns="http://www.w3.org/1999/xhtml"
                    href="http://www.virtualbox.org/">http://www.virtualbox.org/</a></div>
				</div></li></ul></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    xmlns=""
                    id="sect.xen"></a>12.2.1. Xen</h3></div></div></div><div
            class="para">
				Xen <a
              id="id-1.15.5.12.2.1"
              class="indexterm"></a> adalah sebuah solusi "paravirtualization". Ini memperkenalkan lapisan abstraksi tipis, dinamai "hypervisor", antara perangkat keras dan sistem di atas; ini bekerja sebagai wasit yang mengendalikan akses ke perangkat keras dari mesin-mesin virtual. Namun, itu hanya menangani beberapa instruksi, sisanya dieksekusi secara langsung oleh perangkat keras atas nama sistem. Keuntungan utama adalah kinerja tidak menurun, dan sistem berjalan mendekati kecepatan native; kekurangannya adalah kernal dari sistem operasi yang ingin dipakai pada suatu hypervisor Xen perlu diadaptasi untuk berjalan pada Xen.
			</div><div
            class="para">
				Let's spend some time on terms. The hypervisor is the lowest layer, that runs directly on the hardware, even below the kernel. This hypervisor can split the rest of the software across several <span
              class="emphasis"><em>domains</em></span>, which can be seen as so many virtual machines. One of these domains (the first one that gets started) is known as <span
              class="emphasis"><em>dom0</em></span>, and has a special role, since only this domain can control the hypervisor and the execution of other domains. These other domains are known as <span
              class="emphasis"><em>domU</em></span>. In other words, and from a user point of view, the <span
              class="emphasis"><em>dom0</em></span> matches the “host” of other virtualization systems, while a <span
              class="emphasis"><em>domU</em></span> can be seen as a “guest”.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>KULTUR</em></span> Xen dan berbagai versi Linux</strong></p></div></div></div><div
              class="para">
				Xen ini awalnya dikembangkan sebagai seperangkat patch yang berada di luar pohon resmi, dan tidak terintegrasi ke kernel Linux. Pada saat yang sama, beberapa sistem virtualisasi mendatang (termasuk KVM) memerlukan beberapa fungsi generik terkait virtualisasi untuk memfasilitasi integrasi mereka, dan kernel Linux memperoleh set fungsi (dikenal sebagai antarmuka <span
                class="emphasis"><em>paravirt_ops </em></span> atau <span
                class="emphasis"><em>pv_ops</em></span>). Karena patch Xen menduplikasi beberapa fungsionalitas antar muka ini, mereka tidak bisa diterima secara resmi.
			</div><div
              class="para">
				XenSource, perusahaan di belakang Xen, karena itu harus mem-port Xen ke kerangka baru ini, sehingga patch Xen bisa digabungkan ke kernel Linux resmi. Itu berarti banyak penulisan ulang kode, dan meskipun Xensource segera memiliki versi yang bekerja berdasarkan antarmuka paravirt_ops, patch hanya digabungkan ke dalam kernel resmi secara progresif. Penggabungan selesai pada Linux 3.0. <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://wiki.xenproject.org/wiki/XenParavirtOps">http://wiki.xenproject.org/wiki/XenParavirtOps</a></div>
			</div><div
              class="para">
				Karena <span
                class="distribution distribution">Jessie</span> didasarkan pada kernel Linux versi 3.16, paket-paket standar <span
                class="pkg pkg">linux-image-686-pae</span> dan <span
                class="pkg pkg">linux-image-amd64</span> menyertakan kode yang diperlukan, dan patch spesifik distribusi yang diperlukan untuk <span
                class="distribution distribution">Squeeze</span> dan versi sebelumnya dari Debian tidak diperlukan lagi. <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix">http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix</a></div>
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>CATATAN</em></span> Arsitektur yang kompatibel dengan Xen</strong></p></div></div></div><div
              class="para">
				Xen saat ini hanya tersedia untuk arsitektur i386, amd64, arm64, dan armhf.
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>BUDAYA</em></span> Xen dan kernel bukan Linux</strong></p></div></div></div><div
              class="para">
				Xen memerlukan modifikasi ke semua sistem operasi yang ingin berjalan di atasnya; tidak semua kernel memiliki tingkat kedewasaan yang sama dalam hal ini. Banyak yang fungsional-penuh, baik sebagai dom0 maupun domU: Linux 3.0 dan setelahnya, NetBSD 4.0 dan setelahnya, dan OpenSolaris. Yang lain hanya bekerja sebagai domU. Anda dapat memeriksa status dari setiap sistem operasi di wiki Xen: <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen">http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen</a></div> <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://wiki.xenproject.org/wiki/DomU_Support_for_Xen">http://wiki.xenproject.org/wiki/DomU_Support_for_Xen</a></div>
			</div><div
              class="para">
				Namun, jika Xen dapat bergantung pada fungsi perangkat keras yang didedikasikan untuk virtualisasi (yang hanya hadir dalam prosesor yang lebih baru), bahkan sistem operasi tanpa modifikasi dapat berjalan sebagai domU (termasuk Windows).
			</div></div><div
            class="para">
				Menggunakan Xen di bawah Debian memerlukan tiga komponen:
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						Hypervisor itu sendiri. Tergantung dari perangkat keras yang tersedia, paket yang tepat adalah <span
                    class="pkg pkg">xen-hypervisor-4.4-amd64</span>, <span
                    class="pkg pkg">xen-hypervisor-4.4-armhf</span>, atau <span
                    class="pkg pkg">xen-hypervisor-4.4-arm64</span>.
					</div></li><li
                class="listitem"><div
                  class="para">
						Sebuah kernel yang berjalan pada hypervisor itu. Setiap kernel yang lebih baru daripada 3.0 bisa, termasuk versi 3.16 yang ada dalam <span
                    class="distribution distribution">Jessie</span>.
					</div></li><li
                class="listitem"><div
                  class="para">
						Arsitektur i386 juga memerlukan pustaka standar dengan patch yang sesuai yang mengambil keuntungan dari Xen; ini ada dalam paket <span
                    class="pkg pkg">libc6-xen</span>.
					</div></li></ul></div><div
            class="para">
				In order to avoid the hassle of selecting these components by hand, a few convenience packages (such as <span
              class="pkg pkg">xen-linux-system-amd64</span>) have been made available; they all pull in a known-good combination of the appropriate hypervisor and kernel packages. The hypervisor also brings <span
              class="pkg pkg">xen-utils-4.4</span>, which contains tools to control the hypervisor from the dom0. This in turn brings the appropriate standard library. During the installation of all that, configuration scripts also create a new entry in the Grub bootloader menu, so as to start the chosen kernel in a Xen dom0. Note however that this entry is not usually set to be the first one in the list, and will therefore not be selected by default. If that is not the desired behavior, the following commands will change it:
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen
</code></strong><code
              class="computeroutput"># </code><strong
              class="userinput"><code>update-grub
</code></strong></pre><div
            class="para">
				Setelah prapersyaratan ini diinstal, langkah berikutnya adalah untuk menguji perilaku dom0 sendiri; ini melibatkan reboot hypervisor dan kernel Xen. Sistem harus boot dalam cara standar, dengan beberapa tambahan pesan pada konsol selama langkah inisialisasi awal.
			</div><div
            class="para">
				Now is the time to actually install useful systems on the domU systems, using the tools from <span
              class="pkg pkg">xen-tools</span>. This package provides the <code
              class="command">xen-create-image</code> command, which largely automates the task. The only mandatory parameter is <code
              class="literal">--hostname</code>, giving a name to the domU; other options are important, but they can be stored in the <code
              class="filename">/etc/xen-tools/xen-tools.conf</code> configuration file, and their absence from the command line doesn't trigger an error. It is therefore important to either check the contents of this file before creating images, or to use extra parameters in the <code
              class="command">xen-create-image</code> invocation. Important parameters of note include the following:
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--memory</code>, untuk menentukan banyaknya RAM yang didedikasikan bagi sistem yang baru dibuat;
					</div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--size</code> dan <code
                    class="literal">--swap</code>, untuk menentukan ukuran "disk virtual" yang tersedia bagi domU;
					</div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--debootstrap</code>, to cause the new system to be installed with <code
                    class="command">debootstrap</code>; in that case, the <code
                    class="literal">--dist</code> option will also most often be used (with a distribution name such as <span
                    class="distribution distribution">jessie</span>).
					</div><div
                  class="sidebar"><div
                    class="titlepage"><div><div><p
                          class="title"><strong><span
                              class="emphasis"><em>LEBIH JAUH</em></span> Memasang sistem bukan-Debian di domU</strong></p></div></div></div><div
                    class="para">
						In case of a non-Linux system, care should be taken to define the kernel the domU must use, using the <code
                      class="literal">--kernel</code> option.
					</div></div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--dhcp</code> states that the domU's network configuration should be obtained by DHCP while <code
                    class="literal">--ip</code> allows defining a static IP address.
					</div></li><li
                class="listitem"><div
                  class="para">
						Lastly, a storage method must be chosen for the images to be created (those that will be seen as hard disk drives from the domU). The simplest method, corresponding to the <code
                    class="literal">--dir</code> option, is to create one file on the dom0 for each device the domU should be provided. For systems using LVM, the alternative is to use the <code
                    class="literal">--lvm</code> option, followed by the name of a volume group; <code
                    class="command">xen-create-image</code> will then create a new logical volume inside that group, and this logical volume will be made available to the domU as a hard disk drive.
					</div><div
                  class="sidebar"><div
                    class="titlepage"><div><div><p
                          class="title"><strong><span
                              class="emphasis"><em>CATATAN</em></span> Penyimpanan di domU</strong></p></div></div></div><div
                    class="para">
						Seluruh hard disk dapat juga diekspor ke domU, maupun partisi, larik RAID, atau volume logis LVM yang sudah ada sebelumnya. Namun operasi ini tidak diotomatiskan oleh <code
                      class="command">xen-create-image</code>, jadi perlu menyunting berkas konfigurasi image Xen setelah penciptaan awal dengan <code
                      class="command">xen-create-image</code>.
					</div></div></li></ul></div><div
            class="para">
				Setelah pilihan ini dibuat, kita dapat membuat image untuk domU Xen kita nanti:
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</code></strong>
<code
              class="computeroutput">
[...]
General Information
--------------------
Hostname       :  testxen
Distribution   :  jessie
Mirror         :  http://ftp.debian.org/debian/
Partitions     :  swap            128Mb (swap)
                  /               2G    (ext3)
Image type     :  sparse
Memory size    :  128Mb
Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64
Initrd path    :  /boot/initrd.img-3.16.0-4-amd64
[...]
Logfile produced at:
         /var/log/xen-tools/testxen.log

Installation Summary
---------------------
Hostname        :  testxen
Distribution    :  jessie
MAC Address     :  00:16:3E:8E:67:5C
IP-Address(es)  :  dynamic
RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b
Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ
</code></pre><div
            class="para">
				Kita sekarang memiliki mesin virtual, tetapi saat ini tidak berjalan (dan karena itu hanya menggunakan ruang hard disk dom0). Tentu saja, kita dapat membuat lebih banyak image, mungkin dengan parameter yang berbeda.
			</div><div
            class="para">
				Before turning these virtual machines on, we need to define how they'll be accessed. They can of course be considered as isolated machines, only accessed through their system console, but this rarely matches the usage pattern. Most of the time, a domU will be considered as a remote server, and accessed only through a network. However, it would be quite inconvenient to add a network card for each domU; which is why Xen allows creating virtual interfaces, that each domain can see and use in a standard way. Note that these cards, even though they're virtual, will only be useful once connected to a network, even a virtual one. Xen has several network models for that:
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						Model yang paling sederhana adalah model <span
                    class="emphasis"><em>bridge</em></span>; semua kartu jaringan eth0 (baik dalam dom0 dan sistem domU) bersikap seolah-olah mereka secara langsung terhubung ke switch Ethernet.
					</div></li><li
                class="listitem"><div
                  class="para">
						Kemudian ada model <span
                    class="emphasis"><em>routing</em></span>, dimana dom0 berperilaku sebagai router yang berdiri di antara sistem domU dan jaringan eksternal (fisik).
					</div></li><li
                class="listitem"><div
                  class="para">
						Akhirnya, dalam model <span
                    class="emphasis"><em>NAT</em></span>, dom0 lagi-lagi berada di antara sistem domU dan sisa jaringan, tetapi sistem domU tidak langsung dapat diakses dari luar, dan lalu lintas berjalan melalui perjemahan alamat jaringan pada dom0.
					</div></li></ul></div><div
            class="para">
				These three networking nodes involve a number of interfaces with unusual names, such as <code
              class="filename">vif*</code>, <code
              class="filename">veth*</code>, <code
              class="filename">peth*</code> and <code
              class="filename">xenbr0</code>. The Xen hypervisor arranges them in whichever layout has been defined, under the control of the user-space tools. Since the NAT and routing models are only adapted to particular cases, we will only address the bridging model.
			</div><div
            class="para">
				The standard configuration of the Xen packages does not change the system-wide network configuration. However, the <code
              class="command">xend</code> daemon is configured to integrate virtual network interfaces into any pre-existing network bridge (with <code
              class="filename">xenbr0</code> taking precedence if several such bridges exist). We must therefore set up a bridge in <code
              class="filename">/etc/network/interfaces</code> (which requires installing the <span
              class="pkg pkg">bridge-utils</span> package, which is why the <span
              class="pkg pkg">xen-utils-4.4</span> package recommends it) to replace the existing eth0 entry:
			</div><pre
            class="programlisting">auto xenbr0
iface xenbr0 inet dhcp
    bridge_ports eth0
    bridge_maxwait 0
</pre><div
            class="para">
				Setelah reboot untuk memastikan bridge secara otomatis dibuat, kita sekarang dapat memulai domU dengan perkakas kendali Xen, khususnya perintah <code
              class="command">xl</code>. Perintah ini memungkinkan manipulasi yang berbeda pada domain, termasuk menampilkan daftar mereka dan, memulai/menghentikan mereka.
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xl list</code></strong>
<code
              class="computeroutput">Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   463     1     r-----      9.8
# </code><strong
              class="userinput"><code>xl create /etc/xen/testxen.cfg</code></strong>
<code
              class="computeroutput">Parsing config from /etc/xen/testxen.cfg
# </code><strong
              class="userinput"><code>xl list</code></strong>
<code
              class="computeroutput">Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   366     1     r-----     11.4
testxen                                      1   128     1     -b----      1.1</code></pre><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>PERKAKAS</em></span> Pilihan kumpulan perkakas untuk mengelola VM Xen</strong></p></div></div></div><a
              id="id-1.15.5.12.24.2"
              class="indexterm"></a><a
              id="id-1.15.5.12.24.3"
              class="indexterm"></a><div
              class="para">
				In Debian 7 and older releases, <code
                class="command">xm</code> was the reference command line tool to use to manage Xen virtual machines. It has now been replaced by <code
                class="command">xl</code> which is mostly backwards compatible. But those are not the only available tools: <code
                class="command">virsh</code> of libvirt and <code
                class="command">xe</code> of XenServer's XAPI (commercial offering of Xen) are alternative tools.
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>HATI-HATI</em></span> Hanya satu domU per image!</strong></p></div></div></div><div
              class="para">
				While it is of course possible to have several domU systems running in parallel, they will all need to use their own image, since each domU is made to believe it runs on its own hardware (apart from the small slice of the kernel that talks to the hypervisor). In particular, it isn't possible for two domU systems running simultaneously to share storage space. If the domU systems are not run at the same time, it is however quite possible to reuse a single swap partition, or the partition hosting the <code
                class="filename">/home</code> filesystem.
			</div></div><div
            class="para">
				Note that the <code
              class="filename">testxen</code> domU uses real memory taken from the RAM that would otherwise be available to the dom0, not simulated memory. Care should therefore be taken, when building a server meant to host Xen instances, to provision the physical RAM accordingly.
			</div><div
            class="para">
				Voilà! Our virtual machine is starting up. We can access it in one of two modes. The usual way is to connect to it “remotely” through the network, as we would connect to a real machine; this will usually require setting up either a DHCP server or some DNS configuration. The other way, which may be the only way if the network configuration was incorrect, is to use the <code
              class="filename">hvc0</code> console, with the <code
              class="command">xl console</code> command:
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xl console testxen</code></strong>
<code
              class="computeroutput">[...]

Debian GNU/Linux 8 testxen hvc0

testxen login: </code></pre><div
            class="para">
				One can then open a session, just like one would do if sitting at the virtual machine's keyboard. Detaching from this console is achieved through the <span
              class="keycap"><strong>Control</strong></span>+<span
              class="keycap"><strong>]</strong></span> key combination.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>TIPS</em></span> Mendapatkan konsol langsung</strong></p></div></div></div><div
              class="para">
				Sometimes one wishes to start a domU system and get to its console straight away; this is why the <code
                class="command">xl create</code> command takes a <code
                class="literal">-c</code> switch. Starting a domU with this switch will display all the messages as the system boots.
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>ALAT</em></span> OpenXenManager</strong></p></div></div></div><div
              class="para">
				OpenXenManager (dalam paket <span
                class="pkg pkg"> openxenmanager</span>) adalah antarmuka grafis yang memungkinkan manajemen domain Xen jarak jauh melalui API Xen. Itu dapat mengontrol domain Xen dari jauh. Itu menyediakan sebagian besar fitur dari perintah <code
                class="command">xl</code>.
			</div></div><div
            class="para">
				Once the domU is up, it can be used just like any other server (since it is a GNU/Linux system after all). However, its virtual machine status allows some extra features. For instance, a domU can be temporarily paused then resumed, with the <code
              class="command">xl pause</code> and <code
              class="command">xl unpause</code> commands. Note that even though a paused domU does not use any processor power, its allocated memory is still in use. It may be interesting to consider the <code
              class="command">xl save</code> and <code
              class="command">xl restore</code> commands: saving a domU frees the resources that were previously used by this domU, including RAM. When restored (or unpaused, for that matter), a domU doesn't even notice anything beyond the passage of time. If a domU was running when the dom0 is shut down, the packaged scripts automatically save the domU, and restore it on the next boot. This will of course involve the standard inconvenience incurred when hibernating a laptop computer, for instance; in particular, if the domU is suspended for too long, network connections may expire. Note also that Xen is so far incompatible with a large part of ACPI power management, which precludes suspending the host (dom0) system.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>DOKUMENTASI</em></span> Opsi-opsi <code
                        class="command">xl</code></strong></p></div></div></div><div
              class="para">
				Most of the <code
                class="command">xl</code> subcommands expect one or more arguments, often a domU name. These arguments are well described in the <span
                class="citerefentry"><span
                  class="refentrytitle">xl</span>(1)</span> manual page.
			</div></div><div
            class="para">
				Halting or rebooting a domU can be done either from within the domU (with the <code
              class="command">shutdown</code> command) or from the dom0, with <code
              class="command">xl shutdown</code> or <code
              class="command">xl reboot</code>.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>LEBIH JAUH</em></span> Xen tingkat lanjut</strong></p></div></div></div><div
              class="para">
				Xen has many more features than we can describe in these few paragraphs. In particular, the system is very dynamic, and many parameters for one domain (such as the amount of allocated memory, the visible hard drives, the behavior of the task scheduler, and so on) can be adjusted even when that domain is running. A domU can even be migrated across servers without being shut down, and without losing its network connections! For all these advanced aspects, the primary source of information is the official Xen documentation. <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://www.xen.org/support/documentation.html">http://www.xen.org/support/documentation.html</a></div>
			</div></div></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    xmlns=""
                    id="sect.lxc"></a>12.2.2. LXC</h3></div></div></div><a
            id="id-1.15.5.13.2"
            class="indexterm"></a><div
            class="para">
				Even though it is used to build “virtual machines”, LXC is not, strictly speaking, a virtualization system, but a system to isolate groups of processes from each other even though they all run on the same host. It takes advantage of a set of recent evolutions in the Linux kernel, collectively known as <span
              class="emphasis"><em>control groups</em></span>, by which different sets of processes called “groups” have different views of certain aspects of the overall system. Most notable among these aspects are the process identifiers, the network configuration, and the mount points. Such a group of isolated processes will not have any access to the other processes in the system, and its accesses to the filesystem can be restricted to a specific subset. It can also have its own network interface and routing table, and it may be configured to only see a subset of the available devices present on the system.
			</div><div
            class="para">
				These features can be combined to isolate a whole process family starting from the <code
              class="command">init</code> process, and the resulting set looks very much like a virtual machine. The official name for such a setup is a “container” (hence the LXC moniker: <span
              class="emphasis"><em>LinuX Containers</em></span>), but a rather important difference with “real” virtual machines such as provided by Xen or KVM is that there's no second kernel; the container uses the very same kernel as the host system. This has both pros and cons: advantages include excellent performance due to the total lack of overhead, and the fact that the kernel has a global vision of all the processes running on the system, so the scheduling can be more efficient than it would be if two independent kernels were to schedule different task sets. Chief among the inconveniences is the impossibility to run a different kernel in a container (whether a different Linux version or a different operating system altogether).
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>CATATAN</em></span> Batas isolasi LXC</strong></p></div></div></div><div
              class="para">
				Container LXC tidak memberikan tingkat isolasi yang dicapai oleh emulator atau virtualizers yang lebih berat. Khususnya:
			</div><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
						karena kernel dipakai bersama antara sistem host dan container, proses yang dibatasi ke container masih dapat mengakses pesan kernel, yang dapat menyebabkan kebocoran informasi jika pesan dipancarkan oleh kontainer;
					</div></li><li
                  class="listitem"><div
                    class="para">
						untuk alasan yang sama, jika sebuah container terganggu dan kerentanan kernel dieksploitasi, container lain mungkin akan terpengaruh juga;
					</div></li><li
                  class="listitem"><div
                    class="para">
						on the filesystem, the kernel checks permissions according to the numerical identifiers for users and groups; these identifiers may designate different users and groups depending on the container, which should be kept in mind if writable parts of the filesystem are shared among containers.
					</div></li></ul></div></div><div
            class="para">
				Since we are dealing with isolation and not plain virtualization, setting up LXC containers is more complex than just running debian-installer on a virtual machine. We will describe a few prerequisites, then go on to the network configuration; we will then be able to actually create the system to be run in the container.
			</div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.13.7"></a>12.2.2.1. Langkah Pendahuluan</h4></div></div></div><div
              class="para">
					Paket <span
                class="pkg pkg">lxc</span> berisi alat-alat yang diperlukan untuk menjalankan LXC, dan karenanya harus dipasang.
				</div><div
              class="para">
					LXC also requires the <span
                class="emphasis"><em>control groups</em></span> configuration system, which is a virtual filesystem to be mounted on <code
                class="filename">/sys/fs/cgroup</code>. Since Debian 8 switched to systemd, which also relies on control groups, this is now done automatically at boot time without further configuration.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="sect.lxc.network"></a>12.2.2.2. Konfigurasi Jaringan</h4></div></div></div><div
              class="para">
					The goal of installing LXC is to set up virtual machines; while we could of course keep them isolated from the network, and only communicate with them via the filesystem, most use cases involve giving at least minimal network access to the containers. In the typical case, each container will get a virtual network interface, connected to the real network through a bridge. This virtual interface can be plugged either directly onto the host's physical network interface (in which case the container is directly on the network), or onto another virtual interface defined on the host (and the host can then filter or route traffic). In both cases, the <span
                class="pkg pkg">bridge-utils</span> package will be required.
				</div><div
              class="para">
					Kasus sederhana ini hanya sekadar menyunting <code
                class="filename">/etc/network/interfaces</code>, memindah konfigurasi untuk antarmuka fisik (misalnya <code
                class="literal">eth0</code>) ke antarmuka bridge (biasanya <code
                class="literal">br0</code>) dan mengkonfigurasi link antara mereka. Misalnya, jika berkas konfigurasi antarmuka jaringan pada awalnya berisi entri seperti berikut:
				</div><pre
              class="programlisting">auto eth0
iface eth0 inet dhcp</pre><div
              class="para">
					Mereka harus dinonaktifkan dan diganti dengan yang berikut:
				</div><pre
              class="programlisting">#auto eth0
#iface eth0 inet dhcp

auto br0
iface br0 inet dhcp
  bridge-ports eth0</pre><div
              class="para">
					Efek dari konfigurasi ini akan mirip dengan apa yang dapat diperoleh jika container itu adalah mesin yang terhubung ke jaringan fisik yang sama seperti host. Konfigurasi "jbridge" mengelola transit dari frame-frame Ethernet antara semua antarmuka yang dijembatani, yang mencakup fisik <code
                class="literal">eth0</code> maupun antarmuka yang didefinisikan untuk container.
				</div><div
              class="para">
					In cases where this configuration cannot be used (for instance if no public IP addresses can be assigned to the containers), a virtual <span
                class="emphasis"><em>tap</em></span> interface will be created and connected to the bridge. The equivalent network topology then becomes that of a host with a second network card plugged into a separate switch, with the containers also plugged into that switch. The host must then act as a gateway for the containers if they are meant to communicate with the outside world.
				</div><div
              class="para">
					Selain <span
                class="pkg pkg">bridge-utils</span>, konfigurasi "kaya" ini memerlukan paket <span
                class="pkg pkg">vde2</span>; berkas <code
                class="filename">/etc/network/interfaces</code> kemudian menjadi:
				</div><pre
              class="programlisting"># Interface eth0 is unchanged
auto eth0
iface eth0 inet dhcp

# Virtual interface 
auto tap0
iface tap0 inet manual
  vde2-switch -t tap0

# Bridge for containers
auto br0
iface br0 inet static
  bridge-ports tap0
  address 10.0.0.1
  netmask 255.255.255.0</pre><div
              class="para">
					Jaringan kemudian dapat diatur baik secara statis dalam container, atau secara dinamis dengan server DHCP yang berjalan pada host. Server DHCP tersebut perlu dikonfigurasi untuk menjawab pertanyaan pada antarmuka <code
                class="literal">br0</code>.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.13.9"></a>12.2.2.3. Menyiapkan Sistem</h4></div></div></div><div
              class="para">
					Mari kita sekarang menyiapkan sistem berkas yang akan digunakan oleh container. Karena "mesin virtual" ini tidak akan berjalan secara langsung pada perangkat keras, beberapa tweak diperlukan bila dibandingkan dengan sistem berkas standar, terutama bila menyangkut kernel, perangkat, dan konsol. Untungnya, <span
                class="pkg pkg">lxc</span> menyertakan skrip yang kebanyakan mengotomatisasi konfigurasi ini. Sebagai contoh, perintah berikut (yang membutuhkan <span
                class="pkg pkg">debootstrap</span> dan <span
                class="pkg pkg">rsync</span> paket) akan menginstal sebuah container Debian:
				</div><pre
              class="screen"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-create -n testlxc -t debian
</code></strong><code
                class="computeroutput">debootstrap is /usr/sbin/debootstrap
Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... 
Downloading debian minimal ...
I: Retrieving Release 
I: Retrieving Release.gpg 
[...]
Download complete.
Copying rootfs to /var/lib/lxc/testlxc/rootfs...
[...]
Root password is 'sSiKhMzI', please change !
root@mirwiz:~# </code>
</pre><div
              class="para">
					Perhatikan bahwa sistem berkas awalnya dibuat di <code
                class="filename">/var/cache/lxc</code>, kemudian dipindah ke direktori tujuannya. Hal ini memungkinkan membuat container-container identik secara jauh lebih cepat, karena kemudian hanya perlu menyalin.
				</div><div
              class="para">
					Perhatikan bahwa skrip penciptaan templat debian menerima pilihan <code
                class="option">--arch</code> untuk menentukan arsitektur sistem yang akan diinstal dan pilihan <code
                class="option">--release</code> jika Anda ingin menginstal sesuatu yang lain daripada rilis stabil Debian. Anda juga dapat menetapkan variabel lingkungan <code
                class="literal">MIRROR</code> untuk menunjuk ke mirror Debian lokal.
				</div><div
              class="para">
					The newly-created filesystem now contains a minimal Debian system, and by default the container has no network interface (besides the loopback one). Since this is not really wanted, we will edit the container's configuration file (<code
                class="filename">/var/lib/lxc/testlxc/config</code>) and add a few <code
                class="literal">lxc.network.*</code> entries:
				</div><pre
              class="programlisting">lxc.network.type = veth
lxc.network.flags = up
lxc.network.link = br0
lxc.network.hwaddr = 4a:49:43:49:79:20</pre><div
              class="para">
					Entri ini berarti, masing-masing, bahwa suatu antarmuka virtual akan dibuat di dalam container; bahwa itu akan secara otomatis dihidupkan ketika container dimulai; itu akan secara otomatis terhubung ke bridge <code
                class="literal">br0</code> pada host; dan bahwa alamat MAC-nya akan seperti yang ditentukan. Bila entri terakhir ini hilang atau dinonaktifkan, alamat MAC acak akan dibuat.
				</div><div
              class="para">
					Entri lain yang berguna dalam berkas itu adalah pengaturan nama host:
				</div><pre
              class="programlisting">lxc.utsname = testlxc</pre></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.13.10"></a>12.2.2.4. Memulai Container</h4></div></div></div><div
              class="para">
					Sekarang setelah image mesin virtual kita sudah siap, mari kita mulai container:
				</div><pre
              class="screen scale"
              width="94"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-start --daemon --name=testlxc
</code></strong><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-console -n testlxc
</code></strong><code
                class="computeroutput">Debian GNU/Linux 8 testlxc tty1

testlxc login: </code><strong
                class="userinput"><code>root</code></strong><code
                class="computeroutput">
Password: 
Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
root@testlxc:~# </code><strong
                class="userinput"><code>ps auxwf</code></strong>
<code
                class="computeroutput">USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init
root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald
root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D
root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux
root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux
root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux
root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     
root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \_ -bash
root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \_ ps auxfw
root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102
root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e
root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux
root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux
root@testlxc:~# </code></pre><div
              class="para">
					We are now in the container; our access to the processes is restricted to only those started from the container itself, and our access to the filesystem is similarly restricted to the dedicated subset of the full filesystem (<code
                class="filename">/var/lib/lxc/testlxc/rootfs</code>). We can exit the console with <span
                class="keycap"><strong>Control</strong></span>+<span
                class="keycap"><strong>a</strong></span> <span
                class="keycap"><strong>q</strong></span>.
				</div><div
              class="para">
					Note that we ran the container as a background process, thanks to the <code
                class="option">--daemon</code> option of <code
                class="command">lxc-start</code>. We can interrupt the container with a command such as <code
                class="command">lxc-stop --name=testlxc</code>.
				</div><div
              class="para">
					The <span
                class="pkg pkg">lxc</span> package contains an initialization script that can automatically start one or several containers when the host boots (it relies on <code
                class="command">lxc-autostart</code> which starts containers whose <code
                class="literal">lxc.start.auto</code> option is set to 1). Finer-grained control of the startup order is possible with <code
                class="literal">lxc.start.order</code> and <code
                class="literal">lxc.group</code>: by default, the initialization script first starts containers which are part of the <code
                class="literal">onboot</code> group and then the containers which are not part of any group. In both cases, the order within a group is defined by the <code
                class="literal">lxc.start.order</code> option.
				</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>LEBIH JAUH</em></span> Virtualisasi masal</strong></p></div></div></div><div
                class="para">
					Since LXC is a very lightweight isolation system, it can be particularly adapted to massive hosting of virtual servers. The network configuration will probably be a bit more advanced than what we described above, but the “rich” configuration using <code
                  class="literal">tap</code> and <code
                  class="literal">veth</code> interfaces should be enough in many cases.
				</div><div
                class="para">
					It may also make sense to share part of the filesystem, such as the <code
                  class="filename">/usr</code> and <code
                  class="filename">/lib</code> subtrees, so as to avoid duplicating the software that may need to be common to several containers. This will usually be achieved with <code
                  class="literal">lxc.mount.entry</code> entries in the containers configuration file. An interesting side-effect is that the processes will then use less physical memory, since the kernel is able to detect that the programs are shared. The marginal cost of one extra container can then be reduced to the disk space dedicated to its specific data, and a few extra processes that the kernel must schedule and manage.
				</div><div
                class="para">
					We haven't described all the available options, of course; more comprehensive information can be obtained from the <span
                  class="citerefentry"><span
                    class="refentrytitle">lxc</span>(7)</span> and <span
                  class="citerefentry"><span
                    class="refentrytitle">lxc.container.conf</span>(5)</span> manual pages and the ones they reference.
				</div></div></div></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    xmlns=""
                    id="id-1.15.5.14"></a>12.2.3. Virtualisasi dengan KVM</h3></div></div></div><a
            id="id-1.15.5.14.2"
            class="indexterm"></a><div
            class="para">
				KVM, kependekan dari <span
              class="emphasis"><em>Kernel-based Virtual Machine</em></span>, adalah pertama dan terutama sebuah modul kernel yang menyediakan sebagian besar infrastruktur yang dapat digunakan oleh virtualizer, tetapi bukan virtualizer itu sendiri. Kontrol aktual untuk virtualisasi ditangani oleh sebuah aplikasi berbasis QEMU. Jangan khawatir jika bagian ini menyebutkan perintah <code
              class="command">qemu-*</code>: itu masih tentang KVM.
			</div><div
            class="para">
				Tidak seperti sistem virtualisasi lain, KVM digabungkan ke kernel Linux sejak dari awal. Para pengembangnya memilih untuk mengambil keuntungan dari set instruksi prosesor yang didedikasikan untuk virtualisasi (Intel-VT dan AMD-V), yang menjaga KVM ringan, elegan, dan tidak boros sumber daya. Kekurangannya, tentu saja, adalah bahwa KVM tidak bekerja pada sebarang komputer tetapi hanya pada yang memiliki prosesor yang sesuai. Untuk komputer berbasis x86, Anda dapat memastikan bahwa Anda memiliki prosesor seperti itu dengan mencari "vmx" atau "svm" di bendera CPU yang tercantum dalam <code
              class="filename">/proc/cpuinfo</code>.
			</div><div
            class="para">
				Dengan Red Hat secara aktif mendukung perkembangannya, KVM kurang lebih telah menjadi acuan untuk virtualisasi Linux.
			</div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.6"></a>12.2.3.1. Langkah Pendahuluan</h4></div></div></div><a
              id="id-1.15.5.14.6.2"
              class="indexterm"></a><div
              class="para">
					Tidak seperti alat-alat semacam VirtualBox, KVM itu sendiri tidak menyertakan antarmuka pengguna untuk membuat dan mengelola mesin virtual. Paket <span
                class="pkg pkg">qemu-kvm</span> hanya menyediakan program yang bisa memulai sebuah mesin virtual, serta skrip inisialisasi yang memuat modul-modul kernel yang sesuai.
				</div><a
              id="id-1.15.5.14.6.4"
              class="indexterm"></a><a
              id="id-1.15.5.14.6.5"
              class="indexterm"></a><div
              class="para">
					Untungnya, Red Hat juga menyediakan satu set alat lain untuk mengatasi masalah itu, dengan mengembangkan perpustakaan <span
                class="emphasis"><em>libvirt</em></span> dan alat-alat <span
                class="emphasis"><em>manajer mesin virtual</em></span> terkait. Libvirt memungkinkan mengelola mesin virtual dalam cara yang seragam, secara independen dari sistem virtualisasi yang terlibat di balik layar (saat ini mendukung QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare, dan UML). <code
                class="command">virtual-manager</code> adalah sebuah antarmuka grafis yang menggunakan libvirt untuk membuat dan mengelola mesin virtual.
				</div><a
              id="id-1.15.5.14.6.7"
              class="indexterm"></a><div
              class="para">
					Kita pertama kali menginstal paket-paket yang diperlukan, dengan <code
                class="command">apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</code>. <span
                class="pkg pkg">libvirt-bin</span> menyediakan daemon <code
                class="command">libvirtd</code>, yang memungkinkan manajemen mesin virtual (berpotensi remote) yang berjalan pada host, dan memulai VM yang diperlukan ketika host boot. Selain itu, paket ini menyediakan alat bantu baris perintah <code
                class="command">virsh</code>, yang memungkinkan mengendalikan mesin-mesin yang dikelola oleh <code
                class="command">libvirtd</code>.
				</div><div
              class="para">
					Paket <span
                class="pkg pkg">virtinst</span> menyediakan <code
                class="command">virt-install</code>, yang memungkinkan membuat mesin virtual dari baris perintah. Terakhir, <span
                class="pkg pkg">virt-viewer</span> memungkinkan mengakses sebuah konsol grafis VM.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.7"></a>12.2.3.2. Konfigurasi Jaringan</h4></div></div></div><div
              class="para">
					Sama seperti Xen dan LXC, konfigurasi jaringan yang paling sering melibatkan bridge yang mengelompokkan antarmuka jaringan mesin virtual (lihat <a
                class="xref"
                href="sect.virtualization.html#sect.lxc.network">Bagian 12.2.2.2, “Konfigurasi Jaringan”</a>).
				</div><div
              class="para">
					Sebagai alternatif, dan dalam konfigurasi default yang disediakan oleh KVM, mesin virtual diberikan alamat pribadi (di kisaran 192.168.122.0/24), dan NAT diatur sehingga VM dapat mengakses jaringan luar.
				</div><div
              class="para">
					Sisa bagian ini mengasumsikan bahwa host memiliki antarmuka fisik <code
                class="literal">eth0</code> dan bridge <code
                class="literal">br0</code>, dan bahwa yang terdahulu terhubung ke yang terakhir.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.8"></a>12.2.3.3. Instalasi dengan <code
                      class="command">virt-install</code></h4></div></div></div><a
              id="id-1.15.5.14.8.2"
              class="indexterm"></a><div
              class="para">
					Membuat mesin virtual sangat mirip dengan menginstal sistem normal, kecuali bahwa karakteristik mesin virtual dijelaskan dalam baris perintah yang tampaknya tak berujung.
				</div><div
              class="para">
					Secara praktis, ini berarti kita akan menggunakan installer Debian, dengan mem-boot mesin virtual pada drive DVD-ROM virtual yang memetakan ke image DVD Debian yang tersimpan di sistem host. VM akan mengekspor konsol grafisnya lewat protokol VNC (lihat <a
                class="xref"
                href="sect.remote-login.html#sect.remote-desktops">Bagian 9.2.2, “Memakai Desktop Grafis Jarak Jauh”</a> untuk rincian), yang akan memungkinkan kita untuk mengontrol proses instalasi.
				</div><div
              class="para">
					Pertama kita perlu memberitahu libvirtd di mana tempat menyimpan image disk, kecuali bila lokasi default (<code
                class="filename">/var/lib/libvirt/images/</code>) baik-baik saja.
				</div><pre
              class="screen"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>mkdir /srv/kvm</code></strong>
<code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>virsh pool-create-as srv-kvm dir --target /srv/kvm</code></strong>
<code
                class="computeroutput">Pool srv-kvm created

root@mirwiz:~# </code></pre><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>TIPS</em></span> Menambahkan pengguna Anda ke grup libvirt</strong></p></div></div></div><div
                class="para">
					Semua contoh dalam bagian ini mengasumsikan bahwa Anda menjalankan perintah sebagai root. Secara efektif, jika Anda ingin mengontrol daemon libvirt lokal, Anda perlu untuk menjadi root atau menjadi anggota dari kelompok <code
                  class="literal">libvirt</code> (yang tidak terjadi secara default). Jadi jika Anda ingin menghindari menggunakan hak root terlalu sering, Anda dapat menambahkan diri Anda sendiri ke grup <code
                  class="literal">libvirt</code> dan menjalankan berbagai perintah di bawah identitas pengguna Anda.
				</div></div><div
              class="para">
					Mari kita sekarang mulai proses instalasi untuk mesin virtual, dan melihat lebih dekat pada pilihan-pilihan <code
                class="command">virt-install</code> yang paling penting. Perintah ini mendaftarkan mesin virtual dan parameternya di libvirtd, kemudian memulainya sehingga instalasi dapat dilanjutkan.
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virt-install --connect qemu:///system  <span
                    id="virtinst.connect"><img
                      class="callout"
                      src="Common_Content/images/1.png"
                      alt="1" /></span>
               --virt-type kvm           <span
                    id="virtinst.type"><img
                      class="callout"
                      src="Common_Content/images/2.png"
                      alt="2" /></span>
               --name testkvm            <span
                    id="virtinst.name"><img
                      class="callout"
                      src="Common_Content/images/3.png"
                      alt="3" /></span>
               --ram 1024                <span
                    id="virtinst.ram"><img
                      class="callout"
                      src="Common_Content/images/4.png"
                      alt="4" /></span>
               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <span
                    id="virtinst.disk"><img
                      class="callout"
                      src="Common_Content/images/5.png"
                      alt="5" /></span>
               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <span
                    id="virtinst.cdrom"><img
                      class="callout"
                      src="Common_Content/images/6.png"
                      alt="6" /></span>
               --network bridge=br0      <span
                    id="virtinst.network"><img
                      class="callout"
                      src="Common_Content/images/7.png"
                      alt="7" /></span>
               --vnc                     <span
                    id="virtinst.vnc"><img
                      class="callout"
                      src="Common_Content/images/8.png"
                      alt="8" /></span>
               --os-type linux           <span
                    id="virtinst.os"><img
                      class="callout"
                      src="Common_Content/images/9.png"
                      alt="9" /></span>
               --os-variant debianwheezy
</code></strong><code
                class="computeroutput">
Starting install...
Allocating 'testkvm.qcow'             |  10 GB     00:00
Creating domain...                    |    0 B     00:00
Guest installation complete... restarting guest.
</code></pre><div
              class="calloutlist"><table
                border="0"
                summary="Callout list"><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.connect"><img
                          class="callout"
                          src="Common_Content/images/1.png"
                          alt="1" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							Opsi <code
                        class="literal">--connect</code> menyatakan"hypervisor" yang akan dipakai. Bentuknya adalah URL yang memuat sistem virtualisasi (<code
                        class="literal">xen://</code>, <code
                        class="literal">qemu://</code>, <code
                        class="literal">lxc://</code>, <code
                        class="literal">openvz://</code>, <code
                        class="literal">vbox://</code>, dan seterusnya) dan mesin yang harus menjadi host VM (ini dapat dibiarkan kosong dalam kasus hosting lokal). Selain itu, dan dalam kasus QEMU/KVM, setiap pengguna dapat mengelola mesin virtual yang bekerja dengan izin terbatas, dan path URL memungkinkan membedakan mesin "sistem" (<code
                        class="literal">/system</code>) dari (<code
                        class="literal">/session</code>) yang lain.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.type"><img
                          class="callout"
                          src="Common_Content/images/2.png"
                          alt="2" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							Karena KVM dikelola dengan cara yang sama seperti QEMU, <code
                        class="literal">--virt-type kvm</code> mengizinkan menyatakan penggunaan KVM meskipun URL terlihat seperti QEMU.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.name"><img
                          class="callout"
                          src="Common_Content/images/3.png"
                          alt="3" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							Opsi <code
                        class="literal">--name</code> mendefinisikan nama (unik) untuk mesin virtual.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.ram"><img
                          class="callout"
                          src="Common_Content/images/4.png"
                          alt="4" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							Opsi <code
                        class="literal">--ram</code> memungkinkan menentukan banyaknya RAM (dalam MB) yang dialokasikan untuk mesin virtual.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.disk"><img
                          class="callout"
                          src="Common_Content/images/5.png"
                          alt="5" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--disk</code> menyatakan lokasi berkas image yang mewakili hard disk mesin virtual; berkas itu dibuat, kecuali sudah ada, dengan ukuran (dalam GB) yang ditentukan oleh parameter <code
                        class="literal">size</code>. Parameter <code
                        class="literal">format</code> memungkinkan memilih antara beberapa cara untuk menyimpan berkas image. Format default (<code
                        class="literal">raw</code>) adalah satu berkas persis cocok dengan ukuran dan isi disk. Kita memilih format yang lebih maju di sini, yang khusus untuk QEMU dan memungkinkan mulai dengan berkas kecil yang hanya tumbuh ketika mesin virtual mulai benar-benar menggunakan ruang.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.cdrom"><img
                          class="callout"
                          src="Common_Content/images/6.png"
                          alt="6" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							Opsi <code
                        class="literal">--cdrom</code> digunakan untuk menunjukkan di mana menemukan disk optik yang digunakan untuk instalasi. Path bisa berupa path lokal untuk berkas ISO, URL tempat berkas dapat diperoleh, atau perangkat berkas dari drive CD-ROM fisik (yaitu <code
                        class="literal">/dev/cdrom</code>).
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.network"><img
                          class="callout"
                          src="Common_Content/images/7.png"
                          alt="7" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--network</code> menyatakan bagaimana kartu jaringan virtual mengintegrasi dalam konfigurasi jaringan host. Perilaku default (yang secara eksplisit kita paksa dalam contoh kita) adalah mengintegrasikannya ke dalam jaringan bridge apapun yang sudah ada. Jika bridge seperti itu tidak ada, mesin virtual hanya akan mencapai jaringan fisik melalui NAT, sehingga mendapat alamat di subnet pribadi (192.168.122.0/24).
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.vnc"><img
                          class="callout"
                          src="Common_Content/images/8.png"
                          alt="8" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--vnc</code> menyatakan bahwa konsol grafis harus dibuat tersedia menggunakan VNC. Perilaku default untuk server VNC terkait adalah hanya mendengarkan pada antarmuka lokal; jika klien VNC akan dijalankan pada host yang berbeda, membuat koneksi akan memerlukan pengaturan tunnel SSH (lihat <a
                        class="xref"
                        href="sect.remote-login.html#sect.ssh-port-forwarding">Bagian 9.2.1.3, “Menciptakan Tunnel Terenkripsi dengan Penerusan Port”</a>). Sebagai alternatif, <code
                        class="literal">--vnclisten=0.0.0.0</code> dapat digunakan sehingga server VNC dapat diakses dari semua antarmuka; perhatikan bahwa jika Anda melakukannya, Anda benar-benar harus merancang firewall Anda sesuai dengan itu.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.os"><img
                          class="callout"
                          src="Common_Content/images/9.png"
                          alt="9" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							Pilihan <code
                        class="literal">--os-type</code> dan <code
                        class="literal">--os-variant</code> memungkinkan mengoptimalkan beberapa parameter mesin virtual, berdasarkan fitur yang dikenal dari sistem operasi yang disebutkan di sana.
						</div></td></tr></table></div><div
              class="para">
					Pada titik ini, mesin virtual sedang berjalan, dan kita perlu terhubung ke konsol grafis untuk melanjutkan dengan proses instalasi. Jika operasi sebelumnya berjalan dari lingkungan desktop grafis, hubungan ini harus secara otomatis dimulai. Jika tidak, atau jika kita beroperasi jarak jauh, <code
                class="command">virt-viewer</code> dapat dijalankan dari setiap lingkungan grafis untuk membuka konsol grafis (perhatikan bahwa kata sandi root dari host remote diminta dua kali karena operasi memerlukan 2 koneksi SSH):
				</div><pre
              class="screen"><code
                class="computeroutput">$ </code><strong
                class="userinput"><code>virt-viewer --connect qemu+ssh://root@<em
                    class="replaceable">server</em>/system testkvm
</code></strong><code
                class="computeroutput">root@server's password: 
root@server's password: </code></pre><div
              class="para">
					Ketika proses instalasi berakhir, mesin virtual dijalankan ulang, sekarang siap untuk digunakan.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.9"></a>12.2.3.4. Mengelola Mesin dengan <code
                      class="command">virsh</code></h4></div></div></div><a
              id="id-1.15.5.14.9.2"
              class="indexterm"></a><div
              class="para">
					Sekarang setelah instalasi selesai, mari kita lihat bagaimana menangani mesin virtual yang tersedia. Hal pertama yang dicoba adalah untuk bertanya ke <code
                class="command">libvirtd</code> daftar mesin virtual yang dikelolanya:
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system list --all
 Id Name                 State
----------------------------------
  - testkvm              shut off
</code></strong></pre><div
              class="para">
					Mari kita mulai jalankan mesin virtual uji kita:
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system start testkvm
</code></strong><code
                class="computeroutput">Domain testkvm started</code></pre><div
              class="para">
					Kita sekarang bisa mendapatkan petunjuk koneksi untuk konsol grafis (tampilan VNC yang dikembalikan dapat diberikan sebagai parameter ke <code
                class="command">vncviewer</code>):
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system vncdisplay testkvm
</code></strong><code
                class="computeroutput">:0</code></pre><div
              class="para">
					Sub perintah <code
                class="command">virsh</code> lain yang tersedia meliputi:
				</div><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">reboot</code> untuk memulai jalankan lagi sebuah mesin virtual;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">shutdown</code> untuk memicu suatu shutdown yang bersih;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">destroy</code>, untuk menghentikannya secara brutal;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">suspend</code> untuk mengistirahatkannya;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">resume</code> untuk melanjutkan dari istirahat;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">autostart</code> untuk mengaktifkan (atau menonaktifkan, dengan pilihan <code
                      class="literal">--disable</code>) memulai mesin virtual secara otomatis ketika host mulai;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">undefine</code> untuk menghapus semua jejak mesin virtual dari <code
                      class="command">libvirtd</code>.
						</div></li></ul></div><div
              class="para">
					Semua sub perintah ini mengambil sebuah identifier mesin virtual sebagai parameter.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.10"></a>12.2.3.5. Instalasi sistem berbasis RPM dalam Debian dengan yum</h4></div></div></div><div
              class="para">
					Jika mesin virtual dimaksudkan untuk menjalankan Debian (atau salah satu turunannya), sistem dapat diinisialisasi dengan <code
                class="command">debootstrap</code>, seperti dijelaskan di atas. Tetapi jika mesin virtual diinstal dengan sistem berbasis RPM (seperti Fedora, CentOS, atau Scientific Linux), penyiapan akan perlu dilakukan menggunakan utilitas <code
                class="command">yum</code> (tersedia dalam paket dengan nama yang sama).
				</div><div
              class="para">
					Prosedur tersebut perlu memakai <code
                class="command">rpm</code> untuk mengekstrak set awal berkas, termasuk terutama berkas konfigurasi <code
                class="command">yum</code>, dan kemudian memanggil <code
                class="command">yum</code> untuk mengekstrak kumpulan paket sisanya. Tapi karena kita memanggil <code
                class="command">yum</code> dari luar chroot, kita perlu membuat beberapa perubahan sementara. Dalam contoh di bawah ini, chroot target adalah <code
                class="filename">/srv/centos</code>.
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>rootdir="/srv/centos"
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>mkdir -p "$rootdir" /etc/rpm
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>echo "%_dbpath /var/lib/rpm" &gt; /etc/rpm/macros.dbpath
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>rpm --nodeps --root "$rootdir" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</code></strong><code
                class="computeroutput">rpm: RPM should not be used directly install RPM packages, use Alien instead!
rpm: However assuming you know what you are doing...
warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY
# </code><strong
                class="userinput"><code>sed -i -e "s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g" $rootdir/etc/yum.repos.d/*.repo
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>yum --assumeyes --installroot $rootdir groupinstall core
</code></strong><code
                class="computeroutput">[...]
# </code><strong
                class="userinput"><code>sed -i -e "s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g" $rootdir/etc/yum.repos.d/*.repo
</code></strong></pre></div></div></div><ul
        class="docnav"><li
          class="previous"><a
            accesskey="p"
            href="advanced-administration.html"><strong>Sebelumnya</strong>Bab 12. Administrasi Tingkat Lanjut</a></li><li
          class="up"><a
            accesskey="u"
            href="#"><strong>Induk</strong></a></li><li
          class="home"><a
            accesskey="h"
            href="index.html"><strong>Depan</strong></a></li><li
          class="next"><a
            accesskey="n"
            href="sect.automated-installation.html"><strong>Lanjut</strong>12.3. Pemasangan Otomatis</a></li></ul></body></html>
