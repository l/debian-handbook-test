<?xml version='1.0' encoding='utf-8' ?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [
]>
<chapter id="advanced-administration" lang="fa-IR">
	<chapterinfo>
		 <keywordset>
			<keyword>RAID</keyword>
			 <keyword>LVM</keyword>
			 <keyword>FAI</keyword>
			 <keyword>Preseeding</keyword>
			 <keyword>مانیتورینگ</keyword>
			 <keyword>مجازی‌سازی</keyword>
			 <keyword>Xen</keyword>
			 <keyword>LXC</keyword>

		</keywordset>

	</chapterinfo>
	 <title>مدیریت پیشرفته</title>
	 <highlights> <para>
		این فصل به مرور مفاهیمی که تاکنون به آن‌ها پرداخته‌ایم می‌پردازد، اما با رویکردی متفاوت: بجای نصب یک رایانه تکی، به مطالعه نصب-انبوه روی سیستم‌ها می‌پردازیم؛ بجای نصب RAID یا LVM در زمان نصب، اینکار را به صورت دستی در زمان دیگری که نیاز داشته باشیم انجام می‌دهیم. در نهایت، درباره ابزارهای مانیتورینگ و تکنیک‌های مجازی‌سازی صحبت خواهیم کرد. به همین دلیل، مخاطب این فصل روی مدیرسیستم‌های حرفه‌ای تمرکز دارد و به جنبه‌های شخصی و انفرادی این کار کمتر توجه می‌کند.
	</para>
	 </highlights> <section id="sect.raid-and-lvm">
		<title>RAID و LVM</title>
		 <para>
			قسمت <xref linkend="installation" /> از نقطه نظر فرآیند نصب به بررسی این فناوری‌ها و اینکه چگونه می‌توان از ابتدا آن‌ها را به سادگی مدیریت کرد، پرداخت. پس از نصب اولیه، یک مدیرسیستم باید بتواند نیازهای رو به افزایش فضای دیسک را بدون راه‌اندازی فرآیند نصب برطرف کند. برای این منظور آن‌ها باید ابزارهای مورد نیاز برای تغییرات RAID و LVM را درک کنند.
		</para>
		 <para>
			RAID و LVM فناوری‌هایی هستند که دستگاه‌های متصل را به شیوه‌ای انتزاعی از همتای فیزیکی خود جدا می‌سازند (درایوهای هارد-دیسک یا پارتیشن‌ها)؛ اولی با استفاده از تکنیک تکرار برای ایمن‌سازی داده در صورت بروز مشکل سخت‌افزاری و دومی با استفاده از منعطف‌ساختن مدیریت دستگاه‌ها جدا از اندازه واقعی آن‌ها کار می‌کنند. در هر دو مورد، سیستم با دستگاه‌های بلاک-محور جدید روبه‌رو می‌شود که می‌تواند برای ایجاد فایل‌سیستم‌ها یا فضای swap مورد استفاده قرار گیرد، بدون اینکه ضرورتا به یک دیسک فیزیکی نگاشت شوند. RAID و LVM از پیش‌زمینه‌های متفاوتی می‌آیند، اما عملکرد آن‌ها می‌تواند با یکدیگر تداخل داشته باشد که به همین دلیل همراه با هم نام برده می‌شوند.
		</para>
		 <sidebar> <title><emphasis>چشم‌انداز</emphasis> ترکیب RAID و LVM در Btrfs</title>
		 <para>
			از آنجا که RAID و LVM دو زیرسیستم کاملا جدا از یکدیگر در کرنل هستند که در مورد دستگاه‌های بلاک-محور و فایل‌سیستم‌های آنان بکار می‌روند، <emphasis>btrfs</emphasis> فایل‌سیستم جدیدی بحساب می‌آید، که در ابتدا توسط اوراکل توسعه یافت و قصد ترکیب ویژگی‌های RAID و LVM را دارد. با اینکه هنوز برچسب “آزمایشی” روی آن وجود دارد و برخی ویژگی‌هایش به صورت کامل پیاده‌سازی نشده است، در محیط‌های واقعی کاربردهای گوناگونی دارد. <ulink type="block" url="http://btrfs.wiki.kernel.org/" />
		</para>
		 <para>
			از میان ویژگی‌های آن، توانایی ایجاد snapshot از فایل‌سیستم در هر لحظه از زمان وجود دارد. این رونوشت از snapshot به صورت اولیه هیچ فضایی را اشغال نمی‌کند، داده زمانی تکرار می‌شود که یکی از این رونوشت‌ها تغییر کند. فایل‌سیستم همچنین از فشرده‌سازی شفاف فایل‌ها پشتیبانی و از checksum برای اطمینان از جامعیت داده‌های ذخیره شده استفاده می‌کند.
		</para>
		 </sidebar> <para>
			در هر دو مورد RAID و LVM، کرنل یک دستگاه بلاک-محور فراهم می‌کند، مشابه آن‌هایی که برای درایو هارد دیسک یا یک پارتیشن بکار می‌رود. زمانی که یک برنامه، یا قسمتی از کرنل، درخواست دسترسی به چنین دستگاهی را داشته باشد، زیرسیستم مرتبط با آن فرآیند مسیریابی بلاک به لایه فیزیکی مرتبط را برقرار می‌کند. با توجه به پیکربندی، این بلاک می‌تواند در یک یا چند دیسک فیزیکی ذخیره شده باشد و مکان فیزیکی آن ممکن است به صورت مستقیم مرتبط با مکان بلاک در دستگاه مجازی نباشد.
		</para>
		 <section id="sect.raid-soft">
			<title>RAID نرم‌افزاری</title>
			 <indexterm>
				<primary>RAID</primary>
			</indexterm>
			 <para>
				RAID مخفف عبارت <emphasis>Redundant Array of Independent Disks</emphasis> به معنی آرایه‌ای افزونه از دیسک‌های مستقل است. هدف این سیستم پیشگیری از بین رفتن داده در زمان نقص سخت‌افزاری هارد دیسک است. ایده اصلی آن بسیار ساده است: داده بجای اینکه در یک دیسک ذخیره گردد در چند دیسک فیزیکی انباشت می‌شود، همراه با یک سطح قابل پیکربندی از افزونگی. با توجه به این میزان از افزونگی، در زمان بروز یک رویداد سخت‌افزاری ناخواسته روی دیسک، داده می‌تواند از سایر دیسک‌های باقیمانده بازسازی شود.
			</para>
			 <sidebar> <title><emphasis>فرهنگ</emphasis> <foreignphrase>مستقل</foreignphrase> یا <foreignphrase>ارزان</foreignphrase>؟</title>
			 <para>
				حرف I در RAID سابق بر این به معنای <emphasis>inexpensive</emphasis> بود، چرا که RAID امنیت داده را به شدت و بدون نیاز به استفاده از دیسک‌های گران-قیمت بالا می‌برد. اما امروزه با توجه به نگرانی‌های فضای ذخیره‌سازی، از این حرفه بیشتر به معنای <emphasis>independent</emphasis> یاد می‌شود، که الزاما معنای ارزان بودن آن را به یاد نمی‌آورد.
			</para>
			 </sidebar> <para>
				RAID می‌تواند هم به صورت سخت‌‌افزاری (افزونه‌های RAID منطبق با کارت‌های کنترل SCSI یا SATA) هم به صورت نرم‌افزاری (توسط کرنل) پیاده‌سازی شود. جدا از شیوه پیاده‌سازی آن، یک سیستم RAID در صورت وجود افزونگی کافی می‌تواند در زمان بروز نقص دیسک به کار خود ادامه دهد؛ لایه‌های بالایی آن (برنامه‌ها) حتی می‌توانند در صورت بروز مشکل به داده دسترسی داشته باشند. البته که این “حالت ناامن” می‌تواند عملکرد منفی روی سیستم بگذارد و منجر به کاهش سطح افزونگی گردد، بنابراین یک نقص دیسک دیگر، منجر به از دست دادن داده می‌شود. در عمل، تنها یک دیسک تلاش می‌کند که در این حالت تخریبی تا زمان برطرف شدن مشکل قرار بگیرد. زمانی که دیسک جدید جایگزین شود، سیستم RAID می‌تواند با بازسازی داده به حالت امن خود بازگردد. زمانی که آرایه در حالت ناامن یا فاز بازسازی داده قرار می‌گیرد، عملا وقفه‌ای در برنامه‌ها ایجاد نمی‌گردد بجز کاهش سرعت دسترسی به داده.
			</para>
			 <para>
				زمانی که RAID به صورت سخت‌افزاری پیاده‌سازی شود، پیکربندی آن درون ابزار برپایی BIOS قرار می‌گیرد و کرنل یک آرایه RAID را به عنوان یک دیسک مجزا در نظر می‌گیرد، که مانند یک دیسک استاندارد کار خواهد کرد، با این حال نام دستگاه می‌تواند متفاوت باشد (بر اساس درایو بکار رفته).
			</para>
			 <para>
				ما تنها به RAID نرم‌افزاری در این کتاب اشاره می‌کنیم.
			</para>
			 <section id="sect.raid-levels">
				<title>سطوح مختلف RAID</title>
				 <para>
					RAID در حقیقت یک سیستم مجزا نیست، بلکه بازه‌ای از سیستم‌ها در سطوح مختلف است؛ این سطوح با توجه به ساختار و میزان افزونگی داده با یکدیگر فرق دارند. هر چه افزونگی بیشتر باشد، توانایی مواجه به دیسک‌های خراب در زمان نقص سخت‌افزاری بالاتر می‌رود. نقطه مقابل آن زمانی است که فضای موجود برای مجموعه‌ای از دیسک‌ها کاهش یابد؛ که در این صورت به دیسک‌های بیشتری برای ذخیره‌سازی داده نیاز است.
				</para>
				 <variablelist>
					<varlistentry>
						<term>RAID خطی</term>
						 <listitem>
							<para>
								با اینکه زیرسیستم RAID در کرنل از “RAID خطی” پشتیبانی می‌کند، این یک RAID کارآمد بحساب نمی‌آید چرا که شامل هیچ سطحی از افزونگی داده نیست. کرنل صرفا با قرار دادن دیسک‌های مختلف در کنار یکدیگر و ایجاد یک فضای بزرگ‌تر یک دیسک مجازی (یک دستگاه بلاک-محور) بوجود می‌آورد. این تنها عملکرد آن است. از این تنظیم به ندرت استفاده می‌شود (بجز موارد خاص که در ادامه خواهید دید)، بخصوص زمانی که در صورت نبود افزونگی داده، نقص در یک دیسک باعث غیرقابل دسترس شدن داده در تمامی دیسک‌ها و کل سیستم می‌گردد.
							</para>

						</listitem>

					</varlistentry>
					 <varlistentry>
						<term>RAID-0</term>
						 <listitem>
							<para>
								این سطح نیز هیچ افزونگی داده‌ای را فراهم نمی‌کند ولی برخلاف سطح قبل دیسک‌ها به صورت پیوسته پشت سر هم قرار نمی‌گیرند: بلکه به <emphasis>stripes</emphasis> تقسیم می‌شوند و بلاک‌های دستگاه مجازی روی این stripe از دیسک‌های فیزیکی قرار می‌گیرند. برای نمونه، در یک تنظیم RAID-0 با دو دیسک، بلاک‌های شماره زوج از دستگاه مجازی روی دیسک فیزیکی اول و بلاک‌های شماره فرد روی دیسک فیزیکی دوم ذخیره می‌شوند.
							</para>
							 <para>
								هدف این سیستم افزایش قابلیت اعتماد نیست، چرا که (مانند حالت خطی) موجودیت داده به محض بروز نقص در دیسک به خطر می‌افتد، هدف آن افزایش عملکرد دیسک است: طی دسترسی ترتیبی به بخش بزرگی از داده‌های به هم پیوسته، کرنل می‌تواند به صورت موازی عملیات خواندن و نوشتن را روی هر دو دیسک انجام دهد که اینکار منجر به افزایش نرخ انتقال داده می‌گردد. با این حال، استفاده از RAID-0 در حال کاهش است، به صورتی که کاربرد آن با LVM جایگزین شده است (در ادامه خواهید دید).
							</para>

						</listitem>

					</varlistentry>
					 <varlistentry>
						<term>RAID-1</term>
						 <listitem>
							<para>
								این سطح، که با نام “RAID Mirroring” نیز شناخته می‌شود، ساده‌ترین و پرکاربردترین تنظیم مورد استفاده است. در حالت استاندارد، از دو دیسک فیزیکی هم اندازه استفاده می‌کند تا یک فضای منطقی به همان اندازه را فراهم کند. داده به صورت کاملا یکسان روی هر دو دیسک ذخیره می‌شود، که همان عبارت “mirror” است. زمانی که یک دیسک خراب شود داده از دیسک دیگر قابل دسترس است. برای داده‌های بسیار حیاتی، RAID-1 می‌تواند با بیش از دو دیسک تنظیم شود، که تاثیر مستقیم روی نرخ هزینه سخت‌افزار در مقابل فضای موجود خواهد گذاشت.
							</para>
							 <sidebar> <title><emphasis>یادداشت</emphasis> دیسک‌ها و اندازه‌های خوشه</title>
							 <para>
								اگر دو دیسک با اندازه مختلف در این سطح بکار گرفته شوند، از دیسک بزرگ‌تر به طور کامل استفاده نخواهد شد چرا که تنها شامل تمام داده‌های دیسک کوچک‌تر است. بنابراین فضای قابل استفاده در یک تنظیم RAID-1 برابر با کوچک‌ترین دیسک موجود در آرایه است. این الگو در رابطه با سایر سطوح RAID پیشرفته‌تر نیز صادق است، با اینکه ممکن است از فرآیند افزونگی دیگری استفاده کنند.
							</para>
							 <para>
								به همین دلیل، مهم است که در زمان برپایی آرایه‌های RAID (بجز RAID-0 و خطی) از دیسک‌های با اندازه مشابه یا بسیار نزدیک به هم استفاده شود تا هیچ فضای اضافی تلف نگردد.
							</para>
							 </sidebar> <sidebar> <title><emphasis>یادداشت</emphasis> دیسک‌های کمکی</title>
							 <para>
								سطوح RAID که شامل افزونگی داده هستند اجازه استفاده از دیسک‌های بیشتر در آرایه دیسک‌ها را فراهم می‌کنند. از این دیسک‌ها به منظور پشتیبان در زمان بروز نقص برای یکی از دیسک‌های موجود استفاده می‌شود. برای نمونه، در یک تنظیم دو دیسکه همراه با دیسک کمکی، اگر یکی از دیسک‌های اولیه معیوب شود، کرنل به صورت خودکار (و بلافاصله) فضای موجود را با استفاده از دیسک کمکی بازسازی می‌کند، به منظور اینکه اطمینان از عملیات افزونگی داده پس از زمان بازسازی حاصل گردد. از این روش می‌توان به عنوان گام اضافه برای نگهداری از داده‌های بسیار حساس استفاده کرد.
							</para>
							 <para>
								ممکن است به نظر آید که چطور این پیکربندی در مقایسه با استفاده از سه دیسک برای mirroring ممکن است مفید باشد. مزیت استفاده از پیکربندی “دیسک کمکی” این است که می‌تواند بین چندین فضای ذخیره‌سازی RAID به اشتراک گذاشته شود. برای نمونه، می‌توان از سه فضای ذخیره‌سازی mirrored استفاده کرد، همراه با افزونگی داده که در صورت بروز نقص در یکی از دیسک‌ها بکار می‌آید، همراه با هفت دیسک (سه زوج به همراه یک دیسک کمکی) بجای نه دیسک که مورد نیاز سه خانواده سه زوجی است.
							</para>
							 </sidebar> <para>
								این سطح RAID، با وجود هزینه بالا (از آنجا که در بهترین حالت از نصف فضای ذخیره‌سازی استفاده می‌شود) در عمل بسیار مورد استفاده قرار می‌گیرد. درک آن بسیار ساده است و امکان ایجاد پشتیبان‌های ساده وجود دارد: از آنجا که هر دو دیسک شامل محتوای یکسانی هستند، یکی از آن‌ها بدون ایجاد کوچکترین تاثیر منفی روی سیستم می‌تواند تخلیه شود. عملیات خواندن نیز بسیار سریع‌تر خواهد بود چرا که در هر لحظه کرنل به صورت همزمان نصف داده را از هر دو دیسک می‌تواند بخواند، با اینکه عملیات نوشتن آنطور که به نظر می‌آید تاثیر منفی ندارد. در مورد آرایه RAID-1 از N دیسک، حتی در صورت معیوب شدن N-1 دیسک داده کماکان قابل دسترس خواهد بود.
							</para>

						</listitem>

					</varlistentry>
					 <varlistentry>
						<term>RAID-4</term>
						 <listitem>
							<para>
								این سطح RAID، که کاربرد زیادی ندارد، از N دیسک برای ذخیره‌سازی داده مفید و از یک دیسک اضافی برای ذخیره‌سازی اطلاعات افزونگی استفاده می‌کند. اگر این دیسک اضافی معیوب شود، سیستم می‌تواند با آن N دیسک دیگر محتوای خود را بازسازی کند، به صورتی که N-1 دیسک باقیمانده همراه با دیسک “parity” شامل اطلاعات کافی برای بازسازی تمام اطلاعات هستند.
							</para>
							 <para>
								RAID-4 هزینه زیادی ندارد چرا که تنها شامل یک هزینه افزایشی یک در N می‌باشد که تاثیر بسزایی روی عملیات خواندن ندارد ولی عملیات نوشتن را کند می‌کند. علاوه بر این، از آنجا که نوشتن روی هر یک از N دیسک به معنای نوشتن روی دیسک parity است، این دیسک اضافی شاهد نوشتن‌های بیشتری نسبت به سایر دیسک‌ها است و همین دلیل نیز باعث کاهش طول عمر مفید آن می‌گردد. داده روی آرایه RAID-4 تنها در صورت معیوب شدن یک دیسک از N+1 دیسک موجود ایمن خواهد بود.
							</para>

						</listitem>

					</varlistentry>
					 <varlistentry>
						<term>RAID-5</term>
						 <listitem>
							<para>
								RAID-5 مشکل عدم تقارن در RAID-4 را برطرف می‌کند: بلاک‌های parity بین تمام N+1 دیسک دیگر پخش می‌شوند به صورتی که تنها یک دیسک نقش منحصربفرد نداشته باشد.
							</para>
							 <para>
								عملیات خواندن و نوشتن درست مانند RAID-4 است. در اینجا نیز، سیستم تا زمانی بکار خود ادامه می‌دهد که تنها یک دیسک معیوب از N+1 دیسک موجود باشد ولی نه بیشتر.
							</para>

						</listitem>

					</varlistentry>
					 <varlistentry>
						<term>RAID-6</term>
						 <listitem>
							<para>
								RAID-6 می‌تواند به عنوان افزونه‌ای برای RAID-5 در نظر گرفته شود، به صورتی که هر سری از N بلاک شامل دو بلاک افزونگی داده است و هر یک از این N+2 بلاک میان N+2 دیسک تقسیم می‌شود.
							</para>
							 <para>
								این سطح RAID در مقایسه با دو سطح قبلی هزینه بیشتری در پی دارد، ولی امنیت بیشتری نیز به همراه می‌آورد چرا که تا دو درایو از N+2 دیسک موجود در صورت معیوب شدن می‌توانند بکار خود ادامه دهند. نقطه مقابل آن این است که عملیات نوشتن شامل یک بلاک داده و دو بلاک افزونگی دیگر است، که این آرایه را در مقایسه با سطوح دیگر کندتر نیز می‌کند.
							</para>

						</listitem>

					</varlistentry>
					 <varlistentry>
						<term>RAID-1+0</term>
						 <listitem>
							<para>
								این به خودی خود یک سطح RAID بحساب نمی‌آید، بلکه بیشتر یک گروه‌بندی از سطوح دیگر است. با استفاده از 2N دیسک، مجموعه اول به N دیسک RAID-1 تقسیم می‌شود؛ سپس این فضای ذخیره‌سازی به یک واحد مجزا تبدیل می‌شود خواه با “RAID خطی” یا استفاده از LVM. این گزینه آخر چیزی بیش از RAID خالص است، اما مشکلی در استفاده از آن وجود ندارد.
							</para>
							 <para>
								RAID-1+0 می‌تواند از چندین نقص دیسک فرار کند: تا N دیسک از 2N آرایه تعریف شده بالا که برای هر کدام از زوج‌های RAID-1 فراهم شده است، می‌تواند معیوب شود.
							</para>
							 <sidebar id="sidebar.raid-10"> <title><emphasis>مطالعه بیشتر</emphasis> RAID-10</title>
							 <para>
								RAID-10 بیشتر به عنوان مترادفی برای RAID-1+0 استفاده می‌شود، اما در لینوکس به عنوان یک استاندارد بکار گرفته می‌شود. این تنظیم شامل سیستمی است که هر بلاک آن درون دو دیسک متفاوت ذخیره می‌شود، حتی با تعداد دیسک‌های فرد، عملیات رونوشت‌گیری با استفاده از مدل قابل پیکربندی به صورت بهینه انجام می‌شود.
							</para>
							 <para>
								عملکرد کلی سیستم با توجه به سطح افزونگی و مدل پارتیشن‌بندی متفاوت خواهد بود، همچنین فضای ذخیره‌سازی منطقی نیز روی آن تاثیرگذار است.
							</para>
							 </sidebar>
						</listitem>

					</varlistentry>

				</variablelist>
				 <para>
					به طور مشخص، سطح RAID با توجه به محدودیت‌ها و نیازمندی‌های سیستم موجود انتخاب می‌شود. نکته اینکه یک رایانه می‌تواند شامل چندین آرایه RAID منحصربفرد به همراه پیکربندی‌های متفاوت باشد.
				</para>

			</section>
			 <section id="sect.raid-setup">
				<title>برپایی RAID</title>
				 <indexterm>
					<primary><emphasis role="pkg">mdadm</emphasis></primary>
				</indexterm>
				 <para>
					برپایی آرایه‌های RAID نیازمند بسته <emphasis role="pkg">mdadm</emphasis> است؛ که شامل دستور <command>mdadm</command> برای ایجاد و ویرایش این آرایه‌ها می‌باشد همراه با ابزارهای جانبی و اسکریپت‌هایی که آن را با سایر قسمت‌های سیستم از جمله مانیتورینگ یکپارچه می‌سازد.
				</para>
				 <para>
					مثال ما شامل سروری با چندین دیسک است که برخی از آن‌ها استفاده شده و برخی دیگر که آزاد هستند به عنوان RAID بکار گرفته می‌شوند. در حالت اولیه دیسک‌ها و پارتیشن‌های زیر را در اختیار داریم:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							دیسک ۴ گیگابایت <filename>sdb</filename> کاملا موجود است؛
						</para>

					</listitem>
					 <listitem>
						<para>
							دیسک ۴ گیگابایت <filename>sdc</filename> کاملا موجود است؛
						</para>

					</listitem>
					 <listitem>
						<para>
							در دیسک <filename>sdd</filename> تنها پارتیشن ۴ گیگابایت <filename>sdd2</filename> موجود است؛
						</para>

					</listitem>
					 <listitem>
						<para>
							در نهایت، دیسک ۴ گیگابایت <filename>sde</filename> که کاملا موجود است.
						</para>

					</listitem>

				</itemizedlist>
				 <sidebar> <title><emphasis>یادداشت</emphasis> شناسایی آرایه‌های موجود RAID</title>
				 <para>
					فایل <filename>/proc/mdstat</filename> فهرستی از آرایه‌های موجود و شرایط آن‌ها را نگهداری می‌کند. در زمان ایجاد یک آرایه جدید RAID، باید دقت کرد که نام آن با نام آرایه‌ای موجود برابر نباشد.
				</para>
				 </sidebar> <para>
					با استفاده از این دیسک‌ها می‌خواهیم دو فضای ذخیره‌سازی بوجود آوریم، یکی RAID-0 و دیگری RAID-1. بیایید با آرایه RAID-0 شروع کنیم:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</userinput>
<computeroutput>mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
# </computeroutput><userinput>mdadm --query /dev/md0</userinput>
<computeroutput>/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.
# </computeroutput><userinput>mdadm --detail /dev/md0</userinput>
<computeroutput>/dev/md0:
        Version : 1.2
  Creation Time : Wed May  6 09:24:34 2015
     Raid Level : raid0
     Array Size : 8387584 (8.00 GiB 8.59 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:24:34 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

     Chunk Size : 512K

           Name : mirwiz:0  (local to host mirwiz)
           UUID : bb085b35:28e821bd:20d697c9:650152bb
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       1       8       32        1      active sync   /dev/sdc
# </computeroutput><userinput>mkfs.ext4 /dev/md0</userinput>
<computeroutput>mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 2095104 4k blocks and 524288 inodes
Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </computeroutput><userinput>mkdir /srv/raid-0</userinput>
<computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0</userinput>
<computeroutput># </computeroutput><userinput>df -h /srv/raid-0</userinput>
<computeroutput>Filesystem      Size  Used Avail Use% Mounted on
/dev/md0        7.9G   18M  7.4G   1% /srv/raid-0
</computeroutput></screen>
				 <para>
					دستور <command>mdadm --create</command> نیازمند چند پارامتر است: نام آرایه‌ای که قصد ایجادش را داریم (<filename>/dev/md*</filename> که MD به معنی <foreignphrase>Multiple Device</foreignphrase> است)، سطح RAID، تعداد دیسک‌ها (که اجباری است و از RAID-1 به بالا معنا دارد) و درایوهای فیزیکی قابل استفاده. زمانی که دستگاه ایجاد گردد، مانند یک پارتیشن عادی می‌توانیم از آن استفاده کرده، فایل سیستم ایجاد کنیم و آن را متصل سازیم. نکته اینکه ایجاد آرایه RAID-0 روی <filename>md0</filename> تصادفی است و این شماره‌گذاری هیچ ارتباطی به سطوح مختلف RAID ندارد. همچنین امکان ایجاد آرایه‌های نامگذاری شده RAID نیز با استفاده از پارامترهایی نظیر <filename>/dev/md/linear</filename> بجای <filename>/dev/md0</filename> در دستور <command>mdadm</command> وجود دارد.
				</para>
				 <para>
					ایجاد یک آرایه RAID-1 مشابه قبل است که تفاوت آن تنها پس از فرآیند ایجاد مشخص می‌گردد:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</userinput>
<computeroutput>mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%
Continue creating array? </computeroutput><userinput>y</userinput>
<computeroutput>mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md1 started.
# </computeroutput><userinput>mdadm --query /dev/md1</userinput>
<computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
        Version : 1.2
  Creation Time : Wed May  6 09:30:19 2015
     Raid Level : raid1
     Array Size : 4192192 (4.00 GiB 4.29 GB)
  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:30:40 2015
          State : clean, resyncing (PENDING) 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       1       8       64        1      active sync   /dev/sde
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
          State : clean
[...]
</computeroutput></screen>
				 <sidebar> <title><emphasis>نکته</emphasis> RAID، دیسک‌ها و پارتیشن‌ها</title>
				 <para>
					همانطور که در مثال نیز مشخص است، دستگاه‌های RAID بدون پارتیشن‌بندی‌های دیسک نیز می‌توانند ایجاد گردند و نیازمند دیسک‌های کامل نمی‌باشند.
				</para>
				 </sidebar> <para>
					چند نکته باقی می‌ماند. اول، <command>mdadm</command> تشخیص می‌دهد که دستگاه‌های فیزیکی شامل اندازه‌های متفاوت هستند؛ از آنجا که این امر منجر به گم شدن فضا در دیسک بزرگتر می‌شود، تاییدیه آن مورد نیاز است.
				</para>
				 <para>
					مهمتر از آن به حالت دیسک mirror توجه کنید. حالت عادی در آرایه RAID که به صورت mirror باشد مشابهت کامل محتوا در هر دو دیسک است. با این حال، ضمانتی برای بوجود آمدن این حالت در زمان ایجاد آرایه وجود ندارد. زیرمجموعه RAID خود این ضمانت را ایجاد می‌کند و به محض اینکه دستگاه RAID ساخته شود عملیات همگام‌سازی صورت می‌گیرد. بعد از گذشت زمان (که وابسته به اندازه‌ دیسک‌ها است) آرایه RAID به حالت “فعال” یا “تمیز” تغییر می‌یابد. نکته اینکه در زمان این فاز بازسازی، mirror در یک حالت ناپایدار قرار می‌گیرد که افزونگی داده در آن تضمین نمی‌شود . دیسکی که در این بازه زمانی دچار مشکل گردد ممکن است به حذف داده بینجامد. با توجه به این موضوع، قبل از فاز همگام‌سازی معمولا داده‌های بزرگ و حساس روی آرایه RAID قرار نمی‌گیرند. حتی در حالت ناپایدار نیز، <filename>/dev/md1</filename> قابل استفاده است و یک فایل سیستم می‌تواند روی آن ایجاد گردد و داده‌های روی آن قرار گیرند.
				</para>
				 <sidebar> <title><emphasis>نکته</emphasis> آغاز یک mirror در حالت ناپایدار</title>
				 <para>
					بعضی وقت‌ها در زمان آغاز mirror از RAID-1 دو دیسک بلافاصله قابل دسترس نخواهند بود، برای نمونه به این دلیل که یکی از دیسک‌ها برای ذخیره داده‌هایی استفاده شده است که دیگری می‌خواهد از آن mirror بگیرد. در چنین شرایطی، امکان ارجاع یک آرایه RAID-1 ناپایدار با استفاده از پارامتر <filename>missing</filename> بجای نام دستگاه در <command>mdadm</command> وجود دارد. زمانی که از داده در “mirror” رونوشت گرفته شود، دیسک قدیمی می‌تواند به آرایه اضافه گردد. سپس همگام‌سازی صورت می‌گیرد که امکان افزونگی داده با استفاده از دیسک قدیمی را فراهم می‌آورد.
				</para>
				 </sidebar> <sidebar> <title><emphasis>نکته</emphasis> برپایی یک mirror بدون همگام‌سازی</title>
				 <para>
					آرایه‌های RAID-1 اغلب به منظور ایجاد یک دیسک جدید و خالی استفاده می‌شوند. از این رو محتوای اولیه دیسک خیلی حائز اهمیت نیست، از این رو باید دانست داده‌ای که پس از ایجاد فایل سیستم در آرایه قرار می‌گیرد قابل دسترس خواهد بود.
				</para>
				 <para>
					شاید این سوال پیش بیاید که هدف از همگام‌سازی دو دیسک در زمان ایجاد آرایه چیست. چرا به یکسان بودن محتوایی که روی ناحیه‌های این آرایه قرار می‌گیرد اهمیت بدهیم وقتی می‌دانیم تنها پس از نوشتن روی آن است که می‌توان به آن‌ها دسترسی داشت؟
				</para>
				 <para>
					خوشبختانه، این فاز همگام‌سازی می‌تواند با استفاده از گزینه <literal>--assume-clean</literal> در <command>mdadm</command> در نظر گرفته نشود. اگرچه، این گزینه ممکن است منجر به سردرگمی در مواردی شود که داده اولیه خوانده خواهد شد (برای نمونه، وقتی یک فایل سیستم هم اکنون روی دیسک موجود باشد)، به همین دلیل است که به صورت پیشفرض فعال نیست.
				</para>
				 </sidebar> <para>
					اکنون بیایید ببینیم در زمان بروز مشکل برای یکی از آرایه‌های RAID-1 چه اتفاقی می‌افتد. <command>mdadm</command> و به طور خاص گزینه <literal>--fail</literal> آن، امکان شبیه‌سازی این نقص دیسک را بوجود می‌آورد:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde</userinput>
<computeroutput>mdadm: set /dev/sde faulty in /dev/md1
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
    Update Time : Wed May  6 09:39:39 2015
          State : clean, degraded 
 Active Devices : 1
Working Devices : 1
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 19

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       0        0        2      removed

       1       8       64        -      faulty   /dev/sde</computeroutput></screen>
				 <para>
					محتوای آرایه هنوز قابل دسترس است (و در صورت اتصال به فایل سیستم، وقفه‌ای در برنامه‌ها ایجاد نمی‌شود) اما امنیت داده دیگر تضمین نمی‌شود: در صورت بروز نقص برای دیسک <filename>sdd</filename> داده از بین می‌رود. به منظور پیشگیری از این خطر دیسک معیوب را با <filename>sdf</filename> جایگزین می‌کنیم:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>
<computeroutput>mdadm: added /dev/sdf
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
   Raid Devices : 2
  Total Devices : 3
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:48:49 2015
          State : clean, degraded, recovering 
 Active Devices : 1
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 1

 Rebuild Status : 28% complete

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 26

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      spare rebuilding   /dev/sdf

       1       8       64        -      faulty   /dev/sde
# </computeroutput><userinput>[...]</userinput>
<computeroutput>[...]
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
    Update Time : Wed May  6 09:49:08 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 41

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf

       1       8       64        -      faulty   /dev/sde</computeroutput></screen>
				 <para>
					در اینجا نیز، کرنل به صورت خودکار فاز بازسازی آرایه را آغاز می‌کند که طی آن با وجود قابل دسترس بودن، آرایه در یک حالت ناپایدار قرار دارد. زمانی که بازسازی تمام شود، آرایه RAID به حالت عادی خود باز می‌گردد. به منظور سازگاری با حالت کلاسیک RAID که از دو دیسک برای mirror استفاده می‌کند، می‌توان دیسک <filename>sde</filename> را حذف کرد؛
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>
<computeroutput>mdadm: hot removed /dev/sde from /dev/md1
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf</computeroutput></screen>
				 <para>
					از این لحظه، درایو می‌تواند در زمان خاموش شدن سرور یا در صورت پشتیبانی سخت‌افزاری از how-swap به صورت دستی جدا گردد. چنین پیکربندی شامل برخی کنترلرهای SCSI، اغلب دیسک‌های SATA و درایوهای خارجی که روی USB یا Firewire است.
				</para>

			</section>
			 <section id="sect.backup-raid-config">
				<title>پشتیبان‌گیری از پیکربندی</title>
				 <para>
					اکثر اطلاعات جانبی درباره آرایه‌های RAID به صورت مستقیم روی همین دیسک‌ها ذخیره‌سازی می‌شود، تا کرنل در زمان راه‌اندازی اولیه سیستم بتواند به صورت خودکار اجزای آرایه را تشکیل داده و آن را تنظیم کند. با این حال، پشتیبان‌گیری از این پیکربندی توصیه می‌شود چرا که این فرآیند تشخیص اولیه خالی از خطا نیست و تنها انتظار می‌رود که در موارد بسیار معدود دچار نقص گردد. در مثال ما، اگر نقص دیسک <filename>sde</filename> واقعی (در مقابل شبیه‌سازی شده) باشد و سیستم بدون حذف <filename>sde</filename> راه‌اندازی مجدد گردد، این دیسک ممکن است به فعالیت خود پس از عملیات تشخیص اولیه ادامه دهد. کرنل شامل سه دیسک فیزیکی است که هر کدام ادعا می‌کنند نصف فضای RAID را در اختیار دارند. حالت مبهم دیگر ترکیب آرایه‌های RAID از دو سرور مختلف در قالب یک سرور است. اگر این آرایه‌ها قبل از انتقال دیسک‌ها درست کار کنند، کرنل قادر خواهد بود که اجزای آن را شناسایی و پیکربندی کند؛ اما اگر دیسک‌های انتقال یافته درون آرایه <filename>md1</filename> از سرور قدیم قرار بگیرند، در صورتی که سرور جدید آرایه <filename>md1</filename> داشته باشد، یکی از mirrorها نامگذاری مجدد خواهد شد.
				</para>
				 <para>
					از این رو، پشتیبان‌گیری از فایل پیکربندی اهمیت می‌یابد. شیوه استاندارد اینکار ویرایش فایل <filename>/etc/mdadm/mdadm.conf</filename> است که مثالی از آن را در ادامه مشاهده می‌کنید:
				</para>
				 <example id="example.mdadm-conf">
					<title>فایل پیکربندی<command>mdadm</command></title>
					 
<programlisting># mdadm.conf
#
# Please refer to mdadm.conf(5) for information about this file.
#

# by default (built-in), scan all partitions (/proc/partitions) and all
# containers for MD superblocks. alternatively, specify devices to scan, using
# wildcards if desired.
DEVICE /dev/sd*

# auto-create devices with Debian standard permissions
CREATE owner=root group=disk mode=0660 auto=yes

# automatically tag new arrays as belonging to the local system
HOMEHOST &lt;system&gt;

# instruct the monitoring daemon where to send mail alerts
MAILADDR root

# definitions of existing MD arrays
ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464

# This configuration was auto-generated on Thu, 17 Jan 2013 16:21:01 +0100
# by mkconf 3.2.5-3</programlisting>

				</example>
				 <para>
					یکی از جزئیات کاربری آن گزینه <literal>DEVICE</literal> است، که دستگاه‌های مورد نیاز برای جستجوی خودکار اجزای آرایه‌های RAID در سیستم را مشخص می‌کند. در مثال ما، ما گزینه پیشفرض <literal>partitions containers</literal> را با فهرستی از دستگاه‌ها جایگزین کردیم چرا که قصد استفاده از تمام دیسک و نه قسمت‌هایی از پارتیشن آن را برای برخی آرایه‌ها داشتیم.
				</para>
				 <para>
					دو خط آخر در مثال ما به کرنل اجازه می‌دهند که با استفاده از شماره آرایه عملیات تشخیص و راه‌اندازی آن‌ها را انجام دهد. اطلاعات جانبی ذخیره شده روی دیسک برای جمع‌آوری آرایه‌ها کافی است، ولی نه برای تشخیص شماره آن‌ها (و نام دستگاه <filename>/dev/md*</filename> منطبق با آن).
				</para>
				 <para>
					خوشبختانه، این خطوط به صورت خودکار تولید می‌شوند:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>
<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</computeroutput></screen>
				 <para>
					محتوای این دو خط آخر وابسته به تعداد دیسک‌های استفاده شده در آرایه نیست. پس هنگام جایگزینی یک دیسک معیوب با جدید نیازی به تولید مجدد این خطوط نیست. از طرف دیگر، در زمان ایجاد یا حذف یک آرایه RAID، این فایل باید بروزرسانی گردد.
				</para>

			</section>

		</section>
		 <section id="sect.lvm">
			<title>LVM</title>
			 <indexterm>
				<primary>LVM</primary>
			</indexterm>
			 <indexterm>
				<primary>Logical Volume Manager</primary>
			</indexterm>
			 <para>
				LVM که مخفف <emphasis>Logical Volume Manager</emphasis> است، روشی دیگر برای انتزاع دستگاه‌های منطقی از نمونه‌های فیزیکی است که بجای قابلیت اعتماد روی افزایش انعطاف‌پذیری تمرکز دارد. LVM امکان تغییر یک دستگاه منطقی را به شیوه‌ای شفاف برای برنامه‌های کاربردی آن بوجود می‌آورد؛ برای نمونه، امکان افزودن دیسک‌های جدید، مهاجرت داده به آن‌ها و حذف دیسک‌های قدیمی بدون قطع اتصال دستگاه مجازی وجود دارد.
			</para>
			 <section id="sect.lvm-concepts">
				<title>مفاهیم LVM</title>
				 <para>
					این انعطاف‌پذیری از طریق یک سطح انتزاعی همراه با سه مفهوم بدست می‌آید.
				</para>
				 <para>
					اول، PV یا <emphasis>Physical Volume</emphasis> نزدیک‌ترین موجودیت به سخت‌افزار است: می‌تواند پارتیشن‌های روی یک دیسک، یک دیسک کامل یا حتی سایر دستگاه‌های بلاک-محور (از جمله یک آرایه RAID) باشد. به یاد داشته باشید زمانی که یک عنصر فیزیکی به عنوان PV برای LVM تنظیم می‌شود، تنها باید توسط LVM قابل دسترس باشد در غیر اینصورت سیستم دچار سردرگمی می‌گردد.
				</para>
				 <para>
					تعدادی از PVها می‌توانند درون یک خوشه VG یا <emphasis>Volume Group</emphasis> قرار بگیرند که می‌تواند با دیسک‌های مجازی و توسعه‌یافته مقایسه گردد. VGها انتزاعی هستند و درون سلسله‌مراتب <filename>/dev</filename> به عنوان یک فایل ظاهر نمی‌شوند، بنابراین خطری در استفاده مستقیم از آن‌ها وجود ندارد.
				</para>
				 <para>
					سومین مفهوم نیز LV یا <emphasis>Logical Volume</emphasis> نام دارد، که تکه‌ای از یک VG به حساب می‌آید؛ اگر VG را با یک دیسک مقایسه کنیم، LV مانند یک پارتیشن خواهد بود. LV به عنوان یک دستگاه بلاک-محور همراه با مدخلی در <filename>/dev</filename> ظاهر می‌شود، که می‌تواند به عنوان هر پارتیشن فیزیکی دیگر مورد استفاده قرار گیرد (بیشتر در مورد یک فایل سیستم میزبان یا فضای swap).
				</para>
				 <para>
					نکته مهم در تقسیم یک VG به LV این است که کاملا مستقل از اجزای فیزیکی آن (PV) انجام می‌شود. یک VG تنها با یک قسمت فیزیکی (مانند یک دیسک) می‌تواند به چندین دستگاه منطقی تقسیم شود؛ به طور مشابه، یک VG با چندین دیسک فیزیکی می‌تواند به عنوان یک دستگاه منطقی بزرگ ظاهر شود. تنها محدودیت مشخص این است که اندازه کل اختصاص یافته به LVها نمی‌تواند بیشتر از مجموع اندازه PVها در گروه دستگاه‌ها باشد.
				</para>
				 <para>
					اغلب منطقی است که به منظور داشتن همگنی بین اجزای فیزیکی یک VG، آن را به دستگاه‌های مجازی تقسیم کرد که الگوهای مشابهی در کارکرد داشته باشند. برای نمونه، اگر سخت‌افزار موجود شامل دیسک‌های سریع و کند باشد، دیسک‌های سریع می‌توانند درون یک VG و دیسک‌های کند درون دیگری قرار گیرند؛ تکه‌های اولی می‌توانند به برنامه‌هایی اختصاص یابند که نیازمند دسترسی سریع به دیسک هستند، در صورتی که از دومی برای سایر وظایف متداول دیسک استفاده می‌شود.
				</para>
				 <para>
					در هر صورت، به خاطر بسپارید که یک LV به طور مشخص به هیچ PV متصل نیست. امکان تاثیرگذاری روی جایی که داده از یک LV به صورت فیزیکی می‌آید وجود دارد، اما این امکان برای کاربردهای روزانه الزامی نیست. از طرف دیگر، زمانی که مجموعه فیزیکی از اجزای یک VG گسترش می‌یابند، مکان ذخیره‌سازی فیزیکی منطبق با یک LV می‌توانند بین چند دیسک مهاجرت کنند (به صورتی که درون PVهای اختصاص‌یافته به VG قرار داشته باشند).
				</para>

			</section>
			 <section id="sect.lvm-setup">
				<title>برپایی LVM</title>
				 <para>
					بیایید فرآیند گام به گام برپایی LVM برای یک کاربرد متداول را دنبال کنیم: می‌خواهیم یک موقعیت ذخیره‌سازی پیچیده را ساده کنیم. چنین موقعیتی معمولا با گذشت زمان و گره خوردن مقیاس‌های موقتی انباشتگی صورت می‌گیرد. برای این منظور، سروری را در نظر می‌گیریم که نیازهای ذخیره‌سازی آن طی زمان تغییر کرده است که پارتیشن‌های موجود آن بین چندین دیسک فیزیکی مختلف به مانند یک مسیر مارپیچ قرار گرفته‌اند. به عبارت دیگر، پارتیشن‌های زیر موجود هستند:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							درون دیسک <filename>sdb</filename>،‌ یک پارتیشن ۴ گیگابایت به نام <filename>sdb2</filename>؛
						</para>

					</listitem>
					 <listitem>
						<para>
							درون دیسک <filename>sdc</filename>،‌ یک پارتیشن ۳ گیگابایت به نام <filename>sdc3</filename>؛
						</para>

					</listitem>
					 <listitem>
						<para>
							دیسک <filename>sdd</filename>،‌ با ظرفیت ۴ گیگابایت کاملا موجود؛
						</para>

					</listitem>
					 <listitem>
						<para>
							درون دیسک <filename>sdf</filename>،‌ یک پارتیشن ۴ گیگابایت به نام <filename>sdf1</filename> و یک پارتیشن ۵ گیگابایت به نام <filename>sdf2</filename>.
						</para>

					</listitem>

				</itemizedlist>
				 <para>
					علاوه بر این، تصور می‌کنیم که دیسک‌های <filename>sdb</filename> و <filename>sdf</filename> سریع‌تر از دو دیسک دیگر هستند.
				</para>
				 <para>
					هدف ما برپایی یه دستگاه منطقی برای سه برنامه مختلف است: یک سرور فایل که به ۵ گیگابایت فضای ذخیره‌سازی نیاز دارد، یک پایگاه‌داده (۱ گیگابایت) و فضایی برای پشتیبان‌گیری (۱۲ گیگابایت). دوتای اول به عملکرد بالا نیاز دارند اما پشتیبان‌گیری چنین حساسیتی در دسترسی سریع ندارد. تمام این محدودیت‌ها از استفاده پارتیشن‌ها به صورت مستقیم جلوگیری می‌کنند؛ استفاده از LVM می‌تواند اندازه فیزیکی از دستگاه‌ها را انتزاعی کند، که تنها محدودیت آن مجموع فضای ذخیره‌سازی است.
				</para>
				 <para>
					ابزارهای مورد نیاز در بسته <emphasis role="pkg">lvm2</emphasis> و وابستگی‌های آن قرار دارند. زمانی که نصب شوند، برپایی LVM شامل سه گام می‌شود که با سه سطح از مفاهیم آن مرتبط است.
				</para>
				 <para>
					ابتدا دستگاه‌های فیزیکی را با استفاده از <command>pvcreate</command> آماده‌سازی می‌کنیم:
				</para>
				 
<screen id="screen.pvcreate"><computeroutput># </computeroutput><userinput>pvdisplay</userinput>
<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2</userinput>
<computeroutput>  Physical volume "/dev/sdb2" successfully created
# </computeroutput><userinput>pvdisplay</userinput>
<computeroutput>  "/dev/sdb2" is a new physical volume of "4.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/sdb2
  VG Name               
  PV Size               4.00 GiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I

# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</userinput>
<computeroutput>  Physical volume "/dev/sdc3" successfully created
  Physical volume "/dev/sdd" successfully created
  Physical volume "/dev/sdf1" successfully created
  Physical volume "/dev/sdf2" successfully created
# </computeroutput><userinput>pvdisplay -C</userinput>
<computeroutput>  PV         VG   Fmt  Attr PSize PFree
  /dev/sdb2       lvm2 ---  4.00g 4.00g
  /dev/sdc3       lvm2 ---  3.09g 3.09g
  /dev/sdd        lvm2 ---  4.00g 4.00g
  /dev/sdf1       lvm2 ---  4.10g 4.10g
  /dev/sdf2       lvm2 ---  5.22g 5.22g
</computeroutput></screen>
				 <para>
					تا اینجا مشکلی نیست؛ به یاد داشته باشید که یک PV می‌تواند روی یک دیسک کامل یا پارتیشن‌های انفرادی ایجاد گردد. دستور <command>pvdisplay</command> فهرستی از PVها را با دو قالب خروجی ممکن نمایش می‌دهد.
				</para>
				 <para>
					اکنون بیایید این عناصر فیزیکی را با استفاده از <command>vgcreate</command> درون VG قرار دهیم. PV دیسک‌های سریع را درون VG به نام <filename>vg_critical</filename> و دیسک‌های کند را درون VG به نام <filename>vg_normal</filename> قرار می‌دهیم.
				</para>
				 
<screen id="screen.vgcreate"><computeroutput># </computeroutput><userinput>vgdisplay</userinput>
<computeroutput>  No volume groups found
# </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1</userinput>
<computeroutput>  Volume group "vg_critical" successfully created
# </computeroutput><userinput>vgdisplay</userinput>
<computeroutput>  --- Volume group ---
  VG Name               vg_critical
  System ID             
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               8.09 GiB
  PE Size               4.00 MiB
  Total PE              2071
  Alloc PE / Size       0 / 0   
  Free  PE / Size       2071 / 8.09 GiB
  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp

# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</userinput>
<computeroutput>  Volume group "vg_normal" successfully created
# </computeroutput><userinput>vgdisplay -C</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize  VFree 
  vg_critical   2   0   0 wz--n-  8.09g  8.09g
  vg_normal     3   0   0 wz--n- 12.30g 12.30g
</computeroutput></screen>
				 <para>
					در اینجا نیز دستورات واضح هستند و <command>vgdisplay</command> شامل دو قالب خروجی است. به یاد داشته باشید که امکان استفاده از دو پارتیشن یک دیسک فیزیکی درون دو VG مختلف وجود دارد. ما از یک پیشوند <filename>vg_</filename> برای نامگذاری VGها استفاده کردیم، اما چیزی بیشتر از رعایت یک استاندارد نیست.
				</para>
				 <para>
					اکنون دو “دیسک مجازی” به اندازه‌های ۸ و ۱۲ گیگابایت داریم. حال بیایید آن‌ها را به “پارتیشن‌های مجازی” یا LV تقسیم کنیم. اینکار با استفاده از دستور <command>lvcreate</command> و شیوه نگارشی پیچیده‌تر از گام‌های قبلی صورت می‌گیرد:
				</para>
				 
<screen id="screen.lvcreate"><computeroutput># </computeroutput><userinput>lvdisplay</userinput>
<computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical</userinput>
<computeroutput>  Logical volume "lv_files" created
# </computeroutput><userinput>lvdisplay</userinput>
<computeroutput>  --- Logical volume ---
  LV Path                /dev/vg_critical/lv_files
  LV Name                lv_files
  VG Name                vg_critical
  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT
  LV Write Access        read/write
  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400
  LV Status              available
  # open                 0
  LV Size                5.00 GiB
  Current LE             1280
  Segments               2
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0

# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical</userinput>
<computeroutput>  Logical volume "lv_base" created
# </computeroutput><userinput>lvcreate -n lv_backups -L 12G vg_normal</userinput>
<computeroutput>  Logical volume "lv_backups" created
# </computeroutput><userinput>lvdisplay -C</userinput>
<computeroutput>  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_base    vg_critical -wi-a---  1.00g                                           
  lv_files   vg_critical -wi-a---  5.00g                                           
  lv_backups vg_normal   -wi-a--- 12.00g</computeroutput></screen>
				 <para>
					برای ایجاد دستگاه‌های منطقی دو پارامتر مورد نیاز است؛ که باید به صورت گزینه‌ها به <command>lvcreate</command> ارسال گردند. نام LV که قصد ایجاد آن را داریم با گزینه <literal>-n</literal> و اندازه آن با گزینه <literal>-L</literal> مشخص می‌شود. البته، به دستور باید اعلام کنیم که از کدام VG می‌خواهیم استفاده شود.
				</para>
				 <sidebar> <title><emphasis>مطالعه بیشتر</emphasis> گزینه‌های <command>lvcreate</command></title>
				 <para>
					دستور <command>lvcreate</command> شامل چندین گزینه است که چگونگی ایجاد LV را مشخص می‌کند.
				</para>
				 <para>
					ابتدا بیایید گزینه <literal>-l</literal> را توضیح دهیم، که با استفاده از آن اندازه LV می‌تواند به عنوان تعداد بلاک‌ها در مقایسه با واحدهای “انسانی” که در بالا استفاده کردیم مشخص گردد. این بلاک‌ها که به نام PE یا <emphasis>physical extents</emphasis> در زبان LVM مطرح می‌شوند، واحدهای پیوسته از فضای ذخیره‌سازی در PVها می‌باشند که نمی‌توان آن‌ها را بین LVها تقسیم کرد. زمانی که می‌خواهیم یک فضای ذخیره‌سازی با دقت بالا را برای یک LV تعریف کنیم، برای نمونه استفاده از فضای کامل، استفاده از گزینه <literal>-l</literal> بر <literal>-L</literal> اولویت پیدا می‌کند.
				</para>
				 <para>
					همچنین امکان اشاره به مکان فیزیکی یک LV به صورتی که محدوده آن درون یک PV مشخص ذخیره‌سازی شود وجود دارد (البته، با استفاده از گزینه‌های اختصاص‌یافته به VG). از آنجا که می‌دانیم <filename>sdb</filename> از <filename>sdf</filename> سریع‌تر است، اگر بخواهیم برای سرور پایگاه‌داده در مقایسه با سرور فایل برتری قائل شویم می‌توانیم <filename>lv_base</filename> را در‌ آن ذخیره کنیم. پس دستور آن می‌شود: <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command>. به یاد داشته باشید که در صورت نبود محدوده کافی در PV این دستور ناموفق خواهد بود. در مثال ما، برای پیشگیری از این وضعیت، شاید بخواهیم <filename>lv_base</filename> را قبل از <filename>lv_files</filename> ایجاد یا برخی فضای موجود در <filename>sdb2</filename> را با دستور <command>pvmove</command> آزاد کنیم.
				</para>
				 </sidebar> <para>
					گروه‌های مجازی، زمانی که ایجاد گردند، به عنوان فایل‌های دستگاه درون <filename>/dev/mapper/</filename> قرار می‌گیرند:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>ls -l /dev/mapper</userinput>
<computeroutput>total 0
crw------- 1 root root 10, 236 Jun 10 16:52 control
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2
# </computeroutput><userinput>ls -l /dev/dm-*</userinput>
<computeroutput>brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0
brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1
brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2
</computeroutput></screen>
				 <sidebar> <title><emphasis>یادداشت</emphasis> شناسایی خودکار گروه‌های LVM</title>
				 <para>
					زمانی که رایانه راه‌اندازی می‌شود واحد سرویس systemd به نام <filename>lvm2-activation</filename> به اجرای <command>vgchange -aay</command> می‌پردازد که اینکار گروه‌های مجازی را “فعال” می‌کند: ابتدا به پویش دستگاه‌های موجود می‌پردازد؛ آن‌هایی که توسط گروه‌های فیزیکی برای LVM آماده‌سازی شده‌اند درون زیرسیستم آن قرار می‌گیرند، آن‌هایی که متعلق به گروه‌های مجازی باشند گردآوری شده و گروه‌های مجازی آن أغاز و قابل استفاده می‌گردند. بنابراین هنگام ایجاد یا تغییر گروه‌های LVM نیازی به ویرایش فایل‌های پیکربندی نیست.
				</para>
				 <para>
					با این حال، به یاد داشته باشید که ساختار عناصر LVM (گروه‌های فیزیکی و منطقی همراه با گروه‌های دستگاه) در <filename>/etc/lvm/backup</filename> پشتیبان‌گیری می‌شوند، که می‌تواند در زمان بروز مشکل (یا اطلاع از عملکرد آن) مورد استفاده قرار گیرد.
				</para>
				 </sidebar> <para>
					برای ساده‌تر کردن کارها، پیوندهای نمادین متعارف نیز در دایرکتوری‌های شامل VGها ایجاد شده است:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical</userinput>
<computeroutput>total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0
# </computeroutput><userinput>ls -l /dev/vg_normal</userinput>
<computeroutput>total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</computeroutput></screen>
				 <para>
					سپس LVها می‌توانند مانند پارتیشن‌های استاندارد مورد استفاده قرار گیرند:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups</userinput>
<computeroutput>mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 3145728 4k blocks and 786432 inodes
Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d
[...]
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </computeroutput><userinput>mkdir /srv/backups</userinput>
<computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups</userinput>
<computeroutput># </computeroutput><userinput>df -h /srv/backups</userinput>
<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_normal-lv_backups   12G   30M   12G   1% /srv/backups
# </computeroutput><userinput>[...]</userinput>
<computeroutput>[...]
# </computeroutput><userinput>cat /etc/fstab</userinput>
<computeroutput>[...]
/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2
/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2
/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</computeroutput></screen>
				 <para>
					از دید برنامه‌های کاربردی، تعداد بیشمار پارتیشن‌ها اکنون به یک دستگاه بزرگ ۱۲ گیگابایت تبدیل شده است که نام راحت‌تری نیز دارد.
				</para>

			</section>
			 <section id="sect.lvm-over-time">
				<title>LVM در گذر زمان</title>
				 <para>
					با اینکه توانایی گردآوری پارتیشن‌‌ها یا دیسک‌های فیزیکی بسیار متداول است، این تنها مزیت استفاده از LVM نیست. انعطاف‌پذیری آن در گذر زمان و تغییر رویکرد ذخیره‌سازی، مشخص می‌شود. در مثال ما، تصور کنیم که فایل‌های بزرگ جدیدی قرار است درون سرور فایل قرار گیرند که LV اختصاص‌یافته به آن گنجایش کافی را ندارد. از آنجا که از تمام فضای <filename>vg_critical</filename> استفاده نکرده‌ایم، می‌توانیم <filename>lv_files</filename> را گسترش دهیم. برای این منظور، با استفاده از دستور <command>lvresize</command> و <command>resize2fs</command> برای سازگاری فایل سیستم اینکار صورت می‌گیرد:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>
<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files
# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>
<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 5.00g
# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   2   2   0 wz--n- 8.09g 2.09g
# </computeroutput><userinput>lvresize -L 7G vg_critical/lv_files</userinput>
<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).
  Logical volume lv_files successfully resized
# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>
<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 7.00g
# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>
<computeroutput>resize2fs 1.42.12 (29-Aug-2014)
Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.

# </computeroutput><userinput>df -h /srv/files/</userinput>
<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</computeroutput></screen>
				 <sidebar> <title><emphasis>احتیاط</emphasis> تغییر اندازه فایل سیستم‌ها</title>
				 <para>
					تمام فایل سیستم‌ها نمی‌توانند به صورت آنلاین تغییر اندازه یابند؛ تغییر اندازه یک دستگاه ابتدا نیازمند قطع اتصال آن به فایل سیستم سپس اتصال مجدد آن می‌شود. البته، اگر کسی بخواهد فضای اختصاص‌یافته به یک LV را کاهش دهد، ابتدا فایل سیستم باید کاهش پیدا کند؛ این ترتیب در زمان افزایش اندازه برعکس می‌شود: گروه مجازی قبل از فایل سیستم موجود در آن باید افزایش پیدا کند. این فرآیند بسیار واضح است، چرا که در هر زمان اندازه فایل سیستم نباید از اندازه دستگاه بلاک-محور روی آن بیشتر باشد (خواه این دستگاه یک پارتیشن فیزیکی باشد یا یک گروه مجازی).
				</para>
				 <para>
					فایل سیستم‌های ext3، ext4 و zfs بدون نیاز به قطع اتصال می‌توانند افزایش یابند؛ کاهش اندازه نیازمند قطع اتصال است. فایل سیستم reiserfs امکان تغییر اندازه آنلاین را در دو جهت فراهم می‌سازد. ext2 مقدس، اما نیازمند قطع اتصال در دو جهت است.
				</para>
				 </sidebar> <para>
					برای توسعه گروهی که از پایگاه‌داده میزبانی می‌کند نیز به همین ترتیب می‌توان اقدام کرد، با این تفاوت که به انتهای فضای ذخیره‌سازی موجود VG رسیدیم:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>
<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_base 1008M  854M  104M  90% /srv/base
# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree 
  vg_critical   2   2   0 wz--n- 8.09g 92.00m</computeroutput></screen>
				 <para>
					ایرادی ندارد، چرا که LVM امکان افزودن گروه‌های فیزیکی را به گروه‌های دستگاه موجود فراهم می‌سازد. برای نمونه، شاید متوجه شده‌ایم پارتیشن <filename>sdb1</filename>، که خارج از LVM استفاده می‌شود، تنها شامل بایگانی‌هایی است که می‌تواند به <filename>lv_backups</filename> انتقال یابد. اکنون می‌توانیم آن را بازیابی کرده و درون گروه مجازی قرار دهیم، در نتیجه برخی فضای موجود را احیا می‌کنیم. اینکار با استفاده از دستور <command>vgextend</command> صورت می‌گیرد. البته که پارتیشن ابتدا باید به صورت یک گروه فیزیکی آماده شود. زمانی که VG گسترش یافت، از دستورات مشابه می‌توانیم برای افزایش اندازه گروه مجازی و فایل سیستم استفاده کنیم:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>
<computeroutput>  Physical volume "/dev/sdb1" successfully created
# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>
<computeroutput>  Volume group "vg_critical" successfully extended
# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   3   2   0 wz--n- 9.09g 1.09g
# </computeroutput><userinput>[...]</userinput>
<computeroutput>[...]
# </computeroutput><userinput>df -h /srv/base/</userinput>
<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</computeroutput></screen>
				 <sidebar> <title><emphasis>مطالعه بیشتر</emphasis> LVM پیشرفته</title>
				 <para>
					LVM همچنین کاربردهای پیشرفته‌تری نیز دارد که جزئیات آن می‌تواند به صورت دستی وارد شود. برای نمونه، یک مدیر سیستم می‌تواند اندازه بلاک‌های گروه‌های فیزیکی و منطقی را تغییر دهد، همین طور ساختار فیزیکی آن‌ها را. همچنین امکان انتقال بلاک‌ها بین PVها موجود است، برای نمونه به منظور بهبود عملکرد یا آزاد کردن یک PV زمانی که نیاز به استخراج یک دیسک فیزیکی منطبق با خود از VG باشد (خواه با تاثیر روی VG دیگر یا حذف از LVM به صورت کلی). صفحات راهنمای دستورات معمولا واضح بوده و جزئیات بیشتری را مطرح می‌کنند. یک نقطه شروع مناسب صفحه راهنمای <citerefentry><refentrytitle>lvm</refentrytitle>
					 <manvolnum>8</manvolnum></citerefentry> است.
				</para>
				 </sidebar>
			</section>

		</section>
		 <section id="sect.raid-or-lvm">
			<title>RAID یا LVM؟</title>
			 <para>
				RAID و LVM بدون تردید دارای مزایایی هستند که در صورت کنار گذاشتن فرضیه یک رایانه رومیزی همراه با یک هارد دیسک که کارکرد آن در طول زمان تغییر نمی‌کند، مشخص می‌شوند. اگرچه، RAID و LVM در دو جهت مختلف حرکت می‌کنند و اهداف متفاوتی نیز دارند، که گاهی تصمیم‌گیری درباره استفاده صحیح از آن‌ها را دشوار می‌سازد. مناسب‌ترین پاسخ بستگی به نیازمندی‌های فعلی و قابل پیشبینی سیستم در آینده دارد.
			</para>
			 <para>
				موارد ساده‌ای وجود دارد که پرسشی درباره آن‌ها مطرح نمی‌شود. اگر نیازمندی این باشد که داده برابر نقص سخت‌افزاری محافظت گردد، به طور مشخص باید از RAID به صورت آرایه‌ای از دیسک‌ها استفاده کرد، چرا که LVM درباره این مشکل راهکاری ندارد. از طرف دیگر، اگر نیازمندی این باشد که یک طرح ذخیره‌سازی انعطاف‌پذیر از گروه‌های مستقل ساختار فیزیکی دیسک‌ها تشکیل گردد، به طور مشخص باید از LVM استفاده کرد چرا که RAID درباره این مشکل راهکاری ندارد.
			</para>
			 <sidebar> <title><emphasis>یادداشت</emphasis> اگر عملکرد مهم باشد...</title>
			 <para>
				اگر سرعت ورودی/خروجی مطرح باشد، به خصوص در مورد زمان دسترسی، استفاده از LVM و/یا RAID با یکی از ترکیبات موجود ممکن است روی عملکرد تاثیر منفی بگذارد، که اینکار روی تصمیم‌گیری درباره انتخاب هر کدام اثر می‌گذارد. اگرچه، این تفاوت‌ها در عملکرد بسیار ناچیز بوده و تنها در چند مورد خاص قابل اندازه‌گیری می‌باشند. اگر عملکرد مهم باشد، بهترین گزینه استفاده از رسانه ذخیره‌سازی غیرقابل-چرخش است (<indexterm><primary>SSD</primary></indexterm><emphasis>solid-state drives</emphasis> یا SSD)؛ هزینه آن‌ها برای هر مگابایت از دیسک‌ها معمولی بالاتر و ظرفیت کمتری نسبت به آن‌ها دارند، اما عملکرد فوق‌العاده‌ای درباره دسترسی تصادفی به حافظه دارند. اگر الگوی کارکرد شامل عملیات ورودی/خروجی باشد که در میان فایل سیستم پراکنده شده است، برای نمونه پرس و جوهای پیچیده پایگاه‌داده، آنگاه مزیت اجرای آن‌ها روی SSD به مراتب بیشتر از استفاده LVM یا RAID می‌تواند باشد. در این مواقع، انتخاب باید توسط سایر نیازمندی‌ها بجز سرعت صورت بگیرد، چرا که جنبه عملکرد آن به راحتی توسط SSD مدیریت می‌شود.
			</para>
			 </sidebar> <para>
				سومین کارکرد قابل ذکر زمانی است که بخواهیم دو دیسک را در یک گروه بزرگ‌تر قرار دهیم، خواه به دلایل عملکرد یا داشتن یک فایل سیستم بزرگ‌تر از فضای هر کدام از دیسک‌ها). اینکار با استفاده از یک RAID-0 (یا حتی RAID خطی) و گروه LVM انجام می‌شود. در این موقعیت، بجز محدودیت‌های خارجی (برای نمونه، در چارچوب سایر رایانه‌ها بودن اگر آن‌ها نیز تنها از RAID استفاده کنند)، پیکربندی مورد نظر معمولا LVM خواهد بود. راه‌اندازی اولیه به ندرت پیچیده‌تر خواهد بود و این افزایش پیچیدگی در صورت تغییر نیازمندی‌ها یا افزودن دیسک‌ها جدید قابلیت انعطاف‌پذیری بیشتری را فراهم می‌آورد.
			</para>
			 <para>
				البته یک مورد بسیار جالب نیز وجود دارد، که سیستم ذخیره‌سازی هم باید برابر نقص سخت‌افزاری مقاوم هم انعطاف‌پذیری لازم درباره اختصاص گروه‌های مختلف را داشته باشد. هیچ یک از راهکارهای RAID یا LVM به تنهایی نمی‌توانند این نیازمندی را پوشش دهند؛ اینجا است که از هر دو به صورت همزمان استفاده می‌کنیم - یا یکی بر فراز دیگری. طرح کلی در این مورد و با توجه به اینکه این دو فناوری به بلوغ رسیده‌اند این است که ابتدا با گروه‌بندی دیسک‌ها در تعداد آرایه‌های کوچک RAID از افزونگی داده اطمینان حاصل کرد سپس از این آرایه‌ها برای گروه‌های فیزیکی LVM استفاده کنیم؛ پارتیشن‌های منطقی از این LVها برای فایل سیستم بوجود می‌آیند. نتیجه این تنظیم این است که هنگامی که یک دیسک دچار نقص می‌گردد، تنها تعداد کمی از آرایه‌های RAID نیازمند بازسازی هستند، که اینکار زمان سپری شده توسط مدیر سیستم برای بازیابی را کاهش می‌دهد.
			</para>
			 <para>
				بیایید یک مثال واقعی را بررسی کنیم: دپارتمان روابط عمومی در شرکت فالکوت نیازمند یک سیستم برای ویرایش تصویر است، اما هزینه‌های دپارتمان اجازه سرمایه‌گذاری در سخت‌افزارهای گران قیمت را نمی‌دهد. تصمیم گرفته شد که از سخت‌افزار مربوط به کار گرافیکی (مانیتور و کارت گرافیک) و از سخت‌افزار متداول تنها برای ذخیره‌سازی اطلاعات استفاده شود. اگرچه، همانطور که مشخص است، ویدیو دیجیتال نیازمندی ذخیره‌سازی مربوط به خود را دارد: حجم داده قابل ذخیره‌سازی زیاد است، پس نرخ تبادل داده برای خواندن و نوشتن در عملکرد کلی سیستم تاثیرگذار خواهد بود (برای نمونه، بیش از زمان دسترسی متداول). این محدودیت‌ها باید با سخت‌افزار عمومی موجود برطرف گردند که در این مورد دو هارد دیسک ۳۰۰ گیگابایت از نوع SATA می‌باشند؛ داده سیستمی همراه با داده کاربری باید برابر نقص‌های سخت‌افزاری نیز مقاوم گردند. ویدیو کلیپ‌های ویرایش شده باید از امنیت واقعی برخوردار بوده، اما ویدیوهای اولیه که منتظر ویرایش هستند از اهمیت کمتری برخوردارند، چرا که نسخه اصلی آن‌ها روی نوارهای ویدیویی موجود است.
			</para>
			 <para>
				از RAID-1 و LVM به صورت ترکیبی برای رفع این محدودیت‌ها استفاده شده است. دیسک‌ها به دو کنترلر مختلف SATA به منظور بهینه‌سازی دسترسی موازی و کاهش خطر نقص همزمان متصل شده‌اند که به عنوان <filename>sda</filename> و <filename>sdc</filename> ظاهر می‌شوند. آن‌ها با توجه به طرح زیر پارتیشن‌بندی شده‌اند:
			</para>
			 
<screen><computeroutput># </computeroutput><userinput>fdisk -l /dev/sda</userinput>
<computeroutput>
Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x00039a9f

Device    Boot     Start       End   Sectors Size Id Type
/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect
/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris
/dev/sda3        4000185 586099395 582099210 298G 5  Extended
/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect
/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect
/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</computeroutput></screen>
			 <itemizedlist>
				<listitem>
					<para>
						اولین پارتیشن‌های هر دو دیسک (حدود ۱ گیگابایت) به صورت یک آرایه RAID-1 در آمده‌اند، <filename>md0</filename>. از این mirror به طور مستقیم برای ذخیره‌سازی فایل سیستم root استفاده می‌شود.
					</para>

				</listitem>
				 <listitem>
					<para>
						پارتیشن‌های <filename>sda2</filename> و <filename>sdc2</filename> به عنوان swap استفاده شده‌اند، که مجموع فضای ۲ گیگابایت برای swap را فراهم می‌کنند. با ۱ گیگابایت RAM، رایانه مقدار کافی حافظه موجود را خواهد داشت.
					</para>

				</listitem>
				 <listitem>
					<para>
						پارتیشن‌های <filename>sda5</filename> و <filename>sdc5</filename> همراه با <filename>sda6</filename> و <filename>sdc6</filename> هر کدام به یک آرایه RAID-1 به اندازه ۱۰۰ گیگابایت تقسیم شده‌اند که به نام‌های <filename>md1</filename> و <filename>md2</filename> موجود هستند. هر یک از این mirrorها به عنوان گروه‌های فیزیکی برای LVM راه‌اندازی شده‌اند که به گروه آرایه <filename>vg_raid</filename> اختصاص یافته‌اند. بنابراین این VG شامل ۲۰۰ گیگابایت فضای امن است.
					</para>

				</listitem>
				 <listitem>
					<para>
						پارتیشن‌های باقیمانده <filename>sda7</filename> و <filename>sdc7</filename> به طور مستقیم به عنوان گروه‌های فیزیکی <filename>vg_bulk</filename> نامگذاری شده‌اند، که در نهایت فضایی معادل ۲۰۰ گیگابایت را شامل می‌شوند.
					</para>

				</listitem>

			</itemizedlist>
			 <para>
				زمانی که VGها ایجاد شوند، به روشی بسیار منعطف می‌توانند پارتیشن‌بندی گردند. به یاد داشته باشید که LVهای ایجاد شده در <filename>vg_raid</filename> در صورت نقص دیسک‌ها نیز نگهداری می‌شوند که این مورد درباره LVهای ایجاد شده در <filename>vg_bulk</filename> صادق نیست؛ از طرف دیگر، مورد دوم به صورت موازی در اختیار هر دو دیسک قرار می‌گیرد، که امکان خواندن یا نوشتن فایل‌های بزرگ را فراهم می‌آورد.
			</para>
			 <para>
				بنابراین گروه‌های منطقی <filename>lv_usr</filename>، <filename>lv_var</filename> و <filename>lv_home</filename> را در <filename>vg_raid</filename> ایجاد می‌کنیم تا از فایل سیستم‌های متناسب پشتیبانی گردد؛ از یک گروه منطقی دیگر بنام <filename>lv_movies</filename> برای ذخیره‌سازی ویدیوهای ویرایش شده استفاده می‌شود. از VG دیگر به منظور تقسیم <filename>lv_rushes</filename> برای داده‌های ورودی از دوربین‌های دیجیتال و یک <filename>lv_tmp</filename> برای فایل‌های موقت استفاده خواهد شد. مکان ذخیره‌سازی ناحیه کاری خود انتخاب دیگری است: با اینکه عملکرد خوب برای این آرایه مورد نیاز است، آیا ارزش دارد که کار را در قبال یک نقص سخت‌افزاری حین ویرایش ویدیو از دست بدهیم؟ با توجه به پاسخ این پرسش، LV مرتبط با یک VG یا دیگری ایجاد خواهد شد.
			</para>
			 <para>
				اکنون هم افزونگی داده برای داده‌های مهم فراهم شده هم انعطاف‌پذیری بهتری در مورد تقسیم فضای موجود بین برنامه‌ها وجود دارد. زمانی که نرم‌افزار مربوطه نصب شود (برای نمونه، ویرایش کلیپ‌های صوتی) LV که از <filename>/usr/</filename> میزبانی می‌کند می‌تواند به آرامی رشد کند.
			</para>
			 <sidebar> <title><emphasis>یادداشت</emphasis> چرا از سه آرایه RAID-1 استفاده شد؟</title>
			 <para>
				می‌توانستیم از یک آرایه RAID-1 به منظور گروه فیزیکی برای <filename>vg_raid</filename> استفاده می‌کردیم. پس چرا سه تا ایجاد کردیم؟
			</para>
			 <para>
				منطق تقسیم اول (<filename>md0</filename> مقابل دیگران) درباره امنیت داده است: داده‌ای که روی هر دو عنصر یک mirror از RAID-1 نوشته می‌شود کاملا یکسان است و این امکان وجود دارد که با نادیده‌گرفتن ساختار RAID یکی از دیسک‌ها را به صورت مستقیم متصل کنیم. در صورتی که یک باگ کرنل موجود باشد، برای نمونه اگر اطلاعات جانبی LVM خراب شود، امکان راه‌اندازی حداقلی سیستم برای دسترسی به داده حیاتی مانند ساختار دیسک‌های موجود در آرایه‌های RAID و LVM موجود است؛ این اطلاعات جانبی می‌تواند بازسازی شده و فایل‌ها دوباره قابل دسترس شوند، تا سیستم به حالت عادی خود بازگردد.
			</para>
			 <para>
				منطق تقسیم دوم (<filename>md1</filename> مقابل <filename>md2</filename>) از صراحت کمتری برخوردار است، که بیشتر درباره نامشخص بودن آینده دلالت دارد. زمانی که این سیستم جمع‌آوری شد، نیازمندی‌های دقیق ذخیره‌سازی از همان ابتدا مشخص نبود؛ چرا که می‌توانست در گذر زمان تغییر کند. در این مورد، از قبل نمی‌دانیم چه فضایی بابت ذخیره‌سازی ویدیوهای اولیه و کلیپ‌های ویرایش شده نهایی لازم است. اگر یک کلیپ مشخص نیازمند تصاویر اولیه بسیاری باشد و VG اختصاص‌یافته به داده تکراری کمتر از نصف فضای ذخیره‌سازی را داشته باشد، می‌توانیم از برخی فضای استفاده نشده آن بهره ببریم. می‌توانیم یکی از گروه‌های فیزیکی مانند <filename>md2</filename> را از <filename>vg_raid</filename> حذف کرده و به طور مستقیم به <filename>vg_bulk</filename> اختصاص دهیم (اگر مدت زمان انتظار رفته از عملیات به اندازه‌ای کم باشد که این کاهش عملکرد سیستم را تحمل کنیم) یا تنظیم RAID را روی <filename>md2</filename> لغو کرده و اجزای آن یعنی <filename>sda6</filename> و <filename>sdc6</filename> را درون VG دیگر قرار دهیم (که بجای ۱۰۰ می‌شود ۲۰۰ گیگابایت)؛ گروه منطقی <filename>lv_rushes</filename> سپس می‌تواند متناسب با نیازمندی‌های افزایش یابد.
			</para>
			 </sidebar>
		</section>

	</section>
	 <section id="sect.virtualization">
		<title>مجازی‌سازی</title>
		 <indexterm>
			<primary>مجازی‌سازی</primary>
		</indexterm>
		 <para>
			مجازی‌سازی یکی از بزرگترین پیشرفت‌های علوم رایانه در سال‌های اخیر است. این عبارت شامل چندین مفهوم انتزاعی و تکنیکی است که در شبیه‌سازی رایانه‌های مجازی به طوری که مستقل از سخت‌افزار واقعی عمل می‌کنند. یک سرور فیزیکی می‌تواند شامل چندین سرور مجازی باشد که جدا از یکدیگر فعالیت می‌کنند. برنامه‌های کاربردی آن بسیار هستند که از این انزوا مشتق می‌شوند: برای نمونه، محیط‌های آزمایشی همراه با پیکربندی‌های متفاوت یا جداسازی سرویس‌های میزبانی بین چندین ماشین مجازی برای امنیت.
		</para>
		 <para>
			راهکارهای گوناگون مجازی‌سازی هر کدام با نقاط قوت و ضعف خود وجود دارند. تمرکز این کتاب روی Xen، LXC و KVM است اما سایر پیاده‌سازی‌های قابل ذکر عبارتند از:
		</para>
		 <indexterm>
			<primary><emphasis>VMWare</emphasis></primary>
		</indexterm>
		 <indexterm>
			<primary><emphasis>Bochs</emphasis></primary>
		</indexterm>
		 <indexterm>
			<primary><emphasis>QEMU</emphasis></primary>
		</indexterm>
		 <indexterm>
			<primary><emphasis>VirtualBox</emphasis></primary>
		</indexterm>
		 <indexterm>
			<primary><emphasis>KVM</emphasis></primary>
		</indexterm>
		 <indexterm>
			<primary><emphasis>LXC</emphasis></primary>
		</indexterm>
		 <itemizedlist>
			<listitem>
				<para>
					QEMU یک شبیه‌ساز نرم‌افزاری برای یک رایانه کامل است؛ عملکرد آن چیزی بیشتر از سرعت است که می‌توان به آن دست یافت، اما این فرآیند امکان اجرای سیستم عامل‌های تغییرنیافته یا آزمایشی را روی سخت‌افزار شبیه‌سازی شده فراهم می‌کند. همچنین امکان شبیه‌سازی چندین معماری گوناگون سخت‌افزاری را نیز فراهم می‌کند: برای نمونه، یک سیستم <emphasis>amd64</emphasis> می‌تواند یک رایانه <emphasis>arm</emphasis> را شبیه‌سازی کند. QEMU نرم‌افزار آزاد است. <ulink type="block" url="http://www.qemu.org/" />
				</para>

			</listitem>
			 <listitem>
				<para>
					... نیز یک ماشین مجازی آزاد دیگر است، اما تنها به شبیه‌سازی معماری‌های x86 می‌پردازد (i386 و amd64).
				</para>

			</listitem>
			 <listitem>
				<para>
					VMWare یک ماشین مجازی انحصاری است؛ به عنوان یکی از قدیمی‌ترین گزینه‌های موجود، یکی از شناخته‌شده‌ترین راهکارهای مجازی‌سازی است. بر اساس مبانی مشترک با QEMU کار می‌کند. VMWare قابلیت‌های پیشرفته‌ای از جمله snapshot گرفتن از یک ماشین مجازی در حال اجرا را فراهم می‌کند. <ulink type="block" url="http://www.vmware.com/" />
				</para>

			</listitem>
			 <listitem>
				<para>
					VirtualBox یک ماشین مجازی است که تقریبا از نرم‌افزار آزاد تشکیل شده است (برخی اجزای اضافی آن شامل مجوزهای انحصاری می‌باشند). متاسفانه در قسمت “contrib” از دبیان قرار دارد زیرا شامل فایل‌های کامپایل شده‌ای است که بدون استفاده از یک کامپایلر انحصاری قابل بازسازی نیستند. با اینکه از VMWare جوان‌تر و محدود به معماری‌های i386 و amd64 است، شامل ویژگی‌های جالبی از جمله snapshot گرفتن و سایر قابلیت‌های هیجان‌انگیز می‌باشد. <ulink type="block" url="http://www.virtualbox.org/" />
				</para>

			</listitem>

		</itemizedlist>
		 <section id="sect.xen">
			<title>Xen</title>
			 <para>
				Xen <indexterm><primary>Xen</primary></indexterm> یک راهکار “paravirtualization” است که شامل یک لایه انتزاعی سبک به نام “hypervisor” می‌باشد که بین سخت‌افزار و سیستم‌های بالایی قرار می‌گیرد؛ این لایه مانند یک داور دسترسی از ماشین‌های مجازی به سخت‌افزار را کنترل می‌کند. اگرچه، تنها شامل چند دستورالعمل کوتاه است، باقی عملیات به طور مستقیم از طرف سخت‌افزار و به نیابت از سیستم‌های دیگر انجام می‌شوند. مزیت اصلی آن این است که عملکرد کاهش نمی‌یابد و سیستم‌های تقریبا با سرعت اصلی سخت‌افزار اجرا می‌شوند؛ اشکال اصلی آن این است که کرنل‌های سیستم عامل‌ها به منظور استفاده از hypervisor باید سازگار با Xen باشند.
			</para>
			 <para>
				بیایید چند عبارت را بررسی کنیم. hypervisor پایین‌ترین لایه است که به صورت مستقیم روی سخت‌افزار اجرا می‌شود، حتی پایین‌تر از کرنل. این hypervisor می‌تواند سایر نرم‌افزارها را درون چندین <emphasis>دامنه</emphasis> قرار دهد، که می‌توانند به عنوان ماشین‌های مجازی دیده شوند. یکی از این دامنه‌ها (اولین آن‌ها که آغاز می‌شود) به عنوان <emphasis>dom0</emphasis> شناخته می‌شود و نقش ویژه‌ای دارد، چرا که تنها این دامنه می‌تواند hypervisor و اجرای سایر دامنه‌ها را کنترل کند. سایر دامنه‌ها به عنوان <emphasis>domU</emphasis> شناخته می‌شوند. به عبارت دیگر، و از دید کاربر، <emphasis>dom0</emphasis> به عنوان “میزبان” برای سایر سیستم‌های مجازی عمل می‌کند در صورتی که <emphasis>domU</emphasis> به عنوان یک “میهمان” دیده می‌شود.
			</para>
			 <sidebar> <title><emphasis>فرهنگ</emphasis> Xen و نسخه‌های مختلف از لینوکس</title>
			 <para>
				Xen در ابتدا به عنوان چندین اصلاحیه خارج از ساختار اصلی کرنل لینوکس توسعه یافت. در همان زمان، چندین راهکاری مجازی‌سازی دیگر (از جمله KVM) نیازمند چندین عملکرد عمومی مجازی‌سازی بودند تا یکپارچه‌سازی خود را تسهیل نمایند و کرنل لینوکس این مجموعه قابلیت‌ها را گردآوری کرد (که به عنوان رابط <emphasis>paravirt_ops</emphasis> یا <emphasis>pv_ops</emphasis> شناخته می‌شوند). از آنجا که اصلاحیه‌های Xen این قابلیت‌ها را به شیوه‌ای دیگر پیاده‌سازی می‌کردند، نتوانستند به صورت رسمی پذیرفته شوند.
			</para>
			 <para>
				Xensource، شرکتی که پشت Xen است، مجبور شد تا Xen را به یک چارچوب جدید انتقال دهد به صورتی که اصلاحیه‌های Xen بتوانند درون کرنل رسمی لینوکس ادغام شوند. این کار به معنی بازنویسی قسمت اعظمی از کد بود و با اینکه Xensource به نسخه کارآمدی مبتنی بر رابط paravirt_ops رسیده بود، اصلاحیه‌ها با سرعت کمی درون کرنل رسمی قرار می‌گرفتند. این ادغام در لینوکس ۳.۰ کامل شد. <ulink type="block" url="http://wiki.xenproject.org/wiki/XenParavirtOps" />
			</para>
			 <para>
				از آنجا که <emphasis role="distribution">Jessie</emphasis> مبتنی بر نسخه ۳.۱۶ از کرنل لینوکس است، بسته‌های استاندارد <emphasis role="pkg">linux-image-686-pae</emphasis> و <emphasis role="pkg">linux-image-amd64</emphasis> شامل کدهای لازم می‌باشند و اصلاحیه‌های مختص به توزیع‌های <emphasis role="distribution">Squeeze</emphasis> و قبل از آن دیگر مورد نیاز نیستند. <ulink type="block" url="http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix" />
			</para>
			 </sidebar> <sidebar> <title><emphasis>یادداشت</emphasis> معماری‌های سازگار با Xen</title>
			 <para>
				Xen هم اکنون تنها برای معماری‌های i386، amd64، arm64 و armhf موجود می‌باشد.
			</para>
			 </sidebar> <sidebar> <title><emphasis>فرهنگ</emphasis> Xen کرنل‌های غیر از لینوکس</title>
			 <para>
				Xen نیازمند ایجاد تغییرات در تمام سیستم عامل‌هایی است که قصد اجرای روی آن را دارند؛ تمام کرنل‌های به این سطح از بلوغ نرسیده‌اند. بسیاری از آن‌ها کاملا کارآمد هستند، به عنوان dom0 و domU: لینوکس ۳.۰ به بعد، NetBSD ۴.۰ به بعد و OpenSolaris. سایر کرنل‌ها تنها به عنوان domU می‌توانند کار کنند. وضعیت هر کدام از سیستم عامل‌‌ها را می‌توانید در صفحه ویکی Xen مشاهده کنید: <ulink type="block" url="http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen" /> <ulink type="block" url="http://wiki.xenproject.org/wiki/DomU_Support_for_Xen" />
			</para>
			 <para>
				با این حال، اگر Xen بتواند روی قابلیت‌های سخت‌افزاری موجود برای مجازی‌سازی تکیه کند (که تنها در پردازنده‌های جدید مشاهده می‌شوند)، حتی سیستم عامل‌های غیر-قابل تغییر مانند ویندوز نیز می‌توانند به عنوان domU استفاده گردند.
			</para>
			 </sidebar> <para>
				استفاده از Xen تحت دبیان نیازمند سه مولفه است:
			</para>
			 <itemizedlist>
				<listitem>
					<para>
						خود hypervisor. با توجه به سخت‌افزار موجود، بسته مناسب آن یکی از گزینه‌های <emphasis role="pkg">xen-hypervisor-4.4-amd64</emphasis>، <emphasis role="pkg">xen-hypervisor-4.4-armhf</emphasis> یا <emphasis role="pkg">xen-hypervisor-4.4-arm64</emphasis> خواهد بود.
					</para>

				</listitem>
				 <listitem>
					<para>
						کرنلی که روی آن hypervisor اجرا می‌شود. هر کرنل جدیدتر از ۳.۰ اینکار را انجام می‌دهد، از جمله ۳.۱۶ موجود در <emphasis role="distribution">Jessie</emphasis>.
					</para>

				</listitem>
				 <listitem>
					<para>
						معماری i386 همچنین نیازمند یک کتابخانه استاندارد است که بتواند از اصلاحیه‌های موجود در Xen بهره‌مند شود؛ این کتابخانه در بسته <emphasis role="pkg">libc6-xen</emphasis> موجود است.
					</para>

				</listitem>

			</itemizedlist>
			 <para>
				به منظور اینکه انتخاب این مولفه‌ها بدون دردسر انجام شود، چندین بسته کاربردی (از جمله <emphasis role="pkg">xen-linux-system-amd64</emphasis>) ایجاد شده‌اند؛ این بسته‌ها با ترکیب خوبی از hypervisor مناسب و بسته‌های کرنل آن قرار گرفته‌اند. hypervisor همچنین شامل بسته <emphasis role="pkg">xen-utils-4.4</emphasis> است که ابزار لازم برای کنترل آن از طریق dom0 را فراهم می‌آورد. این عمل در حقیقت کتابخانه استاندارد را نصب می‌کند. طی نصب این بسته‌ها، اسکریپت‌های پیکربندی همچنین یک مدخل درون منوی راه‌اندازی Grub ایجاد می‌کنند تا کرنل انتخابی برای آغاز dom0 مشخص گردد. به یاد داشته باشید که این مدخل معمولا به عنوان گزینه اول در فهرست قرار نمی‌گیرد، به همین منظور به صورت پیشفرض انتخاب نمی‌گردد. اگر این عملکرد مطلوب شما نباشد، دستورات زیر می‌توانند آن را تغییر دهند:
			</para>
			 
<screen><computeroutput># </computeroutput><userinput>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen
</userinput><computeroutput># </computeroutput><userinput>update-grub
</userinput></screen>
			 <para>
				زمانی که این پیشنیازها نصب شوند، گام بعدی آزمایش عملکرد خود dom0 است؛ این عمل شامل راه‌اندازی مجدد hypervisor و کرنل Xen می‌باشد. سیستم باید به شیوه استاندارد خود راه‌اندازی شده، همراه با چندین پیام که طی گام‌های اولیه راه‌اندزای در کنسول نمایش می‌یابند.
			</para>
			 <para>
				اکنون زمان آن فرا رسیده است که با استفاده از ابزار موجود در <emphasis role="pkg">xen-tools</emphasis> به نصب سیستم‌های کاربردی روی domU بپردازیم. این بسته شامل دستور <command>xen-create-image</command> می‌باشد که تقریبا این فرآیند را خودکارسازی می‌کند. تنها پارامتر ضروری آن <literal>--hostname</literal> است که نام domU را مشخص می‌کند؛ سایر گزینه‌ها نیز مهم هستند ولی می‌توانند درون فایل پیکربندی <filename>/etc/xen-tools/xen-tools.conf</filename> قرار بگیرند که نبود آن‌ها خطایی را در هنگام اجرای دستور صادر نمی‌کند. بنابراین مهم است که محتوای این فایل را قبل از ایجاد هر image بررسی کرده یا از پارامترهای اضافی هنگام فراخوانی <command>xen-create-image</command> استفاده کنیم. پارامترهای مهم قابل ذکر عبارتند از:
			</para>
			 <itemizedlist>
				<listitem>
					<para>
						<literal>--memory</literal>، برای مشخص کردن میزان RAM اختصاص یافته به سیستم جدید؛
					</para>

				</listitem>
				 <listitem>
					<para>
						<literal>--size</literal> و <literal>--swap</literal>، برای تعریف اندازه "دیسک‌های مجازی" موجود برای domU؛
					</para>

				</listitem>
				 <listitem>
					<para>
						<literal>--debootstrap</literal>، برای نصب شدن سیستم جدید با استفاده از <command>debootstrap</command>؛ در این مورد، از گزینه <literal>--dist</literal> اغلب استفاده می‌شود (همراه با نام یک توزیع مانند <emphasis role="distribution">jessie</emphasis>).
					</para>
					 <sidebar> <title><emphasis>مطالعه بیشتر</emphasis> نصب یک سیستم غیر دبیان در یک domU</title>
					 <para>
						در مورد یک سیستم غیر-لینوکس، برای تعریف کرنل مورد استفاده domU باید دقت کرد، با استفاده از گزینه <literal>--kernel</literal>.
					</para>
					 </sidebar>
				</listitem>
				 <listitem>
					<para>
						<literal>--dhcp</literal> بیان می‌کند که پیکربندی شبکه domU باید از طریق DHCP انجام شود در صورتی که <literal>--ip</literal> امکان استفاده از نشانی ایستای IP را فراهم می‌کند.
					</para>

				</listitem>
				 <listitem>
					<para>
						در انتها، از یک روش ذخیره‌سازی به منظور ایجاد image استفاده کرد (آن‌هایی که به عنوان درایوهای هارد دیسک از domU در نظر گرفته می‌شوند). ساده‌ترین روش، با توجه به گزینه <literal>--dir</literal>، ایجاد یک فایل در dom0 به ازای هر دستگاه domU فراهم‌کننده آن است. برای سیستم‌هایی که از LVM استفاده می‌کنند، گزینه جایگزین استفاده از <literal>--lvm</literal> است، که همراه با نام یک گروه آرایه آورده می‌شود؛ سپس <command>xen-create-image</command> اقدام به ایجاد یک گروه منطقی درون آرایه‌ها می‌کند که این گروه منطقی به عنوان یک درایو هارد دیسک به domU معرفی می‌گردد.
					</para>
					 <sidebar> <title><emphasis>یادداشت</emphasis> ذخیره‌سازی در domU</title>
					 <para>
						علاوه بر پارتیشن‌ها، آرایه‌های RAID و گروه‌های منطقی موجود در LVM، هارد دیسک‌های کامل نیز می‌توانند به domU انتقال یابند. این عملیات توسط <command>xen-create-image</command> به صورت خودکار انجام نمی‌شوند، با این حال، ویرایش فایل پیکربندی Xen معمولا پس از فراخوانی اولیه <command>xen-create-image</command> صورت می‌گیرد.
					</para>
					 </sidebar>
				</listitem>

			</itemizedlist>
			 <para>
				زمانی که این انتخاب‌ّها صورت گیرد، می‌توانیم image خود را برای domU بعدی در Xen ایجاد کنیم:
			</para>
			 
<screen><computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</userinput>
<computeroutput>
[...]
General Information
--------------------
Hostname       :  testxen
Distribution   :  jessie
Mirror         :  http://ftp.debian.org/debian/
Partitions     :  swap            128Mb (swap)
                  /               2G    (ext3)
Image type     :  sparse
Memory size    :  128Mb
Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64
Initrd path    :  /boot/initrd.img-3.16.0-4-amd64
[...]
Logfile produced at:
         /var/log/xen-tools/testxen.log

Installation Summary
---------------------
Hostname        :  testxen
Distribution    :  jessie
MAC Address     :  00:16:3E:8E:67:5C
IP-Address(es)  :  dynamic
RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b
Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ
</computeroutput></screen>
			 <para>
				اکنون دارای یک ماشین مجازی هستیم که اجرا نمی‌شود (و تنها از فضای موجود در هارد دیسک dom0 استفاده می‌کند). البته که می‌توانیم با استفاده از پارامترهای گوناگون دیگر image بیشتری بسازیم.
			</para>
			 <para>
				قبل از اینکه این ماشین‌های مجازی را روشن کنیم باید راجع به چگونگی دسترسی به آن‌ها تصمیم بگیریم. آن‌ها می‌توانند به عنوان ماشین‌های ایزوله شده تنها از طریق کنسول سیستم خود تعریف شوند، اما این روش به ندرت از الگوی کارکرد تبعیت می‌کند. اکثر مواقع، یک domU به عنوان یک سرور راه دور در نظر گرفته می‌شود که تنها از طریق یک شبکه قابل دسترسی است. با این حال، اختصاص یک کارت شبکه به هر domU ممکن است مناسب نباشد؛ به همین دلیل است که Xen امکان ایجاد رابط‌های مجازی که هر دامنه قادر به مشاهده و استفاده استاندارد از آن‌ها باشد را می‌دهد. به یاد داشته باشید که این کارت‌ها، با وجود مجازی بودن، تنها زمانی مفید هستند که به یک شبکه متصل گردند. Xen دارای چندین مدل شبکه برای این منظور است:
			</para>
			 <itemizedlist>
				<listitem>
					<para>
						ساده‌ترین مدل <emphasis>bridge</emphasis> است؛ تمام کارت‌های شبکه eth0 (در سیستم‌های dom0 و domU) طوری عمل می‌کنند گویی به سوئیچ اصلی Ethernet متصل شده‌اند.
					</para>

				</listitem>
				 <listitem>
					<para>
						سپس مدل <emphasis>routing</emphasis> قرار دارد، به صورتی که dom0 به عنوان یک مسیریاب میان سیستم‌های domU و شبکه خارجی (فیزیکی) قرار می‌گیرد.
					</para>

				</listitem>
				 <listitem>
					<para>
						در نهایت، در مدل <emphasis>NAT</emphasis>، dom0 بین سیستم‌های domU و باقی شبکه قرار می‌گیرد، اما سیستم‌های domU به صورت مستقیم از خارج قابل دسترس نیستند و ترافیک از طریق ترجمه نشانی شبکه یا NAT با استفاده از dom0 انتقال می‌یابد.
					</para>

				</listitem>

			</itemizedlist>
			 <para>
				این سه مدل شبکه دارای تعدادی رابط با نام‌های نامتعارف هستند از جمله <filename>vif*</filename>، <filename>veth*</filename>، <filename>peth*</filename> و <filename>xenbr0</filename>. hypervisor موجود در Xen با توجه به لایه تعریف شده آن‌ها را مرتب‌سازی می‌کند که این کار با استفاده از ابزارهای سمت-کاربر صورت می‌گیرد. از آنجا که مدل‌های NAT و routing تنها برای موارد خاص کاربرد دارند، تنها به بررسی مدل bridge می‌پردازیم.
			</para>
			 <para>
				پیکربندی استاندارد بسته‌های Xen تغییری در پیکربندی شبکه در کل سیستم ایجاد نمی‌کند. با این حال، فرآیند پس‌زمینه <command>xend</command> طوری پیکربندی شده است تا رابط‌های مجازی شبکه را با هر شبکه bridge از پیش موجود یکپارچه سازد (در صورت وجود چندین bridge گزینه <filename>xenbr0</filename> اولویت می‌یابد). برای اینکار نیازمند برپایی یک bridge در <filename>/etc/network/interfaces</filename> هستیم تا مدخل موجود eth0 جایگزین گردد (که نیازمند نصب بسته <emphasis role="pkg">bridge-utils</emphasis> می‌باشد، به همین دلیل است که بسته <emphasis role="pkg">xen-utils-4.4</emphasis> توصیه می‌شود):
			</para>
			 
<programlisting>auto xenbr0
iface xenbr0 inet dhcp
    bridge_ports eth0
    bridge_maxwait 0
</programlisting>
			 <para>
				پس از راه‌اندازی مجدد و اطمینان از اینکه bridge به طور خودکار ایجاد شده است، اکنون می‌توانیم domU را با استفاده از ابزار کنترلی Xen، به خصوص دستور <command>xl</command>، آغاز کنیم. این دستور امکان چندین تغییر روی دامنه‌ها را همراه با فهرست‌سازی آن‌ها و آغاز/پایان هر کدام فراهم می‌آورد.
			</para>
			 
<screen><computeroutput># </computeroutput><userinput>xl list</userinput>
<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   463     1     r-----      9.8
# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>
<computeroutput>Parsing config from /etc/xen/testxen.cfg
# </computeroutput><userinput>xl list</userinput>
<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   366     1     r-----     11.4
testxen                                      1   128     1     -b----      1.1</computeroutput></screen>
			 <sidebar> <title><emphasis>ابزار</emphasis> انتخاب جعبه ابزار برای مدیریت ماشین مجازی Xen</title>
			 <indexterm>
				<primary><command>xm</command></primary>
			</indexterm>
			 <indexterm>
				<primary><command>xe</command></primary>
			</indexterm>
			 <para>
				در دبیان ۷ و نسخه‌های قبل از آن، <command>xm</command> ابزار خط فرمان مرجع برای مدیریت ماشین‌های مجازی Xen بود. اکنون با <command>xl</command> جایگزین شده است که در اکثر موارد با آن سازگاری دارد. اما این گزینه‌ها تنها ابزار موجود برای اینکار نیستند: <command>virsh</command> از libvirt و <command>xe</command> از XAPI موجود در XenServer (بسته تجاری Xen) ابزارهای جایگزین هستند.
			</para>
			 </sidebar> <sidebar> <title><emphasis>احتیاط</emphasis> تنها یک domU برای هر image!</title>
			 <para>
				از آنجا که امکان استفاده از چندین سیستم domU مختلف به صورت موازی وجود دارد، تمام آن‌ها نیازمند استفاده از image مخصوص به خود هستند، چرا که برای هر domU انتظار می‌رود که روی سخت‌افزار مختص به خود اجرا گردد (جدا از تکه بسیار کوچک کرنل که با hypervisor تعامل می‌کند). به طور مشخص، امکان استفاده همزمان از دو سیستم domU که از یک فضای ذخیره‌سازی اشتراکی بهره می‌برند وجود ندارد. اگر سیستم‌های domU در یک زمان واحد اجرا نشوند، تقریبا امکان استفاده از یک پارتیشن swap یا پارتیشنی که از فایل سیستم <filename>/home</filename> میزبانی می‌کند وجود دارد.
			</para>
			 </sidebar> <para>
				به یاد داشته باشید که domU ایجاد شده بنام ... از حافظه واقعی گرفته شده از RAM که معمولا برای dom0 در نظر گرفته می‌شود، استفاده می‌کند و نه یک حافظه شبیه‌سازی شده. بنابراین هنگام راه‌اندازی یک سرور مبتی بر Xen باید دقت عمل در تخصیص حافظه به خرج داد.
			</para>
			 <para>
				بسیار خوب! ماشین مجازی ما راه‌اندازی شد. دو روش دسترسی به آن وجود دارد. روش معمول اتصال ... به آن است، مانند یک ماشین حقیقی که از طریق شبکه متصل می‌شویم؛ اینکار نیازمند برپایی یک سرور DHCP یا پیکربندی DNS جداگانه است. روش دیگر، که در صورت اشتباه بودن پیکربندی شبکه می‌تواند تنها روش ممکن باشد، استفاده از کنسول <filename>hvc0</filename> همراه با دستور <command>xl console</command> است:
			</para>
			 
<screen><computeroutput># </computeroutput><userinput>xl console testxen</userinput>
<computeroutput>[...]

Debian GNU/Linux 8 testxen hvc0

testxen login: </computeroutput></screen>
			 <para>
				در این حالت می‌توان یک نشست جداگانه برای دسترسی به ماشین مجازی ایجاد کرد. قطع اتصال این کنسول با استفاده از کلید ترکیبی <keycombo action="simul"><keycap>Control</keycap> <keycap>]</keycap></keycombo> انجام می‌شود.
			</para>
			 <sidebar> <title><emphasis>نکته</emphasis> دسترسی مستقیم به کنسول</title>
			 <para>
				بعضی وقت‌ها شاید بخواهیم پس از راه‌اندازی سیستم domU مستقیم وارد کنسول آن شویم؛ به همین دلیل است که دستور <command>xl create</command> یک گزینه <literal>-c</literal> را نیز می‌پذیرد. آغاز یک domU همراه با این سوئیچ تمام پیام‌های موجود هنگام راه‌اندازی آن را نمایش می‌دهد.
			</para>
			 </sidebar> <sidebar> <title><emphasis>ابزار</emphasis> OpenXenManager</title>
			 <para>
				OpenXenManager (در بسته <emphasis role="pkg">openxenmanager</emphasis>) یک ابزار گرافیکی است که امکان مدیریت دامنه‌های Xen را با استفاده از API آن فراهم می‌کند. همچنین امکان کنترل دامنه‌های Xen از راه‌دور نیز وجود دارد. این بسته تقریبا تمام ویژگی‌های دستور <command>xl</command> را فراهم می‌کند.
			</para>
			 </sidebar> <para>
				زمانی که domU آغاز گردد، می‌تواند مانند هر سرور دیگری مورد استفاده قرار گیرد (چرا که یک سیستم گنو/لینوکس است). اگرچه، وضعیت ماشین مجازی آن برخی قابلیت‌های دیگر را فراهم می‌کند. برای نمونه، یک domU با استفاده از دستورات <command>xl pause</command> و <command>xl unpause</command> می‌تواند به صورت موقت متوقف شده یا ادامه یابد. به یاد داشته باشید که یک domU متوقف شده با اینکه از قدرت پردازنده استفاده نمی‌کند، اما هم اکنون حافظه اختصاص یافته به خود را دارد. استفاده از دستورات <command>xl save</command> و <command>xl restore</command> نیز قابل توجه است: ذخیره‌سازی یک domU تمام منابع اختصاص یافته به آن، از جمله RAM، را آزادسازی می‌کند. در زمان بازگرداندن (یا ادامه، به این منظور) یک domU چیزی به جز گذشت زمان را احساس نمی‌کند. اگر هنگام خاموش کردن dom0 یک domU در حال اجرا باشد، اسکریپت‌های پیکربندی به صورت خودکار domU را ذخیره‌سازی کرده تا در راه‌اندازی بعدی از سر گرفته شود. این عمل البته ناملایمت‌های عملیات hibernate کردن یک رایانه لپ‌تاپ را به همراه دارد، برای نمونه؛ به طور مشخص اگر domU به مدت زمان طولانی در حالت تعلیق باشد، ارتباطات شبکه ممکن است منقضی گردند. به یاد داشته باشید که Xen به شدت ناسازگار با بخش مدیریت قدرت ACPI است، که عملیات متوقف‌سازی سیستم میزبان (dom0) را انجام می‌دهد.
			</para>
			 <sidebar> <title><emphasis>مستندات</emphasis> گزینه‌های <command>xl</command></title>
			 <para>
				اغلب زیردستورات <command>xl</command> شامل یک یا چند آرگومان هستند، که بیشتر شامل نام یک domU می‌شود. این آرگومان‌ها به خوبی در صفحه راهنمای <citerefentry><refentrytitle>xl</refentrytitle>
				 <manvolnum>1</manvolnum></citerefentry> توضیح داده شده‌اند.
			</para>
			 </sidebar> <para>
				متوقف‌سازی یا راه‌اندازی مجدد یک domU می‌تواند از طریق خود آن (با دستور <command>shutdown</command>) یا از طریق dom0 با استفاده از <command>xl shutdown</command> یا <command>xl reboot</command> انجام شود.
			</para>
			 <sidebar> <title><emphasis>مطالعه بیشتر</emphasis> Xen پیشرفته</title>
			 <para>
				Xen دارای قابلیت‌های بسیاری است که نمی‌توان در چند پاراگراف به آن‌ها اشاره کرد. به طور مشخص، سیستم بسیار پویا است و امکان تنظیم چندین پارامتر برای یک دامنه هنگام اجرای آن وجود دارد (از جمله میزان حافظه اختصاص یافته، هارد درایوهای قابل مشاهده، رفتار زمان‌بند وظایف و از این قبیل). یک domU حتی می‌تواند بین چندین سرور انتقال یابد به گونه‌‌ای که نه خاموش گردد و نه ارتباط شبکه را از دست دهد! برای اطلاع از تمام این جنبه‌های پیشرفته، منبع اولیه اطلاعات، مستندات رسمی Xen است. <ulink type="block" url="http://www.xen.org/support/documentation.html" />
			</para>
			 </sidebar>
		</section>
		 <section id="sect.lxc">
			<title>LXC</title>
			 <indexterm>
				<primary>LXC</primary>
			</indexterm>
			 <para>
				با اینکه از آن برای ساخت “ماشین‌های مجازی” استفاده می‌شود، LXC به طور دقیق یک سیستم مجازی‌سازی نیست، بلکه سیستمی برای جدا کردن گروهی از فرآیندها نسبت به یکدیگر می‌باشد که درون یک میزبان اجرا می‌شوند. این سیستم از پیشرفت‌های اخیر در کرنل لینوکس بهره می‌برد، که بنام <emphasis>گروه‌های کنترل</emphasis> شناخته می‌شوند، به این معنی که مجموعه‌های مختلف از فرآیندها که “گروه” نامیده می‌شوند دید متفاوتی نسبت به جنبه‌های کلی سیستم دارند. از جمله این جنبه‌ها می‌توان به شناسه‌های فرآیند، پیکربندی شبکه و نقاط اتصال اشاره کرد. چنین گروهی از فرآیندهای ایزوله‌شده هیچ گونه دسترسی دیگر به سایر فرآیندهای سیستم ندارند و دسترسی آن‌ها به فایل سیستم تنها محدود به مجموعه‌ای کوچک می‌گردد. از این رو می‌تواند رابط شبکه و جدول مسیریابی مربوط به خود را داشته باشد و می‌تواند طوری پیکربندی شود که تنها مجموعه کوچکی از دستگاه‌های موجود در سیستم را مشاهده کند.
			</para>
			 <para>
				این ویژگی‌ها می‌توانند به منظور جدا کردن خانواده فرآیند آغازی توسط <command>init</command> با یکدیگر ترکیب شده که نتیجه نهایی آن مشابه با ماشین مجازی است. نام رسمی چنین تنظیمی “مخزن” است (با توجه به نام LXC که برابر است با: <emphasis>LinuX Containers</emphasis>) اما تفاوت عمده آن با ماشین‌های مجازی “واقعی” مانند Xen یا KVM در نبود یک کرنل دوم است؛ مخزن از همان کرنل سیستم میزبان استفاده می‌کند. اینکار مزایا و معایب خود را دارد: مزایای آن شامل عملکرد فوق‌العاده به دلیل نبود overhead و این حقیقت که کرنل یک دید سراسری نسبت به تمام فرآیندهای اجرایی روی سیستم دارد، به این منظور که عملیات زمان‌بندی می‌تواند به شیوه‌ای موثرتر انجام شود نسبت به حالتی که دو کرنل جداگانه باید مجموعه‌‌های مختلف از وظایف را مدیریت می‌کردند. از میان معایت نیز می‌توان به غیرممکن بودن اجرای یک کرنل مختلف درون یک مخزن اشاره کرد (خواه یک نسخه متفاوت لینوکس خواه یک سیستم عامل دیگر).
			</para>
			 <sidebar> <title><emphasis>یادداشت</emphasis> محدودیت‌های انزوای LXC</title>
			 <para>
				مخازن LXC سطحی از انزوا را مانند شبیه‌سازهای قدرتمند یا مجازی‌سازهای دیگر فراهم نمی‌کنند. به طور مشخص:
			</para>
			 <itemizedlist>
				<listitem>
					<para>
						از آنجا که کرنل بین سیستم میزبان و مخازن مشترک است، فرآیندهایی که محدود به مخازن هستند کماکان می‌توانند به پیام‌های کرنل دسترسی داشته باشند که در صورت انتشار پیام‌ها توسط یک مخزن می‌تواند منجر به افشای اطلاعات شود؛
					</para>

				</listitem>
				 <listitem>
					<para>
						به دلیل مشابه، اگر به یک مخزن نفوذ شود و از یک آسیب‌پذیری کرنل استفاده گردد، سایر مخازن نیز تاثیر منفی می‌پذیرند؛
					</para>

				</listitem>
				 <listitem>
					<para>
						در فایل سیستم، کرنل به بررسی مجوزهای کاربران و گروه‌ها مبتنی بر شناسه‌های عددی می‌پردازد؛ این شناسه‌ها ممکن است به کاربران و گروه‌های مختلف با توجه به مخزن اختصاص یابند که در صورت اشتراکی بودن قسمت‌های قابل نوشتن فایل سیستم بین مخازن باید مورد توجه قرار گیرد.
					</para>

				</listitem>

			</itemizedlist>
			 </sidebar> <para>
				از آنجا که با مفهوم ایزوله کردن و نه یک راهکار مجازی‌سازی ساده سروکار داریم، برپایی مخازن LXC بسیار پیچیده‌تر از اجرای debian-installer در یک ماشین مجازی است. ابتدا چندین پیشنیاز را بررسی کرده سپس به قسمت پیکربندی شبکه می‌رویم؛ در این قسمت است که می‌توانیم سیستم را درون یک مخزن اجرا کنیم.
			</para>
			 <section>
				<title>گام‌های مقدماتی</title>
				 <para>
					بسته <emphasis role="pkg">lxc</emphasis> شامل ابزار مورد نیاز برای نصب و اجرای LXC است.
				</para>
				 <para>
					LXC همچنین به سیستم پیکربندی <emphasis>control groups</emphasis> نیاز دارد که به صورت یک فایل سیستم مجازی به <filename>/sys/fs/cgroup</filename> متصل می‌شود. از آنجا که دبیان ۸ به systemd روی آورده، که خود مبتنی بر گروه‌های کنترل است، اینکار در زمان راه‌اندازی سیستم بدون هیچ پیکربندی خاص صورت می‌گیرد.
				</para>

			</section>
			 <section id="sect.lxc.network">
				<title>پیکربندی شبکه</title>
				 <para>
					هدف از نصب LXC برپایی ماشین‌های مجازی است؛ با اینکه می‌توانیم آن‌ها را به صورت ایزوله در شبکه قرار دهیم و تنها از طریق فایل سیستم با آن‌ها تعامل کنیم، اکثر موارد کاربردی شامل دسترسی حداقلی شبکه به مخازن است. در حالت معمولی، به هر مخزن یک رابط مجازی شبکه اختصاص می‌یابد که از طریق bridge به یک رابط حقیقی شبکه متصل است. این رابط مجازی هم می‌تواند به رابط فیزیکی میزبان (که در این صورت مخزن به صورت مستقیم در شبکه قرار می‌گیرد) هم می‌تواند به رابط مجازی دیگری در میزبان متصل شود (که میزبان کار فیلتر و مسیریابی ترافیک را انجام می‌دهد). در هر صورت، بسته <emphasis role="pkg">bridge-utils</emphasis> مورد نیاز خواهد بود.
				</para>
				 <para>
					مورد اول به سادگی ویرایش فایل <filename>/etc/network/interfaces</filename>، انتقال پیکربندی برای رابط فیزیکی (برای نمونه <literal>eth0</literal>) به رابط bridge (معمولا <literal>br0</literal>) و پیکربندی پیوند بین آن‌ها است. برای نمونه، اگر فایل پیکربندی رابط شبکه شامل مدخل‌های زیر باشد:
				</para>
				 
<programlisting>auto eth0
iface eth0 inet dhcp</programlisting>
				 <para>
					آن‌ها باید غیرفعال شده و با مدخل‌های زیر جایگزین گردند:
				</para>
				 
<programlisting>#auto eth0
#iface eth0 inet dhcp

auto br0
iface br0 inet dhcp
  bridge-ports eth0</programlisting>
				 <para>
					تاثیر این پیکربندی مشابه با حالتی خواهد بود که مخازن به صورت ماشین‌هایی به شبکه فیزیکی یکسانی از طریق میزبان متصل می‌شدند. پیکربندی “bridge” انتقال فریم‌های Ethernet را بین تمام رابط‌های bridged مدیریت می‌کند که شامل <literal>eth0</literal> همراه با رابط‌های تعریف شده برای مخازن می‌باشد.
				</para>
				 <para>
					در مواری که این پیکربندی نمی‌تواند استفاده شود (برای نمونه اگر هیچ نشانی عمومی IP نتواند به مخازن اختصاص یابد)، یک رابط مجازی <emphasis>tap</emphasis> ایجاد و به bridge متصل می‌شود. معادل توپولوژی شبکه سپس به میزبانی با یک کارت شبکه ثانویه تبدیل شده که به یک سوئیچ جداگانه متصل است، همراه با مخازن متصل به آن سوئیچ. میزبان باید به صورت یک gateway برای مخازنی عمل کند که قصد ارتباط با دنیای خارج را دارند.
				</para>
				 <para>
					علاوه بر <emphasis role="pkg">bridge-utils</emphasis>، این پیکربندی “غنی” نیازمند بسته <emphasis role="pkg">vde2</emphasis> است؛ فایل <filename>/etc/network/interfaces</filename> سپس به صورت زیر در می‌آید:
				</para>
				 
<programlisting># Interface eth0 is unchanged
auto eth0
iface eth0 inet dhcp

# Virtual interface 
auto tap0
iface tap0 inet manual
  vde2-switch -t tap0

# Bridge for containers
auto br0
iface br0 inet static
  bridge-ports tap0
  address 10.0.0.1
  netmask 255.255.255.0</programlisting>
				 <para>
					شبکه می‌تواند یا به صورت ایستا درون مخازن یا به صورت پویا از طریق سرور DHCP درون میزبان برپا شود. چنین سرور DHCP باید طوری پیکربندی شود که به پرس و جوهای موجود در رابط <literal>br0</literal> پاسخ دهد.
				</para>

			</section>
			 <section>
				<title>برپایی سیستم</title>
				 <para>
					اکنون بیاید فایل سیستم مورد نیاز مخزن را برپا کنیم. از آنجا که این “ماشین مجازی” به صورت مستقیم روی سخت‌افزار اجرا نخواهد شد، در مقایسه با یک فایل سیستم استاندارد رعایت برخی نکات ضروری است، به خصوص تا آنجا که به کرنل، دستگاه‌ها و کنسول‌ها مربوط باشد. خوشبختانه <emphasis role="pkg">lxc</emphasis> شامل اسکریپت‌هایی است که اکثر این پیکربندی را به صورت خودکار انجام می‌دهند. برای نمونه، دستورات پیش رو (که نیازمند بسته‌های <emphasis role="pkg">debootstrap</emphasis> و <emphasis role="pkg">rsync</emphasis> هستند) اقدام به نصب یک مخزن دبیان می‌کنند:
				</para>
				 
<screen><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-create -n testlxc -t debian
</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap
Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... 
Downloading debian minimal ...
I: Retrieving Release 
I: Retrieving Release.gpg 
[...]
Download complete.
Copying rootfs to /var/lib/lxc/testlxc/rootfs...
[...]
Root password is 'sSiKhMzI', please change !
root@mirwiz:~# </computeroutput>
</screen>
				 <para>
					به یاد داشته باشید که فایل سیستم به صورت اولیه در <filename>/var/cache/lxc</filename> ایجاد شد، سپس به مقصد خود انتقال یافت. اینکار امکان ایجاد مخازن مشابه را با سرعت بیشتری فراهم می‌کند، چرا که تنها عملیات رونوشت‌گیری مورد نیاز است.
				</para>
				 <para>
					به یاد داشته باشید که اسکریپت ایجاد قالب دبیان یک گزینه <option>--arch</option> به منظور تعیین معماری سیستم و یک گزینه <option>--release</option> به منظور نصب نسخه‌ای بجز نسخه انتشار اصلی از دبیان را قبول می‌کند. همچنین می‌توانید با استفاده از متغیر محیطی <literal>MIRROR</literal> از یک mirror مخصوص به دبیان استفاده کنید.
				</para>
				 <para>
					فایل سیستم تازه ایجاد شده اکنون شامل یک سیستم پایه دبیان است و مخزن پیشفرض آن هیچ رابط شبکه‌ای ندارد (بجز گزینه loopback). از آنجا که این مورد نظر ما نیست، به ویرایش فایل پیکربندی مخزن (<filename>/var/lib/lxc/testlxc/config</filename>) پرداخته و چندین مدخل <literal>lxc.network.*</literal> را در آن ایجاد می‌کنیم:
				</para>
				 
<programlisting>lxc.network.type = veth
lxc.network.flags = up
lxc.network.link = br0
lxc.network.hwaddr = 4a:49:43:49:79:20</programlisting>
				 <para>
					وجود این مدخل‌ها به این معنی است که یک رابط مجازی برای مخزن ایجاد خواهد شد؛ که به صورت خودکار هنگام آغاز مخزن شروع می‌شوند؛ که به صورت خودکار به bridge موجود در میزبان بنام <literal>br0</literal> متصل می‌شوند؛ که نشانی MAC آن مطابق بالا خواهد بود. در صورت فقدان یا غیرفعال بودن این گزینه آخر، از یک نشانی MAC تصادفی استفاده خواهد شد.
				</para>
				 <para>
					یک مدخل مفید دیگر در آن فایل تنظیم نام میزبان است:
				</para>
				 
<programlisting>lxc.utsname = testlxc</programlisting>

			</section>
			 <section>
				<title>آغاز مخزن</title>
				 <para>
					اکنون که image ماشین مجازی آماده است، بیایید مخزن را آغاز کنیم:
				</para>
				 
<screen role="scale" width="94"><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-start --daemon --name=testlxc
</userinput><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-console -n testlxc
</userinput><computeroutput>Debian GNU/Linux 8 testlxc tty1

testlxc login: </computeroutput><userinput>root</userinput><computeroutput>
Password: 
Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
root@testlxc:~# </computeroutput><userinput>ps auxwf</userinput>
<computeroutput>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init
root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald
root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D
root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux
root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux
root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux
root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     
root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \_ -bash
root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \_ ps auxfw
root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102
root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e
root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux
root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux
root@testlxc:~# </computeroutput></screen>
				 <para>
					اکنون درون مخزن هستیم؛ دسترسی به فرآیندها تنها محدود به آن‌هایی است که توسط مخزن آغاز شده باشند و دسترسی به فایل سیستم تنها بخش کوچکی از فایل سیستم کامل <filename>/var/lib/lxc/testlxc/rootfs</filename> را شامل می‌شود. با استفاده از کلید ترکیبی <keycombo action="simul"><keycap>Control</keycap> <keycap>a</keycap></keycombo> <keycombo><keycap>q</keycap></keycombo> می‌توانیم از کنسول خارج شویم.
				</para>
				 <para>
					به یاد داشته باشید که مخزن را به عنوان یک فرآیند پس‌زمینه اجرا کردیم، به لطف گزینه <option>--daemon</option> از <command>lxc-start</command>. با استفاده از دستور <command>lxc-stop --name=testlxc</command> می‌توانیم مخزن را متوقف کنیم.
				</para>
				 <para>
					بسته <emphasis role="pkg">lxc</emphasis> شامل یک اسکریپت راه‌اندازی است که به صورت خودکار یک یا چند مخزن را در زمان راه‌اندازی میزبان آغاز می‌کند (مبتنی بر <command>lxc-autostart</command> است که به صورت خودکار مخازن شامل گزینه <literal>lxc.start.auto</literal> برابر ۱ را راه‌اندازی می‌کند). با استفاده از <literal>lxc.start.order</literal> و <literal>lxc.group</literal> می‌توان کنترل بیشتری روی ترتیب اجرای مخازن اعمال کرد: به صورت پیشفرض، اسکریپت راه‌اندازی ابتدا مخازنی را آغاز می‌کند که جزو گروه <literal>onboot</literal> باشند سپس به سراغ مخازن دیگر می‌رود). در هر دو مورد، ترتیب درون هر گروه توسط گزینه <literal>lxc.start.order</literal> مشخص می‌شود.
				</para>
				 <sidebar> <title><emphasis>مطالعه بیشتر</emphasis> مجازی‌سازی انبوه</title>
				 <para>
					از آنجا که LXC یک سیستم ایزوله‌کردن سبک به حساب می‌آید، می‌تواند به منظور میزبانی از سرورهای مجازی انبوه سازگار شود. پیکربندی شبکه از آنچه در این قسمت توضیح دادیم به مراتب پیچیده‌تر خواهد بود اما پیکربندی “غنی” با استفاده از رابط‌های <literal>tap</literal> و <literal>veth</literal> در اکثر موارد به شیوه توضیح داده شده کافی خواهد بود.
				</para>
				 <para>
					به منظور پیشگیری از نصب مجدد نرم‌افزارهایی که برای چندین مخزن کاربردی هستند، معقول بنظر می‌رسد که قسمتی از فایل سیستم مانند <filename>/usr</filename> و <filename>/lib</filename> را به اشتراک بگذاریم. اینکار معمولا با مدخل‌های <literal>lxc.mount.entry</literal> درون فایل پیکربندی مخازن انجام می‌شود. یک تاثیر جانبی جالب این است که فرآیندها از حافظه فیزیکی کمتری استفاده می‌کنند، چرا که کرنل قادر به تشخیص برنامه‌هایی است که به صورت اشتراکی کار می‌کنند. هزینه حاشیه‌ای یک مخزن اضافی دیگر می‌تواند به فضای دیسک اختصاص یافته به داده خاص و چند فرآیند اضافی که کرنل برای زمان‌بندی و مدیریت استفاده می‌کند، کاهش یابد.
				</para>
				 <para>
					البته، تمام گزینه‌های موجود را بررسی نکردیم؛ اطلاعات جامع بیشتر از طریق صفحات راهنمای <citerefentry> <refentrytitle>lxc</refentrytitle>
					 <manvolnum>7</manvolnum> </citerefentry> و <citerefentry> <refentrytitle>lxc.container.conf</refentrytitle>
					 <manvolnum>5</manvolnum></citerefentry> همراه با سایر مراجع آن قابل دسترس است.
				</para>
				 </sidebar>
			</section>

		</section>
		 <section>
			<title>مجازی‌سازی با KVM</title>
			 <indexterm>
				<primary>KVM</primary>
			</indexterm>
			 <para>
				KVM، که مخفف عبارت <emphasis>Kernel-based Virtual Machine</emphasis> است، در درجه اول یک افزونه کرنل به حساب می‌آید که اکثر زیرساخت مورد نیاز یک مجازی‌ساز را فراهم می‌کند اما خود یک مجازی‌ساز نیست. کنترل واقعی مجازی‌سازی توسط برنامه‌ای مبتنی بر QEMU انجام می‌شود. نگران نباشید اگر در این قسمت دستورات مربوط به <command>qemu-*</command> را مشاهده کنید: تمام آن‌ها مرتبط با KVM هستند.
			</para>
			 <para>
				برخلاف سایر سیستم‌های مجازی‌سازی، KVM از ابتدا درون کرنل لینوکس قرار گرفت. توسعه‌دهندگان آن تصمیم گرفتند از مجموعه دستورالعمل‌های پردازنده برای مجازی‌سازی (Intel-VT و AMD-V) استفاده کنند که اینکار باعث می‌شود KVM سبک، ظریف و سازگار با منابع پایین باشد. نقطه ضعف آن این است که KVM روی هر رایانه‌ای نمی‌تواند اجرا شود بلکه فقط برخی پردازنده‌های خاص از آن پشتیبانی می‌کنند. برای رایانه‌های مبتنی بر x86، می‌توانید به دنبال پرچم‌های مخصوص پردازنده به نام “vmx” یا “svm” در فایل <filename>/proc/cpuinfo</filename> بگردید.
			</para>
			 <para>
				با پشتیبانی مداوم Red Hat از توسعه آن، KVM کم و بیش به مرجع مجازی‌سازی در لینوکس تبدیل شده است.
			</para>
			 <section>
				<title>گام‌های مقدماتی</title>
				 <indexterm>
					<primary><command>virt-install</command></primary>
				</indexterm>
				 <para>
					KVM برخلاف ابزاری مانند VirtualBox، شامل رابط کاربری برای مدیریت ماشین‌های مجازی نیست. بسته <emphasis role="pkg">qemu-kvm</emphasis> تنها شامل یک فایل اجرایی برای آغاز یک ماشین مجازی است، همراه با اسکریپت‌های راه‌اندازی که اقدام به بارگیری افزونه‌های مناسب کرنل می‌کنند.
				</para>
				 <indexterm>
					<primary>libvirt</primary>
				</indexterm>
				 <indexterm>
					<primary><emphasis role="pkg">virt-manager</emphasis></primary>
				</indexterm>
				 <para>
					خوشبختانه، Red Hat برای غلبه بر این مشکل مجموعه ابزاری فراهم کرده است که شامل کتابخانه <emphasis>libvirt</emphasis> و ابزارهای <emphasis>virtual machine manager</emphasis> می‌شوند. libvirt امکان مدیریت ماشین‌های مجازی را به یک شیوه یکسان فراهم می‌کند، جدا از سیستم مجازی‌سازی که در پشت صحنه قرار دارد (هم اکنون از QEMU، KVM، Xen، LXC، OpenVZ، VirtualBox، VMWare و UML پشتیبانی می‌کند). <command>virtual-manager</command> یک رابط گرافیکی است که با استفاده از libvirt ماشین‌های مجازی را مدیریت می‌کند.
				</para>
				 <indexterm>
					<primary><emphasis role="pkg">virtinst</emphasis></primary>
				</indexterm>
				 <para>
					ابتدا با استفاده از <command>apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</command> بسته‌های مورد نیاز را نصب می‌کنیم. <emphasis role="pkg">libvirt-bin</emphasis> فرآیند پس‌زمینه <command>libvirtd</command> را فراهم می‌کند، که امکان مدیریت (معمولا راه دور) ماشین‌های مجازی اجرای در سیستم میزبان را فراهم کرده و ماشین‌های مجازی مورد نیاز را در زمان راه‌اندازی میزبان آغاز می‌کند. علاوه بر این، این بسته ابزار خط-فرمان <command>virsh</command> را فراهم می‌کند که امکان کنترل ماشین‌های <command>libvirtd</command> را بوجود می‌آورد.
				</para>
				 <para>
					بسته <emphasis role="pkg">virtinst</emphasis> شامل <command>virt-install</command> می‌شود که امکان ایجاد ماشین‌های مجازی از خط فرمان را فراهم می‌کند. در نهایت، <emphasis role="pkg">virt-viewer</emphasis> اجازه دسترسی به کنسول گرافیکی یک ماشین مجازی را بوجود می‌آورد.
				</para>

			</section>
			 <section>
				<title>پیکربندی شبکه</title>
				 <para>
					درست مانند Xen و LXC، متداول‌ترین پیکربندی شبکه شامل یک bridge که رابط‌های شبکه ماشین‌های مجازی را گروه‌بندی می‌کند، می‌باشد (<xref linkend="sect.lxc.network" /> را مشاهده کنید).
				</para>
				 <para>
					به طور متقابل، در پیکربندی پیشفرض فراهم شده توسط KVM، یک نشانی خصوصی به ماشین مجازی اختصاص می‌یابد (در محدوده 192.168.122.0/24) و NAT طوری تنظیم می‌شود که ماشین مجازی بتواند به شبکه خارجی دسترسی داشته باشد.
				</para>
				 <para>
					باقیمانده این قسمت با توجه به اینکه میزبان دارای یک رابط فیزیکی <literal>eth0</literal> و bridge <literal>br0</literal> است ادامه می‌یابد، به طوری که اولی به دومی متصل شده است.
				</para>

			</section>
			 <section>
				<title>نصب با <command>virt-install</command></title>
				 <indexterm>
					<primary><command>virt-install</command></primary>
				</indexterm>
				 <para>
					ایجاد یک ماشین مجازی بسیار شبیه یک سیستم عادی است، با این تفاوت که ویژگی‌های ماشین مجازی به صورت گزینه‌های بی‌پایان در خط فرمان قرار می‌گیرند.
				</para>
				 <para>
					در عمل، یعنی از یک نصب کننده دبیان استفاده خواهیم کرد، با راه‌اندازی ماشین مجازی روی یک درایو DVD-ROM که به یک تصویر از DVD دبیان ذخیره شده در سیستم میزبان نگاشت شده است. ماشین مجازی از طریق پروتکل VNC کنسول گرافیکی خود را آماده می‌کند (<xref linkend="sect.remote-desktops" /> را مشاهده کنید) که اینکار به ما اجازه می‌دهد فرآیند نصب را کنترل کنیم.
				</para>
				 <para>
					ابتدا باید به libvirtd بگوییم تصاویر دیسک را در کجا ذخیره کند، مگر مکان پیشفرض <filename>/var/lib/libvirt/images/</filename> مناسب باشد.
				</para>
				 
<screen><computeroutput>root@mirwiz:~# </computeroutput><userinput>mkdir /srv/kvm</userinput>
<computeroutput>root@mirwiz:~# </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>
<computeroutput>Pool srv-kvm created

root@mirwiz:~# </computeroutput></screen>
				 <sidebar> <title><emphasis>نکته</emphasis> افزودن کاربر خود به گروه libvirt</title>
				 <para>
					تمام نمونه‌های این قسمت فرض را بر این می‌گذارند که شما به عنوان root دستورات را اجرا می‌کنید. اگر قصد کنترل یک libvirt محلی را دارید، یا باید root باشید یا عضوی از گروه <literal>libvirt</literal> (که به صورت پیشفرض فعال نیست). بنابراین به منظور جلوگیری از اجرای تمام دستورات به عنوان root می‌توانید با افزودن کاربر خود به گروه <literal>libvirt</literal> تمام دستورات آن را تحت مجوز کاربری خود اجرا کنید.
				</para>
				 </sidebar> <para>
					اکنون بیایید فرآیند نصب ماشین مجازی را آغاز کرده و نگاهی بر مهم‌ترین گزینه‌های <command>virt-install</command> بیندازیم. این دستور، ماشین مجازی و پارامترهای آن را در libvirtd ثبت می‌کند سپس به اجرای آن پرداخته تا فرآیند نصب ادامه یابد.
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id="virtinst.connect"></co>
               --virt-type kvm           <co id="virtinst.type"></co>
               --name testkvm            <co id="virtinst.name"></co>
               --ram 1024                <co id="virtinst.ram"></co>
               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <co id="virtinst.disk"></co>
               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <co id="virtinst.cdrom"></co>
               --network bridge=br0      <co id="virtinst.network"></co>
               --vnc                     <co id="virtinst.vnc"></co>
               --os-type linux           <co id="virtinst.os"></co>
               --os-variant debianwheezy
</userinput><computeroutput>
Starting install...
Allocating 'testkvm.qcow'             |  10 GB     00:00
Creating domain...                    |    0 B     00:00
Guest installation complete... restarting guest.
</computeroutput></screen>
				 <calloutlist>
					<callout arearefs="virtinst.connect">
						<para>
							گزینه <literal>--connect</literal> مشخص می‌کند از کدام “hypervisor” استفاده شود. فرم استفاده از آن شامل یک URL همراه با سیستم مجازی‌سازی مرتبط (<literal>xen://</literal>، <literal>qemu://</literal>، <literal>lxc://</literal>، <literal>openvz://</literal>، <literal>vbox://</literal>) و ماشینی که باید از آن میزبانی کند می‌باشد (در صورت استفاده از localhost می‌تواند خالی باشد). علاوه بر این و در مورد QEMU/KVM، هر کاربر می‌تواند با استفاده از مجوزهای محدودشده ماشین‌های مجازی را مدیریت کند و مسیر URL امکان تفاوت قائل شدن بین ماشین‌های “سیستم” (<literal>/system</literal>) را از دیگر (<literal>/session</literal>) فراهم می‌کند.
						</para>

					</callout>
					 <callout arearefs="virtinst.type">
						<para>
							از آنجا که KVM به شیوه مشابه QEMU مدیریت می‌شود، <literal>--virt-type kvm</literal> امکان مشخص کردن استفاده از KVM با وجود تشابه با URL QEMU را فراهم می‌کند.
						</para>

					</callout>
					 <callout arearefs="virtinst.name">
						<para>
							گزینه <literal>--name</literal> یک نام (منحصربفرد) برای ماشین مجازی تعریف می‌کند.
						</para>

					</callout>
					 <callout arearefs="virtinst.ram">
						<para>
							گزینه <literal>--ram</literal> میزان RAM (به مگابایت) اختصاص یافته به ماشین مجازی را تعریف می‌کند.
						</para>

					</callout>
					 <callout arearefs="virtinst.disk">
						<para>
							گزینه <literal>--disk</literal> مکان فایل تصویری که قرار است هارد دیسک ماشین مجازی در آن قرار گیرد را تعریف می‌کند؛ این فایل با استفاده از پارامتر <literal>size</literal> (به گیگابایت) در صورت موجود نبودن، ایجاد می‌گردد. پارامتر <literal>format</literal> امکان ذخیره‌سازی فایل تصویر را در قالب‌های گوناگون بوجود می‌آورد. قالب پیشفرض (<literal>raw</literal>) یک فایل تکی است که با محتوا و اندازه دیسک سازگاری داشته باشد. در اینجا از یک قالب پیشرفته‌تر استفاده کرده‌ایم، که مختص به QEMU می‌باشد و امکان شروع با یک فایل کوچک را می‌دهد که به مرور زمان و نیاز ماشین مجازی به فضای بیشتر، بزرگ‌تر می‌شود.
						</para>

					</callout>
					 <callout arearefs="virtinst.cdrom">
						<para>
							گزینه <literal>--cdrom</literal> به منظور یافتن دیسک نوری برای فرآیند نصب استفاده می‌شود. مسیر می‌تواند شامل یک مسیر محلی برای فایل ISO، یک URL که فایل می‌تواند از آنجا دریافت شود یا فایل دستگاه مربوط به یک درایو فیزیکی CD-ROM باشد (<literal>/dev/cdrom</literal>).
						</para>

					</callout>
					 <callout arearefs="virtinst.network">
						<para>
							گزینه <literal>--network</literal> مشخص می‌کند کارت مجازی شبکه چطور با پیکربندی سیستم میزبان ادغام شود. عملکرد پیشفرض آن (که در این نمونه به صورت صریح بیان کرده‌ایم) ادغام آن با شبکه bridge از قبل موجود در سیستم است. اگر چنین bridge موجود نباشد، ماشین مجازی تنها با استفاده از NAT می‌تواند به شبکه فیزیکی دسترسی یابد، بنابراین یک نشانی در محدوده زیرشبکه 192.168.122.0/24 دریافت می‌کند.
						</para>

					</callout>
					 <callout arearefs="virtinst.vnc">
						<para>
							گزینه <literal>--vnc</literal> بیان می‌کند که کنسول گرافیکی باید توسط VNC قابل ارائه باشد. عملکرد پیشفرض سرور VNC این است که تنها به رابط local گوش دهد؛ اگر برنامه VNC در یک میزبان دیگر قرار داشته باشد، برقراری ارتباط نیازمند برپایی تونل SSH می‌باشد (<xref linkend="sect.ssh-port-forwarding" /> را مشاهده کنید). به همین ترتیب، از <literal>--vnclisten=0.0.0.0</literal> می‌توان برای دسترسی به سرور VNC از طریق تمام رابط‌های شبکه استفاده کرد؛ به یاد داشته باشید که در این صورت باید از یک طراحی firewall بهره‌مند شوید.
						</para>

					</callout>
					 <callout arearefs="virtinst.os">
						<para>
							گزینه‌های <literal>--os-type</literal> و <literal>--os-variant</literal>، با توجه به برخی از ویژگ‌های سیستم عامل اشاره شده، امکان بهینه‌سازی چندین پارامتر ماشین مجازی را فراهم می‌کنند.
						</para>

					</callout>

				</calloutlist>
				 <para>
					در این نقطه، ماشین مجازی در حال اجرا است و به منظور ادامه فرآیند نصب باید به کنسول گرافیکی متصل شویم. اگر عملیات قبل از طریق یک میزکار گرافیکی صورت گرفته باشد، این ارتباط به صورت مستقیم برقرار می‌شود. در غیر اینصورت، یا در حالتی که از راه دور اینکار را انجام می‌دهیم، <command>virt-viewer</command> با استفاده از هر محیط گرافیکی برای باز کردن کنسول گرافیکی می‌تواند اجرا شود (به یاد داشته باشید که دو مرتبه گذرواژه root درخواست می‌شود چرا که ۲ ارتباط SSH مورد نیاز است):
				</para>
				 
<screen><computeroutput>$ </computeroutput><userinput>virt-viewer --connect qemu+ssh://root@<replaceable>server</replaceable>/system testkvm
</userinput><computeroutput>root@server's password: 
root@server's password: </computeroutput></screen>
				 <para>
					زمانی که فرآیند نصب به پایان برسد، ماشین مجازی راه‌اندازی مجدد می‌گردد تا قابل استفاده شود.
				</para>

			</section>
			 <section>
				<title>مدیریت ماشین‌های مجازی با <command>virsh</command></title>
				 <indexterm>
					<primary><command>virsh</command></primary>
				</indexterm>
				 <para>
					اکنون که نصب به پایان رسیده است، بیایید چگونگی مدیریت ماشین‌های مجازی را بررسی کنیم. اولین کاری که باید بکنیم پرسش از <command>libvirtd</command> برای فهرستی از ماشین‌های مجازی موجود است:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all
 Id Name                 State
----------------------------------
  - testkvm              shut off
</userinput></screen>
				 <para>
					بیایید ماشین مجازی آزمایشی خود را آغاز کنیم:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>virsh -c qemu:///system start testkvm
</userinput><computeroutput>Domain testkvm started</computeroutput></screen>
				 <para>
					اکنون می‌توانیم دستورالعمل‌های ارتباط به کنسول گرافیکی را دریافت کنیم (نمایش VNC بازگشتی می‌تواند به عنوان پارامتر <command>vncviewer</command> استفاده شود):
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm
</userinput><computeroutput>:0</computeroutput></screen>
				 <para>
					سایر دستورات <command>virsh</command> عبارتند از:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							<literal>reboot</literal> برای راه‌اندازی مجدد یک ماشین مجازی؛
						</para>

					</listitem>
					 <listitem>
						<para>
							<literal>shutdown</literal> برای درخواست یک shutdown تمیز؛
						</para>

					</listitem>
					 <listitem>
						<para>
							<literal>destroy</literal> برای توقف خشن آن؛
						</para>

					</listitem>
					 <listitem>
						<para>
							<literal>suspend</literal> برای توقف عادی آن؛
						</para>

					</listitem>
					 <listitem>
						<para>
							<literal>resume</literal> برای ادامه فعالیت آن؛
						</para>

					</listitem>
					 <listitem>
						<para>
							<literal>autostart</literal> برای فعال کردن (یا غیر فعال کردن با گزینه <literal>--disable</literal>) راه‌اندازی ماشین مجازی به صورت خودکار در زمان راه‌اندازی میزبان؛
						</para>

					</listitem>
					 <listitem>
						<para>
							<literal>undefine</literal> برای حذف تمام نشانه‌های ماشین مجازی از <command>libvirtd</command>.
						</para>

					</listitem>

				</itemizedlist>
				 <para>
					تمام این دستورات شناسه ماشین مجازی را به عنوان یک پارامتر دریافت می‌کنند.
				</para>

			</section>
			 <section>
				<title>نصب یک سیستم مبتنی بر RPM در دبیان با استفاده از yum</title>
				 <para>
					اگر قرار باشد ماشین مجازی به منظور اجرای دبیان (یا یکی از توزیع‌های آن) راه‌اندازی گردد، سیستم می‌تواند با استفاده از <command>debootstrap</command> همانطور که توضیح داده شد راه‌اندازی شود. اما اگر قرار باشد ماشین مجازی به منظور اجرای یک سیستم مبتنی بر RPM (مانند Fedora، CentOS یا Scientific Linux) راه‌اندازی گردد، اینکار باید با استفاده از ابزار <command>yum</command> صورت گیرد (که در بسته‌ای با همین نام قرار دارد).
				</para>
				 <para>
					این فرآیند شامل استفاده از <command>rpm</command> به منظور استخراج مجموعه‌ای از فایل‌ها، شامل فایل‌های پیکربندی <command>yum</command>، سپس فراخوانی <command>yum</command> برای استخراج سایر بسته‌های باقیمانده می‌باشد. اما از آنجا که فراخوانی <command>yum</command> خارج از chroot صورت می‌گیرد، باید برخی تغییرات موقتی را ایجاد کنیم. در نمونه زیر، chroot هدف عبارت است از <filename>/srv/centos</filename>.
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>rootdir="/srv/centos"
</userinput><computeroutput># </computeroutput><userinput>mkdir -p "$rootdir" /etc/rpm
</userinput><computeroutput># </computeroutput><userinput>echo "%_dbpath /var/lib/rpm" &gt; /etc/rpm/macros.dbpath
</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root "$rootdir" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!
rpm: However assuming you know what you are doing...
warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY
# </computeroutput><userinput>sed -i -e "s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g" $rootdir/etc/yum.repos.d/*.repo
</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core
</userinput><computeroutput>[...]
# </computeroutput><userinput>sed -i -e "s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g" $rootdir/etc/yum.repos.d/*.repo
</userinput></screen>

			</section>

		</section>

	</section>
	 <section id="sect.automated-installation">
		<title>نصب خودکار</title>
		 <indexterm>
			<primary>گسترش</primary>
		</indexterm>
		 <indexterm>
			<primary>نصب</primary>
			<secondary>نصب خودکار</secondary>
		</indexterm>
		 <para>
			مدیر سیستم‌های شرکت فالکوت، مانند سایر مدیر سیستم‌های خدمات بزرگ IT، به ابزاری نیاز دارند که فرآیند نصب (یا بازنصب) ماشین‌های خود را در کمترین زمان و در صورت ممکن به صورت خودکار انجام دهند.
		</para>
		 <para>
			این نیازمندی‌ها توسط طیف گسترده‌ای از راه حل‌ها برطرف می‌شوند. از یک طرف ابزار عمومی مانند SystemImager با ایجاد یک تصویر از ماشین نمونه، آن را روی ماشین‌های هدف گسترش می‌دهد؛ از طرف دیگر، نصب کننده دبیان این قابلیت را دارد که با استفاده از یک فایل پیکربندی خاص به پرسش‌های مطرح شده طی فرآیند نصب به صورت خودکار پاسخ دهد. به عنوان یک راه حل ترکیبی، ابزاری مانند FAI، که مخفف <emphasis>Fully Automatic Installer</emphasis> است، ماشین‌ها را با استفاده از سیستم بسته‌بندی نصب، اما از زیرساخت خود به منظور فرآیندهای پیچیده‌تر مانند راه‌اندازی، پارتیشن‌بندی و پیکربندی استفاده می‌کند.
		</para>
		 <para>
			هر یک از این راه حل‌ها نقاط ضعف و قوت خود را دارند: SystemImager مستقل از سیستم‌های بسته‌بندی کار می‌کند که این امر به مدیریت مجموعه‌ای بزرگ از ماشین‌ها با توزیع‌های مختلف لینوکس منجر می‌شود. همچنین شامل یک سیستم بروزرسانی است که نیازمند نصب مجدد نمی‌باشد، اما این سیستم تنها در صورتی قابل اعتماد خواهد بود که هیچ از یک ماشین‌های زیر مجموعه آن به تنهایی تغییر نکرده باشند؛ به عبارت دیگر، کاربر نباید نرم‌افزاری را بروزرسانی یا نصب کند. به همین شکل، بروزرسانی‌های امنیتی نباید به صورت خودکار صورت پذیرند، چرا که باید توسط یک تصویر مرجع و مرکزی از SystemImager مدیریت شوند. این راه حل نیازمند یکپارچه بودن ماشین‌های هدف از نقطه نظر معماری رایانه است، در غیر اینصورت از تصاویر بسیار گوناگونی برای مدیریت آن باید استفاده شود (یک تصویر i386 با ماشین powerpc سازگار نیست).
		</para>
		 <para>
			از طرف دیگر، یک نصب خودکار با استفاده از debian-installer می‌تواند با توجه به نیاز هر ماشین تغییر یابد: برنامه نصب کننده کرنل و بسته‌های نرم‌افزاری مناسب را از مخازن خود دریافت، سخت‌افزار موجود را شناسایی، تمام هارد دیسک را به منظور استفاده بهینه از فضا پارتیشن‌بندی، سیستم مورد نیاز دبیان را نصب و یک راه‌انداز مناسب را تنظیم می‌کند. اگرچه، نصب‌کننده استاندارد تنها نسخه‌های استاندارد دبیان را همراه با سیستم پایه و مجموعه‌ای از “وظایف” انتخابی را نصب می‌کند؛ اینکار از نصب یک سیستم به خصوص همراه با برنامه‌های بسته‌بندی نشده جلوگیری می‌کند. برای رفع این نیازهای خاص نیاز به سفارشی‌سازی نصب کننده است... خوشبختانه، نصب کننده بسیار ماژولار بوده و ابزارهایی برای خودکارسازی این فرآیند سفارشی‌سازی وجود دارند، به خصوص simple-CDD که مخفف <emphasis>Custom Debian Derivative</emphasis> است. این ابزار، با این حال تنها بخش اولیه فرآیند نصب را مدیریت می‌کند؛ این مشکل بزرگی نخواهد بود چرا که ابزار APT امکان گسترش بهینه بروزرسانی‌ها را فراهم می‌کنند.
		</para>
		 <para>
			به منظور تمرکز روی debian-installer و simple-CDD، تنها به بررسی کوتاه FAI می‌پردازیم و ابزار SystemImager را نادیده می‌گیریم (که دیگر در دبیان وجود ندارد)، چرا که این ابزارها در محیط دبیان بسیار متداول هستند.
		</para>
		 <section id="sect.fai">
			<title>نصب‌کننده تمام خودکار (FAI)</title>
			 <indexterm>
				<primary>نصب‌کننده تمام خودکار (FAI)</primary>
			</indexterm>
			 <para>
				<foreignphrase>نصب‌کننده تمام خودکار</foreignphrase> احتمالا قدیمی‌ترین سیستم گسترش خودکار برای دبیان باشد، که جایگاه آن به عنوان یک مرجع را مشخص می‌کند؛ اما طبیعت بسیار سازگار آن به نوعی پیچیدگی‌های درونی‌اش را جبران می‌کند.
			</para>
			 <para>
				FAI نیازمند یک سیستم سرور به منظور نگهداری از اطلاعات راه‌اندازی برای ماشین‌های است که قصد دارند از طریق شبکه به آن متصل گردند. این سرور نیازمند بسته <emphasis role="pkg">fai-server</emphasis> (یا <emphasis role="pkg">fai-quickstart</emphasis>، که عناصر مورد نیاز برای یک پیکربندی استاندارد را گردآوری می‌کند) است.
			</para>
			 <para>
				FAI از یک رویکرد مشخص برای تعریف پروفایل‌های قابل نصب استفاده می‌کند. بجای رونوشت‌گیری ساده از یک مرجع قابل نصب، FAI یک نصب‌کننده تمام عیار است، که با استفاده از مجموعه فایل‌ها و اسکریپت‌های ذخیره‌شده در سرور قابل پیکربندی می‌باشد؛ مکان پیشفرض <filename>/srv/fai/config/</filename> به صورت خودکار ایجاد نمی‌شود، پس مدیر سیستم در کنار سایر فایل‌ها باید آن را ایجاد کند. در اکثر موارد، این فایل‌ها توسط نمونه‌هایی که در بسته مستندات <emphasis role="pkg">fai-doc</emphasis> وجود دارد سفارشی‌سازی می‌شوند، به خصوص دایرکتوری <filename>/usr/share/doc/fai-doc/examples/simple/</filename>.
			</para>
			 <para>
				زمانی که پروفایل‌ها تعریف شوند، دستور <command>fai-setup</command> عناصر مورد نیاز نصب‌کننده FAI را تولید می‌کند؛ اینکار اغلب به معنی آماده‌سازی یا بروزرسانی یک سیستم حداقلی (NFS-root) در حین فرآیند نصب است. گزینه جایگرین آن ایجاد یک CD قابل اجرا با استفاده از <command>fai-cd</command> است.
			</para>
			 <para>
				ایجاد تمام این فایل‌های پیکربندی نیازمند درک درستی از چگونگی عملکرد FAI می‌باشد. یک فرآیند متداول نصب از گام‌های زیر تشکیل شده است:
			</para>
			 <itemizedlist>
				<listitem>
					<para>
						دریافت یک کرنل از شبکه و راه‌اندازی آن؛
					</para>

				</listitem>
				 <listitem>
					<para>
						اتصال فایل سیستم root از NFS؛
					</para>

				</listitem>
				 <listitem>
					<para>
						اجرای <command>/usr/sbin/fai</command>، که باقی فرآیند نصب را کنترل می‌کند (از این رو گام‌های بعدی توسط این اسکریپت فراخوانی می‌شوند)؛
					</para>

				</listitem>
				 <listitem>
					<para>
						رونوشت‌گیری فضای پیکربندی از سرور درون <filename>/fai/</filename>؛
					</para>

				</listitem>
				 <listitem>
					<para>
						اجرای <command>fai-class</command>. اسکریپت‌های <filename>/fai/class/[0-9][0-9]*</filename> به ترتیب اجرا می‌شوند و نام “کلاس‌های” منطبق با ماشین مورد نظر می‌باشند را بر می‌گردانند؛ از این اطلاعات برای ادامه فرآیند نصب استفاده می‌شود. اینکار موجب انعطاف‌پذیری در تعریف سرویس‌های مورد نیاز برای نصب و پیکربندی می‌شود.
					</para>

				</listitem>
				 <listitem>
					<para>
						دریافت تعدادی از متغیرهای پیکربندی، با توجه به کلاس‌های مربوطه؛
					</para>

				</listitem>
				 <listitem>
					<para>
						پارتیشن‌بندی دیسک‌ها و فرمت کردن پارتیشن‌ها، بر اساس اطلاعات فراهم شده در <filename>/fai/disk_config/<replaceable>class</replaceable></filename>؛
					</para>

				</listitem>
				 <listitem>
					<para>
						اتصال پارتیشن‌های مذکور؛
					</para>

				</listitem>
				 <listitem>
					<para>
						نصب سیستم پایه؛
					</para>

				</listitem>
				 <listitem>
					<para>
						گردآوری پایگاه‌داده Debconf با استفاده از <command>fai-debconf</command>؛
					</para>

				</listitem>
				 <listitem>
					<para>
						دریافت فهرست بروزرسانی‌های موجود برای APT؛
					</para>

				</listitem>
				 <listitem>
					<para>
						نصب بسته‌های فهرست شده در <filename>/fai/package_config/<replaceable>class</replaceable></filename>؛
					</para>

				</listitem>
				 <listitem>
					<para>
						اجرای اسکریپت‌های پس از پیکربندی، <filename>/fai/scripts/<replaceable>class</replaceable>/[0-9][0-9]*</filename>؛
					</para>

				</listitem>
				 <listitem>
					<para>
						ثبت گزارش‌های نصب، قطع اتصال پارتیشن‌ها و راه‌اندازی مجدد.
					</para>

				</listitem>

			</itemizedlist>

		</section>
		 <section id="sect.d-i-preseeding">
			<title>گردآوری debian-installer</title>
			 <indexterm>
				<primary>preseed</primary>
			</indexterm>
			 <indexterm>
				<primary>preconfiguration</primary>
			</indexterm>
			 <para>
				در انتها، بهترین ابزار برای نصب سیستم‌های دبیان به طور منطقی باید نصب‌کننده رسمی دبیان باشد. به همین دلیل است، که از ابتدای آن، debian-installer برای کاربرد خودکاری سازی طراحی شده است که از زیرساخت فراهم شده توسط <emphasis role="pkg">debconf</emphasis> استفاده می‌کند. گزینه دوم، از یک طرف امکان کاهش تعداد پرسش‌های مطرح شده را فراهم می‌کند (پرسش‌های پنهان با پاسخ‌های پیشفرض جواب داده می‌شوند)، از طرف دیگر پاسخ‌های پیشفرض به صورت جداگانه ارائه می‌شوند، به این منظور که فرآیند نصب به صورت غیر-تعاملی انجام شود. این ویژگی آخر به نام <emphasis>preseeding</emphasis> شناخته می‌شود.
			</para>
			 <sidebar> <title><emphasis>مطالعه بیشتر</emphasis> Debconf همراه با یک پایگاه‌داده مرکزی</title>
			 <indexterm>
				<primary><command>debconf</command></primary>
			</indexterm>
			 <para>
				Preseeding امکان فراهم کردن مجموعه‌ای از پاسخ‌ها به پرسش‌های Debconf در زمان نصب را می‌دهد، اما این پاسخ‌ها ایستا بوده و در گذر زمان تغییر نمی‌کنند. از آنجا که ماشین‌های نصب-شده نیازمند بروزرسانی هستند و ممکن است به پاسخ‌های جدید نیاز باشد، فایل پیکربندی <filename>/etc/debconf.conf</filename> می‌تواند به منظور استفاده Debconf از منابع خارجی داده (مانند یک دایرکتوری سرور LDAP یا فایلی که از طریق NFS یا Samba مورد نیاز باشد)، پیکربندی شود. منابع داده خارجی متفاوتی می‌توانند در یک زمان تعریف شوند تا یکدیگر را کامل کنند. پایگاه‌داده محلی به منظور دسترسی خواندنی-نوشتنی و پایگاه‌داده‌های راه دور به منظور دسترسی فقط-خواندنی محدود می‌شوند. صفحه راهنمای <citerefentry><refentrytitle>debconf.conf</refentrytitle>
				 <manvolnum>5</manvolnum></citerefentry> به تشریح تمام احتمالات موجود در این زمینه می‌پردازد (به بسته <emphasis role="pkg">debconf-doc</emphasis> نیاز دارید).
			</para>
			 </sidebar> <section>
				<title>استفاده از یک فایل Preseed</title>
				 <para>
					مکان‌های مختلفی وجود دارد که یک نصب‌کننده می‌تواند فایل preseed را دریافت کند:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							در initrd که برای راه‌اندازی ماشین استفاده شده است؛ در این مورد، عملیات preseed در ابتدای فرآیند نصب صورت می‌گیرد و تمام پرسش‌ها می‌توانند نادیده گرفته شوند. این فایل باید بنام <filename>preseed.cfg</filename> در اولین سطح دایرکتوری initrd قرار گیرد.
						</para>

					</listitem>
					 <listitem>
						<para>
							در رسانه راه‌اندازی (مانند CD یا USB)؛ عملیات preseed به محض اتصال رسانه صورت می‌گیرد، یعنی درست پس از پرسش‌های مربوط به زبان و ساختار صفحه کلید. پارامتر راه‌اندازی <literal>preseed/file</literal> می‌تواند برای شناسایی مکان فایل preseed (برای نمونه، <filename>/cdrom/preseed.cfg</filename> در هنگام نصب از CD-ROM یا <filename>/hd-media/preseed.cfg</filename> در هنگام نصب از USB) مورد استفاده قرار گیرد.
						</para>

					</listitem>
					 <listitem>
						<para>
							از طریق شبکه؛ عملیات preseed پس از پیکربندی (خودکار) شبکه صورت می‌گیرد؛ پارامتر راه‌اندازی مرتبط با آن عبارت است از <literal>preseed/url=http://<replaceable>server</replaceable>/preseed.cfg</literal>.
						</para>

					</listitem>

				</itemizedlist>
				 <para>
					در یک نگاه، قرار دادن فایل preseed درون initrd ممکن است جالب‌ترین گزینه به نظر آید؛ اگرچه، کمتر از این حالت استفاده می‌شود چرا که ایجاد یک initrd قابل نصب بسیار دشوار است. دو راهکار جایگزین دیگر، بیشتر متداول هستند، به خصوص که پارامترهای راه‌اندازی روشی دیگر برای آماده‌سازی پاسخ‌ها به اولین پرسش‌های فرآیند نصب را فراهم می‌کنند. روش مرسوم برای ذخیره‌سازی این پارامترهای راه‌اندازی و جلوگیری از نوشتن هر کدام در زمان نصب، قرار دادن آن‌ها در پیکربندی مرتبط با <command>isolinux</command> (در مورد CD-ROM) یا <command>syslinux</command> (در مورد USB) است.
				</para>

			</section>
			 <section>
				<title>ایجاد یک فایل Preseed</title>
				 <para>
					یک فایل preseedاز نوع متنی است که در هر خط آن پاسخ به یک پرسش از Debconf قرار دارد. هر خط به چهار فیلد که با فاصله (space یا tab) از یکدیگر جدا می‌شوند، تقسیم شده است. برای نمونه، در مورد <literal>d-i mirror/suite string stable</literal>:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							فیلد اول “مالک” پرسش به حساب می‌آید؛ “d-i” برای پرسش‌هایی استفاده می‌شود که مرتبط با فرآیند نصب هستند، اما می‌تواند در مورد نام بسته‌های موجود دبیان نیز بکار رود؛
						</para>

					</listitem>
					 <listitem>
						<para>
							فیلد دوم یک شناسه برای پرسش به حساب می‌آید؛
						</para>

					</listitem>
					 <listitem>
						<para>
							فیلد سوم نوع پرسش را مشخص می‌کند؛
						</para>

					</listitem>
					 <listitem>
						<para>
							فیلد چهارم و آخرین فیلد نیز پاسخ به پرسش را شامل می‌شود. به یاد داشته باشید که از فیلد سوم توسط یک space باید جدا شود؛ اگر بیش از یک فاصله بکار رود، به عنوان بخشی از پاسخ در نظر گرفته می‌شود.
						</para>

					</listitem>

				</itemizedlist>
				 <para>
					ساده‌ترین روش برای ایجاد یک فایل preseed نصب یک سیستم به صورت دستی است. سپس <command>debconf-get-selections --installer</command> پاسخ‌های مرتبط با آن را فراهم می‌کند. پاسخ‌های مرتبط با سایر بسته‌ها نیز توسط <command>debconf-get-selections</command> گردآوری می‌شوند. اگرچه، راهکار بهتر در این مورد نوشتن فایل preseed به صورت دستی است، که از یک فایل نمونه و مستندات مرجع می‌توان استفاده کرد: با چنین رویکردی، تنها پرسش‌هایی که پاسخ‌های پیشفرض داشته باشند می‌توانند آماده‌سازی شوند؛ استفاده از پارامتر راه‌اندازی <literal>priority=critical</literal> به Debconf دستور می‌دهد که تنها به پرسش‌های حیاتی پاسخ دهد و از پاسخ‌های پیشفرض برای سایر پرسش‌ها استفاده کند.
				</para>
				 <sidebar> <title><emphasis>مستندات</emphasis> ضمیمه راهنمای نصب</title>
				 <para>
					راهنمای نصب، که به صورت آنلاین موجود است، شامل مستندات کامل درباره استفاده از یک فایل preseed در یک ضمیمه جداگانه است. همچنین شامل یک فایل نمونه همراه با توضیحات می‌باشد، که می‌تواند به عنوان پایه‌ای برای سفارشی‌سازی‌های محلی استفاده شود. <ulink type="block" url="https://www.debian.org/releases/jessie/amd64/apb.html" /> <ulink type="block" url="https://www.debian.org/releases/jessie/example-preseed.txt" />
				</para>
				 </sidebar>
			</section>
			 <section>
				<title>ایجاد یک رسانه راه‌اندازی سفارشی‌</title>
				 <para>
					دانستن اینکه یک فایل preseed در کجا ذخیره شود خوب است، اما کافی نیست: مدیر سیستم باید به شیوه‌ای رسانه‌ راه‌اندازی نصب را تغییر دهد که پارامترهای راه‌اندازی تغییر کرده و فایل preseed به آن اضافه شود.
				</para>
				 <section>
					<title>راه‌اندازی از طریق شبکه</title>
					 <para>
						زمانی که یک رایانه از طریق شبکه راه‌اندازی می‌شود، سروری که عناصر راه‌اندازی را ارسال می‌کند همچنین شامل پارامترهای راه‌اندازی نیز می‌باشد. بنابراین، تغییرات مورد نظر باید در پیکربندی PXE سرور راه‌اندازی اعمال شوند؛ به طور خاص، در فایل پیکربندی <filename>/tftpboot/pxelinux.cfg/default</filename>. برپایی راه‌اندازی شبکه برای اینکار یک پیشنیاز به حساب می‌آید: برای جزئیات بیشتر راهنمای نصب را مشاهده کنید. <ulink type="block" url="https://www.debian.org/releases/jessie/amd64/ch04s05.html" />
					</para>

				</section>
				 <section>
					<title>آماده‌سازی یک USB قابل اجرا</title>
					 <para>
						زمانی که یک حافظه قابل راه‌اندازی آماده‌سازی شد (<xref linkend="sect.install-usb" /> را مشاهده کنید)، برخی عملیات اضافی مورد نیاز است. فرض می‌کنیم که محتوای حافظه در <filename>/media/usbdisk/</filename> قرار دارد:
					</para>
					 <itemizedlist>
						<listitem>
							<para>
								رونوشت گرفتن از فایل preseed در <filename>/media/usbdisk/preseed.cfg</filename>
							</para>

						</listitem>
						 <listitem>
							<para>
								ویرایش <filename>/media/usbdisk/syslinux.cfg</filename> و افزودن پارامترهای راه‌اندازی مورد نیاز (مثال زیر را مشاهده کنید).
							</para>

						</listitem>

					</itemizedlist>
					 <example>
						<title>فایل syslinux.cfg و پارامترهای عملیات آماده‌سازی</title>
						 
<programlisting>default vmlinuz
append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --</programlisting>

					</example>

				</section>
				 <section>
					<title>ایجاد یک تصویر CD-ROM</title>
					 <indexterm>
						<primary>debian-cd</primary>
					</indexterm>
					 <para>
						یک حافظه USB از نوع رسانه‌های خواندنی-نوشتنی است، پس تغییر فایل در آن و افزودن پارامترها کار آسانی است. در مورد CD-ROM، این عملیات از پیچیدگی بیشتری برخوردار است چرا که نیازمند تولید مجدد تصویر ISO از آن می‌باشد. این وظیفه توسط <emphasis role="pkg">debian-cd</emphasis> مدیریت می‌شود، اما استفاده از این ابزار برای اینکار مطلوب نیست: نیازمند یک mirror محلی و درک از گزینه‌های موجود در <filename>/usr/share/debian-cd/CONF.sh</filename> است؛ حتی در این صورت، <command>make</command> چندین بار باید فراخوانی شود. بنابراین مطالعه <filename>/usr/share/debian-cd/README</filename> به شدت توصیه می‌گردد.
					</para>
					 <para>
						با این تفاسیر، debian-cd به شیوه مشابهی عمل می‌کند: یک دایرکتوری “image” همراه با محتوای دقیق از CD-ROM تولید، سپس با استفاده از ابزاری مانند <command>genisoimage</command>، <command>mkisofs</command> یا <command>xorriso</command> به فایل ISO تبدیل می‌شود. دایرکتوری image پس از گام <command>make image-trees</command> در debian-cd نهایی می‌گردد. در این نقطه، فایل preseed را درون دایرکتوری متناسب آن قرار می‌دهیم (معمولا <filename>$TDIR/$CODENAME/CD1/</filename>، که پارامترهای $TDIR و $CODENAME توسط فایل پیکربندی <filename>CONF.sh</filename> تعریف می‌شوند). CD-ROM از <command>isolinux</command> به عنوان راه‌انداز خود استفاده می‌کند که فایل پیکربندی آن باید توسط آنچه که debian-cd تولید کرده است سازگار باشد، تا بتوان پارامترهای راه‌اندازی مورد نیاز را وارد کرد (فایل مشخص آن عبارت است از <filename>$TDIR/$CODENAME/boot1/isolinux/isolinux.cfg</filename>). در انتها فرآیند “عادی” می‌تواند ادامه یابد و می‌توان تصویر ISO را با استفاده از <command>make image CD=1</command> (یا <command>make images</command> در صورت نیاز به چندین CD-ROM) ایجاد کنیم.
					</para>

				</section>

			</section>

		</section>
		 <section id="sect.simple-cdd">
			<title>Simple-CDD: یک راهکار جامع</title>
			 <indexterm>
				<primary>simple-cdd</primary>
			</indexterm>
			 <para>
				استفاده از یک فایل preseed به تنهایی تمام پیشنیازهای فرآیندهای بزرگ توسعه را محقق نمی‌کند. با اینکه امکان اجرای چند اسکریپت در انتهای فرآیند نصب وجود دارد، مجموعه بسته‌هایی که باید نصب گردند به سادگی قابل دسترس نمی‌باشند (معمولا، فقط “task” می‌تواند انتخاب شود)؛ مهمتر اینکه، این روش منجر به نصب بسته‌های رسمی از دبیان می‌شود و سایر بسته‌های محلی را نادیده می‌گیرد.
			</para>
			 <para>
				از طرف دیگر، debian-cd می‌تواند بسته‌های خارجی را یکپارچه‌سازی کند و debian-installer می‌تواند با درج گام‌های جدید در فرآیند نصب توسعه یابد. با ترکیب این قابلیت‌ها باید بتوانیم یک نصب‌کننده سفارشی را برای نیازهای خود ایجاد کنیم؛ همچنین باید قادر باشد برخی سرویس‌ها را پس از نصب بسته‌های آن‌ها پیکربندی کند. خوشبختانه، این یک حالت فرضی نیست، زیرا دقیقا کاری است که Simple-CDD (در بسته <emphasis role="pkg">simple-cdd</emphasis>) انجام می‌دهد.
			</para>
			 <para>
				هدف Simple-CDD این است که هر فردی بتواند یک توزیع مشتق شده از دبیان را با انتخاب مجموعه‌ای از بسته‌های موجود، پیکربندی آن‌ها با Debconf، افزودن نرم‌افزار خاص و اجرای اسکریپت‌های سفارشی در انتهای فرآیند نصب، ایجاد کند. این رویکرد با فلسفه “سیستم عامل جهانی” (شعار دبیان) سازگاری دارد، چرا که هر فردی می‌تواند آن را با نیاز خود سازگار سازد.
			</para>
			 <section>
				<title>ایجاد پروفایل‌ها</title>
				 <para>
					Simple-CDD “پروفایل” را تعریف می‌کند که با مفهوم “کلاس” در FAI سازگار هستند و یک ماشین می‌تواند چندین پروفایل داشته باشد (که در زمان نصب مشخص می‌شوند). یک پروفایل توسط مجموعه فایل‌های <filename>profiles/<replaceable>profile</replaceable>.*</filename> تعریف می‌شود:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							فایل <filename>.description</filename> شامل توضیح یک خطی درباره پروفایل است؛
						</para>

					</listitem>
					 <listitem>
						<para>
							فایل <filename>.packages</filename> شامل بسته‌هایی است که در صورت انتخاب شدن پروفایل به شیوه خودکار نصب می‌گردند؛
						</para>

					</listitem>
					 <listitem>
						<para>
							فایل <filename>.downloads</filename> شامل بسته‌هایی است که درون رسانه نصب ذخیره‌سازی می‌شوند، اما الزامی در نصب آن‌ها وجود ندارد؛
						</para>

					</listitem>
					 <listitem>
						<para>
							فایل <filename>.preseed</filename> شامل اطلاعات آماده‌سازی برای پرسش‌های Debconf (برای نصب‌کننده و/یا بسته‌ها) می‌باشد؛
						</para>

					</listitem>
					 <listitem>
						<para>
							فایل <filename>.postinst</filename> شامل اسکریپتی است که در انتهای فرآیند نصب اجرا می‌شود؛
						</para>

					</listitem>
					 <listitem>
						<para>
							در انتها، فایل <filename>.conf</filename> امکان تغییر برخی پارامترهای Simple-CDD را بر اساس پروفایل‌های موجود در آن فراهم می‌کند.
						</para>

					</listitem>

				</itemizedlist>
				 <para>
					پروفایل <literal>default</literal> نقش ویژه‌ای دارد چرا که همیشه انتخاب می‌شود؛ شامل حداقل‌های مورد نیاز توسط Simple-CDD است. تنها موردی که در این پروفایل سفارشی می‌شود پارامتر آماده‌سازی <literal>simple-cdd/profiles</literal> است: امکان رد کردن پرسش، که توسط Simple-CDD، درباره پروفایل قابل نصب را فراهم می‌کند.
				</para>
				 <para>
					به یاد داشته باشید که دستورات باید از دایرکتوی والد <filename>profiles</filename> فراخوانی شوند.
				</para>

			</section>
			 <section>
				<title>پیکربندی و استفاده از <command>build-simple-cdd</command></title>
				 <indexterm>
					<primary><command>build-simple-cdd</command></primary>
				</indexterm>
				 <sidebar> <title><emphasis>نگاه سریع</emphasis> فایل پیکربندی همراه با جزئیات</title>
				 <para>
					یک نمونه از فایل پیکربندی Simple-CDD، همراه با تمام پارامترهای ممکن، درون <filename>/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz</filename> قرار دارد. از این فایل می‌توان به عنوان نقطه آغاز برای ایجاد فایل پیکربندی سفارشی استفاده کرد.
				</para>
				 </sidebar> <para>
					Simple-CDD به پارامترهای بسیاری برای عملکرد جامع نیاز دارد. آن‌ها اغلب درون یک فایل پیکربندی قرار دارند، که <command>build-simple-cdd</command> می‌تواند با گزینه <literal>--conf</literal> به آن اشاره کند، همچنین می‌توانند با استفاده از پارامترهای انحصاری به <command>build-simple-cdd</command> ارجاع شوند. در اینجا به این دستور و پارامترهای مورد نیاز آن نگاهی می‌اندازیم:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							پارامتر <literal>profiles</literal> فهرستی از پروفایل‌های قابل اجرا در CD-ROM ایجاد شده را فهرست می‌کند؛
						</para>

					</listitem>
					 <listitem>
						<para>
							بر اساس فهرست بسته‌های مورد نیاز، Simple-CDD فایل‌های متناسب با آن‌ها را از سرور اشاره شده در <literal>server</literal> دانلود کرده و آن‌ها را درون یک mirror موقت قرار می‌دهد (که در ادامه به debian-cd داده می‌شود)؛
						</para>

					</listitem>
					 <listitem>
						<para>
							بسته‌های سفارشی موجود در <literal>local_packages</literal> نیز درون همین mirror محلی قرار می‌گیرند؛
						</para>

					</listitem>
					 <listitem>
						<para>
							سپس debian-cd (درون یک مکان پیشفرض که می‌تواند با متغیر <literal>debian_cd_dir</literal> پیکربندی شود) همراه با فهرستی از بسته‌ها به منظور یکپارچه‌سازی اجرا می‌شود؛
						</para>

					</listitem>
					 <listitem>
						<para>
							زمانی که debian-cd دایرکتوری خود را آماده کند، Simple-CDD برخی تغییرات را در این دایرکتوری انجام می‌دهد:
						</para>
						 <itemizedlist>
							<listitem>
								<para>
									فایل‌هایی که شامل پروفایل‌ها هستند درون یک دایرکتوری زیر مجموعه <filename>simple-cdd</filename> (واقع در CD-ROM نهایی) قرار می‌گیرند؛
								</para>

							</listitem>
							 <listitem>
								<para>
									سایر فایل‌های فهرست شده در پارامتر <literal>all_extras</literal> نیز افزوده می‌شوند؛
								</para>

							</listitem>
							 <listitem>
								<para>
									پارامترهای راه‌اندازی طوری تنظیم می‌شوند که عملیات preseed فعال گردد. در صورت ذخیره‌سازی اطلاعات لازم در متغیرهای <literal>language</literal> و <literal>country</literal>، پرسش‌های مربوط به زبان و کشور نادیده گرفته می‌شوند.
								</para>

							</listitem>

						</itemizedlist>

					</listitem>
					 <listitem>
						<para>
							سپس debian-cd فایل نهایی ISO را تولید می‌کند.
						</para>

					</listitem>

				</itemizedlist>

			</section>
			 <section>
				<title>تولید یک فایل ISO</title>
				 <para>
					زمانی که یک فایل پیکربندی ایجاد و پروفایل‌های خود را تعریف کردیم، گام باقیمانده فراخوانی <command>build-simple-cdd --conf simple-cdd.conf</command> است. پس از چند دقیقه، فایل نهایی را در <filename>images/debian-8.0-amd64-CD-1.iso</filename> دریافت می‌کنیم.
				</para>

			</section>

		</section>

	</section>
	 <section id="sect.monitoring">
		<title>مانیتورینگ</title>
		 <para>
			مانیتورینگ یک عبارت عمومی است و فعالیت‌های مرتبط با آن اهداف گوناگونی را دنبال می‌کنند: از یک طرف، پیگیری منابع مصرفی فراهم شده توسط ماشین امکان پیشبینی میزان اشباع و بروزرسانی‌های متعاقب با آن را فراهم می‌کند؛ از طرف دیگر، هشدار به مدیر سیستم به محض اینکه یک سرویس از دسترس خارج شود یا به درستی کار نکند به معنی رفع سریع‌تر مشکلات در زمان بروز حادثه است.
		</para>
		 <para>
			<emphasis>Munin</emphasis> با نمایش نمودارهای گرافیکی برای مقادیر مختلف از پارامترهای متعدد (حافظه مصرفی، فضای اشغال شده دیسک، بار پردازنده، ترافیک شبکه، بار وب سرور/پایگاه‌داده و از این قبیل) ناحیه اول را پوشش می‌دهد. <emphasis>Nagios</emphasis> با بررسی مداوم سرویس‌ها و نحوه کارکرد و قابل دسترس بودن آن‌ها، همراه با ارسال پیام به مدیر سیستم با استفاده از کانال‌های مناسب (ایمیل، پیامک و از این قبیل) ناحیه دوم را پوشش می‌دهد. هر دو ابزار ساختاری ماژولار دارند که به توسعه هر یک از آن‌ها و افزودن پارامترها یا سرویس‌های خاص کمک می‌کند.
		</para>
		 <sidebar> <title><emphasis>جایگزین</emphasis> Zabbix، یک ابزار مانیتورینگ یکپارچه</title>
		 <indexterm>
			<primary>Zabbix</primary>
		</indexterm>
		 <para>
			با اینکه استفاده از Munin و Nagios بسیار متداول است، آن‌ها تنها بازیکنان موجود در میدان مانیتورینگ نیستند و هر کدام نیز تنها نصف وظایف (نمودار از یک طرف، هشدار از طرف دیگر) را انجام می‌دهند. Zabbix، از طرف دیگر هر دو بخش مانیتورینگ را یکپارچه می‌کند؛ همچنین شامل یک رابط وب به منظور پیکربندی جنبه‌های مختلف مانیتورینگ می‌باشد. طی چند سال گذشته فراز و نشیب‌های بسیاری را پشت سر گذاشته و هم اکنون به عنوان یک رقیب جدی به حساب می‌آید. در سرور مانیتورینگ، می‌توانید <emphasis role="pkg">zabbix-server-pgsql</emphasis> یا <emphasis role="pkg">zabbix-server-mysql</emphasis> را همراه با <emphasis role="pkg">zabbix-frontend-php</emphasis> به منظور دسترسی به رابط وب نصب کنید. در سیستم‌های میزبان که قصد مانیتور کردن آن‌ها را دارید می‌توانید <emphasis role="pkg">zabbix-agent</emphasis> را نصب کنید که داده‌ها را به سمت سرور می‌فرستد. <ulink type="block" url="http://www.zabbix.com/" />
		</para>
		 </sidebar> <sidebar> <title><emphasis>جایگزین</emphasis> Icinga، یک fork از Nagios</title>
		 <indexterm>
			<primary>Icinga</primary>
		</indexterm>
		 <para>
			به موجب اختلاف نظر در مدل توسعه انتخابی برای Nagios (که توسط یک شرکت کنترل می‌شود)، تعدادی از توسعه‌دهندگان آن را fork و از نام جدید Icinga استفاده کردند. Icinga کماکان ـ تا جای ممکن - با پیکربندی‌ها و پلاگین‌های Nagios سازگار، اما ویژگی‌های اضافی را به آن افزوده است. <ulink type="block" url="http://www.icinga.org/" />
		</para>
		 </sidebar> <section id="sect.munin">
			<title>راه‌اندازی Munin</title>
			 <indexterm>
				<primary>Munin</primary>
			</indexterm>
			 <para>
				هدف Munin مانیتور کردن ماشین‌های متعدد است؛ بنابراین، طبیعی است که از معماری کلاینت/سرور استفاده کند. میزبان مرکزی - یا grapher - داده را از تمام میزبان‌های قابل مانیتور کردن دریافت کرده و نمودارهای گرافیکی تولید می‌کند.
			</para>
			 <section>
				<title>پیکربندی میزبان‌ها برای مانیتور شدن</title>
				 <para>
					اولین گام نصب بسته <emphasis role="pkg">munin-node</emphasis> است. فرآیند پس‌زمینه‌ای که توسط این بسته نصب می‌شود به درگاه ۴۹۴۹ گوش کرده و داده‌های دریافتی از پلاگین‌های فعال را ارسال می‌کند. هر پلاگین یک برنامه ساده است که توضیح مرتبط با داده دریافتی همراه با آخرین مقدار بدست آمده را باز می‌گرداند. پلاگین‌ها در مسیر <filename>/usr/share/munin/plugins/</filename> ذخیره شده‌اند اما تنها آن‌هایی که به صورت پیوند نمادین در <filename>/etc/munin/plugins/</filename> قرار داشته باشند، استفاده می‌گردند.
				</para>
				 <para>
					زمانی که بسته نصب شود، مجموعه‌ای از پلاگین‌های فعال بر اساس نرم‌افزار موجود و پیکربندی فعلی میزبان تشخیص داده می‌شوند. اگرچه، این پیکربندی خودکار وابسته به قابلیتی است که هر پلاگین باید فراهم کرده باشد و بهتر است که نتایج را به صورت دستی مرور و ویرایش کنیم. مرور <ulink url="http://gallery.munin-monitoring.org">گالری پلاگین</ulink> می‌تواند جالب باشد با این وجود که همه پلاگین‌ها ممکن است شامل مستندات جامع نباشند. با این حال، تمام پلاگین‌ها اسکریپت هستند که اکثر آن‌ها ساده بوده و دارای توضیحات خوبی می‌باشند. مرور <filename>/etc/munin/plugins/</filename> شیوه خوبی برای اطلاع از کارکرد هر پلاگین و تشخیص اینکه کدام یک باید حذف شود می‌باشد. به طور مشابه، فعال‌سازی یک پلاگین جالب در <filename>/usr/share/munin/plugins/</filename> به سادگی ایجاد پیوند نمادین با استفاده از <command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command> می‌باشد. به یاد داشته باشید اگر نام پلاگین به زیرخط یا “_” تمام شود، پلاگین نیازمند یک پارامتر است. این پارامتر باید در نام مرتبط با پیوند نمادین ذخیره‌سازی شود؛ برای نمونه، پلاگین “if_” باید همراه با پیوند نمادین <filename>if_eth0</filename> فعال‌سازی شود، تا بتواند ترافیک شبکه رابط eth0 را مانیتور کند.
				</para>
				 <para>
					زمانی که تمام پلاگین‌ها راه‌اندازی شوند، پیکربندی فرآیند پس‌زمینه به منظور تعریف کنترل دسترسی به داده‌های گردآوری شده باید بروزرسانی گردد. اینکار شامل عبارت‌های <literal>allow</literal> در فایل <filename>/etc/munin/munin-node.conf</filename> می‌شود. پیکربندی پیشفرض به صورت <literal>allow ^127\.0\.0\.1$</literal> است که تنها اجازه دسترسی به میزبان محلی را می‌دهد. یک مدیرسیستم معمولا خطی مشابه را همراه با نشانی IP میزبان grapher می‌افزاید، سپس اقدام به راه‌اندازی مجدد فرآیند پس‌زمینه با استفاده از <command>service munin-node restart</command> می‌کند.
				</para>
				 <sidebar> <title><emphasis>مطالعه بیشتر</emphasis> ایجاد پلاگین‌های محلی</title>
				 <para>
					Munin شامل مستندات کاملی درباره چگونگی عملکرد و توسعه پلاگین‌ها است. <ulink type="block" url="http://munin-monitoring.org/wiki/plugins" />
				</para>
				 <para>
					یک پلاگین بهتر است در شرایط مشابه با فراخوانی توسط munin-node مورد آزمون قرار گیرد؛ این عمل با اجرای <command>munin-run <replaceable>plugin</replaceable></command> به عنوان root شبیه‌سازی می‌شود. یک پارامتر دوم احتمالی که به این دستور داده می‌شود (از جمله <literal>config</literal>) به عنوان یک پارامتر به پلاگین فرستاده می‌شود.
				</para>
				 <para>
					زمانی که یک پلاگین توسط پارامتر <literal>config</literal> فراخوانی می‌شود، باید خود را با بازگرداندن مجموعه‌ای از فیلدها تعریف کند:
				</para>
				 
<screen><computeroutput>$ </computeroutput><userinput>sudo munin-run load config
</userinput><computeroutput>graph_title Load average
graph_args --base 1000 -l 0
graph_vlabel load
graph_scale no
graph_category system
load.label load
graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run "immediately").
load.info 5 minute load average
</computeroutput></screen>
				 <para>
					فیلدهای موجود مختلف توسط “مرجع پلاگین” موجود در قسمت “راهنمای Munin” توضیح داده شده‌اند. <ulink type="block" url="http://munin.readthedocs.org/en/latest/reference/plugin.html" />
				</para>
				 <para>
					زمانی که بدون پارامتر فراخوانی می‌شود، پلاگین به سادگی آخرین مقدار محاسبه شده را باز می‌گرداند؛ برای نمونه، اجرای <command>sudo munin-run load</command> می‌تواند مقدار <literal>load.value 0.12</literal> را باز گرداند.
				</para>
				 <para>
					در نهایت، زمانی که یک پلاگین توسط پارامتر <literal>autoconf</literal> فراخوانی می‌شود، باید مقدار “yes” (گزارش خروج ۰) یا “no” (گزارش خروج ۱) با توجه به اینکه آیا پلاگین باید در این میزبان فعال شود یا خیر را باز گرداند.
				</para>
				 </sidebar>
			</section>
			 <section>
				<title>پیکربندی Grapher</title>
				 <para>
					“grapher” در واقع رایانه‌ای است که داده‌ها را گردآوری کرده و نمودارهای مرتبط با آن را رسم می‌کند. نرم‌افزار مورد نیاز در بسته <emphasis role="pkg">munin</emphasis> قرار دارد. پیکربندی استاندارد <command>munin-cron</command> را هر ۵ دقیقه یکبار اجرا کرده، تا اطلاعات از تمام میزبان‌های موجود در <filename>/etc/munin/munin.conf</filename> گردآوری شوند (فقط میزبان محلی به صورت پیشفرض قرار دارد)، داده‌های بدست آمده را در فایل‌های RRD، که مخفف <emphasis>Round Robin Database</emphasis> و مناسب ذخیره‌سازی داده‌های متغیر در طول زمان است، ذخیره‌سازی می‌کند که این فایل‌ها در مسیر <filename>/var/lib/munin/</filename> قرار دارند و در نهایت یک صفحه HTML همراه با نمودارها در <filename>/var/cache/munin/www/</filename> ایجاد می‌کند.
				</para>
				 <para>
					بنابراین تمام ماشین‌های مانیتور شده باید در فایل پیکربندی <filename>/etc/munin/munin.conf</filename> قرار داشته باشند. هر ماشین به عنوان یک قسمت کامل همراه با نام آن و حداقل یک مدخل <literal>address</literal> که شامل نشانی IP ماشین است، قرار می‌گیرد.
				</para>
				 
<programlisting>[ftp.falcot.com]
    address 192.168.0.12
    use_node_name yes</programlisting>
				 <para>
					قسمت‌ها می‌توانند پیچیده‌تر باشند، تا با ترکیب داده‌های بدست آمده از چند ماشین نمودارهای اضافی رسم گردد. مثال‌های موجود در فایل پیکربندی نقطه آغاز مناسبی برای سفارشی‌کردن این فرآیند هستند.
				</para>
				 <para>
					آخرین گام انتشار صفحات تولید شده است؛ اینکار نیازمند پیکربندی سرور وب به گونه‌ای است که محتوای <filename>/var/cache/munin/www/</filename> از طریق یک وبسایت قابل دسترس باشد. دسترسی به این وبسایت می‌تواند با استفاده از مکانیزم احرازهویت یا کنترل دسترسی مبتنی بر IP مدیریت شود. برای جزئیات مرتبط <xref linkend="sect.http-web-server" /> را مشاهده کنید.
				</para>

			</section>

		</section>
		 <section id="sect.nagios">
			<title>راه‌اندازی Nagios</title>
			 <indexterm>
				<primary>Nagios</primary>
			</indexterm>
			 <para>
				برخلاف Munin، الزامی بر نصب Nagios روی میزبان‌های مانیتور شده نیست؛ از Nagios بیشتر به منظور بررسی موجود بودن سرویس‌های شبکه استفاده می‌شود. برای نمونه، Nagios می‌تواند به یک سرور وب متصل شده و بررسی کند یک صفحه مشخص در زمان مشخص قابل دسترس است یا خیر.
			</para>
			 <section>
				<title>نصب</title>
				 <para>
					اولین گام در راه‌اندازی Nagios نصب بسته‌های <emphasis role="pkg">nagios3</emphasis>، <emphasis role="pkg">nagios-plugins</emphasis> و <emphasis role="pkg">nagios3-doc</emphasis> است. نصب بسته‌ها منجر به پیکربندی یک رابط وب و ایجاد کاربر <literal>nagiosadmin</literal> می‌شود (که برای آن گذرواژه درخواست خواهد شد). افزودن سایر کاربران به سادگی درج آن‌ها در فایل <filename>/etc/nagios3/htpasswd.users</filename> با استفاده از دستور <command>htpasswd</command> در آپاچی است. اگر در زمان نصب پرسشی از طرف Debconf مطرح نشد، می‌توان از <command>dpkg-reconfigure nagios3-cgi</command> برای تعریف گذرواژه <literal>nagiosadmin</literal> استفاده کرد.
				</para>
				 <para>
					نشانی <literal>http://<replaceable>server</replaceable>/nagios3/</literal> در مرورگر، رابط وب مربوط به آن را نمایش می‌دهد؛ به طور مشخص، به یاد داشته باشید که Nagios برخی پارامترهای ماشینی که در آن اجرا می‌شود را مانیتور می‌کند. با این حال، برخی ویژگی‌های تعاملی از جمله افزودن دیدگاه به یک میزبان ممکن است کار نکند. این ویژگی‌های در پیکربندی پیشفرض برای Nagios غیرفعال هستند، که بنا بر دلایل امنیتی بسیار محدود کننده است.
				</para>
				 <para>
					همانطور که در <filename>/usr/share/doc/nagios3/README.Debian</filename> توضیح داده شده است، فعال‌سازی برخی ویژگی‌ها نیازمند ویرایش <filename>/etc/nagios3/nagios.cfg</filename> و تنظیم پارامتر <literal>check_external_commands</literal> آن به “1” است. همچنین نیاز داریم تا مجوزهای نوشتن روی دایرکتوری که توسط Nagios استفاده می‌شود را با استفاده از دستورات مشابه زیر تنظیم کنیم:
				</para>
				 
<screen><computeroutput># </computeroutput><userinput>service nagios3 stop</userinput>
<computeroutput>[...]
# </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios3/rw
</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios3
</userinput><computeroutput># </computeroutput><userinput>service nagios3 start</userinput>
<computeroutput>[...]</computeroutput></screen>

			</section>
			 <section>
				<title>پیکربندی</title>
				 <para>
					رابط وب Nagios بسیار کاربردی است، اما اجازه تغییر پیکربندی یا افزودن میزبان‌ها و سرویس‌های قابل مانیتور را نمی‌دهد. پیکربندی کلی از طریق فایل‌های اشاره شده در فایل پیکربندی مرکزی <filename>/etc/nagios3/nagios.cfg</filename> مدیریت می‌شود.
				</para>
				 <para>
					بدون درک از مفاهیم Nagios نباید مستقیم سراغ این فایل‌ها رفت. پیکربندی شامل اشیایی از نوع زیر می‌باشد:
				</para>
				 <itemizedlist>
					<listitem>
						<para>
							یک <emphasis>host</emphasis> ماشینی است که باید مانیتور شود؛
						</para>

					</listitem>
					 <listitem>
						<para>
							یک <emphasis>hostgroup</emphasis> مجموعه‌ای از میزبان‌ها است که برای نمایش یا در نظر گرفتن برخی عناصر پیکربندی، باید گروه‌بندی شوند؛
						</para>

					</listitem>
					 <listitem>
						<para>
							یک <emphasis>service</emphasis> شامل عنصری قابل آزمون مرتبط با یک میزبان یا گروه میزبانی است. در اکثر موارد برای بررسی یک سرویس شبکه بکار می‌رود، اما می‌تواند برای بررسی محدوده مجاز برخی پارامترها نیز استفاده شود (برای نمونه، فضای آزاد دیسک یا بار پردازنده)؛
						</para>

					</listitem>
					 <listitem>
						<para>
							یک <emphasis>servicegroup</emphasis> مجموعه‌ای از سرویس‌ها است که برای نمایش باید گروه‌بندی شوند؛
						</para>

					</listitem>
					 <listitem>
						<para>
							یک <emphasis>contact</emphasis> شخصی است که می‌تواند هشدارها را دریافت کند؛
						</para>

					</listitem>
					 <listitem>
						<para>
							یک <emphasis>contactgroup</emphasis> مجموعه‌ای از چنین افرادی است؛
						</para>

					</listitem>
					 <listitem>
						<para>
							یک <emphasis>timeperiod</emphasis> بازه‌ای از زمان است که طی آن برخی سرویس‌ها باید بررسی شوند؛
						</para>

					</listitem>
					 <listitem>
						<para>
							یک <emphasis>command</emphasis> دستوری است که به منظور بررسی یک سرویس مشخص فراخوانی می‌شود.
						</para>

					</listitem>

				</itemizedlist>
				 <para>
					با توجه به نوع، هر شی شامل قابلیت‌هایی است که می‌تواند سفارشی شود. فهرست کامل آن بسیار طولانی است، اما مهم‌ترین قابلیت‌ها همان روابط بین اشیا است.
				</para>
				 <para>
					یک <emphasis>service</emphasis> از <emphasis>command</emphasis> استفاده می‌کند تا وضعیت یک قابلیت موجود در<emphasis>host</emphasis> (یا <emphasis>hostgroup</emphasis>) را طی بازه <emphasis>timeperiod</emphasis> بررسی کند. در صورت بروز مشکل، Nagios یک هشدار به تمام اعضای موجود در <emphasis>contactgroup</emphasis> مرتبط با سرویس ارسال می‌کند. با توجه به کانال تعریف شده برای هر فرد در شی <emphasis>contact</emphasis>، پیام برای وی فرستاده می‌شود.
				</para>
				 <para>
					یک سیستم ارث‌گرایی امکان اشتراک‌گذاری مجموعه‌ای از قابلیت‌ها را بین چندین شی مختلف بدون تکرار اطلاعات فراهم می‌کند. علاوه بر این، پیکربندی پیشفرض شامل برخی از اشیای استاندارد می‌باشد؛ در اکثر موارد، تعریف میزبان‌ها، سرویس‌ها و افراد جدید به سادگی مشتق شدن از این اشیای عمومی است. فایل‌های موجود در <filename>/etc/nagios3/conf.d/</filename> منبع خوبی از اطلاعات درباره چگونگی کارکرد این اشیا است.
				</para>
				 <para>
					مدیرسیستم‌های شرکت فالکوت از پیکربندی زیر استفاده می‌کنند:
				</para>
				 <example>
					<title>فایل <filename>/etc/nagios3/conf.d/falcot.cfg</filename></title>
					 
<programlisting>define contact{
    name                            generic-contact
    service_notification_period     24x7
    host_notification_period        24x7
    service_notification_options    w,u,c,r
    host_notification_options       d,u,r
    service_notification_commands   notify-service-by-email
    host_notification_commands      notify-host-by-email
    register                        0 ; Template only
}
define contact{
    use             generic-contact
    contact_name    rhertzog
    alias           Raphael Hertzog
    email           hertzog@debian.org
}
define contact{
    use             generic-contact
    contact_name    rmas
    alias           Roland Mas
    email           lolando@debian.org
}

define contactgroup{
    contactgroup_name     falcot-admins
    alias                 Falcot Administrators
    members               rhertzog,rmas
}

define host{
    use                   generic-host ; Name of host template to use
    host_name             www-host
    alias                 www.falcot.com
    address               192.168.0.5
    contact_groups        falcot-admins
    hostgroups            debian-servers,ssh-servers
}
define host{
    use                   generic-host ; Name of host template to use
    host_name             ftp-host
    alias                 ftp.falcot.com
    address               192.168.0.6
    contact_groups        falcot-admins
    hostgroups            debian-servers,ssh-servers
}

# 'check_ftp' command with custom parameters
define command{
    command_name          check_ftp2
    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35
}

# Generic Falcot service
define service{
    name                  falcot-service
    use                   generic-service
    contact_groups        falcot-admins
    register              0
}

# Services to check on www-host
define service{
    use                   falcot-service
    host_name             www-host
    service_description   HTTP
    check_command         check_http
}
define service{
    use                   falcot-service
    host_name             www-host
    service_description   HTTPS
    check_command         check_https
}
define service{
    use                   falcot-service
    host_name             www-host
    service_description   SMTP
    check_command         check_smtp
}

# Services to check on ftp-host
define service{
    use                   falcot-service
    host_name             ftp-host
    service_description   FTP
    check_command         check_ftp2
}</programlisting>

				</example>
				 <para>
					این فایل پیکربندی دو میزبان مانیتور شده را تعریف می‌کند. اولی سرور وب است که بررسی‌های موجود برای درگاه‌های ۸۰ و ۴۴۳ انجام می‌شود. Nagios همچنین بررسی می‌کند که سرور SMTP روی درگاه ۲۵ اجرا شود. دومی سرور FTP است که بررسی به منظور دریافت پاسخ ظرف مدت ۲۰ ثانیه در آن صورت می‌گیرد. فراتر از این تاخیر، یک <emphasis>warning</emphasis> صادر می‌شود؛ فراتر از ۳۰ ثانیه، هشدار به منزله بحرانی در نظر گرفته می‌شود. رابط وب Nagios همچنین نشان می‌دهد که سرویس SSH در حال مانیتور شدن است: این اطلاعات از میزبان‌هایی می‌آید که متعلق به گروه <literal>ssh-servers</literal> هستند. سرویس استاندارد منطبق با آن در فایل <filename>/etc/nagios3/conf.d/services_nagios2.cfg</filename> تعریف شده است.
				</para>
				 <para>
					کاربرد ارث‌گرایی را به یاد داشته باشید: یک شی می‌تواند از شی دیگری با استفاده از “use <replaceable>parent-name</replaceable>” ارث‌بری کند. شی والد باید قابل شناسایی باشد، که نیازمند اختصاص قابلیت “name <replaceable>identifier</replaceable>” به آن می‌باشد. اگر قرار بر واقعی نبودن شی والد باشد، اما تنها به صورت والد عمل کند، اختصاص قابلیت “register 0” به آن به Nagios می‌گوید که از آن شی صرف نظر کند، که در این صورت نبود برخی پارامترهای مورد نیاز آن مشکلی را بوجود نمی‌آورد.
				</para>
				 <sidebar> <title><emphasis>مستندات</emphasis> فهرستی از قابلیت‌های شی</title>
				 <para>
					درک عمیق‌تر از روش‌های دیگری برای پیکربندی Nagios می‌تواند با استفاده از مستندات موجود در بسته <emphasis role="pkg">nagios3-doc</emphasis> حاصل شود. این مستندات به صورت مستقیم از رابط وب، در پیوند “Documentation” موجود در گوشه بالا سمت چپ قابل دسترسی هستند. شامل فهرستی از تمام انواع اشیا همراه با تمام قابلیت‌های موجود در آن‌ها می‌شود. همچنین توضیح می‌دهد چطور پلاگین‌های جدید را ایجاد کنیم.
				</para>
				 </sidebar> <sidebar> <title><emphasis>مطالعه بیشتر</emphasis> آزمون‌های راه‌دور با استفاده از NRPE</title>
				 <para>
					بسیاری پلاگین‌های Nagios امکان بررسی برخی پارامترهای محلی برای یک میزبان را فراهم می‌کنند؛ اگر ماشین‌های زیادی به این بررسی‌ها نیاز داشته باشند در صورتی که یک نصب مرکزی تمام اطلاعات را گردآوری کند، پلاگین NRPE یا <emphasis>Nagios Remote Plugin Executor</emphasis> باید راه‌اندازی گردد. بسته <emphasis role="pkg">nagios-nrpe-plugin</emphasis> باید در سرور Nagios و بسته <emphasis role="pkg">nagios-nrpe-server</emphasis> باید در میزبان‌هایی که نیازمند برخی آزمون‌ها هستند، نصب شود. گزینه دوم پیکربندی خود را از <filename>/etc/nagios/nrpe.cfg</filename> دریافت می‌کند. این فایل همراه با نشانی‌های IP ماشین‌هایی که قصد اجرای این آزمون‌ها را دارند، باید شامل آزمون‌هایی باشد که از راه‌دور آغاز می‌شوند. در طرف Nagios، فعال‌سازی این آزمون‌های جدید به سادگی افزودن‌ سرویس‌های متناظر با آن‌ها با استفاده از دستور <emphasis>check_nrpe</emphasis> است.
				</para>
				 </sidebar>
			</section>

		</section>

	</section>
</chapter>

