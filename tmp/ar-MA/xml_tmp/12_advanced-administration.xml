<?xml version='1.0' encoding='utf-8' ?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [
]>
<chapter id="advanced-administration">
  <chapterinfo>
    <mediaobject condition="pdf">
      <imageobject>
        <imagedata fileref="images/chap-advanced-administration.png" scalefit="1" />
      </imageobject>
    </mediaobject>
    <keywordset>
      <keyword>RAID</keyword>
      <keyword>LVM</keyword>
      <keyword>FAI</keyword>
      <keyword>تغذية</keyword>
      <keyword>المراقبة</keyword>
      <keyword>الحوسبة الظاهرية</keyword>
      <keyword>‏Xen</keyword>
      <keyword>‏LXC</keyword>
    </keywordset>
  </chapterinfo>
  <title>الإدارة المتقدمة</title>
  <highlights>
    <para>يعيد هذا الفصل النظر في بعض القضايا التي ناقشناها سابقاً، لكن من وجهة نظر مختلفة: سوف ندرس تجهيز الأنظمة الكبيرة بدلاً من تجهيز حاسوب مفرد؛ وسوف نتعلم ضبط LVM و RAID يدوياً بدل الضبط الآلي عند التثبيت، حتى نتمكن من تعديل الخيارات التي حددناها سابقاً. أخيراً، سوف نتحدث عن أدوات المراقبة وتقنيات المحاكاة. أي أن هذا الفصل موجَّه لمديري النظم المحترفين أكثر مما يركز على ما يهم الأفراد الذين يديرون شبكة منزلية.</para>
  </highlights>
  <section id="sect.raid-and-lvm">
    <title>‏RAID وLVM</title>

    <para>استعرض <xref linkend="installation" /> هذه التقنيات من وجهة نظر برنامج التثبيت، والطريقة التي دمجت فيها هذه التقنيات حتى يكون إعدادها سهلاً منذ البداية. يجب على مدير النظام أن يستطيع معالجة الحاجات المتزايدة للمساحة التخزينية بعد التثبيت الأولي للنظام، دون اللجوء إلى عملية إعادة التثبيت المكلفة (من ناحية الوقت والجهد). أي أن مدير النظام يجب أن يستخدم الأدوات المطلوبة لتعديل نظامي LVM و RAID بمهارة.</para>

    <para>تستخدم تقنيتا LVM و RAID لعزل الحيز التخزيني المتاح لنظام الملفات عن الحيز التخزيني الفيزيائي (الأقراص الصلبة الفعلية أو الأقسام partitions)؛ تحمي تقنية RAID البيانات من خلال التخزين الفائض، بينما تجعل تقنية LVM إدارة البيانات أكثر مرونة واستقلالاً عن السعة الحقيقية للأقراص التي تحميل تلك البيانات. في الحالتين، يعتمد النظام على أجهزة تخزينية جديدة، يمكن استخدامها لإنشاء نظم ملفات أو مساحات swap، دون أن ترتبط بقرص فيزيائي واحد. إن جذور التقنيتين مختلفة كثيرًا، لكن وظائفهما متشابهة نوعًا ما، ولهذا غالبًا ما تذكران معًا.</para>

    <sidebar>
      <title><emphasis>منظور</emphasis> Btrfs يجمع بين LVM وRAID </title>

      <para>في حين أن RAID و LVM هما نظامان فرعيان للنواة يعملان بين أجهزة التخزين وبين نظام الملفات، <emphasis>btrfs</emphasis> هو نظام ملفات جديد، طورته أوراكل في البداية، يهدف للجمع بين مزايا RAID و LVM وأكثر من ذلك. إن معظم أجزاء النظام جاهزة، بالرغم من أنها لا تزال تعتبر ”تجريبية“ لأن تطويرها غير مكتمل (بعض المزايا لم تصل مرحلة التطبيق بعد)، وقد شهد بعض الاستخدامات في البيئات الإنتاجية. <ulink type="block" url="http://btrfs.wiki.kernel.org/" /></para>

      <para>من المزايا التي تستحق الذكر هي إمكانية أخذ لقطة snapshot لشجرة نظام الملفات عند أي لحظة زمنية. هذه اللقطة لا تحجز أي مساحة على القرص، إذا أن البيانات لا تنسخ قبل أن تجرى بعض التغييرات عليها. كما أن نظام الملفات يعالج أيضًا الضغط الشفاف للملفات، وهناك checksums تضمن سلامة كافة البيانات المخزنة.</para>
    </sidebar>

    <para>في حال استخدام RAID أو LVM، توفر النواة ملف جهاز تخزيني (كتلي) block device file، يشبه الملفات التي تمثل الأقراص الصلبة أو أقسام الأقراص. عندما يحتاج أحد التطبيقات، أو أحد أجزاء النواة، للوصول إلى كتلة block من جهاز تخزيني من هذا النوع، يعمل النظام الفرعي المناسب (نظام LVM أو RAID) على توجيه هذه الكتلة إلى الطبقة الفيزيائية الموافقة. وحسب إعداد النظام، يمكن أن تُخزَّن هذه الكتلة على قرص فيزيائي واحد أو أكثر، كما أن موقعها الفيزيائي قد لا يرتبط بموقعها ضمن الجهاز المنطقي.</para>
    <section id="sect.raid-soft">
      <title>‏Software RAID</title>
      <indexterm><primary>RAID</primary></indexterm>

      <para>كلمة RAID تعني <emphasis>Redundant Array of Independent Disks</emphasis>. يهدف هذا النظام إلى حماية البيانات من الضياع في حال عطب القرص الصلب. المبدأ العام بسيط جدًا: تخزن البيانات على عدة أقراص فيزيائية بدلًا من تخزينها على قرص واحد، ويكون مستوى التخزين الفائض قابلاً للضبط. بالاعتماد على هذا التخزين الفائض، يمكن استعادة البيانات دون أية خسارة حتى في حال تعطل أحد الأقراص بشكل غير متوقع.</para>

      <sidebar>
        <title><emphasis>ثقافة</emphasis> <foreignphrase>Independent</foreignphrase> أو <foreignphrase>inexpensive</foreignphrase>؟</title>

	<para>كان حرف I في الاختصار RAID يرمز لكلمة <emphasis>inexpensive</emphasis>، لأن RAID قدمت نقلة نوعية في أمان البيانات دون الاضطرار لشراء أقراص متطورة باهظة الثمن. إلا أنها اليوم تروج على أنها تشير إلى <emphasis>independent</emphasis>، ربما حتى لا تعطي انطباعاً غير مرغوب بالفقر.</para>
      </sidebar>

      <para>يمكن تطبيق RAID باستخدام عتاد خاص (وحدات RAID مدمجة في متحكِّمات SCSI أو SATA) أو برمجيًا (عبر النواة). سواء كان النظام يعتمد على العتاد أو البرمجيات، يستطيع RAID أن يبقى في الخدمة عند عطب أحد الأقراص إذا كان هناك تخزين فائض كاف؛ إذا يمكن للطبقة العليا (التطبيقات) أن تستمر بالوصول إلى البيانات بغض النظر عن العطل. طبعاً، يمكن أن يؤثر ”وضع degraded“ هذا على الأداء، كما أن الفائض التخزيني ينخفض، ما يعني إمكانية خسارة البيانات إذا حصل عطل آخر في الأقراص. ولهذا لا يتم الاعتماد على degraded mode عمليًا إلا خلال المدة اللازمة لاستبدال القرص المعطوب. يستطيع نظام RAID إعادة بناء المعلومات اللازمة للعودة إلى الوضع الآمن بعد تثبيت القرص الجديد. لن تلاحظ البرمجيات أي شيء، أو ربما تشعر ببعض البطء في سرعة الوصول إلى البيانات عندما تكون المصفوفة في الوضع degraded أو أثناء مرحلة إعادة بناء البيانات المفقودة.</para>

      <para>عندما يعتمد على العتاد لبناء مصفوفات RAID، فغالباً ما يتم إعداد النظام عبر أداة إعداد BIOS، وتعتبر النواة مصفوفة RAID كقرص واحد، يعمل مثل قرص فيزيائي قياسي، إلا أن اسم الجهاز قد يختلف (تبعاً لبرنامج التعريف).</para>

      <para>سوف نركز على RAID البرمجي فقط في هذا الكتاب.</para>

      <section id="sect.raid-levels">
        <title>مستويات RAID المختلفة</title>

	<para>في الواقع RAID ليس نظاماً واحداً، بل مجموعة من النظم لكل منها مستوى؛ وتختلف المستويات عن بعضها بالتنظيم وكمية الفائض التي تقدمها. كلما كان الفائض أكبر كلما كان النظام أكثر مقاومة للأعطال، ذلك لأن النظام سيبقى في الخدمة مع المزيد من الأقراص المعطوبة. الناحية السلبية هي أن المساحة التخزينية المتاحة للاستعمال تصغر؛ وذلك بسبب الحاجة لأقراص أكثر لتخزين الكمية نفسها من البيانات.</para>
        <variablelist>
          <varlistentry>
            <term>‏Linear RAID</term>
            <listitem>
	      <para>مع أن نظام RAID الفرعي في النواة يدعم إنشاء ”Linear RAID“، إلا أن هذا النوع ليس RAID أصلاً، إذا أن هذا الإعداد ليس فيه أي فائض. كل ما يحدث هو أن النواة تجمع عدة أقراص مع بعضها بأسلوب end-to-end (نهاية القرص الأول مع بداية الثاني وهكذا) وتقدم مجموع الحجم التخزيني بشكل قرص ظاهري واحد (one block device). هذه هي وظيفته كلها. نادرًا ما يستخدم هذا النمط وحده (اقرأ الفقرات التالية لتتعرف على الحالات الاستثنائية)، خصوصًا أن افتقاره للفائض يعني أن تعطل أحد الأقراص سيودي بالمجموع التخزيني كله، مع بياناته.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>‏RAID-0</term>
            <listitem>
	      <para>لا يقدم هذا المستوى أية فائض أيضًا، لكن الأقراص لا تتقاطر خلف بعضها بشكل بسيط: بل تقسم إلى شرائط <emphasis>stripes</emphasis>، ويتم تخزين أجزاء القرص الظاهري على الشرائط بشكل متناوب بين الأقراص الفيزيائية. في نظام RAID-0 ذو قرصين، مثلًا، تُخَزَّن الأجزاء الزوجية من القرص الظاهري على القرص الفيزيائي الأول، والأجزاء الفردية على القرص الفيزيائي الثاني.</para>

	      <para>لا يسعى هذا النظام لزيادة الوثوقية، نظرًا لأن كافة البيانات ستضيع إذا فشل أحد الأقراص (كما في حالة Linear RAID)، لكنه يهدف لرفع الأداء: سوف تتمكن النواة أثناء الوصول التسلسلي لكميات كبيرة من البيانات المستمرة من القراءة من القرصين معًا (أو الكتابة عليهما معًا) على التوازي، وهو ما يزيد مستوى نقل البيانات. على أية حال، فإن استخدام RAID-0 في تناقص، بعد أن احتلّ LVM مكانه في تحقيق هذه الميزة (انظر لاحقاً).</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>‏RAID-1</term>
            <listitem>
	      <para>يعرف هذا المستوى أيضًا باسم ”RAID mirroring“، وهو الأبسط والأكثر انتشاراً. يعتمد هذا المستوى –في شكله المعياري– على قرصين فيزيائيين لهما السعة ذاتها، ويعطي قرصًا منطقيًا له نفس السعة أيضًا. تخزن البيانات نفسها على القرصين، ولذلك كان ”mirror“ هو الاسم الثاني لهذا المستوى. إذا تعطّل أحد القرصين، تبقى البيانات متوفرة على الآخر. يمكن طبعاً إعداد RAID-1 على أكثر من قرصين بالنسبة للبيانات الهامة جدًا، لكن هذا سيزيد نسبة الكلفة للمساحة التخزينية.</para>

              <sidebar>
                <title><emphasis>ملاحظة</emphasis> سعة الأقراص وسعة العنقود</title>

		<para>إذا تم إعداد قرصين من سعتين مختلفتين في مرآة RAID-1، لن يستخدم القرص الأكبر بشكل كامل، لأنه سيحوي نفس البيانات التي يحويها القرص الأصغر فقط. أي أن المساحة المتوفرة للاستخدام في قرص RAID-1 الناتج ستطابق سعة أصغر قرص في المصفوفة. هذا القانون ينطبق على مستويات RAID اللاحقة أيضًا، رغم أن الفائض مخزن بأسلوب مختلف.</para>

		<para>لذلك كان مهماً أن تجمع الأقراص ذات السعات المتساوية أو المتقاربة جدًا عند إعداد مصفوفات RAID (ما عدا RAID-0 و Linear RAID)، حتى تتجنب الهدر في الموارد.</para>
              </sidebar>

              <sidebar>
                <title><emphasis>ملاحظة</emphasis> الأقراص الاحتياطية</title>

		<para>يمكن إضافة أقراص أكثر مما هو مطلوب للمصفوفة في مستويات RAID التي تحتوي على فائض. يمكن استخدام الأقراص الإضافية كبديل عندما يتعطّل أحد الأقراص الرئيسية. مثلًا، في حالة تطبيق مرآة بقرصين مع قرص احتياطي واحد، سوف تعيد النواة بناء المرآة تلقائيًا (وفوريًا) باستخدام القرص الاحتياطي إذا تعطّل أحد القرصين الرئيسيين. يمكن اعتماد هذا الأسلوب كخط أمان إضافي للبيانات الحساسة.</para>

		<para>قد يتساءل المرء عن سبب تفضيل هذا الأسلوب على إعداد مرآة بثلاثة أقراص ببساطة. إن ميزة إعداد ”القرص الاحتياطي“ هي إمكانية مشاركة القرص الاحتياطي بين عدة مصفوفات RAID. يمكن مثلًا، إعداد ثلاثة مصفوفات RAID-1، مع ضمان حماية الفائض حتى في حال تعطل أحد الأقراص باستخدام سبعة أقراص فقط (ثلاثة أزواج واحتياطي واحد)، بدلاً من تسعة أقراص كنا سنحتاجها لإعداد ثلاثة مرايا ثلاثية.</para>
              </sidebar>

	      <para>بالرغم من ارتفاع كلفة هذا المستوى (نظراً لأن المساحة التخزينية المتاحة تساوي نصف المساحة الفيزيائية في أحسن الأحوال)، إلا أنه استخدامه منتشر عملياً. فهم هذا المستوى بسيط، وهو يؤدي عملية نسخ احتياطي بسيطة جدًا: بما أن القرصين يخزنان المحتوى نفسه، يمكن فصل أحدهما مؤقتًا دون التأثير على عمل النظام. غالبًا ما يكون أداء الأقراص عند القراءة مرتفعاً، لأن النواة تستطيع قراءة نصف البيانات من كل قرص على التوازي، في حين لا ينخفض الأداء كثيراً عند الكتابة. تبقى البيانات متاحة في مصفوفة RAID-1 ذات N قرص، حتى في حال تعطل N-1 قرص.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>‏RAID-4</term>
            <listitem>
	      <para>هذا المستوى من RAID غير منتشر كثيراً. يستخدم هذا المستوى N قرص لتخزين البيانات المفيدة، وقرص إضافي لتخزين معلومات فائضة. إذا تعطل القرص الإضافي، يستطيع النظام إعادة بناء محتوياته اعتمادًا على الأقراص الأخرى. أما إذا تعطل أحد أقراص المعلومات فيستخدم النظام الأقراص المتبقية منها (N-1 قرص) مع القرص الإضافي (قرص الازدواجية – ‎“parity” disk) لإعادة بناء البيانات المفقودة.</para>

	      <para>إن كلفة RAID-4 ليست مرتفعة جداً بما أن الزيادة في الكلفة هي 1 إلى N كما أنه تأثيره على سرعة القراءة غير ملحوظ، لكن أداء الكتابة ينخفض. من ناحية أخرى، عند كل عملية كتابة على أحد أقراص المعلومات يجب الكتابة على قرص الازدواجية أيضًا، ما قد يؤدي لتقصير عمره بشكل كبير. تبقى البيانات في مصفوفة RAID-4 بأمان في حال عطب قرص واحد (من المصفوفة كلها ذات N+1 قرص).</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>‏RAID-5</term>
            <listitem>
	      <para>يعالج المستوى RAID-5 مشكلة اللاتناظر التي يعاني منها RAID-4: حيث تنتشر معلومات الازدواجية على جميع الأقراص في مصفوفة N+1، ولا يوجد دور محدد لأي قرص منها.</para>

	      <para>أداء القراءة والكتابة مطابق لأداء RAID-4. كما أن النظام هنا أيضًا يتحمل تعطل قرص واحد فقط (من أصل N+1 قرص).</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>‏RAID-6</term>
            <listitem>
	      <para>يمكن اعتبار RAID-6 كامتداد للمستوى RAID-5، إذ أن كل سلسلة مؤلفة من N كتلة تحتاج إلى كتلتين فائضتين، وكل سلسلة من N+2 كتلة تنتشر على N+2 قرص.</para>

	      <para>كلفة هذا المستوى أعلى بقليل من المستويين السابقين، لكنه يزيد مستوى الأمان إذا يستطيع العمل حتى لو تعطل قرصين (من أصل N+2) دون تأثر البيانات. الجانب السلبي هو أن عمليات الكتابة على الأقراص تحتاج لكتابة كتلة بيانات واحدة وكتلتين فائضتين، وهذا يجعل الكتابة أبطأ.</para>
            </listitem>
          </varlistentry>
          <varlistentry>
            <term>‏RAID-1+0</term>
            <listitem>
	      <para>للأمانة العلمية هذا ليس مستوى RAID، لكنه تركيب لمستويين وراء بعضهما. إذا كان لدينا N‏×2 قرص، يمكننا أن نجمع كل زوج منها للحصول على N قرص من مستوى RAID-1؛ ثم نجمع هذه الأقراص في قرص واحد إما باستخدام ”linear RAID“ أو عبر LVM. إذا استخدمنا LVM فإننا نتجاوز حدود RAID، لكن هذه ليست مشكلة في الواقع.</para>

	      <para>تتحمل مصفوفات RAID-1+0 تعطل عدة أقراص: فالمصفوفة الموضحة سابقاً يمكن أن تتحمل تعطل N قرص إذا كانت تحوي ‎2×‎N قرص، بشرط أن ينجو قرص واحد على الأقل من كل زوج من أقراص RAID-1.</para>

              <sidebar id="sidebar.raid-10">
                <title><emphasis>التعمق أكثر</emphasis> RAID-10</title>

		<para>يعتبر RAID-10 كمرادف للمستوى RAID-1+0 عموماً، لكن هناك خاصية في لينكس تجعل الثاني حالة خاصة من الأول. يسمح هذا الإعداد ببناء نظام تُخزَّن فيه كل كتلة على قرصين مختلفين، حتى لو كان عدد الأقراص في النظام فردياً، ويتبع توزيع النسخ على الأقراص نموذجاً محدداً يمكن تعديله.</para>

		<para>سيختلف مستوى الأداء تبعاً لنموذج التقسيم المتبع ومستوى الفائض، وحمل الحيز التخزيني المنطقي.</para>
              </sidebar>
            </listitem>
          </varlistentry>
        </variablelist>

	<para>من الواضح أن اختيار مستوى RAID الملائم يعتمد على متطلبات وقيود كل تطبيق. لاحظ أن الحاسوب الواحد يمكن أن يحوي عدة مصفوفات RAID ذات مستويات مختلفة.</para>
      </section>
      <section id="sect.raid-setup">
        <title>إعداد RAID</title>
        <indexterm><primary><emphasis role="pkg">mdadm</emphasis></primary></indexterm> 

	<para>يحتاج إعداد RAID لحزمة <emphasis role="pkg">mdadm</emphasis>؛ التي توفر الأمر <command>mdadm</command> الذي يستخدم لإنشاء وتعديل مصفوفات RAID، كما توفر أيضًا سكربتات وأدوات تدمج البرنامج في أجزاء نظام التشغيل الأخرى، بما فيه نظام المراقبة.</para>

	<para>مثالنا هو مُخدِّم فيه عدد من الأقراص، بعضها مستخدم، والباقي متاح لإعداد مصفوفة RAID. هذه هي الحالة الإبتدائية للأقراص والأقسام:</para>
        <itemizedlist>
          <listitem>
	    <para>القرص <filename>sdb</filename>، ‏4 غ.ب، متاح بالكامل؛</para>
          </listitem>
          <listitem>
	    <para>القرص <filename>sdc</filename>، ‏4 غ.ب، متاح بالكامل أيضاً؛</para>
          </listitem>
          <listitem>
	    <para>القسم <filename>sdd2</filename> من القرص <filename>sdd</filename> متاح (حوالي 4 غ.ب)؛</para>
          </listitem>
          <listitem>
	    <para>أخيراً، القرص <filename>sde</filename>، أيضاً 4 غ.ب متاح بالكامل.</para>
          </listitem>
        </itemizedlist>

        <sidebar>
          <title><emphasis>ملاحظة</emphasis> التعرف على أقراص RAID القديمة</title>

	  <para>يسرد الملف <filename dir="ltr">/proc/mdstat</filename> جميع أقراص RAID السابقة وحالاتها. يجب أن تنتبه إلى عدم استخدام اسم قرص مستخدم مسبقًا عند إنشاء قرص جديد.</para>
        </sidebar>

	<para>سوف نستخدم هذه العناصر الفيزيائية لبناء حيزين تخزينيين، أحدهما RAID-0، والآخر RAID-1 (مرآة). دعنا نبدأ ببناء حيز RAID-0:</para>

        <screen><computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</userinput>
<computeroutput>mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
# </computeroutput><userinput>mdadm --query /dev/md0</userinput>
<computeroutput>/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.
# </computeroutput><userinput>mdadm --detail /dev/md0</userinput>
<computeroutput>/dev/md0:
        Version : 1.2
  Creation Time : Wed May  6 09:24:34 2015
     Raid Level : raid0
     Array Size : 8387584 (8.00 GiB 8.59 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:24:34 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

     Chunk Size : 512K

           Name : mirwiz:0  (local to host mirwiz)
           UUID : bb085b35:28e821bd:20d697c9:650152bb
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       1       8       32        1      active sync   /dev/sdc
# </computeroutput><userinput>mkfs.ext4 /dev/md0</userinput>
<computeroutput>mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 2095104 4k blocks and 524288 inodes
Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </computeroutput><userinput>mkdir /srv/raid-0</userinput>
<computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0</userinput>
<computeroutput># </computeroutput><userinput>df -h /srv/raid-0</userinput>
<computeroutput>Filesystem      Size  Used Avail Use% Mounted on
/dev/md0        7.9G   18M  7.4G   1% /srv/raid-0
</computeroutput></screen>

	<para>يحتاج الأمر <command>mdadm --create</command> عدة متغيرات: اسم الحيز الذي سيتم إنشاؤه (<filename dir="ltr">/dev/md*</filename>، حيث ترمز md إلى <foreignphrase>Multiple Device</foreignphrase>―”أجهزة متعددة“)، ومستوى RAID، وعدد الأقراص (هذا المتغير إلزامي رغم أنه لا يفيد إلا مع مستويات RAID-1 وما فوق)، والأجهزة الفيزيائية التي ستستخدم. بعد إنشاء الحيز، يمكننا استخدامه كما نستخدم أي قسم عادي، فيمكن إنشاء نظام ملفات عليه، وربطه بشجرة الملفات، وغير ذلك. لاحظ أن إنشاء حيز RAID-0 على <filename>md0</filename> هو محض صدفة، وترقيم المصفوفة لا يشترط أن يتعلق بمستوى RAID المختار. كما يمكن إنشاء مصفوفات RAID بأسماء محددة، عبر إعطاء <command>mdadm</command> متغير مثل <filename dir="ltr">/dev/md/linear</filename> بدلاً من <filename dir="ltr">/dev/md0</filename>.</para>

	<para>يتم إنشاء RAID-1 بأسلوب مشابه، ولا تظهر الاختلافات إلا بعد عملية الإنشاء:</para>

        <screen><computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</userinput>
<computeroutput>mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%
Continue creating array؟ </computeroutput><userinput>y</userinput>
<computeroutput>mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md1 started.
# </computeroutput><userinput>mdadm --query /dev/md1</userinput>
<computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
        Version : 1.2
  Creation Time : Wed May  6 09:30:19 2015
     Raid Level : raid1
     Array Size : 4192192 (4.00 GiB 4.29 GB)
  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:30:40 2015
          State : clean, resyncing (PENDING) 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       1       8       64        1      active sync   /dev/sde
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
          State : clean
[...]
</computeroutput></screen>

        <sidebar>
          <title><emphasis>تلميح</emphasis> RAID والأقراص والأقسام</title>

	  <para>كما هو واضح من المثال، يمكن بناء أجهزة RAID من أقسام الأقراص، ولا يشترط استخدام أقراص كاملة.</para>
        </sidebar>

	<para>هناك بضعة ملاحظات. أولاً، يلاحظ <command>mdadm</command> اختلاف سعة العناصر الفيزيائية؛ وبما أن هذا يعني ضياع بعض المساحة من العنصر الأكبر، يطلب من المستخدم تأكيد العملية.</para>

	<para>الأهم من هذا هو حالة المرآة. لاحظ كيف كانت resyncing ثم انتقلت إلى active. إن الحالة الطبيعية لمرآة RAID هي أن تتطابق محتويات القرصين. لكن لا شيء يضمن هذا التطابق عند إنشاء المصفوفة أول مرة، ولذلك يعمل نظام RAID الفرعي على ضمان هذا بنفسه، ويبدأ طور مزامنة المحتويات بعد إنشاء المصفوفة مباشرة. بعد فترة من الزمن (تختلف المدة حسب حجم الأقراص الفعلي...)، تنتقل مصفوفة RAID إلى حالة ”active“ أو ”clean“. لاحظ أن المصفوفة تكون في الوضع degraded خلال طور إعادة البناء، وأن الفائض التخزيني غير جاهز بعد. إذا تعطل قرص أثناء مرحلة الخطر تلك، فسوف يؤدي ذلك إلى خسارة البيانات كلها. لكن نادرًا ما تستخدم مصفوفات RAID الجديدة لتخزين كميات كبيرة من البيانات الحساسة قبل أن تنتهي مرحلة تهيئتها الأولية. لاحظ أيضًا أن <filename dir="ltr">/dev/md1</filename> جاهز للاستخدام حتى في وضع degraded، وأنه يمكن إنشاء نظام ملفات عليه، كما يمكن نسخ البيانات إليه أيضًا.</para>

        <sidebar>
          <title><emphasis>تلميح</emphasis> إنشاء مرآة في وضع degraded</title>

	  <para>أحيانًا لا يكون القرصان جاهزين فورًا لحظة إنشاء مرآة RAID-1، مثلاً يمكن أن أحد القرصين الذين نريد استخدامهما مستخدم أصلاً لتخزين البيانات التي نريد نقلها إلى المصفوفة. في مثل هذه الحالات، من الممكن إنشاء مصفوفة RAID-1 في الوضع degraded باستخدام قرص واحد من خلال تمرير <filename>missing</filename> كمعامل للأمر <command>mdadm</command> بدلاً من تمرير اسم الملف الذي يمثل القرص. بعد نسخ البيانات إلى ”المرآة“، يمكن إضافة القرص القديم إلى المصفوفة. عندها تبدأ عملية المزامنة، للوصول إلى الحالة الآمنة التي أردناها في البداية.</para>
        </sidebar>

        <sidebar>
          <title><emphasis>تلميح</emphasis> إعداد مرآة بدون مزامنة</title>

	  <para>تستخدم مصفوفات RAID-1 بعد إنشائها غالبًا كأقراص جديدة، وتعامل على أنها فارغة. أي أن المحتويات الأولية للقرص عديمة القيمة، لأن كل ما نحتاجه هو أن نتأكد أننا سوف نستطيع لاحقاً الوصول البيانات التي سنكتبها بعد إنشاء الحيز التخزيني الجديد، خصوصاً نظام الملفات.</para>

	  <para>قد يتساءل المرء عندئذ عن فائدة مزامنة الأقراص عند إنشائها. ما الفرق إذا كانت محتويات المصفوفة متزامنة إذا كنا لن نقرأ من المصفوفة شيئًا قبل محوها وتهيئتها؟</para>

	  <para>لحسن الحظ، يمكن تفادي طور المزامنة هذا بتمرير الخيار‎ <literal dir="ltr">--assume-clean</literal> للأمر <command>mdadm</command>. لكن هذا الخيار قد يسبب مفاجآت لو حاولنا قراءة البيانات الأولية (مثلاً إذا كانت الأقراص الفيزيائية تحوي نظام ملفات مسبقًا)، لذلك فإن هذا الخيار معطل افتراضيًا.</para>
        </sidebar>

	<para>دعنا نرى ما سيحدث عندما يتعطل أحد عناصر مصفوفة RAID-1. يمكن محاكاة عطب قرص ما باستخدام الخيار <literal dir="ltr">--fail</literal> مع الأمر <command>mdadm</command>:</para>

        <screen><computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde</userinput>
<computeroutput>mdadm: set /dev/sde faulty in /dev/md1
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
    Update Time : Wed May  6 09:39:39 2015
          State : clean, degraded 
 Active Devices : 1
Working Devices : 1
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 19

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       0        0        2      removed

       1       8       64        -      faulty   /dev/sde</computeroutput></screen>

	<para>تبقى محتويات المصفوفة متاحة (وإذا كانت مرتبطة بشجرة الملفات، فلن تشعر التطبيقات بشيء)، لكن البيانات لم تعد بأمان: فإذا تعطل القرص <filename>sdd</filename> أيضًا، سوف تضيع البيانات. نحن لا نريد أن نخاطر بذلك، ولهذا سوف نستبدل القرص المعطوب بقرص جديد، <filename>sdf</filename>:</para>

        <screen><computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>
<computeroutput>mdadm: added /dev/sdf
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
   Raid Devices : 2
  Total Devices : 3
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:48:49 2015
          State : clean, degraded, recovering 
 Active Devices : 1
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 1

 Rebuild Status : 28% complete

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 26

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      spare rebuilding   /dev/sdf

       1       8       64        -      faulty   /dev/sde
# </computeroutput><userinput>[...]</userinput>
<computeroutput>[...]
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
    Update Time : Wed May  6 09:49:08 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 41

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf

       1       8       64        -      faulty   /dev/sde</computeroutput></screen>

	<para>هنا أيضاً تبدأ النواة طور إعادة بناء تلقائيًا، وتبقى المصفوفة خلال هذا الطور في الوضع degraded أيضًا لكنها متاحة للوصول. ترجع مصفوفة RAID-1 إلى الحالة الطبيعية فور انتهاء إعادة البناء. يمكن عندها أن نخبر النظام أننا سوف نزيل القرص <filename>sde</filename> من المصفوفة، حتى تبقى كمرآة RAID كلاسيكية بقرصين فقط:</para>

        <screen><computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>
<computeroutput>mdadm: hot removed /dev/sde from /dev/md1
# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>
<computeroutput>/dev/md1:
[...]
    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf</computeroutput>
</screen>

	<para>عند هذه اللحظة يمكن فصل القرص الفيزيائي عند إيقاف تشغيل المخدم، أو يمكن حتى فصلها مباشرة إذا كان العتاد يسمح بالتبديل الساخن hot-swap. تسمح بعض متحكمات SCSI، ومعظم أقراص SATA، والسواقات الخارجية التي تعمل عبر USB أو Firewire بهذا النوع من التبديل.</para>
      </section>
      <section id="sect.backup-raid-config">
        <title>النسخ الاحتياطي للإعدادات</title>

	<para>تُحفَظ معظم البيانات الفوقية (meta-data) الخاصة بمصفوفات RAID مباشرة على الأقراص التي تنتمي لهذه المصفوفات، حتى تتعرف النواة على المصفوفات ومكوناتها وتجمعها آليًا عند إقلاع النظام. لكن الأفضل أخذ نسخة احتياطية عن هذه البيانات، لأن عملية التعرف هذه قد تفشل، ومن المتوقع ألا تفشل هذه العملية إلا في الظروف الحساسة. فلو كان عطل القرص <filename>sde</filename> في مثالنا حقيقيًا (وليس ظاهريًا كما فعلنا) ثم أعيد تشغيل النظام دون إزالة هذا القرص <filename>sde</filename>، فقد يعود هذا القرص إلى العمل ثانية نتيجة عملية الاستكشاف أثناء إعادة الإقلاع. سوف تصطدم النواة إذًا بثلاثة أقراص فيزيائية، كلٌّ منها يدعي أنه يحوي نصف الحيز التخزيني المقابل للمصفوفة نفسها. أو يمكن أن يحدث التباس عند دمج مصفوفات RAID من مخدمين إلى مخدم واحد فقط. إذا كانت هذه المصفوفات تعمل بشكل صحيح قبل نقل الأقراص، سوف تتمكن النواة من التعرف على الأزواج وجمعها بشكل صحيح؛ لكن إذا كانت الأقراص على المخدم القديم مجموعة مع بعضها في مصفوفة اسمها <filename>md1</filename>، وكان المخدم الجديد يحوي <filename>md1</filename> أيضًا، فسوف تعاد تسمية إحدى المرآتين.</para>

	<para>إذاً لا بد من أخذ نسخة احتياطية عن الإعدادات، حتى لو كانت للاستئناس فقط. الطريقة المعيارية لعمل هذا هي تحرير الملف <filename dir="ltr">/etc/mdadm/mdadm.conf</filename>، إليك مثالاً عن هذا الملف:</para>

        <example id="example.mdadm-conf">
          <title>ملف إعداد <command>mdadm</command></title>

          <programlisting># mdadm.conf
#
# Please refer to mdadm.conf(5) for information about this file.
#

# by default (built-in), scan all partitions (/proc/partitions) and all
# containers for MD superblocks. alternatively, specify devices to scan, using
# wildcards if desired.
DEVICE /dev/sd*

# auto-create devices with Debian standard permissions
CREATE owner=root group=disk mode=0660 auto=yes

# automatically tag new arrays as belonging to the local system
HOMEHOST &lt;system&gt;

# instruct the monitoring daemon where to send mail alerts
MAILADDR root

# definitions of existing MD arrays
ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464

# This configuration was auto-generated on Thu, 17 Jan 2013 16:21:01 +0100
# by mkconf 3.2.5-3
</programlisting>
        </example>

	<para>أحد أهم التفاصيل هو خيار <literal>DEVICE</literal>، الذي يعدد الأجهزة التي يفحصها النظام بحثًا عن مكونات مصفوفات RAID عند الإقلاع. لقد استبدلنا في مثالنا القيمة الافتراضية – <literal>partitions containers</literal> – بلائحة واضحة تسرد أسماء ملفات الأجهزة، ذلك لأننا اخترنا استخدام بعض الأقراص الكاملة وليس الأقسام فقط.</para>

	<para>آخر سطرين في مثالنا يسمحان للنواة بإسناد رقم الحيز المناسب إلى المصفوفة المناسبة. إن البيانات الفوقية المخزنة على الأقراص نفسها تكفي لإعادة جمع المصفوفات، لكنها لا تكفي لمعرفة رقم الحيز (ولا معرفة اسم <filename dir="ltr">/dev/md*</filename> الموافق للجهاز).</para>

	<para>لحسن الحظ، يمكن توليد هذه الأسطر آليًا:</para>

        <screen><computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>
<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</computeroutput>
</screen>

	<para>لا تعتمد محتويات هذه السطور على الأقراص المتضمنة في المصفوفة. فلا حاجة إلى إعادة توليدها عند استبدال قرص معطوب بآخر جديد. لكن يجب الانتباه إلى تحديث الملف عند إنشاء مصفوفة RAID جديدة أو حذف واحدة قديمة.</para>
      </section>
    </section>
    <section id="sect.lvm">
      <title>LVM</title>
      <indexterm><primary>LVM</primary></indexterm>
      <indexterm><primary>Logical Volume Manager</primary></indexterm>

      <para><emphasis>Logical Volume Manager</emphasis> ًأو اختصارا LVM هو أسلوب آخر لعزل الأقراص التخزينية المنطقية عن الأقراص الفيزيائية، وهو يركز على زيادة المرونة بدلاً من زيادة الوثوقية. يسمح LVM بتغيير القرص المنطقي بشكل شفاف بالنسبة للتطبيقات؛ فمثلاً، يمكن إضافة أقراص فيزيائية جديدة، ونقل البيانات إليها، وإزالة القديمة، دون فصل القرص المنطقي عن شجرة الملفات.</para>
      <section id="sect.lvm-concepts">
        <title>مفاهيم LVM</title>

	<para>هذه المرونة نحرزها من خلال مستوى من العزل يشمل ثلاثة مفاهيم.</para>

	<para>الأول هو PV، أي <emphasis>Physical Volume</emphasis> (الحيز الفيزيائي) وهو أقرب وحدة إلى العتاد: يمكن أن يتألف من قسم من أحد الأقراص، أو قرص كامل، أو أي جهاز كتلي آخر (بما في ذلك مصفوفات RAID على سبيل المثال). لاحظ أنه عندما يتم إعداد عنصر فيزيائي ليشغل دور PV في LVM، فيجب التعامل معه من LVM فقط، وإلا فإن النظام سوف يضطرب.</para>

	<para>يمكن تجميع عدة PV ضمن VG ‏(<emphasis>Volume Group</emphasis>)، التي يمكن أن نعتبرها بمثابة أقراص ظاهرية قابلة للتوسعة. إن VGs مكونات مجردة، ولا تظهر بشكل ملفات أجهزة في فرع <filename dir="ltr">/dev</filename>، لذلك لا يمكن استخدامها مباشرة.</para>

	<para>النوع الثالث من المكونات هو LV‏ (<emphasis>Logical Volume</emphasis> – الحيز المنطقي)، وهو قطعة من VG؛ فإذا اعتبرنا VG بمثابة قرص، عندها يقابل LV القسم من القرص. يظهر LV كجهاز كتلي له مدخلة في <filename dir="ltr">/dev</filename>، ويمكن استخدامه كما يستخدم أي قسم فيزيائي آخر (لاستضافة نظام ملفات أو مساحة swap عادة).</para>

	<para>أهم شيء هنا هو أن تقسيم VG إلى LVs مستقل تمامًا عن المكونات الفيزيائية للـ VG (وهي PVs). يمكن تقسيم VG يتألف من مكون فيزيائي واحد (قرص مثلاً) إلى دزينة من الأقراص المنطقية؛ كما يمكن أن يتألف VG من العديد من الأقراص الفيزيائية ثم يظهر كحيز منطقي كبير مفرد. القيد الوحيد طبعاً هو أن الحجم الكلي المتاح للتخزين على LVs لا يمكن أن يكون أكبر من السعة الكلية للحيزات الفيزيائية في الـVG.</para>

	<para>إلا أن المنطق يطلب شيئًا من التجانس بين المكونات الفيزيائية للـVG، وأن تقسم الـVG إلى حيزات منطقية لها استخدامات متشابهة. مثلاً، إذا كان العتاد المتوفر يحوي أقراصًا سريعة وأخرى بطيئة، فيمكن تجميع السريعة منها في VG واحدة والأقراص البطيئة في أخرى؛ يمكن تخصيص أجزاء من الأولى للتطبيقات التي تحتاج وصولاً سريعًا للبيانات، بينما تبقى الأخرى للمهام الأقل إلحاحاً.</para>

	<para>وعلى أية حال، تذكر أن LV لا يرتبط مباشرة بأي PV معيّن. من الممكن التأثير على موقع تخزين بيانات أحد الحيزات المنطقية فيزيائيًا، لكن هذه الإمكانية ليست جوهرية في الاستخدامات العادية. وعلى صعيد آخر: عندما تتطور المكونات الفيزيائية للـVG، يمكن تهجير مواقع التخزين الفيزيائية لأحد LVs بين الأقراص (مع البقاء ضمن PVs المخصصة للـVG بالطبع).</para>
      </section>
      <section id="sect.lvm-setup">
        <title>إعداد LVM</title>

	<para>دعنا الآن نتبع –خطوة بخطوة– طريقة إعداد LVM لحالة استخدام نموذجية: حيث نريد تبسيط حالة تخزينية معقدة. تحدث هذه الحالات عادة بعد تاريخ طويل ومعقد من تراكم التدابير المؤقتة. سوف ندرس كمثال حالة مخدم تغيرت فيه الحاجات التخزينية مع الزمن، وانتهى المطاف بمتاهة من الأقسام المتاحة الموزعة على عدد من الأقراص المستخدمة جزئيًا. بكلام واضح أكثر، الأقسام التالية هي المتاحة:</para>
        <itemizedlist>
          <listitem>
	    <para>من القرص <filename>sdb</filename>، القسم <filename>sdb2</filename>، الحجم 4 غ.ب؛</para>
          </listitem>
          <listitem>
	    <para>من القرص <filename>sdc</filename>، القسم <filename>sdc3</filename>، الحجم 3 غ.ب؛</para>
          </listitem>
          <listitem>
	    <para>القرص <filename>sdd</filename>، متاح بالكامل، 4 غ.ب؛</para>
          </listitem>
          <listitem>
	    <para>من القرص <filename>sdf</filename>، القسم <filename>sdf1</filename>، ‏4 غ.ب؛ والقسم <filename>sdf2</filename>، ‏5 غ.ب.</para>
          </listitem>
        </itemizedlist>

	<para>بالإضافة لذلك، دعنا نفترض أن القرصين <filename>sdb</filename> و<filename>sdf</filename> أسرع من البقية.</para>

	<para>هدفنا هو إعداد ثلاثة حيزات منطقية لثلاثة تطبيقات: مخدم ملفات يحتاج 5 غ.ب. من المساحة التخزينية، وقاعدة بيانات (1 غ.ب) وبعض المساحة للنسخ الاحتياطية (12 غ.ب). يحتاج التطبيقان الأوليان أداء جيداً، بينما النسخ الاحتياطية أقل حرجاً من حيث الحاجة لسرعة النقل. تمنعنا كل هذه القيود من استخدام الأقسام المتاحة مباشرة كما هي؛ لكن يمكن أن يسمح استخدام LVM بعزل الحجم الفيزيائي للأجهزة، بحيث يبقى القيد الوحيد هو المساحة الكلية المتوفرة فقط.</para>

	<para>الأدوات المطلوبة كلها في حزمة <emphasis role="pkg">lvm2</emphasis> واعتمادياتها. بعد تثبيتها، يتطلب إعداد LVM ثلاث خطوات، تقابل المستويات الثلاث للمفاهيم.</para>

	<para>أولاً، نجهز الحيزات الفيزيائية باستخدام <command>pvcreate</command>:</para>

        <screen id="screen.pvcreate"><computeroutput># </computeroutput><userinput>pvdisplay</userinput>
<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2</userinput>
<computeroutput>  Physical volume "/dev/sdb2" successfully created
# </computeroutput><userinput>pvdisplay</userinput>
<computeroutput>  "/dev/sdb2" is a new physical volume of "4.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/sdb2
  VG Name               
  PV Size               4.00 GiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I

# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</userinput>
<computeroutput>  Physical volume "/dev/sdc3" successfully created
  Physical volume "/dev/sdd" successfully created
  Physical volume "/dev/sdf1" successfully created
  Physical volume "/dev/sdf2" successfully created
# </computeroutput><userinput>pvdisplay -C</userinput>
<computeroutput>  PV         VG   Fmt  Attr PSize PFree
  /dev/sdb2       lvm2 ---  4.00g 4.00g
  /dev/sdc3       lvm2 ---  3.09g 3.09g
  /dev/sdd        lvm2 ---  4.00g 4.00g
  /dev/sdf1       lvm2 ---  4.10g 4.10g
  /dev/sdf2       lvm2 ---  5.22g 5.22g
</computeroutput></screen>

	<para>حتى الآن، كل شيء على ما يرام؛ لاحظ أنه يمكن إعداد PV على قرص كامل كما يمكن ذلك على أقسام الأقراص. يسرد الأمر <command>pvdisplay</command> الحيزات الفيزيائية الموجودة، وذلك في صيغتين مختلفتين للخرج، كما هو موضح أعلاه.</para>

	<para>دعنا الآن نجمع هذه العناصر الفيزيائية في VG باستخدام <command>vgcreate</command>. سوف نجمع الحيزات الفيزيائية من الأقراص السريعة فقط في مجموعة اسمها <filename>vg_critical</filename>؛ أما المجموعة الأخرى، <filename>vg_normal</filename>، فسوف تحوي عناصر سريعة وأخرى بطيئة.</para>

        <screen id="screen.vgcreate"><computeroutput># </computeroutput><userinput>vgdisplay</userinput>
<computeroutput>  No volume groups found
# </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1</userinput>
<computeroutput>  Volume group "vg_critical" successfully created
# </computeroutput><userinput>vgdisplay</userinput>
<computeroutput>  --- Volume group ---
  VG Name               vg_critical
  System ID             
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               8.09 GiB
  PE Size               4.00 MiB
  Total PE              2071
  Alloc PE / Size       0 / 0   
  Free  PE / Size       2071 / 8.09 GiB
  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp

# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</userinput>
<computeroutput>  Volume group "vg_normal" successfully created
# </computeroutput><userinput>vgdisplay -C</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize  VFree 
  vg_critical   2   0   0 wz--n-  8.09g  8.09g
  vg_normal     3   0   0 wz--n- 12.30g 12.30g
</computeroutput>
</screen>

	<para>الأوامر هنا أيضًا واضحة جداً ( كما أن <command>vgdisplay</command> يوفر صيغتين للخرج). لاحظ أنه من الممكن استخدام قسمين من القرص الفيزيائي نفسه في مجموعتين مختلفتين. لاحظ أيضًا أننا استخدمنا بادئة <filename dir="ltr">vg_</filename> عند تسمية VGs التي أنشأناها ولكن هذا مجرد اصطلاح.</para>

	<para>لدينا الآن ”قرصين ظاهريين“، أحجامهما تقريبًا 8 غ.ب و 12 غ.ب على التوالي. دعنا الآن نقطعهما إلى ”أقسم ظاهرية“ (LVs). نحتاج الأمر <command>lvcreate</command>، ونحتاج أيضًا تعليمة أكثر تعقيداً بقليل:</para>

        <screen id="screen.lvcreate"><computeroutput># </computeroutput><userinput>lvdisplay</userinput>
<computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical</userinput>
<computeroutput>  Logical volume "lv_files" created
# </computeroutput><userinput>lvdisplay</userinput>
<computeroutput>  --- Logical volume ---
  LV Path                /dev/vg_critical/lv_files
  LV Name                lv_files
  VG Name                vg_critical
  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT
  LV Write Access        read/write
  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400
  LV Status              available
  # open                 0
  LV Size                5.00 GiB
  Current LE             1280
  Segments               2
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0

# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical</userinput>
<computeroutput>  Logical volume "lv_base" created
# </computeroutput><userinput>lvcreate -n lv_backups -L 12G vg_normal</userinput>
<computeroutput>  Logical volume "lv_backups" created
# </computeroutput><userinput>lvdisplay -C</userinput>
<computeroutput>  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_base    vg_critical -wi-a---  1.00g                                           
  lv_files   vg_critical -wi-a---  5.00g                                           
  lv_backups vg_normal   -wi-a--- 12.00g</computeroutput></screen>

	<para>يوجد معاملان مطلوبان عند إنشاء الحيزات المنطقية؛ ويجب تمريرهما إلى الأمر <command>lvcreate</command> كخيارات. الأول هو اسم LV الذي سوف ننشئه ويحدد بالخيار <literal dir="ltr">-n</literal>، والثاني هو حجم LV ويعطى عمومًا بالخيار <literal dir="ltr">-L</literal>. نحتاج أيضًا إعلام الأمر باسم VG التي يطبق عليها طبعًا، وهذا هو المعامل الأخير في التعليمة.</para>

        <sidebar>
          <title><emphasis>التعمق أكثر</emphasis> خيارات <command>lvcreate</command></title>

	  <para>للأمر <command>lvcreate</command> العديد من الخيارات تسمح بضبط عملية إنشاء LV.</para>

	  <para>دعنا أولاً نشرح الخيار <literal dir="ltr">-l</literal>، الذي يسمح بتحديد حجم الحيز المنطقي كعدد من الكتل (بدلاً من استخدام الواحدات ”البشرية“ كما فعلنا في المثال السابق). هذه الكتل (التي تدعى PEs، أي <emphasis>physical extents</emphasis>، بحسب مصطلحات LVM) هي وحدات متجاورة من المساحة التخزينية في الحيزات الفيزيائية، ولا يمكن أن تقسم الواحدة منها بين الحيزات المنطقية. عندما يحتاج المرء لتحديد السعة التخزينية للحيز المنطقي بدقة أكبر، مثلاً لاستخدام كامل المساحة المتوفرة، سيكون الخيار <literal dir="ltr">-l</literal> مفضلاً على الخيار <literal dir="ltr">-L</literal> غالبًا.</para>

	  <para>من الممكن أيضاً الإشارة إلى الموقع الفيزيائي لتخزين LV، بحيث تخزن ”استطالاته“ (extents) على PV معين (مع البقاء ضمن الحيزات الفيزيائية المخصصة للـVG طبعاً). نظراً لأننا نعلم أن <filename>sdb</filename> أسرع من <filename>sdf</filename>، ربما نريد تخزين <filename>lv_base</filename> هناك إذا أردنا منح الأفضلية لمخدم قاعدة البيانات على مخدم الملفات. يصبح الأمر كالتالي: <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command>. لاحظ أن هذا الأمر قد يفشل إذا لم يحو الحيز الفيزيائي عدداً كافياً من الاستطالات الحرة. في هذا المثال، لعلنا سنحتاج إلى إنشاء <filename>lv_base</filename> قبل <filename>lv_files</filename> لتفادي هذا الموقف ― أو إلى تحرير بعض المساحة على <filename>sdb2</filename> باستخدام الأمر <command>pvmove</command>.</para>
        </sidebar>

	<para>ينتهي المطاف بالحيزات المنطقية بعد إنشائها كملفات أجهزة كتلية في <filename dir="ltr">/dev/mapper/</filename>:</para>

        <screen><computeroutput># </computeroutput><userinput>ls -l /dev/mapper</userinput>
<computeroutput>total 0
crw------- 1 root root 10, 236 Jun 10 16:52 control
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2
# </computeroutput><userinput>ls -l /dev/dm-*</userinput>
<computeroutput>brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0
brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1
brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2
</computeroutput></screen>

        <sidebar>
          <title><emphasis>ملاحظة</emphasis> التعرف الآلي على حيزات LVM</title>

          <para>عند إقلاع الحاسب، تنفذ وحدة الخدمة <filename>lvm2-activation</filename> التابعة لنظام systemd الأمر <command>vgchange -aay</command> ”لتنشيط activate“ مجموعات الحيزات: حيث يفحص الأجهزة المتوفرة؛ وتُسجَّل الأجهزة التي تمت تهيئتها كحيزات فيزيائية ضمن نظام LVM الفرعي، وتجمَع الحيزات التي تنتمي لمجموعات في مجموعاتها، ثم تنشط الحيزات المنطقية وتصبح متوفرة. لا حاجة إذاً لتحرير أي ملفات إعداد عند إنشاء أو تعديل حيزات LVM.</para>

	  <para>لكن لاحظ أن خريطة عناصر LVM ( الحيزات الفيزيائية والمنطقية، والمجموعات) تُنسَخ احتياطيًا إلى <filename dir="ltr">/etc/lvm/backup</filename>، وهذه قد تفيد في حال حدوث مشكلة (أو لاختلاس النظر تحت الغطاء).</para>
        </sidebar>

	<para>لتسهيل الأمور، يتم إنشاء اختصارات رمزيّة أيضًا في مجلدات بأسماء VGs نفسها:</para>

        <screen><computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical</userinput>
<computeroutput>total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0
# </computeroutput><userinput>ls -l /dev/vg_normal</userinput>
<computeroutput>total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</computeroutput></screen>

	<para>يمكن استخدام LVs عندها مثل أي قسم نظامي تماماً:</para>

        <screen><computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups</userinput>
<computeroutput>mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 3145728 4k blocks and 786432 inodes
Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d
[...]
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </computeroutput><userinput>mkdir /srv/backups</userinput>
<computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups</userinput>
<computeroutput># </computeroutput><userinput>df -h /srv/backups</userinput>
<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_normal-lv_backups   12G   30M   12G   1% /srv/backups
# </computeroutput><userinput>[...]</userinput>
<computeroutput>[...]
# </computeroutput><userinput>cat /etc/fstab</userinput>
<computeroutput>[...]
/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2
/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2
/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</computeroutput></screen>

	<para>من وجهة نظر التطبيقات، تم تحويل الأقسام الصغيرة العديدة إلى حيز كبير واحد بحجم 12غ.ب، وله اسم ألطف.</para>
      </section>
      <section id="sect.lvm-over-time">
        <title>‏LVM مع الزمن</title>

	<para>بالرغم من أن ميزة جمع الأقراص أو الأقسام الفيزيائية مفيدة، إلا أنها ليست الميزة الأساسية لاستخدام LVM. لا تبدو المرونة التي تحصل عليها من LVM واضحة إلا بعد مرور فترة من الزمن بشكل خاص، عندما تتغير الحاجات. في مثالنا السابق، دعنا نفترض أن هناك ملفات جديدة كبيرة يجب تخزينها، وأن الحيز المنطقي المخصص لمخدم الملفات صغير جداً عليها. بما أننا لم نستهلك كامل المساحة الحرة المتوفرة على <filename>vg_critical</filename>، يمكننا توسعة <filename>lv_files</filename>. سوف نستخدم الأمر <command>lvresize</command> لذلك الغرض، ثم نستخدم <command>resize2fs</command> لملائمة نظام الملفات مع الحجم الجديد:</para>

        <screen><computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>
<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files
# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>
<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 5.00g
# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   2   2   0 wz--n- 8.09g 2.09g
# </computeroutput><userinput>lvresize -L 7G vg_critical/lv_files</userinput>
<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).
  Logical volume lv_files successfully resized
# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>
<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 7.00g
# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>
<computeroutput>resize2fs 1.42.12 (29-Aug-2014)
Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.

# </computeroutput><userinput>df -h /srv/files/</userinput>
<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</computeroutput></screen>

        <sidebar>
          <title><emphasis>تحذير</emphasis> تحجيم نظم الملفات</title>

	  <para>لا تدعم جميع نظم الملفات التحجيم أثناء الاتصال (online resizing)؛ بالتالي يجب فصل نظام الملفات أولاً (unmount) ثم إعادة ربطه بعد إنهاء العملية. طبعاً إذا كان هناك رغبة بتصغير المساحة المخصصة لأحد الحيزات المنطقية، فيجب تقليص نظام الملفات اولاً؛ أما في حال التكبير فيكون الترتيب معكوساً: حيث يجب تكبير الحيز المنطقي قبل توسعة نظام الملفات داخله. هذا منطقي تمامًا، فلا يمكن أن يكون حجم نظام الملفات أكبر من حجم الجهاز الكتلي الذي يحويه بأي حال من الأحوال (سواء كان الجهاز قسمًا فيزيائياً أو كان حيز تخزين منطقي).</para>

	  <para>يمكن توسعة نظم الملفات ext3، وext4 و xfs دون فصل الاتصال (online)؛ أما التقليص فيحتاج الفصل عن شجرة الملفات. يسمح نظام الملفات reiserfs بالتحجيم أثناء الاتصال في الاتجاهين. أما صاحب الجلالة ext2 فلا يسمح بأي منهما، ويحتاج للفصل في جميع الحالات.</para>
        </sidebar>

	<para>يمكننا توسعة الحيز الذي يستضيف قاعدة البيانات بنفس الأسلوب، لولا أننا وصلنا لحدود المساحة المتاحة على المجموعة:</para>

        <screen><computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>
<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_base 1008M  854M  104M  90% /srv/base
# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree 
  vg_critical   2   2   0 wz--n- 8.09g 92.00m</computeroutput>
</screen>

	<para>لا مشكلة، حيث يسمح LVM بإضافة حيزات فيزيائية إلى المجموعات القائمة مسبقًا. مثلاً، ربما لاحظنا أن القسم <filename>sdb1</filename> الذي كان يستخدم خارج نظام LVM حتى الآن، كان يحتوي على أرشيفات يمكن نقلها إلى <filename>lv_backups</filename>. يمكننا الآن إعادة استخدام القسم ودمجه في مجموعة الحيزات الحالية، واستثمار بعض المساحة الحرة. هذه هي وظيفة الأمر <command>vgextend</command>. طبعاً يجب تهيئة القسم كحيز فيزيائي قبل ذلك. بعد توسيع المجموعة، يمكننا استخدام أوامر مشابهة للسابقة لتمديد الحيز المنطقي وتوسعة نظام الملفات بعد ذلك:</para>

        <screen><computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>
<computeroutput>  Physical volume "/dev/sdb1" successfully created
# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>
<computeroutput>  Volume group "vg_critical" successfully extended
# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>
<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   3   2   0 wz--n- 9.09g 1.09g
# </computeroutput><userinput>[...]</userinput>
<computeroutput>[...]
# </computeroutput><userinput>df -h /srv/base/</userinput>
<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</computeroutput></screen>

        <sidebar>
          <title><emphasis>التعمق أكثر</emphasis> LVM متقدم</title>

	  <para>يسمح LVM باستخدامات متقدمة أكثر، حيث يمكن تحديد الكثير من التفاصيل يدوياً. مثلاً، يستطيع مدير النظام تعديل حجم الكتل التي تتركب منها الحيزات الفيزيائية والمنطقية، كما يستطيع ضبط تخطيطها الفيزيائي (physical layout). من الممكن أيضًا نقل الكتل بين الحيزات الفيزيائية، لضبط الأداء بدقة مثلاً، أو تحرير PV معين عند الحاجة لإخراج القرص الفيزيائي الموافق من المجموعة (سواء لنقله إلى VG أخرى أو إزالته من LVM بالكامل) كتيبات التعليمات التي تصف الأوامر واضحة ومفصلة بشكل عام. صفحة <citerefentry><refentrytitle>lvm</refentrytitle> <manvolnum>8</manvolnum></citerefentry>‎ هي نقطة بدء جيدة.</para>
        </sidebar>
      </section>
    </section>
    <section id="sect.raid-or-lvm">
      <title>‏RAID أو LVM؟</title>

      <para>يقدم كلٌّ من RAID وLVM ميزات لا تقبل الجدل عندما يبتعد المرء عن الحالة البسيطة للحاسوب المكتبي ذي القرص الواحد حيث لا تتغير الاستخدامات مع مرور الزمن. لكن RAID وLVM يتباعدان في اتجاهين مختلفين، وتتباعد أهدافهما، ومن المقبول أن يتسائل المرء عن أي التقنيتين يجب أن يتبناها. الإجابة الأنسب ستعتمد طبعاً على الحاجات الحالية والمتوقعة.</para>

      <para>هناك عدة حالات بسيطة حيث لا تظهر فيها أي تساؤلات فعلية. إذا كان الهدف هو حماية البيانات من عطب العتاد، فالحل طبعاً هو إعداد RAID مع مصفوفة أقراص ذات فائض تخزيني، نظرًا لأن LVM لا يعالج هذه المشكلة أبداً. من جهة أخرى، إذا كان هناك حاجة لتصميم تخزيني مرن تستقل فيه الحيزات التخزينية عن المخطط الفيزيائي للأقراص، عندها RAID لا يساعد كثيراً وLVM هو الخيار الطبيعي.</para>

      <sidebar>
        <title><emphasis>ملاحظة</emphasis> إذا كان الأداء مهمًا…</title>

	<para>إذا كانت سرعة الدخل والخرج جوهرية، خصوصًا من ناحية أزمنة الوصول، فإن استخدام LVM أو RAID أو جمعهما معاً بإحدى الطرق قد يؤثر على الأداء، وأحياناً يجب أخذ هذا بعين الاعتبار عند اختيار إحدى التقنيتين. إلا أن هذه الاختلافات في الأداء صغيرة حقاً، ولا يمكن قياسها إلا في حالات قليلة. إذا كان الأداء مهمًا، فإن أكبر زيادة يمكن الحصول عليها تكون باستخدام وسائط تخزين غير ميكانيكية (سواقات الحالة الصلبة SSD – ‏<indexterm><primary>SSD</primary></indexterm><emphasis>solid-state drives</emphasis>)؛ كلفة الميغابايت في هذه الوسائط أعلى من كلفته في الأقراص الصلبة العادية، كما أن سعتها أصغر عادة، لكنها تقدم أداء باهراً للوصول العشوائي. إذا كان نمط الاستخدام يشتمل على العديد من عمليات الدخل والخرج المنتشرة على أنحاء نظام الملفات، كما في حالة قواعد البيانات التي تجرى عليها استعلامات معقدة مثلاً، فإن جدوى تشغيلها على SSD أكبر بكثير من استخدام LVM بدلاً من RAID أو العكس. يجب اتخاذ القرار في هذه الحالات اعتماداً على معايير أخرى غير السرعة، نظراً لأن موضوع الأداء يمكن معالجته بسهولة باستخدام SSD.</para>
      </sidebar>

      <para>حالة الاستخدام الثالثة الجديرة بالاهتمام هي عندما يحتاج المرء جمع قرصين في حيز تخزيني واحد، وذلك بهدف زيادة الأداء أو للحصول على نظام ملفات أكبر من سعة الأقراص المتوفرة. يمكن معالجة هذه الحالة باستخدم RAID-0 (أو حتى linear-RAID) أو باستخدام LVM. في هذه الحالة، يقع الاختيار على LVM ما لم تكن هناك قيود إضافية (الانسجام مع بقية الحواسيب إذا كانت تعتمد على RAID مثلاً). الإعداد الأولي لنظام LVM أكثر تعقيداً بقليل، ولكن المرونة الإضافية التي يوفرها تعوض هذه الزيادة الطفيفة في التعقيدات عندما تتغير المتطلبات التخزينية أو إذا دعت الحاجة لإضافة أقراص جديدة.</para>

      <para>ثم نصل طبعاً إلى حالة الاستخدام الشيقة حقاً، وهي عندما نحتاج نظاماً تخزينياً يقاوم أعطال العتاد ومرناً من ناحية توزيع الحيزات التخزينية. لا يستطيع RAID وحده ولا LVM معالجة المتطلبين معاً؛ هذه هي الحالة التي نستخدم فيها الاثنين في الوقت نفسه — أو بالأحرى، نستخدم أحدهما فوق الآخر. أكثر طريقة مستخدمة منذ وصل RAID و LVM إلى مرحلة النضج هي ضمان حماية البيانات أولاً من خلال جمع الأقراص في عدد صغير من مصفوفات RAID الكبيرة، ثم استخدام هذه المصفوفات كحيزات فيزيائية لنظام LVM؛ بعدها تقطع LVs إلى أقسام منطقية لإنشاء نظم الملفات. إن ميزة هذا الأسلوب هي أنه عندما يتعطل قرص ما، سنحتاج لإعادة بناء عدد صغير من مصفوفات RAID، وبالتالي اختصار الوقت الذي يقضيه مدير النظام في الاستعادة.</para>

      <para>لنأخذ مثلاً حقيقياً: يحتاج قسم العلاقات العامة في شركة فلكوت محطة عمل لتحرير الفيديو، لكن ميزانية القسم لا تسمح بشراء عتاد متطور بالكامل. اتخذ القرار بتفضيل العتاد المخصص لأعمال الجرافيك (الشاشة وبطاقة الفيديو)، والاكتفاء بالعتاد العادي بالنسبة لوسائط التخزين. لكن، كما هو معلوم، يحتاج الفيديو الرقمي بعض المتطلبات الخاصة فيما يتعلق بوشائط التخزين: فكمية البيانات المخزنة كبيرة، كما أن معدل النقل عند قراءة أو كتابة هذه البيانات مهم ويؤثر على الأداء الكلي للنظام (أهميته أكبر من أهمية زمن الوصول النموذجي مثلاً). يجب تلبية هذه المتطلبات باستخدام عتاد عادي، في هذه الحالة لدينا قرصين صلبين SATA سعة كل منهما 300 غيغابايت؛ يجب أيضًا أن تقاوم بيانات النظام وبعض من بيانات المستخدم أعطال العتاد، إذ يجب أن تبقى مقاطع الفيديو المحررة بأمان، لكن اللقطات (rushes) التي تنتظر التحرير أقل أهميةً، بما أنها لا تزال متوفرة على شرائط الفيديو.</para>

      <para>سوف نجمع RAID-1 و LVM معاً لإيفاء هذه الشروط. سوف نصل القرصين إلى متحكمي SATA مختلفين لتحسين الوصول المتوازي وتخفيف خطر الأعطال المتزامنة، بالتالي سوف يظهر القرصان باسمي <filename>sda</filename> و<filename>sdc</filename>. سوف نقطِّع القرصين وفق المخطط التالي:</para>

      <screen><computeroutput># </computeroutput><userinput>fdisk -l /dev/sda</userinput>
<computeroutput>
Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x00039a9f

Device    Boot     Start       End   Sectors Size Id Type
/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect
/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris
/dev/sda3        4000185 586099395 582099210 298G 5  Extended
/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect
/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect
/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</computeroutput></screen>
      <itemizedlist>
        <listitem>
	  <para>جمعنا القسمين الأولين من كل قرص (حوالي 1 غ.ب) في حيز RAID-1، هو <filename>md0</filename>. هذه المرآة ستستخدم مباشرة لتخزين نظام الملفات الجذر.</para>
        </listitem>
        <listitem>
	  <para>استخدمنا القسمين <filename>sda2</filename> و<filename>sdc2</filename> كقسمي swap، ما منحنا مساحة تبديل سعتها الكلية 2 غ.ب. ومع 1 غ.ب من الذاكرة RAM، أصبحت كمية الذاكرة المتوفرة لمحطة العمل مريحة.</para>
        </listitem>
        <listitem>
	  <para>جمعنا القسمين <filename>sda5</filename> و<filename>sdc5</filename>، كما جمعنا <filename>sda6</filename> و<filename>sdc6</filename> في حيزي RAID-1 حجم كل منهما حوالي 100 غ.ب، هما <filename>md1</filename> و<filename>md2</filename>. تمت تهيئة كل من هاتين المرآتين كحيز LVM فيزيائي، وتم تخصيصهما للمجموعة <filename>vg_raid</filename>. هذه VG تحوي تقريبًا 200 غ.ب من المساحة المؤمنة.</para>
        </listitem>
        <listitem>
	  <para>استخدمنا القسمين المتبقيين، <filename>sda7</filename> و<filename>sdc7</filename>، مباشرة بشكل حيزات فيزيائية، وخصصناهما لمجموعة حيزات أخرى تدعى <filename>vg_bulk</filename>، حيث أصبحت تحوي تقريبًا 200 غ.ب من المساحة.</para>
        </listitem>
      </itemizedlist>

      <para>بعد إنشاء VGs، يمكن تقطيعها بطريقة مرنة جداً. يجب أن نأخذ بعين الاعتبار أن LVs التي ننشئها في <filename>vg_raid</filename> ستبقى محفوظة حتى لو تعطل أحد القرصين، لكن هذا لا ينطبق على LVs التي ننشئها في <filename>vg_bulk</filename>؛ من ناحية أخرى، سوف تحجز الحيزات المنطقية في <filename>vg_bulk</filename> على القرصين على التوازي، ما يسمح بسرعات قراءة أو كتابة أكبر للملفات الكبيرة.</para>

      
      <para>إذن سوف ننشئ الحيزات المنطقية <filename>lv_usr</filename> و<filename>lv_var</filename> و<filename>lv_home</filename> على <filename>vg_raid</filename>، لتخزين نظم الملفات المقابلة لها؛ وسنستخدم حيز منطقي آخر كبير باسم <filename>lv_movies</filename> لتخزين النسخ النهائية من الأفلام بعد التحرير. سوف نقسم الـVG الأخرى إلى حيز كبير باسم <filename>lv_rushes</filename>، للبيانات القادمة مباشرة من كميرات الفيديو الرقمية، و<filename>lv_tmp</filename> للملفات المؤقتة. تحديد موقع مساحة العمل ليس خياراً واضحاً تماماً: في حين أن الأداء الجيد مطلوب لذلك القسم، هل يستحق هذا المخاطرة بخسارة العمل إذا تعطل أحد الأقراص أثناء جلسة التحرير؟ اعتماداً على إجابة ذلك السؤال، سوف ننشئ الحيز المنطقي المناسب على إحدى المجموعتين.</para>

      <para>الآن أصبح لدينا بعض الفائض يضمن لنا حماية البيانات الهامة ومرونة كبيرة في توزيع المساحة المتوفرة بين التطبيقات. على فرض أن هناك حاجة لتثبيت برمجيات جديدة لاحقًا (لتحرير المقاطع الصوتية مثلاً)، يمكن توسيع الحيز المنطقي المقابل لنظام ملفات <filename>/usr/</filename> بسهولة.</para>

      <sidebar>
        <title><emphasis>ملاحظة</emphasis> لماذا ثلاثة حيزات RAID-1؟</title>

	<para>كان يمكن إعداد حيز RAID-1 واحد فقط ليعمل كحيز فيزيائي نضع عليه <filename>vg_raid</filename>. فلم أنشأنا ثلاثة منها إذاً؟</para>

	<para>السبب وراء القسم الأول (فصل <filename>md0</filename> عن البقية) هو أمان البيانات: فالبيانات التي تكتب على مرايا RAID-1 هي نفسها على جميع الأقراص، ولذلك يمكن تجاوز طبقة RAID وربط أحد أقراص المصفوفة مباشرة. في حال مواجهة علة في النواة مثلاً، أو إذا تضررت البيانات الفوقية التي تُعرّف LVM، يمكن عندها إقلاع نظام أصغري يسمح بالوصول إلى البيانات الحساسة مثل مخطط الأقراص في حيزات RAID وLVM؛ يمكن حينئذ إعادة بناء البيانات الفوقية والوصول للملفات ثانية، بحيث يعود النظام إلى حالته النظامية.</para>

	<para>أما السبب وراء القسم الثاني (فصل <filename>md1</filename> عن <filename>md2</filename>) فهو أقل وضوحاً، والداعي له هو عدم ثقتنا بطبيعة التغييرات التي سنحتاجها في المستقبل. قد لا نعرف الحاجات التخزينية للمستخدمين بدقة عند تجميع محطة العمل أول مرة، كما يمكن أن تتغير هذه الحاجات مع مرور الزمن. في حالتنا، لا يمكننا معرفة الأحجام التخزينية اللازمة للقطات الخام (rushes) ومقاطع الفيديو المكتملة مسبقاً. إذا احتاج أحد المقاطع لعدد كبير من اللقطات، وكان أكثر من نصف VG المخصصة للحيزات المؤمنة فارغاً، يمكننا إعادة استخدام بعض المساحة غير اللازمة منها. يمكننا إزالة أحد الحيزات الفيزيائية، ولنقل <filename>md2</filename>، من <filename>vg_raid</filename> ثم نضيفه إلى <filename>vg_bulk</filename> مباشرة (إذا كانت المدة المتوقعة لإنهاء العملية قصيرة بحيث نستطيع قبول الانخفاض المؤقت في الأداء)، أو نلغي مصفوفة RAID على <filename>md2</filename> وندمج مكوناتها (<filename>sda6</filename> و<filename>sdc6</filename>) مع VG غير المؤمنة (التي ستكبر بمقدار 200 غ.ب بدلاً من 100 غ.ب)؛ بعدها يمكن توسعة الحيز المنطقي <filename>lv_rushes</filename> حسب الحاجة.</para>
      </sidebar>
    </section>
  </section>
  <section id="sect.virtualization">
    <title>الحوسبة الظاهرية</title>
    <indexterm><primary>الحوسبة الظاهرية (virtualization)</primary></indexterm> 

    <para>الحوسبة الظاهرية (virtualization) هي إحدى أهم تطورات الحوسبة في السنوات الأخيرة. يغطي المصطلح العديد من المفاهيم والتقنيات المستخدمة لمحاكاة الحواسيب الظاهرية بدرجات متفاوتة من الاستقلال عن العتاد الفعلي. يمكن لمخدم فيزيائي واحد عندها أن يستضيف العديد من الأنظمة التي تعمل في الوقت نفسه بمعزل عن بعضها. تطبيقات هذه التقنية عديدة، وهي مشتقة غالبًا من فكرة العزل: كاختبار بيئات لها إعدادات مختلفة مثلاً، أو فصل الخدمات المقدمة عبر حواسيب ظاهرية (virtual) مختلفة لزيادة الأمن.</para>

    <para>هناك الكثير من حلول الحوسبة الظاهرية، لكل منها ميزاته وعيوبه. يركز هذا الكتاب على Xen، و LXC، وKVM، لكن هناك حلول أخرى تستحق الذكر منها:</para>
    <indexterm><primary><emphasis>VMWare</emphasis></primary></indexterm>
    <indexterm><primary><emphasis>Bochs</emphasis></primary></indexterm>
    <indexterm><primary><emphasis>QEMU</emphasis></primary></indexterm>
    <indexterm><primary><emphasis>VirtualBox</emphasis></primary></indexterm>
    <indexterm><primary><emphasis>KVM</emphasis></primary></indexterm>
    <indexterm><primary><emphasis>LXC</emphasis></primary></indexterm>
    <itemizedlist>
      <listitem>
	<para>QEMU هو محاك برمجي لحاسوب كامل؛ الأداء بعيد عن السرعة التي تحصل عليها من العمل بشكل مباشر على العتاد (natively)، لكنه يسمح بتشغيل نظم تشغيل غير معدلة أو نظم تجريبية على عتاد ظاهري. كما يسمح أيضًا بمحاكاة معماريات عتادية مختلفة: مثلاً، يستطيع نظام <emphasis>amd64</emphasis> محاكاة حاسوب <emphasis>arm</emphasis>.‏ QEMU برنامج حر. <ulink type="block" url="http://www.qemu.org/" /></para>
      </listitem>
      <listitem>
	<para>Bochs هو نظام محاكاة حر آخر، لكنه يحاكي معماريات x86 فقط (i386 و amd64).</para>
      </listitem>
      <listitem>
	<para>VMWare هو نظام محاكاة احتكاري (مملوك – proprietary)؛ بما أنه أقدم الحلول المتوفرة فهو أيضًا أكثرها شهرة. يعتمد على مبادئ تشبه مبادئ QEMU. يقدم VMWare ميزات متقدمة مثل أخذ لقطة (snapshot) لحالة حاسوب ظاهري قيد العمل. <ulink type="block" url="http://www.vmware.com/" /></para>
      </listitem>
      <listitem>
        <para>VirtualBox هو نظام محاكاة معظمه برمجيات حرة (رغم أن بعض المكونات الإضافية متوفرة برخص احتكارية). للأسف فهو مصنف في قسم المشتركات ”contrib“ لأنه يحوي بعض الملفات المترجمة مسبقاً والتي لا يمكن إعادة بناؤها دون استخدام مترجم مملوك. رغم أنه أقل عمراً من VMWare ومقيد بمعماريتي i386 و amd64، إلا أنه يتضمن مع ذلك ميزة snapshot وبعض الميزات المشوقة الأخرى. <ulink type="block" url="http://www.virtualbox.org/" /></para>
      </listitem>
    </itemizedlist>
    <section id="sect.xen">
      <title>‏Xen</title>

      <para>Xen<indexterm><primary>Xen</primary></indexterm> هو حل محاكاة ”شبه ظاهرية – paravirtualization“. يقدم Xen طبقة عزل رقيقة، تدعى ”المشرف – hypervisor“، بين العتاد والأنظمة العليا؛ تعمل بمثابة مرجع يتحكم بالوصول للعتاد من الحواسيب الظاهرية. لكنها تعالج عدداً قليلاً من التعليمات، أما البقية فتنفذ مباشرة على العتاد بالنيابة عن الأنظمة الظاهرية. الميزة الأساسية هي أن مستوى الأداء لا ينخفض، والنظم تعمل بسرعات تقترب من السرعة الأصلية؛ لكن نقطة الضعف هي أن نوى نظم التشغيل التي يمكن استخدامها مع مشرف Xen يجب تعديلها لتناسب العمل على Xen.</para>

      <para>لنمض بعض الوقت في التعرف على المصطلحات. المُشرف هو أدنى طبقة، يعمل مباشرة على العتاد، بل تحت النواة حتى. يستطيع هذا المشرف تقسيم البرمجيات الأخرى إلى عدة نطاقات domains، التي يمكن اعتبارها كحواسيب ظاهرية متعددة. يدعى أحد هذه النطاقات (أول نطاق يتم تشغيله) باسم dom0، ويتمتع بدور خاص، حيث يستطيع هذا النطاق فقط التحكم بالمشرف وتنفيذ النطاقات الأخرى. تعرف هذه النطاقات الأخرى باسم domU. بكلمات أخرى، من وجهة نظر المستخدم، يقابل dom0 ”المستضيف – host“ في نظم المحاكاة الأخرى، بينما يمكن اعتبار domU على أنه ”الضيف – guest“.</para>

      <sidebar>
        <title><emphasis>ثقافة</emphasis> Xen والإصدارات المختلفة من لينكس</title>

	<para>تم تطوير Xen أساسًا كمجموعة من الترقيعات التي بقيت خارج الشجرة الرسمية، ولم تدمج في النواة لينكس. في الوقت نفسه، تطلبت عدة نظم محاكاة جديدة (بما فيها KVM) بعض الدوال العامة المتعلقة بالمحاكاة لتسهيل دمجها، وأضيفت هذه الدوال إلى النواة لينكس (التي تعرف بواجهة <emphasis>paravirt_ops</emphasis> أو <emphasis>pv_ops</emphasis>). وبما أن رقع Xen كانت تكرر بعض وظائف هذه الواجهة، لم يعد قبولها رسميًا ممكنًا.</para>

	<para>كان على Xensource، وهي الشركة وراء تطوير Xen، نقل Xen لإطار العمل الجديد هذا، حتى يمكن دمج رقع Xen في شجرة النواة لينكس الرسمية. هذا يعني الكثير من إعادة كتابة الكود، وبالرغم من أن Xensource وصلت سريعاً إلى نسخة فعالة اعتماداً على واجهة paravirt_ops، إلا أن الرقع لم تدمج إلا تدريجيًا في النواة الرسمية. تم إكمال الدمج في لينكس 3.0. <ulink type="block" url="http://wiki.xenproject.org/wiki/XenParavirtOps" /></para>

	<para>بما أن <emphasis role="distribution">جيسي</emphasis> تعتمد على الإصدار 3.16 من النواة لينكس، فإن الحزم النظامية <emphasis role="pkg">linux-image-686-pae</emphasis> و<emphasis role="pkg">linux-image-amd64</emphasis> تحوي الكود اللازم، والترقيع الخاص بالتوزيعة الذي كان لازماً مع <emphasis role="distribution">سكويز</emphasis> والنسخ السابقة من دبيان لم يعد مطلوباً. <ulink type="block" url="http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix" /></para>
      </sidebar>

      <sidebar>
        <title><emphasis>ملاحظة</emphasis> المعماريات المتوافقة مع Xen</title>

        <para>حالياً Xen متاح فقط لمعماريات i386 وamd64 وarm64 وarmhf.</para>
      </sidebar>

      <sidebar>
        <title><emphasis>ثقافة</emphasis> Xen والنوى المختلفة عن لينكس</title>

	<para>يحتاج Xen لتعديل جميع نظم التشغيل التي يريد المرء تشغيلها عليه؛ لا تتمتع جميع النوى بدرجة النضج نفسها في هذا المجال. العديد من النوى تعمل بالكامل، سواء في dom0 أو domU: مثل لينكس 3.0 وما بعد، وNetBSD 4.0 وما بعد، وOpenSolaris. أما النوى الأخرى تعمل فقط في domU. يمكنك التحقق من حالة نظم التشغيل المختلفة في ويكي Xen:‏ <ulink type="block" url="http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen" /> <ulink type="block" url="http://wiki.xenproject.org/wiki/DomU_Support_for_Xen" /></para>

	<para>لكن إذا كان Xen يستطيع الاعتماد على تعليمات العتاد المختصة بالمحاكاة (المتوفرة فقط في المعالجات الأحدث)، فيمكن تشغيل النظم غير المعدلة أيضًا في domU (بما في ذلك Windows).</para>
      </sidebar>

      <para>استخدام Xen في دبيان يحتاج ثلاثة مكونات:</para>
      <itemizedlist>
        <listitem>
	  <para>المشرف نفسه. الحزمة المناسبة هي إما <emphasis role="pkg">xen-hypervisor-4.4-amd64</emphasis> أو <emphasis role="pkg">xen-hypervisor-4.4-armhf</emphasis> أو <emphasis role="pkg">xen-hypervisor-4.4-arm64</emphasis>. حسب العتاد المستخدم.</para>
        </listitem>
        <listitem>
	  <para>نواة تعمل فوق المشرف. أي نواة أحدث من 3.0 سوف تعمل، بما في ذلك الإصدارة 3.16 المعتمدة في <emphasis role="distribution">جيسي</emphasis>.</para>
        </listitem>
        <listitem>
	  <para>معمارية i386 تحتاج أيضًا لمكتبة قياسية مع الترقيعات المناسبة للاستفادة من Xen؛ هذه متوفرة في الحزمة <emphasis role="pkg">libc6-xen</emphasis>.</para>
        </listitem>
      </itemizedlist>

      <para>لتفادي عناء اختيار هذه المكونات يدويًا، تم توفير عدد من الحزم المريحة للمستخدم (مثل <emphasis role="pkg">xen-linux-system-amd64</emphasis>)؛ كل من هذه الحزم تسحب تجميعة من حزم المشرف والنواة معروفة بتناسبها. يحضر المشرف معه أيضًا حزمة <emphasis role="pkg">xen-utils-4.4</emphasis>، التي تحوي أدوات للتحكم بالمشرف من dom0. تحضر هذه الحزمة بدورها المكتبة القياسية المناسبة. خلال تثبيت كل هذا، تنشئ سكربتات الإعداد أيضًا مدخلة جديدة في قائمة محمل الإقلاع Grub، لبدء تشغيل النواة المختارة لنطاق dom0. لكن لاحظ أن هذه المدخلة لا تكون الأولى عادة في القائمة، ولذلك لن تحدد افتراضيًا. إذا لم يكن هذا السلوك مرغوبًا، يمكن تغييره بالأوامر التالي:</para>

      <screen><computeroutput># </computeroutput><userinput>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen
</userinput><computeroutput># </computeroutput><userinput>update-grub
</userinput>
</screen>

      <para>بعد تثبيت هذه المتطلبات، يأتي دور اختبار سلوك dom0 نفسه؛ هذا يحتاج إعادة الإقلاع إلى المشرف ونواة Xen. يجب أن يقلع النظام بالأسلوب العادي، مع بعض الرسائل الإضافية على الشاشة خلال خطوات التهيئة المبكرة.</para>

      <para>الآن حان وقت تثبيت أنظمة مفيدة على نطاقات domU، باستخدام الأدوات من حزمة <emphasis role="pkg">xen-tools</emphasis>. توفر هذه الحزمة الأمر <command>xen-create-image</command>، الذي يؤتمت معظم المهمة. البارامتر الإجباري الوحيد هو <literal dir="ltr">--hostname</literal>، لإعطاء اسم للنطاق domU؛ الخيارات الأخرى هامة، لكن يمكن تخزينها في ملف الضبط <filename dir="ltr">/etc/xen-tools/xen-tools.conf</filename>، وغيابها من سطر الأوامر لا يسبب خطأ. من المهم إذاً التحقق من محتويات هذا الملف قبل إنشاء الصور، أو استخدام بارامترات إضافية عند استدعاء <command>xen-create-image</command>. نذكر من البارامترات الهامة:</para>
      <itemizedlist>
        <listitem>
	  <para><literal dir="ltr">--memory</literal>، لتحديد كمية RAM المخصصة للنظام الجديد؛</para>
        </listitem>
        <listitem>
	  <para><literal dir="ltr">--size</literal> و <literal dir="ltr">--swap</literal>، لتحديد حجم ”الأقراص الظاهرية“ المتاحة للـ domU؛</para>
        </listitem>
        <listitem>
	  <para><literal dir="ltr">--debootstrap</literal>، لتثبيت النظام الجديد مع <command>debootstrap</command>؛ في تلك الحالة، يستخدم خيار <literal dir="ltr">--dist</literal> أيضًا أغلب الأحيان (مع اسم توزيعة ما مثل <emphasis role="distribution">jessie</emphasis>).</para>

          <sidebar>
            <title><emphasis>التعمق أكثر</emphasis> تثبيت نظام آخر غير دبيان في domU</title>

	    <para>في حال تثبيت نظام تشغيل لا يعتمد على نواة لينكس، يجب الانتباه لتحديد النواة التي يجب أن يستخدمها domU، عبر استخدام الخيار <literal dir="ltr">--kernel</literal>.</para>
          </sidebar>
        </listitem>
        <listitem>
	  <para>يبين <literal dir="ltr">--dhcp</literal> أن الحصول على إعدادات الشبكة في domU يتم من خلال DHCP بينما يسمح <literal dir="ltr">--ip</literal> بتحديد عنوان IP ستاتيكي (ثابت).</para>
        </listitem>
        <listitem>
	  <para>أخيراً، يجب اختيار طريقة التخزين للصور المنشأة (التي سيراها domU على أنها أقراص صلبة). أبسط طريقة، التي تقابل الخيار <literal dir="ltr">--dir</literal>، هي إنشاء ملف على dom0 لكل جهاز يجب تقديمه للـ domU. هناك بديل للأنظمة التي تستخدم LVM، وهو استخدام الخيار <literal dir="ltr">--lvm</literal>، متبوعاً باسم مجموعة حيزات (VG)؛ عندئذ سينشئ <command>xen-create-image</command> حيزاً منطقيًا جديداً داخل تلك المجموعة، وسيكون هذا الحيز الجديد متاحاً للـ domU بشكل قرص صلب.</para>

          <sidebar>
            <title><emphasis>ملاحظة</emphasis> التخزين في domU</title>

	    <para>يمكن تصدير أقراص صلبة كاملة إلى domU، كما يمكن تصدير أقسام الأقراص، أو مصفوفات RAID أو حيزات LVM منطقية موجودة مسبقًا. لكن هذه العمليات لا يديرها الأمر <command>xen-create-image</command>، لذلك يجب تحرير ملف إعداد صورة Xen بعد إنشاءه أولاً باستخدام <command>xen-create-image</command>.</para>
          </sidebar>
        </listitem>
      </itemizedlist>

      <para>بعد تحديد هذه الخيارات، يمكننا إنشاء صورة domU:</para>

      <screen><computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</userinput>
<computeroutput>
[...]
General Information
--------------------
Hostname       :  testxen
Distribution   :  jessie
Mirror         :  http://ftp.debian.org/debian/
Partitions     :  swap            128Mb (swap)
                  /               2G    (ext3)
Image type     :  sparse
Memory size    :  128Mb
Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64
Initrd path    :  /boot/initrd.img-3.16.0-4-amd64
[...]
Logfile produced at:
         /var/log/xen-tools/testxen.log

Installation Summary
---------------------
Hostname        :  testxen
Distribution    :  jessie
MAC Address     :  00:16:3E:8E:67:5C
IP-Address(es)  :  dynamic
RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b
Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ
</computeroutput></screen>

      <para>لدينا الآن حاسوب ظاهري، لكنه لا يعمل حاليًا (وبالتالي فهو لا يشغل سوى المساحة على القرص الصلب في dom0). طبعاً يمكننا إنشاء المزيد من الصور، وربما استخدمنا بارامترات أخرى.</para>

      <para>قبل تشغيل هذه الحواسيب الظاهرية، علينا تحديد طريقة الوصول إليها. يمكن طبعاً اعتبارها حواسيب منفصلة، ونصل إليها فقط من خلال سطر أوامر النظام، لكن هذا نادراً ما يناسب نموذج الاستخدام. في معظم الأحيان، يعتبر domU كمخدم بعيد، ويتم الوصول إليه عبر الشبكة فقط. لكن من الصعب جداً إضافة بطاقة شبكة من أجل كل domU؛ ولذلك يسمح Xen بإنشاء واجهات شبكة ظاهرية، يستطيع كل نطاق أن يراها ويستعملها بالطريقة القياسية. لاحظ أن هذه البطاقات، بالرغم من أنها ظاهرية، إلا أنها غير مفيدة ما لم تتصل بأي شبكة، حتى لو كانت شبكة ظاهرية. لدى Xen عدة نماذج شبكية لهذا الغرض:</para>
      <itemizedlist>
        <listitem>
	  <para>أبسط نموذج هو النموذج الجسري <emphasis>bridge</emphasis> model؛ وفيه تعمل جميع بطاقات eth0 (في أنظمة dom0 وdomU على حد سواء) كما لو كانت موصولة مباشرة مع تحويلة إيثرنت Ethernet switch.</para>
        </listitem>
        <listitem>
	  <para>بعدها يأتي نموذج التوجيه <emphasis>routing</emphasis> model، حيث يعمل dom0 كموجه (راوتر) ما بين أنظمة domU والشبكة الخارجية (الفيزيائية).</para>
        </listitem>
        <listitem>
	  <para>أخيراً، نموذج <emphasis>NAT</emphasis>، وفيه يصل dom0 ثانية بين أنظمة domU وباقي عناصر الشبكة، لكن لا يمكن الوصول مباشرة من الخارج إلى أنظمة domU، وتمر البيانات عبر dom0 باستخدام network address translation (ترجمة عنوان الشبكة).</para>
        </listitem>
      </itemizedlist>

      <para>هذه الأنماط الثلاثة تحتاج عدداً من الواجهات ذات المسميات الغريبة، مثل <filename dir="ltr">vif*</filename>، و<filename dir="ltr">veth*</filename>، ‏<filename dir="ltr">peth*</filename> وأيضًا <filename>xenbr0</filename>. يرتب مشرف Xen هذه الواجهات في التخطيط الذي يعرفه المستخدم، حيث يتم التحكم بأدوات من فضاء المستخدم (user-space tools). سوف نقتصر على شرح النموذج الجسري، بما أن نموذج NAT ونموذج التوجيه يناسبان بعض الحالات الخاصة فقط.</para>

      <para>الإعداد القياسي لحزم Xen لا يؤثر على إعداد الشبكة للنظام. لكن خدمة <command>xend</command> معدّة لدمج الواجهات الشبكية الظاهرية في أي جسر شبكة سابق (يأخذ <filename>xenbr0</filename> الأولوية إذا كان هناك أكثر من جسر واحد). علينا إذاً إعداد جسر في <filename dir="ltr">/etc/network/interfaces</filename> (وهذا يحتاج تثبيت حزمة <emphasis role="pkg">bridge-utils</emphasis>، ولهذا السبب توصي بها حزمة <emphasis role="pkg">xen-utils-4.4</emphasis>) لاستبدال المدخلة السابقة eth0:</para>

      <programlisting>auto xenbr0
iface xenbr0 inet dhcp
    bridge_ports eth0
    bridge_maxwait 0
    </programlisting>

      <para>بعد إعادة التشغيل للتأكد أن الجسر يُنشَأ آليَّاً، يمكننا الآن تشغيل domU باستخدام أدوات التحكم بـXen، بالأخص الأمر <command>xl</command>. يسمح هذا الأمر بإجراء العديد من التعديلات على النطاقات، مثل سردها أو تشغيلها وإيقافها.</para>

      <screen><computeroutput># </computeroutput><userinput>xl list</userinput>
<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   463     1     r-----      9.8
# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>
<computeroutput>Parsing config from /etc/xen/testxen.cfg
# </computeroutput><userinput>xl list</userinput>
<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   366     1     r-----     11.4
testxen                                      1   128     1     -b----      1.1</computeroutput></screen>

      <sidebar>
        <title><emphasis>أدوات</emphasis> اختيار أدوات إدارة أجهزة Xen الظاهرية</title>
        <indexterm><primary><command>xm</command></primary></indexterm>
        <indexterm><primary><command>xe</command></primary></indexterm>
        <para>في دبيان 7 والإصدارات الأقدم، كانت <command>xm</command> هي الأداة النصية المعيارية لإدارة أجهزة Xen الظاهرية. لكنها استبدلت الآن بالأداة <command>xl</command> المتوافقة مع الأداة القديمة بشكل كبير. لكن هناك أدوات أخرى متاحة: من الأدوات البديلة هناك <command>virsh</command> التابعة لمشروع libvirt و<command>xe</command> من XAPI (النسخة التجارية من Xen التي تقدمها XenServer).</para>
      </sidebar>

      <sidebar>
        <title><emphasis>تحذير</emphasis> domU واحد فقط لكل صورة!</title>

	<para>مع أنه من الممكن طبعاً تشغيل أكثر من domU معاً على التوازي، إلا أن كل واحد منهم يحتاج استخدام صورة خاصة به، بما أن كل واحد من domU يعتقد أنه يعمل على عتاد خاص به (بغض النظر عن الجزء الصغير من النواة الذي يتخاطب مع المشرف). على الأخص، لا يمكن لنظامي domU يعملان في الوقت نفسه أن يتشاركا المساحة التخزينية. على أية حال، إذا كانت أنظمة domU لن تعمل في الوقت نفسه، فمن الممكن إعادة استخدام قسم swap ذاته، أو القسم الذي يحوي نظام الملفات <filename dir="ltr">/home</filename>.</para>
      </sidebar>

      <para>لاحظ أن النطاق <filename>testxen</filename> يستهلك ذاكرة حقيقية من الـRAM المتاحة للنطاق dom0، وليست ذاكرة ظاهرية. يجب أخذ الحيطة إذن عند بناء مخدم لاستضافة نسخ Xen، وتزويده بذاكرة فيزيائية مناسبة.</para>

      <para>ڤوالا! آلتنا الظاهرية قيد الإقلاع. يمكننا الوصول إليها بإحدى طريقتين. الطريقة المعتادة هي الاتصال بها ”عن بعد“ عبر الشبكة، كما كنا سنتصل بأي حاسب حقيقي؛ هذا يحتاج عادة مخدم DHCP أو بعض إعدادات DNS. الطريقة الأخرى، ولعلها الطريقة الوحيدة إذا كانت إعدادات الشبكة غير صحيحة، هي استخدام طرفية <filename>hvc0</filename>، باستخدام الأمر <command>xl console</command>:</para>

      <screen><computeroutput># </computeroutput><userinput>xl console testxen</userinput>
<computeroutput>[...]

Debian GNU/Linux 8 testxen hvc0

testxen login: </computeroutput></screen>

      <para>بعدها يمكنك بدء جلسة، كما لو كنت تجلس وراء لوحة مفاتيح الحاسب الظاهري. يتم الانفصال عن هذه الطرفية بالمفتاحين <keycombo action="simul"><keycap>Control</keycap> <keycap>]</keycap></keycombo>‎.</para>

      <sidebar>
        <title><emphasis>تلميح</emphasis> الوصول للطرفية مباشرة</title>

	<para>أحياناً يرغب المرء بتشغيل domU والوصول إلى طرفية النظام فوراً؛ ولذلك يقبل الأمر <command>xl create</command> الخيار <literal dir="ltr">-c</literal>. تشغيل domU مع هذا الخيار سوف يعرض كل الرسائل مع إقلاع النظام.</para>
      </sidebar>

      <sidebar>
        <title><emphasis>أدوات</emphasis> OpenXenManager</title>

	<para>OpenXenManager (من الحزمة <emphasis role="pkg">openxenmanager</emphasis>) هي واجهة رسومية تسمح بإدارة نطاقات Xen عن بعد بالاستفادة من Xen API. تستطيع هذه الواجهة إذن التحكم بنطاقات Xen عن بعد، وهي توفر معظم مزايا الأمر <command>xl</command>.</para>
      </sidebar>

      <para>بعد أن يعمل domU، يمكن استخدامه مثل أي مخدم آخر (بما أنه نظام غنو/لينكس في النهاية). لكن بما أنه حاسب ظاهري فهذه الحالة تسمح ببعض المزايا الإضافية. مثلاً، يمكن إيقاف عمل domU مؤقتاً ثم استكماله، بالأمرين <command>xl pause</command> و<command>xl unpause</command>. لاحظ أن الذاكرة المخصصة للنطاق domU تبقى محجوزة أثناء الإيقاف المؤقت، رغم أنه لا يستهلك أي طاقة حسابية من المعالج. الأمران <command>xl save</command> و<command>xl restore</command> جديران بالاهتمام أيضاً: حفظ domU يحرر الموارد التي كان يستهلكها، بما في ذلك ذاكرة RAM. لا يلاحظ domU عند استعادته (أو استكمال عمله) أي شيء إلا مرور الزمن. إذا كان domU يعمل عند إيقاف تشغيل dom0، فسوف تحفظ سكربتات الحزمة حالة domU آلياً، وتستعيدها عند الإقلاع التالي. هذا يؤدي طبعاً للمتاعب التي تظهر عادة عند إسبات الحاسب المحمول. على سبيل المثال؛ إذا تعلق domU لفترة طويلة، فقد تلغى اتصالاته الشبكية. لاحظ أيضاً أن Xen حتى الآن غير متوافق مع شريحة واسعة من واجهة ACPI لإدارة الطاقة، ما يحول دون إمكانية إسبات النظام المستضيف (dom0).</para>

      <sidebar>
        <title><emphasis>توثيق</emphasis> خيارات <command>xl</command></title>

	<para>تحتاج معظم أوامر <command>xl</command> الفرعية إلى متغير واحد أو أكثر، غالباً هي اسم domU. هذه المتغيرات مشروحة بشكل جيد في صفحة التعليمات <citerefentry><refentrytitle>xl</refentrytitle> <manvolnum>1</manvolnum></citerefentry>‎.</para>
      </sidebar>

      <para>يمكن إيقاف أو إعادة تشغيل domU إما من داخل domU نفسه (بالأمر <command>shutdown</command>) أو من dom0، بالأمر <command>xl shutdown</command> أو <command>xl reboot</command>.</para>

      <sidebar>
        <title><emphasis>التعمق أكثر</emphasis> خيارات Xen المتقدمة</title>

	<para>يملك Xen ميزات أكثر بكثير مما يمكننا شرحه في هذه المقاطع القليلة. على وجه الخصوص، النظام ديناميكي جداً، ويمكن تعديل العديد من بارامترات النطاق (مثل كمية الذاكرة المخصصة، الأقراص الصلبة المرئية، سلوك جدولة المهام، وغيرها) أثناء عمل النطاق. بل يمكن أيضًا تهجير domU بين المخدمات دون إيقاف تشغيله، ودون انقطاع اتصاله عن الشبكة! المصدر الرئيسي للمعلومات لجميع هذه المزايا المتقدمة هو توثيق Xen الرسمي. <ulink type="block" url="http://www.xen.org/support/documentation.html" /></para>
      </sidebar>
    </section>
    <section id="sect.lxc">
      <title>‏LXC</title>
      <indexterm><primary>LXC</primary></indexterm> 

      <para>بالرغم من أن LXC يستخدم لبناء ”حواسيب ظاهرية“، إلا أن LXC –إذا تحرينا الدقة– ليس نظام محاكاة، بل هو نظام لعزل مجموعات من العمليات عن بعضها مع أنها تعمل على نفس الحاسب المستضيف. يستفيد هذا النظام من مجموعة من التطورات الحديثة في النواة لينكس، التي تعرف باسم مجموعات التحكم—<emphasis>control groups</emphasis>، التي تسمح لعدة زمر مختلفة من العمليات التي تدعى ”المجموعات“ برؤية بعض مظاهر النظام الكلي بشكل مختلف. من أبرز هذه المظاهر هي أرقام تعريف العمليات PIDs، وإعدادات الشبكة، ونقاط الربط في نظام الملفات. لا تستطيع أي مجموعة عمليات معزولة مثل هذه الوصول بأي شكل إلى العمليات الأخرى في النظام، كما يمكن تقييد وصولها إلى نظام الملفات بجزء فرعي محدد. يمكن لها أن تملك واجهة شبكية وجدول توجيه خاصين بها، ويمكن ضبطها حتى ترى مجموعة جزئية فقط من الأجهزة المتاحة المتصلة بالنظام.</para>

      <para>يمكن جمع هذه المزايا لعزل عائلة كاملة من العمليات بدءاً من العملية <command>init</command>، وستشبه المجموعة الناتجة حاسوباً ظاهرياً. الاسم الرسمي لهذا الوضع هو ”حاوية—container“ (ومن هنا جاء اسم LXC: ‏<emphasis>LinuX Containers</emphasis>)، لكن الفرق الهام بينها وبين الحواسيب الظاهرية ”الحقيقية“ التي يقدمها Xen أو KVM هو عدم وجود نواة ثانية؛ فالحاوية تستخدم نواة النظام نفسها تماماً. ينطوي هذا على محاسن ومساوئ: من المزايا الأداء الممتاز لعدم وجود عبئ حقيقي، والواقع أن النواة ترى جميع العمليات الجارية في النظام، وبالتالي فإن جدولة المهام ستكون أكثر فعالية مما لو استخدمنا نواتين مستقلتين وكل منهما ستجدول مجموعة مختلفة من المهام. أول العيوب هو استحالة استخدام نواة مختلفة في الحاوية (سواء نسخة مختلفة من لينكس أو نظام تشغيل مختلف بالكامل).</para>

      <sidebar>
        <title><emphasis>ملاحظة</emphasis> حدود العزل في LXC</title>

	<para>حاويات LXC لا توفر درجة العزل التي تحصل عليها عند استخدام محاكيات أو حلول حوسبة ظاهرية أثقل. على وجه الخصوص:</para>
        <itemizedlist>
          <listitem>
	    <para>بما أن النواة مشتركة بين النظام المستضيف والحاويات، فإن العمليات المحجوزة في الحاويات ستبقى تصل لرسائل النواة، ما قد يؤدي لتسرب المعلومات إذا بثت الحاوية الرسائل؛</para>
          </listitem>
          <listitem>
	    <para>للأسباب ذاتها، إذا تم اختراق حاوية وتم استغلال ثغرة في النواة، فقد تتأثر الحاويات الأخرى أيضًا؛</para>
          </listitem>
          <listitem>
	    <para>في نظام الملفات، تتحقق النواة من الصلاحيات وفقاً للمعرفات العددية للمستخدمين والمجموعات؛ وربما كانت هذه المعرفات تشير لمستخدمين ومجموعات مختلفة حسب الحاوية، ويجب أخذ هذا بعين الاعتبار عند مشاركة أجزاء قابلة للكتابة من نظام الملفات بين عدد من الحاويات.</para>
          </listitem>
        </itemizedlist>
      </sidebar>

      <para>بما أننا نتعامل مع تقنية عزل وليست محاكاة وحسب، فإن إعداد حاويات LXC أعقد من تشغيل مثبت دبيان على جهاز ظاهري. سوف نشرح بعض المتطلبات الأولية، ثم نتجه إلى إعداد الشبكة؛ وبعدها سوف نتمكن من إنشاء النظام الذي سيعمل ضمن الحاوية.</para>
      <section>
        <title>الخطوات الأولية</title>

	<para>تحوي الحزمة <emphasis role="pkg">lxc</emphasis> الأدوات اللازمة لتشغيل LXC، ويجب تثبيتها إذن.</para>

	<para>يحتاج LXC أيضاً لنظام مجموعات التحكم <emphasis>control groups</emphasis> للإعداد، وهو نظام ملفات ظاهري يتم ربطه على <filename dir="ltr">/sys/fs/cgroup</filename>. بما أن دبيان 8 قد انتقلت إلى systemd، الذي يعتمد أيضاً على مجموعات التحكم، فهذا يتم تلقائياً أثناء الإقلاع دون الحاجة لأي عمليات إضافية.</para>
      </section>
      <section id="sect.lxc.network">
        <title>إعداد الشبكة</title>

	<para>الهدف من تثبيت LXC هو إعداد أجهزة ظاهرية؛ وفي حين أننا نستطيع تركها معزولة عن الشبكة طبعاً، والتخاطب معها عبر نظام الملفات فقط، إلا أن معظم حالات الاستخدام تحتاج إعطاء الحاويات وصولاً محدوداً للشبكة على الأقل. في الحالة النموذجية، كل حاوية سيكون لها واجهة شبكية ظاهرية، تتصل بالشبكة الحقيقية عبر جسر. يمكن وصل هذه الواجهة الظاهرية إما مباشرة مع الواجهة الشبكية الفيزيائية للمستضيف (وفي تلك الحالة تتصل الحاوية مباشرة بالشبكة)، أو مع واجهة ظاهرية أخرى معرفة لدى المستضيف (ويمكن للمستضيف بعدها توجيه حركة الشبكة أو حجبها). في كلا الحالتين، سوف نحتاج للحزمة <emphasis role="pkg">bridge-utils</emphasis>.</para>

	<para>أبسط حالة تتلخص بتحرير <filename dir="ltr">/etc/network/interfaces</filename>، ونقل إعدادات الواجهة الفيزيائية (<literal>eth0</literal> مثلاً) إلى واجهة جسرية (عادة <literal>br0</literal>)، وإعداد الوصلة بينهما. على سبيل المثال، إذا كان ملف إعداد الواجهة الشبكية في البداية يحوي مدخلات تشبه ما يلي:</para>

        <programlisting>auto eth0
iface eth0 inet dhcp</programlisting>

	<para>فيجب تعطيلها واستبدالها بالتالي:</para>

        <programlisting>#auto eth0
#iface eth0 inet dhcp

auto br0
iface br0 inet dhcp
  bridge-ports eth0</programlisting>

	<para>إن نتيجة هذا الإعداد ستشبه ما نحصل عليه لو كانت الحاويات أجهزة تتصل بشبكة المستضيف الفيزيائية نفسها. يدير الإعداد ”الجسري“ حركة إطارات الإيثرنت بين جميع الواجهات المجسَّرة، بما فيها الواجهة الفيزيائية <literal>eth0</literal> بالإضافة للواجهات الظاهرية المعرفة في الحاويات.</para>

	<para>في الحالات التي لا يمكن فيها استخدام هذا الإعداد (مثلاً إذا لم يكن هناك مجال لتعيين عناوين IP عامة للحاويات)، سيتم إنشاء واجهة <emphasis>tap</emphasis> ظاهرية ووصلها مع الجسر. عندها يصبح مخطط الشبكة الموافق لهذا الإعداد هو كأن المستضيف له بطاقة شبكة إضافية متصلة بتحويلة (switch) منفصلة، والحاويات تتصل أيضًا بتلك التحويلة. على المستضيف عندها العمل كبوابة للحاويات إذا كانت تريد التواصل مع العالم الخارجي.</para>

	<para>هذا الإعداد ”الغني“ يحتاج –بالإضافة إلى حزمة <emphasis role="pkg">bridge-utils</emphasis>– إلى الحزمة <emphasis role="pkg">vde2</emphasis>؛ عندئذ يصبح ملف <filename dir="ltr">/etc/network/interfaces</filename> كما يلي:</para>

        <programlisting># Interface eth0 is unchanged
auto eth0
iface eth0 inet dhcp

# Virtual interface 
auto tap0
iface tap0 inet manual
  vde2-switch -t tap0

# Bridge for containers
auto br0
iface br0 inet static
  bridge-ports tap0
  address 10.0.0.1
  netmask 255.255.255.0
</programlisting>

	<para>بعدها يمكن إعداد الشبكة إما ستاتيكيًا في الحاويات، أو ديناميكيًا باستخدام مخدم DHCP يعمل على المستضيف. إذا استخدم مخدم DHCP فيجب إعداده لإجابة الطلبات على الواجهة <literal>br0</literal>.</para>
      </section>
      <section>
        <title>إعداد النظام</title>

	<para>دعنا الآن نضبط نظام الملفات الذي ستستخدمه الحاوية. بما أن هذا ”الجهاز الظاهري“ لن يعمل على العتاد مباشرة، فيجب إجراء بعض التعديلات على نظام الملفات حتى يتناسب مع تنظيم أنظمة الملفات القياسية، خصوصاً بالنسبة للنواة والأجهزة والطرفيات. لحسن الحظ، تحوي <emphasis role="pkg">lxc</emphasis> سكربتات تؤتمت معظم عملية الضبط هذه. مثلاً، يمكن استخدام الأوامر التالية (التي تحتاج الحزمتين <emphasis role="pkg">debootstrap</emphasis> و <emphasis role="pkg">rsync</emphasis>) لتثبيت حاوية دبيان:</para>

        <screen><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-create -n testlxc -t debian
</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap
Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... 
Downloading debian minimal ...
I: Retrieving Release 
I: Retrieving Release.gpg 
[...]
Download complete.
Copying rootfs to /var/lib/lxc/testlxc/rootfs...
[...]
Root password is 'sSiKhMzI', please change !
root@mirwiz:~# </computeroutput>
        </screen>

	<para>لاحظ أن إنشاء نظام الملفات يتم أولاً في <filename dir="ltr">/var/cache/lxc</filename>، ثم ينقل إلى المجلد الوجهة. هذا يسمح بإنشاء حاويات متطابقة أسرع بكثير، نظراً لأنك تحتاج للنسخ فقط لا أكثر.</para>

	<para>لاحظ أيضًا أن سكربت إنشاء قالب دبيان يقبل خيار <option dir="ltr">--arch</option> لتحديد معمارية النظام الذي سيتم تثبيته وخيار <option dir="ltr">--release</option> إذا كنت تريد تثبيت إصدار آخر غير الإصدار المستقر الحالي من دبيان. يمكنك أيضًا ضبط متغير البيئة <literal>MIRROR</literal> ليشير إلى مرآة دبيان محلية.</para>

	<para>يحوي نظام الملفات المنشأ حديثاً نظام دبيان أصغري، ولا تملك الحاوية افتراضياً أي واجهة شبكية (عدا واجهة loopback). بما أن هذا غير مرغوب، سوف نعدل ملف إعداد الحاوية (<filename dir="ltr">/var/lib/lxc/testlxc/config</filename>) ونضيف بضعة مدخلات <literal dir="ltr">lxc.network.*</literal>:</para>

        <programlisting>lxc.network.type = veth
lxc.network.flags = up
lxc.network.link = br0
lxc.network.hwaddr = 4a:49:43:49:79:20
</programlisting>

	<para>هذه المدخلات تعني، على الترتيب، أنه سيتم إنشاء واجهة شبكية ظاهرية في الحاوية؛ وسيتم تنشيطها آليًا كلما تم تشغيل تلك الحاوية؛ وأنها ستتصل تلقائياً بالجسر <literal>br0</literal> على المستضيف؛ وأن عنوان MAC الخاص بها سيكون كما هو محدد. إذا كانت هذه المدخلة الأخيرة ناقصة أو معطلة، سيتم توليد عنوان MAC عشوائي.</para>

	<para>من المدخلات المفيدة أيضًا التي يمكن إضافتها لهذا الملف هي تعيين اسم المستضيف hostname:</para>

<programlisting>lxc.utsname = testlxc
</programlisting>

      </section>
      <section>
        <title>تشغيل الحاوية</title>

	<para>الآن وبعد أن أصبحت صورة الجهاز الظاهري جاهزة، دعنا نشغل الحاوية:</para>

        <screen role="scale" width="94"><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-start --daemon --name=testlxc
</userinput><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-console -n testlxc
</userinput><computeroutput>Debian GNU/Linux 8 testlxc tty1

testlxc login: </computeroutput><userinput>root</userinput><computeroutput>
Password: 
Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
root@testlxc:~# </computeroutput><userinput>ps auxwf</userinput>
<computeroutput>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init
root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald
root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D
root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux
root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux
root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux
root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     
root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \_ -bash
root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \_ ps auxfw
root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102
root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e
root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux
root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux
root@testlxc:~# </computeroutput></screen>

	<para>نحن الآن داخل الحاوية؛ ووصولنا إلى العمليات مقيد بالعمليات التي بدأت من داخل الحاوية نفسها، كما أن وصولنا إلى نظام الملفات مقيد إلى المجموعة الجزئية المعينة لهذه الحاوية من نظام الملفات الكامل (<filename dir="ltr">/var/lib/lxc/testlxc/rootfs</filename>). يمكننا الخروج من الطرفية باستخدام <keycombo action="simul"><keycap>Control</keycap> <keycap>a</keycap></keycombo> <keycombo><keycap>q</keycap></keycombo>.</para>

	<para>لاحظ أننا بدأنا الحاوية كعملية في الخلفية، بفضل الخيار <option dir="ltr">--daemon</option> للأمر <command>lxc-start</command>. يمكننا مقاطعة الحاوية بالأمر <command>lxc-stop --name=testlxc</command>.</para>

	<para>تحوي الحزمة <emphasis role="pkg">lxc</emphasis> سكربت تهيئة يستطيع تشغيل حاوية واحدة أو أكثر آلياً عند إقلاع المستضيف (يعتمد السكربت على أمر <command>lxc-autostart</command> الذي يشغل كل الحاويات التي يكون خيار <literal>lxc.start.auto</literal> فيها مضبوطاً على القيمة 1). يمكن التحكم بدقة أكبر بترتيب التشغيل من خلال <literal>lxc.start.order</literal> و<literal>lxc.group</literal>: افتراضياً، يبدأ السكربت أولاً بتشغيل الحاويات التي تنتمي للمجموعة <literal>onboot</literal> ثم الحاويات التي لا تنتمي لأي مجموعة. وفي كلا الحالتين، يتحدد الترتيب فيما بين أعضاء المجموعة الواحدة من خلال الخيار <literal>lxc.start.order</literal>.</para>

        <sidebar>
          <title><emphasis>التعمق أكثر</emphasis> المحاكاة العملاقة</title>

	  <para>بما أن LXC هو نظام عزل خفيف جداً، يمكن تكييفه للاستضافة الكبيرة للعديد من المخدمات الظاهرية. لعل إعداد الشبكة سيكون أعقد بقليل مما شرحناه هنا، لكن الإعداد ”الغني“ باستخدام واجهات <literal>tap</literal> و<literal>veth</literal> يجب أن يكون كافيًا في العديد من الحالات.</para>

	  <para>ربما كان مناسباً أيضًا مشاركة أجزاء من نظام الملفات، مثل <filename dir="ltr">/usr</filename> و<filename dir="ltr">/lib</filename>، لتفادي تكرار البرمجيات المشتركة بين عدة حاويات. هذا يحقق عادة باستخدام مدخلات <literal>lxc.mount.entry</literal> في ملف إعداد الحاويات. هناك أثر جانبي جميل هنا وهو أن العمليات ستستهلك ذاكرة أقل في هذه الحالة، لأن النواة تقدر على اكتشاف البرامج المشتركة. عندئذ يمكن تخفيض الكلفة الهامشية لإضافة حاوية جديدة للمساحة التخزينية المخصصة لبياناتها، والعمليات القليلة الإضافية التي ستديرها النواة وتجدولها.</para>

	  <para>لم نشرح كافة الخيارات المتاحة، بالطبع؛ يمكنك الحصول على معلومات أوسع من صفحات الكتيبات <citerefentry> <refentrytitle>lxc</refentrytitle> <manvolnum>7</manvolnum> </citerefentry>‎ و<citerefentry> <refentrytitle>lxc.container.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry>‎ والصفحات التي تشيران إليها.</para>
        </sidebar>
      </section>
    </section>
    <section>
      <title>المحاكاة في KVM</title>
      <indexterm><primary>KVM</primary></indexterm>

      <para>KVM، التي ترمز إلى <emphasis>Kernel-based Virtual Machine</emphasis>، هي أولاً وأخيراً وحدة من وحدات النواة توفر معظم البنية التحتية التي يمكن أن يستفيد منها برنامج المحاكاة، لكنها ليست محاكيًا. التحكم الفعلي بالمحاكاة يتم من خلال تطبيق مبني على QEMU. لا تقلق إذا كان هذا القسم يذكر أوامر تبدأ ب <command dir="ltr">qemu-*</command>: نحن لا نزال نتحدث عن KVM.</para>

      <para>لقد دمجت KVM منذ البداية في النواة لينكس، بخلاف نظم المحاكاة الأخرى. اختر مطوروها استغلال مجموعات تعليمات المعالج المخصصة للمحاكاة (Intel-VT و AMD-V)، ما جعل KVM خفيفة الوزن، وأنيقة وغير شرهة للموارد. الجانب السلبي، طبعاً، هو أن KVM لا تعمل إلا على الحواسيب التي تملك معالجات مناسبة. بالنسبة للحواسيب ذات معمارية x86، يمكنك التأكد أن المعالج مناسب عن طريق البحث عن ”vmx“ أو ”svm“ في أعلام المعالج المذكورة في <filename dir="ltr">/proc/cpuinfo</filename>.</para>

      <para>مع دعم Red Hat النشط لتطوير KVM، فقد أصبحت بشكل أو بآخر المرجع في الحوسبة الظاهرية في لينكس.</para>
      <section>
        <title>الخطوات الأولية</title>
        <indexterm><primary><command>virt-install</command></primary></indexterm>

	<para>بعكس الأدوات الأخرى مثل VirtualBox، لا تقدم KVM نفسها أي واجهة للمستخدم لإنشاء وإدارة الحواسيب الظاهرية. تقدم حزمة <emphasis role="pkg">qemu-kvm</emphasis> برنامجاً تنفيذيًا قادراً على تشغيل حاسوب ظاهري، بالإضافة إلى سكربت تهيئة يحمل وحدات النواة المناسبة.</para>
        <indexterm><primary>libvirt</primary></indexterm>
        <indexterm><primary><emphasis role="pkg">virt-manager</emphasis></primary></indexterm>

	<para>لحسن الحظ، توفر Red Hat أيضًا مجموعة أخرى من الأدوات لمعالجة هذه المشكلة، من خلال تطوير المكتبة <emphasis>libvirt</emphasis> وأدوات <emphasis>virtual machine manager</emphasis> المقترنة بها. تسمح libvirt بإدارة الحواسيب الظاهرية بأسلوب قياسي، بغض النظر عن نظام المحاكاة المستخدم وراء الكواليس (حاليًا هناك دعم لنظم QEMU، وKVM، وXen، وLXC، وOpenVZ، وVirtualBox، وVMWare وأيضاً UML). ‏<command>virtual-manager</command> هي واجهة رسومية تعتمد على libvirt لإنشاء وإدارة الحواسيب الظاهرية.</para>
        <indexterm><primary><emphasis role="pkg">virtinst</emphasis></primary></indexterm>

	<para>سوف نثبت الحزم المطلوبة أولاً، بالأمر <command>apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</command>. تقدم الحزمة <emphasis role="pkg">libvirt-bin</emphasis> الخدمة <command>libvirtd</command>، التي تسمح بالإدارة (البعيدة ربما) للحواسيب الظاهرية التي تعمل على المستضيف، وتشغيل الحواسيب الظاهرية المناسبة عند إقلاع المستضيف. بالإضافة لذلك، توفر هذه الحزمة أداة <command>virsh</command> ذات الواجهة النصية، التي تسمح بالتحكم بالإجهزة التي تديرها خدمة <command>libvirtd</command>.</para>

	<para>تقدم الحزمة <emphasis role="pkg">virtinst</emphasis> الأداة <command>virt-install</command>، التي تسمح بإنشاء الحواسيب الظاهرية من سطر الأوامر. أخيراً، يسمح <emphasis role="pkg">virt-viewer</emphasis> بالوصول إلى الطرفية الرسومية للحاسب الظاهري.</para>
      </section>
      <section>
        <title>إعداد الشبكة</title>

	<para>كما في Xen و LXC، أكثر الخيارات شيوعاً عند إعداد الشبكة هو استخدام جسر يجمع الواجهات الشبكية لعدة حواسيب ظاهرية (انظر <xref linkend="sect.lxc.network" />).</para>

	<para>أو يمكن، كما هو الإعداد الافتراضي الذي تقدمه KVM، إعطاء الحاسب الظاهري عنوناً داخلياً (ضمن المجال 192.168.122.0/24)، وإعداد NAT حتى يتمكن الجهاز الظاهري من الوصول إلى الشبكة الخارجية.</para>

	<para>سنفترض في تتمة هذا القسم أن المستضيف لديه واجهة فيزيائية <literal>eth0</literal> وجسر <literal>br0</literal>، وأن الأولى متصلة مع الأخير.</para>
      </section>
      <section>
        <title>التثبيت باستخدام <command>virt-install</command></title>
        <indexterm><primary><command>virt-install</command></primary></indexterm>

	<para>يشبه إنشاء حاسب ظاهري تثبيت النظم العادية كثيراً، عدا أن مواصفات الحواسب الظاهري تُحدَّد في أمر طويل جداً.</para>

	<para>عملياً، هذا يعني أننا سنستخدم برنامج تثبيت دبيان، من خلال إقلاع الحاسب الظاهري من سواقة DVD-ROM ظاهرية ترتبط مع صورة DVD دبيان مخزنة على النظام المستضيف. سوف يُصدِّر الجهاز الظاهري طرفيته الرسومية عبر بروتوكول VNC (انظر <xref linkend="sect.remote-desktops" /> للتفاصيل)، ما يسمح لنا بالتحكم بعملية التثبيت.</para>

	<para>نحتاج أولاً إخبار libvirtd عن موقع تخزين صور الأقراص، ما لم يكن الموقع الافتراضي (<filename>/var/lib/libvirt/images/</filename>) مناسباً.</para>

        <screen><computeroutput>root@mirwiz:~# </computeroutput><userinput>mkdir /srv/kvm</userinput>
<computeroutput>root@mirwiz:~# </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>
<computeroutput>Pool srv-kvm created

root@mirwiz:~# </computeroutput></screen>

        <sidebar>
          <title><emphasis>تلميح</emphasis> أضف مستخدمك إلى المجموعة libvirt</title>
          <para>تفترض كافة الأمثلة في هذا القسم أنك تنفذ الأوامر بصلاحيات الجذر. وبالتالي، إذا أردت التحكم بخدمة libvirt محلية، عليك إما أن تكون الجذر أو أن تكون عضواً في المجموعة <literal>libvirt</literal> (الحالة الافتراضية هي أنك لا تنتمي لهذه المجموعة). فإذا أردت تفادي استخدام صلاحيات الحذر كثيراً، يمكنك إضافة نفسك إلى المجموعة <literal>libvirt</literal> وتنفيذ الأوامر المختلفة بصلاحيات مستخدمك العادي.</para>
        </sidebar>

	<para>دعنا نبدأ الآن عملية تثبيت الحاسب الظاهري، وإلقاء نظرة قريبة على أهم خيارات <command>virt-install</command>. هذا الأمر يسجل الجهاز الظاهري وبارامتراته عند libvirtd، ثم يشغله حتى نتابع عملية التثبيت.</para>

        <screen><computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id="virtinst.connect"></co>
               --virt-type kvm           <co id="virtinst.type"></co>
               --name testkvm            <co id="virtinst.name"></co>
               --ram 1024                <co id="virtinst.ram"></co>
               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <co id="virtinst.disk"></co>
               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <co id="virtinst.cdrom"></co>
               --network bridge=br0      <co id="virtinst.network"></co>
               --vnc                     <co id="virtinst.vnc"></co>
               --os-type linux           <co id="virtinst.os"></co>
               --os-variant debianwheezy
</userinput><computeroutput>
Starting install...
Allocating 'testkvm.qcow'             |  10 GB     00:00
Creating domain...                    |    0 B     00:00
Guest installation complete... restarting guest.
</computeroutput></screen>
        <calloutlist>
          <callout arearefs="virtinst.connect">
	    <para>يحدد خيار <literal dir="ltr">--connect</literal> ”المشرف“ المستخدم. شكله هو شكل URL يحوي اسم نظام المحاكاة (<literal dir="ltr">xen://</literal>‏، <literal dir="ltr">qemu://</literal>‏، <literal dir="ltr">lxc://</literal>‏، <literal dir="ltr">openvz://</literal>‏، <literal dir="ltr">vbox://</literal>، وهكذا) والحاسب الذي يجب أن يستضيف الجهاز الظاهري (يمكن ترك هذا فارغًا في حالة  الاستضافة المحلية). لالإضافة لذلك، في حالة استخدام QEMU/KVM، يستطيع كل مستخدم إدارة الحواسيب الظاهرية ولكن بصلاحيات مقيدة، ويسمح مسار URL بتمييز حواسيب ”النظام“ (<literal dir="ltr">/system</literal>) من الحواسيب الظاهرية (<literal dir="ltr">/session</literal>).</para>
          </callout>
          <callout arearefs="virtinst.type">
	    <para>بما أن طريقة إدارة KVM تطابق طريقة إدارة QEMU، فإن الخيار <literal dir="ltr">--virt-type kvm</literal> يسمح بتحديد استخدام KVM بالرغم من أن URL يبدو وكأنه QEMU.</para>
          </callout>
          <callout arearefs="virtinst.name">
	    <para>خيار <literal dir="ltr">--name</literal> يحدد اسمًا (فريداً) للجهاز الظاهري.</para>
          </callout>
          <callout arearefs="virtinst.ram">
	    <para>يسمح خيار <literal dir="ltr">--ram</literal> بتحديد كمية الذاكرة (بالميغابايت) المخصصة للجهاز الظاهري.</para>
          </callout>
          <callout arearefs="virtinst.disk">
	    <para>يحدد <literal dir="ltr">--disk</literal> موقع ملف الصورة التي تمثل القرص الصلب لجهازنا الظاهري؛ سوف يتم إنشاء ذلك الملف –ما لم يكن موجوداً مسبقاً– بالحجم المحدد بالبارامتر <literal>size</literal> (بالغيغابايت). يسمح المتغير <literal>format</literal> باختيار إحدى الصيغ المتعددة لتخزين ملفات الصور. الصيغة الافتراضية (<literal>raw</literal>) هي ملف وحيد يطابق القرص بالحجم والمحتويات تماماً. لقد اخترنا صيغة متقدمة أكثر هنا، هذه الصيغة خاصة بـ QEMU وهي تسمح بالبدء مع ملف صغير يكبر فقط عندما يبدأ الجهاز الظاهري باستهلاك المساحة فعلاً.</para>
          </callout>
          <callout arearefs="virtinst.cdrom">
	    <para>يستخدم خيار <literal dir="ltr">--cdrom</literal> للإشارة إلى موقع القرص الضوئي المستخدم للتثبيت. يمكن أن يكون المسار مساراً محلياً لصورة ISO، أو URL يمكن الحصول منه على الملف، أو ملف جهاز يمثل سواقة CD-ROM فيزيائية (مثل <literal dir="ltr">/dev/cdrom</literal>).</para>
          </callout>
          <callout arearefs="virtinst.network">
	    <para>يحدد <literal dir="ltr">--network</literal> طريقة دمج بطاقة الشبكة الظاهرية في إعدادات الشبكة في المستضيف. السلوك الافتراضي (الذي  حددنا استخدامه صراحة في مثالنا) هو دمجها في أي جسر شبكي سابق. إذا لم يكن هناك أي جسر من قبل، فلن يستطيع الجهاز الظاهري الوصول إلى الشبكة الفيزيائية إلا من خلال NAT، لذلك يأخذ عنواناً ضمن مجال شبكة فرعية داخلية (192.168.122.0/24).</para>
          </callout>
          <callout arearefs="virtinst.vnc">
	    <para>يصرح <literal dir="ltr">--vnc</literal> أن الطرفية الرسومية يجب أن تكون متاحة عبر استخدام VNC. السلوك الافتراضي لمخدم VNC المرفق هو الإنصات إلى الواجهة المحلية فقط؛ إذا كان عميل VNC سيعمل على حاسب آخر، فإن الاتصال يحتاج لإعداد نفق SSH (انظر <xref linkend="sect.ssh-port-forwarding" />). أو يمكن استخدام <literal dir="ltr">--vnclisten=0.0.0.0</literal> حتى يصبح الوصول لمخدم VNC ممكناً من جميع الواجهات؛ لكن انتبه إلى أنك إذا استخدمت هذا الخيار، فعليك تصميم الجدار الناري بما يتناسب معه.</para>
          </callout>
          <callout arearefs="virtinst.os">
	    <para>يسمح الخياران <literal dir="ltr">--os-type</literal> و<literal dir="ltr">--os-variant</literal> بتحسين بعض متغيرات الجهاز الظاهري، اعتماداً على بعض المزايا المعروفة لنظام التشغيل المذكور هنا.</para>
          </callout>
        </calloutlist>

	<para>عند هذه النقطة، بدأ الجهاز الظاهري يعمل، ونحتاج الاتصال بالطرفية الرسومية  لمتابعة عملية التثبيت. إذا تم تنفيذ العملية السابقة من بيئة سطح مكتب رسومية، فيجب أن يبدأ هذا الاتصال آلياً. إذا لم يحدث هذا، أو إذا كنا نعمل عن بعد، يمكن تشغيل <command>virt-viewer</command> من أي بيئة رسومية لفتح الطرفية الرسومية (لاحظ أن كلمة سر الجذر للنظام البعيد ستطلب مرتين لأن العملية تحتاج لاتصالي SSH):</para>

        <screen><computeroutput>$ </computeroutput><userinput>virt-viewer --connect qemu+ssh://root@<replaceable>server</replaceable>/system testkvm
</userinput><computeroutput>root@server's password: 
root@server's password: </computeroutput>
</screen>

	<para>عند انتهاء عملية التثبيت، تتم إعادة تشغيل الجهاز الظاهري، ويصبح جاهزاً عند ذلك للاستخدام.</para>
      </section>
      <section>
        <title>إدارة الأجهزة باستخدام <command>virsh</command></title>
        <indexterm><primary><command>virsh</command></primary></indexterm>

	<para>بعد أن انتهينا من التثبيت، دعنا نرى كيف ندير الأجهزة الظاهرية المتوفرة. أول شيئ سنجربه هو طلب قائمة بالأجهزة التي تديرها <command>libvirtd</command>:</para>

        <screen><computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all</userinput>
 Id Name                 State
----------------------------------
  - testkvm              shut off

</screen>

	<para>دعنا نبدأ تشغيل جهازنا التجريبي:</para>

        <screen><computeroutput># </computeroutput><userinput>virsh -c qemu:///system start testkvm
</userinput><computeroutput>Domain testkvm started</computeroutput>
</screen>

	<para>يمكننا الآن الحصول على تعليمات الاتصال بالطرفية الرسومية (يمكن تمرير لوحة عرض VNC المعادة كمتغير للبرنامج <command>vncviewer</command>):</para>

        <screen><computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm
</userinput><computeroutput>:0</computeroutput>
</screen>

	<para>من أوامر <command>virsh</command> الفرعية المتاحة أيضاً:</para>
        <itemizedlist>
          <listitem>
	    <para><literal>reboot</literal> لإعادة إقلاع الجهاز الظاهري؛</para>
          </listitem>
          <listitem>
	    <para><literal>shutdown</literal> لبدء عملية إيقاف تشغيل نظيفة؛</para>
          </listitem>
          <listitem>
	    <para><literal>destroy</literal>، لإيقاف عمل الجهاز الظاهري قسراً؛</para>
          </listitem>
          <listitem>
	    <para><literal>suspend</literal> لإيقاف عمله مؤقتاً؛</para>
          </listitem>
          <listitem>
	    <para><literal>resume</literal> لاستكمال عمله؛</para>
          </listitem>
          <listitem>
	    <para><literal>autostart</literal> لتفعيل (أو تعطيل، إذا استخدم الخيار <literal dir="ltr">--disable</literal>) تشغيل الجهاز الظاهري تلقائياً عند إقلاع المستضيف؛</para>
          </listitem>
          <listitem>
	    <para><literal>undefine</literal> لإزالة كافة آثار الجهاز الظاهري من <command>libvirtd</command>.</para>
          </listitem>
        </itemizedlist>

	<para>جميع هذه الأوامر الفرعية تأخذ الاسم المُعِّرف للجهاز الظاهري كمتغير لها.</para>
      </section>
      <section>
        <title>تثبيت نظام مبني على RPM في دبيان باستخدام yum</title>

	<para>إذا كان الجهاز الظاهري سيعمل بنظام دبيان (أو أحد مشتقاته)، يمكن تهيئة النظام باستخدام <command>debootstrap</command>، كما شرحناه سابقاً. أما إذا كان الجهاز الظاهري سيعمل بنظام مبني على RPM (مثل فيدورا، أو CentOS أو Scientific Linux)، يجب إتمام التثبيت باستخدام أداة <command>yum</command> (المتوفرة في الحزمة ذات الاسم نفسه).</para>
	
        <para>تحتاج العملية لاستخدام <command>rpm</command> لاستخراج مجموعة من الملفات، من أهمها ملفات إعداد <command>yum</command>، ثم استدعاء <command>yum</command> لفك الضغط عن بقية الحزم. لكن بما أننا سوف نستدعي <command>yum</command> من خارج chroot، علينا إجراء بعض التغييرات المؤقتة. في المثال التالي، كان chroot الهدف هو <filename dir="ltr">/srv/centos</filename>.</para>

        <screen><computeroutput># </computeroutput><userinput>rootdir="/srv/centos"
</userinput><computeroutput># </computeroutput><userinput>mkdir -p "$rootdir" /etc/rpm
</userinput><computeroutput># </computeroutput><userinput>echo "%_dbpath /var/lib/rpm" &gt; /etc/rpm/macros.dbpath
</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root "$rootdir" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!
rpm: However assuming you know what you are doing...
warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY
# </computeroutput><userinput>sed -i -e "s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g" $rootdir/etc/yum.repos.d/*.repo
</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core
</userinput><computeroutput>[...]
# </computeroutput><userinput>sed -i -e "s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g" $rootdir/etc/yum.repos.d/*.repo
</userinput></screen>
      </section>
    </section>
  </section>
  <section id="sect.automated-installation">
    <title>التثبيت المؤتمت</title>
    <indexterm><primary>تنصيب</primary></indexterm>
    <indexterm><primary>تثبيت</primary><secondary>التثبيت المؤتمت</secondary></indexterm>

    <para>يحتاج مدراء النظم في شركة فلكوت، كما هو حال مدراء النظم في العديد من شركات الخدمات التقنية الكبيرة، لأدوات تساعدهم على تثبيت (أو إعادة تثبيت) النظام على الأجهزة الجديدة بسرعة، وبصورة آلية إذا أمكن.</para>

    <para>يمكن تلبية هذه الحاجة بطيف واسع من الحلول. فالأدوات العامة مثل SystemImager تعالج هذه القضية بإنشاء صورة بالاعتماد على جهاز نموذجي، ثم نشر تلك الصورة على الأجهزة المستهدفة، وعلى النهاية الأخرى من الطيف، هناك برنامج تثبيت دبيان القياسي الذي يمكن تغذيته بملف إعداد يجيب على الأسئلة المطروحة أثناء عملية التثبيت. وكنوع من الحلول الوسط، يمكن استخدام أداة هجينة مثل FAI (‏<emphasis>Fully Automatic Installer</emphasis>) لتثبيت النظام على الأجهزة باستخدام نظام إدارة الحزم، لكنها تستخدم بنية تحتية خاصة بها للمهام المتعلقة بالنشر واسع النطاق massive deployment (مثل الإقلاع، وتقطيع الأقراص، وإعداد النظام وما شابه).</para>

    <para>لكل من هذه الأدوات محاسن ومساوئ. يعمل SystemImager بشكل مستقل عن أي نظام حزم معين، وهذا يسمح له بإدارة مجموعات كبيرة من الأجهزة باستخدام عدة توزيعات لينكس مختلفة. كما أنه يتضمن نظام تحديث لا يحتاج إعادة تثبيت النظام، لكن لا يمكن الاعتماد على نظام التحديث هذا إلا إذا لم تعدّل الأجهزة بشكل مستقل؛ أي يجب ألا يحدث المستخدمون وحدهم أي برمجية، كما لا يجب أن يثبتوا برمجيات إضافية. كما يجب عدم أتمتة التحديثات الأمنية، بل يجب أن تمر عبر الصورة المركزية التي يديرها SystemImager. هذا الحل يتطلب أيضًا أن تكون الأجهزة المستهدفة متجانسة، وإلا يجب الاحتفاظ بعدد من الصور المختلفة وإدارتها (صورة i386 لن تتناسب مع جهاز powerpc، وهكذا).</para>

    <para>أما التثبيت المؤتمت باستخدام مثبت دبيان  فيستطيع التكيف مع خصائص كل جهاز؛ إذ أن المثبت سيجلب النواة والحزم البرمجية المناسبة من المستودعات الموافقة، وسيتعرف على العتاد المتوفر، ويقطع كامل القرص الصلب للاستفادة من كل المساحة التخزينية المتاحة، ثم يثبت نظام دبيان ويعد محمل إقلاع ملائم. لكن المثبت القياسي لا يثبت إلا نسخ دبيان ”القياسية“، التي تحوي النظام الأساسي مع مجموعة من ”المهام“ المحددة مسبقًا؛ وهذا يمنع تثبيت نظام مخصص مع تطبيقات غير محزمة. لتلبية هذا المتطلب بالذات يجب تخصيص المثبت… لحسن الحظ، المثبت تجزيئي كثيراً (modular)، وهناك أدوات لأتمتة معظم العمل المطلوب لهذا التخصيص، أهمها simple-CDD (حيث CDD هي اختصار <emphasis>Custom Debian Derivative</emphasis>—مشتق مخصص من دبيان). وحتى simple-CDD يعالج التثبيت الأولي فقط؛ لكن هذه ليست مشكلة عادة بما أن أدوات APT تسمح بالنشر الفعال للتحديثات لاحقاً.</para>

    <para>سوف نقدم شرحاً مقتضبًا فقط عن FAI، وسنتجاوز SystemImager بالكامل (الذي لم يعد متوفراً في دبيان)، وذلك للتركيز أكثر على مثبت دبيان وsimple-CDD، وهي الحلول الأكثر جاذبية عند العمل مع نظم دبيان.</para>
    <section id="sect.fai">
      <title>‏Fully Automatic Installer (FAI)‎</title>
      <indexterm><primary>Fully Automatic Installer (FAI)</primary></indexterm>

      <para>لعل <foreignphrase>Fully Automatic Installer</foreignphrase> أقدم نظم النشر المؤتمت لأنظمة دبيان، وهذا ما يفسر ذكر هذه الأداة كثيراً؛ إلا أن طبيعته فائقة المرونة بالكاد تغطي تعقيد استخدامه.</para>

      <para>يحتاج FAI لنظام يعمل كمخدم لتخزين معلومات النشر ويسمح للأجهزة المستهدفة بالإقلاع عبر الشبكة. يحتاج هذا المخدم حزمة <emphasis role="pkg">fai-server</emphasis> (أو <emphasis role="pkg">fai-quickstart</emphasis> التي تثبت أيضًا العناصر المطلوبة للإعداد القياسي).</para>

      <para>يستخدمُ FAI أسلوباً خاصاً لتعريف البروفايلات المتنوعة التي يمكن تثبيتها. بدلاً من النسخ البسيط للنظام المرجعي، يوفر FAI مثبتاً متكاملاً يمكن تخصيصه بالكامل عبر مجموعة من الملفات والسكربتات المخزنة على المخدم؛ لا يتم إنشاء الموقع الافتراضي <filename>/srv/fai/config/</filename> آليًا، لذلك يجب أن ينشئه مدير النظام بالإضافة لجميع الملفات اللازمة. في معظم الأحيان تكون هذه الملفات نسخاً مخصصة عن ملفات الأمثلة المتوفرة في الحزمة <emphasis role="pkg">fai-doc</emphasis> وبالأخص في المجلد <filename dir="ltr">/usr/share/doc/fai-doc/examples/simple/</filename>.</para>

      <para>بعد تعريف البروفايلات، يجب تنفيذ الأمر <command>fai-setup</command> لتوليد العناصر المطلوبة لبدء التثبيت باستخدام FAI؛ هذا يعني تحضير أو تحديث نظام أصغري (NFS-root) يستخدم خلال التثبيت. أو يمكن توليد CD إقلاعي للتثبيت باستخدام <command>fai-cd</command>.</para>

      <para>لإنشاء كل ملفات الضبط هذه يجب فهم طريقة عمل FAI. تتألف عملية التثبيت النموذجية من الخطوات التالية:</para>
      <itemizedlist>
        <listitem>
	  <para>إحضار النواة عبر الشبكة، وإقلاعها؛</para>
        </listitem>
        <listitem>
	  <para>ربط نظام الملفات الجذر عبر NFS‏ (nfsroot المذكور سابقاً)؛</para>
        </listitem>
        <listitem>
	  <para>تنفيذ <command dir="ltr">/usr/sbin/fai</command> الذ يتحكم بتتمة العملية (أي أن الخطوات التالية سينفذها هذا السكربت)؛</para>
        </listitem>
        <listitem>
	  <para>نسخ مساحة الإعداد من المخدم إلى <filename>/fai/</filename>؛</para>
        </listitem>
        <listitem>
	  <para>استدعاء <command>fai-class</command>. سوف تُنفَّذ السكربتات <filename dir="ltr">/fai/class/[0-9][0-9]*</filename> بالدور، وتعيد أسماء ”الفئات“ (classes) التي يجب تطبيقها على الجهاز الذي تجري عليه عملية التثبيت؛ سوف تعمل هذه المعلومات كأساس للخطوات التالية. هذا يسمح ببعض المرونة في تعريف الخدمات التي سوف تُثبَّت وتُضبَط.</para>
        </listitem>
        <listitem>
	  <para>قراءة عدد من متغيرات الضبط، وذلك تبعاً للفئات (classes) المحددة؛</para>
        </listitem>
        <listitem>
	  <para>تقسيم الأقراص وتهيئة الأقسام الناتجة، حسب المعلومات المتوفرة في <filename dir="ltr">/fai/disk_config/<replaceable>class</replaceable></filename>؛</para>
        </listitem>
        <listitem>
	  <para>ربط الأقسام السابقة؛</para>
        </listitem>
        <listitem>
	  <para>تثبيت أساس النظام؛</para>
        </listitem>
        <listitem>
	  <para>تغذية قاعدة بيانات Debconf باستخدام <command>fai-debconf</command>؛</para>
        </listitem>
        <listitem>
	  <para>الحصول على قائمة الحزم المتاحة لأداة APT؛</para>
        </listitem>
        <listitem>
	  <para>تثبيت الحزم المذكورة في <filename dir="ltr">/fai/package_config/<replaceable>class</replaceable></filename>؛</para>
        </listitem>
        <listitem>
	  <para>تنفيذ السكربتات التالية للإعداد، <filename dir="ltr">/fai/scripts/<replaceable>class</replaceable>/[0-9][0-9]*</filename>؛</para>
        </listitem>
        <listitem>
	  <para>حفظ سجلات التثبيت، فصل أقسام الأقراص الصلبة، ثم إعادة الإقلاع؛</para>
        </listitem>
      </itemizedlist>
    </section>
    <section id="sect.d-i-preseeding">
      <title>تغذية مثبت دبيان</title>
      <indexterm><primary>تغذية</primary></indexterm>
      <indexterm><primary>إعداد مسبق</primary></indexterm>

      <para>في النهاية، يجب –منطقياً– أن يبقى مُثبِّت دبيان الرسمي أفضل أداة لتثبيت أنظمة دبيان. ولهذا السبب تم تصميم مثبت دبيان منذ البداية للاستخدام المؤتمت، بالاستفادة من مزايا البنية التحتية التي تقدمها <emphasis role="pkg">debconf</emphasis>. تسمح الأخيرة بتقليل عدد الأسئلة المطروحة من جهة (تأخذ الأسئلة المخفية الإجابات الافتراضية آلياً)، ومن جهة أخرى، توفير الإجابات الافتراضية بشكل مستقل، حتى تتاح إمكانية التثبيت غير التفاعلي. هذه الميزة الأخيرة تعرف باسم <emphasis>preseeding</emphasis>—التغذية، التي تعني ”الإعداد المسبق“ ببساطة.</para>

      <sidebar>
        <title><emphasis>التعمق أكثر</emphasis> Debconf مع قاعدة بيانات مركزية</title>
        <indexterm><primary><command>debconf</command></primary></indexterm>

	<para>تسمح التغذية بالإجابة على أسئلة Debconf التي تطرحها أثناء التثبيت، لكن هذه الأجوبة ثابتة ولا تتطور بمرور الزمن. بما أن النظم المثبتة مسبقًا قد تحتاج للترقية، وقد تطرح أسئلة جديدة أثناء العملية، فيمكن ضبط ملف الإعداد <filename dir="ltr">/etc/debconf.conf</filename> بحيث تستخدم Debconf مصادر بيانات خارجية (مثل مخدم LDAP directory، أو ملف بعيد تصل إليه عبر NFS أو Samba). يمكن تعريف عدة مصادر خارجية للبيانات في الوقت نفسه، وسوف تكمل هذه المصادر بعضها البعض. ستبقى قاعدة البيانات المحلية قيد الاستخدام (لاستخدامها للقراءة والكتابة)، أما قواعد البيانات الخارجية فتقتصر الصلاحيات فيها على القراءة فقط عادة. تشرح صفحة التعليمات <citerefentry><refentrytitle>debconf.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry>‎ كافة الاحتمالات بالتفصيل (ستحتاج للحزمة <emphasis role="pkg">debconf-doc</emphasis>).</para>
      </sidebar>
      <section>
        <title>استخدام ملف تغذية</title>

	<para>يستطيع المُثبّت الحصول على ملف التغذية من العديد من الأماكن:</para>
        <itemizedlist>
          <listitem>
	    <para>من initrd المستخدمة لإقلاع الجهاز، في هذه الحالة، تتم التغذية منذ بداية التثبيت الأولية، وسوف يتم تجاوز جميع الأسئلة. يجب فقط تسمية الملف preseed.cfg وتخزينه في جذر initrd.</para>
          </listitem>
          <listitem>
	    <para>من وسيط الإقلاع (CD أو مفتاح USB)؛ وتحدث التغذية فور ربط الوسيط التخزيني، أي  مباشرة بعد السؤال عن اللغة وتخطيط لوحة المفاتيح. يمكن استخدام متغير الإقلاع <literal>preseed/file</literal> للإشارة إلى موقع ملف التغذية (مثلا، <filename dir="ltr">/cdrom/preseed.cfg</filename> عند التثبيت من قرص CD-ROM، أو <filename dir="ltr">/hd-media/preseed.cfg</filename> في حال استخدام مفتاح USB).</para>
          </listitem>
          <listitem>
	    <para>من الشبكة؛ عندها لا تتم التغذية إلا بعد إعداد الشبكة (الأوتوماتيكي)؛ عندها يجب استخدام متغير الإقلاع <literal>preseed/url=http://<replaceable>server</replaceable>/preseed.cfg</literal>.</para>
          </listitem>
        </itemizedlist>

	<para>كنظرة أولية، يبدو تضمين ملف التغذية في initrd أنه الحل الأكثر جاذبية؛ لكنه نادراً ما يستخدم عملياً، لأن توليد initrd للمثبت معقد جداً. الحلين الآخرين أكثر انتشاراً بكثير، خصوصًا أنك تستطيع استخدام المتغيرات الإقلاعية كطريق بديل لتغذية الأسئلة الأولى لعملية التثبيت. جرت العادة أن تحفظ هذه المتغيرات في إعدادات <command>isolinux</command> (في حال استخدام CD-ROM) أو <command>syslinux</command> (ذاكرة USB) بدلاً من كتابتها يدوياً عند كل عملية تثبيت.</para>
      </section>
      <section>
        <title>إنشاء ملف التغذية</title>

	<para>ملف التغذية هو ملف نصي عادي، كل سطر منه يحوي إجابة لسؤال واحد من أسئلة Debconf. يفصل السطر إلى أربعة أقسام تفصلها مسافات بيضاء (علامة مسافة space أو علامة جدولة tab)، فمثلاً <literal>d-i mirror/suite string stable</literal>:</para>
        <itemizedlist>
          <listitem>
	    <para>الحقل الأول هو ”صاحب“ السؤال؛ تستخدم ”d-i“ للأسئلة المتعلقة بالمثبت، لكن يمكن أن تكتب اسم حزمة للأسئلة التي تطرحها حزم دبيان؛</para>
          </listitem>
          <listitem>
	    <para>الحقل الثاني هو معرف للسؤال؛</para>
          </listitem>
          <listitem>
	    <para>الثالث، نوع السؤال؛</para>
          </listitem>
          <listitem>
	    <para>الحقل الرابع والأخير يحوي قيمة الإجابة. لاحظ أن هذا الحقل يجب فصله عن سابقه بمسافة واحدة؛ وإذا كان هناك أكثر من واحدة ستعتبر المسافات اللاحقة جزءاً من الإجابة.</para>
          </listitem>
        </itemizedlist>

	<para>أبسط طريقة لكتابة ملف تغذية هي تثبيت النظام يدوياً. ثم يعطيك الأمر <command>debconf-get-selections --installer</command> الإجابات المتعلقة بالمثبت. يمكن الحصول على الإجابات المتعلقة بالحزم الأخرى بالأمر <command>debconf-get-selections</command>. لكن الحل الأفضل هو أن تكتب ملف التغذية يدوياً، بالاعتماد على مثال وعلى الوثائق: بهذا الشكل يمكن تغذية الأسئلة التي تحتاج تغيير إجاباتها الافتراضية فقط؛ واستخدام متغير الإقلاع <literal>priority=critical</literal> سوف يفرض على Debconf أن تطرح الأسئلة الحرجة فقط، وأن تستخدم الإجابات الافتراضية لبقية الأسئلة.</para>

        <sidebar>
          <title><emphasis>توثيق</emphasis> الملحق في دليل التثبيت</title>

	  <para>يتضمن دليل التثبيت، المتاح على شبكة الإنترنت، توثيقاً مفصلاً عن استخدام ملفات التغذية في ملحق خاص. كما يتضمن مثالاً عن ملف تغذية مفصلاً ومزوداً بالتعليقات، يمكن الاستفادة منه كأساس للتخصيصات المحلية. <ulink type="block" url="https://www.debian.org/releases/jessie/amd64/apb.html" /> <ulink type="block" url="https://www.debian.org/releases/jessie/example-preseed.txt" /></para>
        </sidebar>
      </section>
      <section>
        <title>إنشاء وسيط إقلاعي مخصص</title>

	<para>من الجيد أن يعرف المرء مكان تخزين ملف التغذية، لكن مكان التخزين ليس كل شيء: يجب تعديل وسيط الإقلاع –بشكل أو بآخر– لتغيير متغيرات الإقلاع وإضافة ملف التغذية.</para>
        <section>
          <title>الإقلاع من الشبكة</title>

	  <para>عند إقلاع الحاسب من الشبكة، يعرف المخدم الذي يرسل عناصر التهيئة متغيرات الإقلاع أيضاً. أي يجب أن يتم التعديل على إعداد PXE لمخدم الإقلاع؛ وبالتحديد أكثر، في ملف الإعداد <filename dir="ltr">/tftpboot/pxelinux.cfg/default</filename>. إن إعداد الإقلاع عبر الشبكة هو متطلب أساسي؛ انظر دليل التثبيت لمزيد من التفاصيل. <ulink type="block" url="https://www.debian.org/releases/jessie/amd64/ch04s05.html" /></para>
        </section>
        <section>
          <title>تحضير ذاكرة USB إقلاعية</title>

	  <para>بعد تجهيز الذاكرة الإقلاعية (انظر <xref linkend="sect.install-usb" />)، يجب تنفيذ بعض العمليات الإضافية. على فرض أن محتويات الذاكرة متاحة في <filename>/media/usbdisk/</filename>:</para>
          <itemizedlist>
            <listitem>
	      <para>انسخ ملف التغذية إلى <filename dir="ltr">/media/usbdisk/preseed.cfg</filename></para>
            </listitem>
            <listitem>
	      <para>حرر الملف <filename dir="ltr">/media/usbdisk/syslinux.cfg</filename> وأضف المتغيرات الإقلاعية اللازمة (انظر المثال التالي).</para>
            </listitem>
          </itemizedlist>

          <example>
            <title>ملف syslinux.cfg وبارامترات التغذية</title>

            <programlisting>default vmlinuz
append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --</programlisting>
          </example>
        </section>
        <section>
          <title>إنشاء صورة CD-ROM</title>
          <indexterm><primary>debian-cd</primary></indexterm>

	  <para>ذاكرة USB هي وسيط تخزين يقبل القراءة والكتابة، لذلك كانت إضافة الملف إليها وتعديل بعض المتغيرات فيها عملية سهلة. لكن في حالة استخدام CD-ROM، فالعملية معقدة أكثر، لأننا نحتاج توليد صورة ISO كاملة. هذه المهمة تحتاج الأداة <emphasis role="pkg">debian-cd</emphasis>، لكن استخدام هذه الأداة مزعج نوعًا ما: تحتاج الأداة لمرآة محلية، كما تحتاج لفهم جميع الخيارات في <filename dir="ltr">/usr/share/debian-cd/CONF.sh</filename>؛ وحتى بعد ذلك، يجب استدعاء <command>make</command> عدة مرات. عليك إذن قراءة <filename dir="ltr">/usr/share/debian-cd/README</filename>.</para>

	  <para>تعمل debian-cd دائماً بنفس الأسلوب: يتم توليد مجلد ”صورة“ فيه محتويات القرص الليزري نفسها، ثم يحوّل إلى ملف ISO بأداة مثل <command>genisoimage</command> أو <command>mkisofs</command> أو <command>xorriso</command>. يُختَم المجلد بعد الخطوة <command>make image-trees</command> التابعة لحزمة debian-cd. عند هذه النقطة، سوف نزرع ملف التغذية في المجلد المناسب (عادة <filename dir="ltr">$TDIR/$CODENAME/CD1/</filename>، حيث ‎$TDIR و$CODENAME هما متغيران يعرفهما ملف الإعداد <filename>CONF.sh</filename>). تستخدم الأقراص الليزرية <command>isolinux</command> كمحمل للإقلاع، ويجب ضبط ملف الإعداد ليتناسب مع ما ولدته debian-cd، وإدخال متغيرات الإقلاع المطلوبة (الملف المقصود هو <filename dir="ltr">$TDIR/$CODENAME/boot1/isolinux/isolinux.cfg</filename>). بعدها يمكن متابعة العملية ”الاعتيادية“، ويمكننا توليد صورة ISO بالأمر <command>make image CD=1</command> (أو <command>make images</command> إذا كنا سنولد عدة CD-ROMs).</para>
        </section>
      </section>
    </section>
    <section id="sect.simple-cdd">
      <title>‏Simple-CDD: كل الحلول في حل واحد</title>
      <indexterm><primary>simple-cdd</primary></indexterm>

      <para>ببساطة إن استخدام ملف التغذية لا يكفي لتلبية كافة المطالب التي قد تظهر عند النشر واسع النطاق. وبالرغم أنه يمكن تنفيذ بضعة سكربتات عند نهاية عملية التثبيت العادية، إلا أن مجموعة الحزم التي ستثبت ليست مرنة بما يكفي (أساساً لا يمكن إلا اختيار ”المهام“)؛ وأهم من هذا، لا يمكن إلا تثبيت حزم دبيان الرسمية، ولا يسمح بالحزم المولدة محليًا.</para>

      <para>وعلى صعيد آخر، تستطيع debian-cd دمج الحزم الخارجية، كما يمكن توسيع مثبت دبيان بإدخال خطوات جديدة في عملية التثبيت. بجمع هذه الإمكانيات، يفترض أن نستطيع إنشاء مثبت مخصص يلبي حاجاتنا؛ بل يفترض أن يتمكن أيضًا من ضبط بعض الخدمات بعد تثبيت الحزم المطلوبة. لحسن الحظ، هذه ليست فرضية بلا برهان، بل هي وظيفة Simple-CDD (في الحزمة <emphasis role="pkg">simple-cdd</emphasis>) تماماً.</para>

      <para>الهدف من Simple-CDD هو السماح لأي شخص بإنشاء توزيعة مشتقة من دبيان بسهولة، بتحديد مجموعة جزئية من الحزم المتوفرة، وإعدادها مسبقاً باستخدام Debconf، وإضافة برمجيات معينة، وتنفيذ سكربتات مخصصة عند نهاية عملية التثبيت. هذا يوافق فلسفة ”نظام التشغيل العالمي“، حيث يستطيع أي شخص تعديله ليناسب حاجاته الشخصية.</para>
      <section>
        <title>تعريف البروفايلات</title>

	<para>يعرف Simple-CDD ”بروفايلات“ تقابل مفهوم ”الفئات – classes“ في FAI، ويمكن إعطاء الجهاز عدة بروفايلات (تُحدَّد أثناء التثبيت). يعرف البروفايل بمجموعة من ملفات <filename dir="ltr">profiles/<replaceable>profile</replaceable>.*</filename>:</para>
        <itemizedlist>
          <listitem>
	    <para>ملف <filename dir="ltr">.description</filename> يحوي سطراً واحداً يصف البروفايل؛</para>
          </listitem>
          <listitem>
	    <para>ملف <filename dir="ltr">.packages</filename> يسرد أسماء الحزم التي ستثبت تلقائيًا عند تحديد هذا البروفايل؛</para>
          </listitem>
          <listitem>
	    <para>ملف <filename dir="ltr">.downloads</filename> يسرد أسماء الحزم التي ستخزن على وسيط التثبيت، لكن لا يشترط تثبيتها؛</para>
          </listitem>
          <listitem>
	    <para>ملف <filename dir="ltr">.preseed</filename> يحوي معلومات التغذية لأسئلة Debconf (للمثبت أو للحزم)؛</para>
          </listitem>
          <listitem>
	    <para>ملف <filename dir="ltr">.postinst</filename> يحوي سكربتًا يعمل عند نهاية التثبيت؛</para>
          </listitem>
          <listitem>
	    <para>أخيراً، ملف <filename dir="ltr">.conf</filename> يسمح بتعديل بعض متغيرات Simple-CDD اعتماداً على البروفايلات التي ستضمَّن في الصورة.</para>
          </listitem>
        </itemizedlist>

	<para>البروفايل <literal>default</literal> له دور خاص، لأنه محدد دوماً؛ ولذلك يحوي الحد الأدنى المطلوب لعمل Simple-CDD. الشيء الوحيد الذي يخصص عادة في هذا البروفايل هو متغير التغذية <literal>simple-cdd/profiles</literal>: هذا يسمح بتفادي طلب Simple-CDD تحديد البروفايل الذي يريد تثبيته من المستخدم.</para>

	<para>لاحظ أيضًا أنه يجب استدعاء الأوامر من المجلد الأب للمجلد <filename>profiles</filename>.</para>
      </section>
      <section>
        <title>إعداد واستخدام <command>build-simple-cdd</command></title>
        <indexterm><primary><command>build-simple-cdd</command></primary></indexterm>

        <sidebar>
          <title><emphasis>نظرة سريعة</emphasis> ملف إعداد مفصل</title>

	  <para>هناك مثال عن ملف إعداد Simple-CDD فيه كل المتغيرات الممكنة، مضمن في الحزمة (<filename dir="ltr">/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz</filename>). يمكن استخدام هذا الملف كنقطة انطلاق عند إنشاء ملفات إعداد مخصصة.</para>
        </sidebar>

	<para>يحتاج Simple-CDD للكثير من المتغيرات ليعمل بشكل كامل. غالبًا ما تجمع هذه المتغيرات في ملف إعداد، وبعدها نمرره للأمر <command>build-simple-cdd</command> بالخيار <literal dir="ltr">--conf</literal>، لكن يمكن أيضاً تحديد قيم هذه المتغيرات باستخدام بارمترات خاصة تعطى للأمر <command>build-simple-cdd</command>. إليك نظرة عامة عن عمل هذا الأمر، وعن تأثير متغيراته المختلفة:</para>
        <itemizedlist>
          <listitem>
	    <para>يحدد المتغير <literal>profiles</literal> البروفايلات التي ستضمن في صورة CD-ROM المولدة؛</para>
          </listitem>
          <listitem>
	    <para>اعتماداً على قائمة الحزم المطلوبة سوف ينزل Simple-CDD الملفات المناسبة من المخدم المذكور في <literal>server</literal>، ويجمعها في مرآة جزئية (التي ستعطى لاحقًا إلى debian-cd).</para>
          </listitem>
          <listitem>
	    <para>تدمج الحزم المخصصة المذكورة في <literal>local_packages</literal> أيضًا في هذه المرآة المحلية؛</para>
          </listitem>
          <listitem>
	    <para>بعدها تستدعى debian-cd (ويستخدم موقع افتراضي يمكن تعديله بالمتغير <literal>debian_cd_dir</literal>)، وتعطى قائمة بالحزم المراد دمجها؛</para>
          </listitem>
          <listitem>
	    <para>بعدما جهزت debian-cd المجلد، تطبق Simple-CDD بعض التعديلات عليه:</para>
            <itemizedlist>
              <listitem>
		<para>تضاف الملفات التي تحوي البروفايلات إلى مجلد فرعي باسم <filename>simple-cdd</filename> (وسوف يظهر في القرص النهائي)؛</para>
              </listitem>
              <listitem>
		<para>تضاف الملفات الأخرى المذكورة في المتغير <literal>all_extras</literal> أيضًا؛</para>
              </listitem>
              <listitem>
		<para>تضبط متغيرات الإقلاع لتفعيل التغذية. يتم تفادي الأسئلة عن اللغة والبلد إذا كانت المعلومات المطلوبة مخزنة في المتغيرين <literal>language</literal> و<literal>country</literal>.</para>
              </listitem>
            </itemizedlist>
          </listitem>
          <listitem>
	    <para>تولد debian-cd صورة ISO النهائية.</para>
          </listitem>
        </itemizedlist>
      </section>
      <section>
        <title>توليد صورة ISO</title>

	<para>بعدما كتبنا ملف الإعداد وعرفنا البروفايلات، تبقى خطوة استدعاء <command>build-simple-cdd --conf simple-cdd.conf</command>. بعد عدة دقائق، نحصل على الصورة المطلوبة في <filename>images/debian-8.0-amd64-CD-1.iso</filename>.</para>
      </section>
    </section>
  </section>
  <section id="sect.monitoring">
    <title>المراقبة</title>

    <para>المراقبة هي مصطلح عام، ونشاطات المراقبة المتنوعة لها أهداف عدة: فمن ناحية أولى، تسمح متابعة استهلاك موارد الحاسب بتوقع الإشباع والتطويرات اللاحقة له؛ ومن ناحية أخرى، فإن تنبيه مدير النظام فور خروج إحدى الخدمات عن العمل أو عدم عملها بشكل صحيح يعني أن إصلاح المشاكل التي تحدث قد يتم أبكر.</para>

    <para>يغطي <emphasis>Munin</emphasis> الناحية الأولى، من خلال عرض مخططات بيانية للقيم التاريخية لعدد من المتغيرات (الذاكرة المستخدمة، مساحة القرص المحجوزة، حمل المعالج، نشاط الشبكة، حمل Apache/MySQL، وهكذا). أما <emphasis>Nagios</emphasis> فيغطي الناحية الأخرى، من خلال التحقق المنتظم من عمل الخدمات وتوفرها، وإرسال تنبيهات عبر القنوات المناسبة (بريد إلكتروني، رسائل نصية، وهكذا). لكل منهما تصميم تجزيئي يسهل إنشاء إضافات جديدة لمراقبة متغيرات أو خدمات محددة.</para>

    <sidebar>
      <title><emphasis>بدائل</emphasis> Zabbix، أداة مراقبة متكاملة</title>
      <indexterm><primary>Zabbix</primary></indexterm>

      <para>رغم أن استخدام Munin و Nagios شائع جداً، إلا أنهما ليسا اللاعبين الوحيدين في مجال المراقبة، كما أن كل منهما يعالج نصف المهمة فقط (الأول يتولى الرسوم البيانية، والثاني التنبيهات). أما Zabbix فيجمع بين الاثنتين؛ كما أن له واجهة وب لضبط النواحي الأكثر استخداماً. لقد تطور Zabbix في قفزات كبيرة خلال السنوات القليلة الماضية، ويمكن اعتباره منافساً حقيقياً. لاستخدامه عليك تثبيت <emphasis role="pkg">zabbix-server-pgsql</emphasis> (أو <emphasis role="pkg">zabbix-server-mysql</emphasis>) على مخدم المراقبة، وربما أيضاً <emphasis role="pkg">zabbix-frontend-php</emphasis> للحصول على واجهة وب. أما على الأجهزة التي ستراقبها فعليك تثبيت <emphasis role="pkg">zabbix-agent</emphasis> الذي يرسل البيانات إلى المخدم. <ulink type="block" url="http://www.zabbix.com/" /></para>
    </sidebar>

    <sidebar>
      <title><emphasis>بدائل</emphasis> Icinga، مشتق من Nagios</title>
      <indexterm><primary>Icinga</primary></indexterm>

      <para>اشتق عدد من المطورين Nagios نتيجة تباين الآراء بخصوص نموذج تطوير Nagios (الذي تتحكم به شركة)، واختاروا Icinga كاسم لهم. لا يزال Icinga متوافقاً مع إعدادات Nagios وإضافاته —حتى الآن— إلا أنه يضيف بعض المزايا الخاصة أيضاً. <ulink type="block" url="http://www.icinga.org/" /></para>
    </sidebar>
    <section id="sect.munin">
      <title>إعداد Munin</title>
      <indexterm><primary>Munin</primary></indexterm>

      <para>يهدف Munin لمراقبة العديد من الأجهزة؛ وبالتالي، من الطبيعي أن يعتمد بنية مخدم/عميل. يجمع المستضيف المركزي –راسم البيانات (the grapher)– المعطيات من جميع حواسيب المراقبة، ويولد المخططات البيانية الزمنية.</para>
      <section>
        <title>إعداد الأجهزة للمراقبة</title>

	<para>الخطوة الأولى هي تثبيت الحزمة <emphasis role="pkg">munin-node</emphasis>. تنصت الخدمة التي تثبتها هذه الحزمة إلى المنفذ 4949 وترد بإرسال البيانات التي تجمعها كافة الملحقات الفعالة. كل ملحق هو برنامج بسيط يعيد وصفاً للبيانات التي يجمعها بالإضافة إلى آخر قيمة مقاسة. تخزن الملحقات في <filename>/usr/share/munin/plugins/</filename>، لكن لا تستخدم منها إلا التي لها رابط رمزي في المجلد <filename>/etc/munin/plugins/</filename>.</para>

	<para>عند تثبيت الحزمة، تعرف مجموعة من الملحقات الفعالة اعتماداً على البرمجيات المتوفرة والإعداد الحالي للمستضيف. لكن هذا الإعداد الآلي يعتمد على ميزة يجب أن يوفرها كل ملحق، ولذلك كان من المستحسن مراجعة وتعديل النتائج يدوياً. قد يفيد تصفح <ulink url="http://gallery.munin-monitoring.org">معرض الملحقات</ulink> ولو لم يكن هناك توثيقاً شاملاً لجميع الملحقات. على أي حال، جميع الملحقات هي سكربتات ومعظمها بسيط جداً وفيه تعليقات توضيحية جيدة. إن تصفح <filename>/etc/munin/plugins/</filename> إذن هو طريق جيدة لأخذ فكرة عن مهمة كل ملحق وتحديد الملحقات التي يجب إزالتها. كما أن تفعيل ملحق مفيد تجده في <filename>/usr/share/munin/plugins/</filename> لا يحتاج إلا إنشاء رابط رمزي بالأمر <command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command>‎. لاحظ أنه عندما ينتهي اسم الملحق بشرطة منخفضة ”_“ (underscore)، فهذا يعني أن الملحق يحتاج متغيراً حتى يعمل. يجب تخزين قيمة هذا المتغير في اسم الرابط الرمزي؛ مثلاً، يجب تفعيل الملحق ”if_‎“ بالرابط <filename>if_eth0</filename>، وعندها سيراقب نشاط الشبكة على الواجهة الشبكية eth0.</para>

	<para>بعد إعداد جميع الملحقات بشكل صحيح، يجب تغيير إعدادات الخدمة لتحديد صلاحيات الوصول للبيانات المجموعة. يتم هذا من خلال استخدام تعليمة التوجيه <literal>allow</literal> في الملف <filename>/etc/munin/munin-node.conf</filename> الإعداد الافتراضي هو <literal dir="ltr">allow ^127\.0\.0\.1$</literal>، وهو يسمح بالوصول فقط للمستضيف المحلي. في العادة سيضيف مدير النظام سطراً مشابهاً يحوي عنوان IP للمستضيف راسم البيانات، وبعدها يعيد تشغيل الخدمة بالأمر <command>service munin-node restart</command>.</para>

        <sidebar>
          <title><emphasis>التعمق أكثر</emphasis> إنشاء ملحقات محلية</title>

	  <para>يوفر Munin توثيقاً مفصلاً عن أسلوب عمل الملحقات، وكيفية تطوير الملحقات الجديدة. <ulink type="block" url="http://munin-monitoring.org/wiki/plugins" /></para>

	  <para>أفضل اختبار للملحق هو عند تشغيله في الظروف نفسها التي يعمل فيها عندما تستدعيه الخدمة munin-node؛ ويمكن محاكاة هذا باستدعاء الأمر <command>munin-run <replaceable>plugin</replaceable></command> بصلاحيات الجذر. إذا تم تمرير متغير ثان لهذا الأمر (مثل <literal>config</literal>) فسوف يعطى للملحق كمتغير.</para>

	  <para>عند استدعاء الملحق مع المتغير <literal>config</literal>، عليه توصيف نفسه عبر إعادة زمرة من الحقول:</para>

          <screen><computeroutput>$ </computeroutput><userinput>sudo munin-run load config
</userinput><computeroutput>graph_title Load average
graph_args --base 1000 -l 0
graph_vlabel load
graph_scale no
graph_category system
load.label load
graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run "immediately").
load.info 5 minute load average
</computeroutput>
</screen>

	  <para>يشرح ”مرجع الملحقات Plugin reference“ مختلف الحقول المتاحة، هذا الدليل متوفر ضمن ”Munin guide“.‏ <ulink type="block" url="http://munin.readthedocs.org/en/latest/reference/plugin.html" /></para>

	  <para>عند استدعاء الملحلق دون أي متغيرات، سوف يعيد آخر قيمة مقاسة ببساطة؛ مثلاً، تنفيذ <command>sudo munin-run load</command> سوف يعيد القيمة <literal>load.value 0.12</literal>.</para>

	  <para>أخيراً، عند استدعاء الملحق مع المتغير <literal>autoconf</literal>، عليه أن يعيد ”yes“ (مع حالة الخروج—exit status ‏0) إذا كان تفعيل الملحق واجباً على هذا المستضيف، أو ”no“ (مع حالة الخروج 1) في الحالة المعاكسة.</para>
        </sidebar>
      </section>
      <section>
        <title>إعداد راسم البيانات</title>

	<para>”راسم البيانات“ هو ببساطة حاسوب يجمع البيانات ويولد الرسوم البيانية الموافقة. البرنامج المطلوب متوفر في الحزمة <emphasis role="pkg">munin</emphasis>. يشغل الإعداد الافتراضي <command>munin-cron</command> (مرة كل 5 دقائق)، الذي يجمع البيانات من كافة الأجهزة المذكورة في <filename dir="ltr">/etc/munin/munin.conf</filename> (المستضيف المحلي هو الوحيد المذكور افتراضيًا)، ويحفظ البيانات التاريخية في ملفات RRD (‏<emphasis>Round Robin Database</emphasis>، وهي صيغة ملفات مصممة لحفظ البيانات التي تتغير مع الزمن) محفوظة في <filename>/var/lib/munin/</filename> ويولد صفحة HTML تحوي المخططات البيانية في المجلد <filename>/var/cache/munin/www/</filename>.</para>

	<para>يجب إذن ذكر جميع الأجهزة المراقبة في ملف الضبط <filename dir="ltr">/etc/munin/munin.conf</filename>. كل جهاز يذكر في قسم كامل مع اسم يقابل الجهاز ومدخلة <literal>address</literal> واحدة على الأقل هي مدخلة العنوان التي تعطي عنوان IP المناسب.</para>

        <programlisting>[ftp.falcot.com]
    address 192.168.0.12
    use_node_name yes
</programlisting>

	<para>يمكن أن تصبح الأقسام معقدة أكثر وتضاف إليها معلومات وصف مخططات بيانية إضافية لتوليدها بجمع البيانات من عدة أجهزة. العينات الموفرة في ملف الضبط هي نقاط بدء جيدة للتخصيص.</para>

	<para>آخر خطوة هي نشر الصفحات المولدة؛ وهذا يحتاج إعداد مخدم وب حتى تتاح محتويات <filename>/var/cache/munin/www/</filename> على موقع وب. سيكون الوصول لهذا الموقع مقيَّداً غالباً، إما باستخدام نظام مصادقة أو بتقييد الوصول حسب عناوين IP. انظر <xref linkend="sect.http-web-server" /> لمزيد من التفاصيل.</para>
      </section>
    </section>
    <section id="sect.nagios">
      <title>إعداد Nagios</title>
      <indexterm><primary>Nagios</primary></indexterm>

      <para>لا يشترط Nagios تثبيت أي شيء على الأجهزة المراقبة بخلاف Munin؛ بل يستخدم Nagios –معظم الأحيان– للتحقق من توفر الخدمات الشبكية. مثلاً، يمكن أن يتصل Nagios بمخدم الوب ويتحقق أنه يستطيع الحصول على صفحة وب معينة خلال مدة زمنية محددة.</para>
      <section>
        <title>التثبيت</title>

	<para>أول خطوة في إعداد Nagios هي تثبيت الحزم <emphasis role="pkg">nagios3</emphasis>، و<emphasis role="pkg">nagios-plugins</emphasis>، و<emphasis role="pkg">nagios3-doc</emphasis>. عملية التثبيت لهذه الحزم تتضمن إعداد واجهة وب وإنشاء مستخدم أولي باسم <literal>nagiosadmin</literal> (ويطلب منك تحديد كلمة السر لهذا الحساب). يمكن إضافة مستخدمين آخرين بسهولة بإضافتهم إلى ملف <filename dir="ltr">/etc/nagios3/htpasswd.users</filename> بالأمر <command>htpasswd</command> الذي يوفره مخدم الوب أباتشي. إذا لم يظهر سؤال Debconf عن كلمة السر أثناء التثبيت، فيمكن استخدام <command>dpkg-reconfigure nagios3-cgi</command> لتعريف كلمة السر لحساب <literal>nagiosadmin</literal>.</para>

	<para>تفتح واجهة الوب بتوجيه مستعرض الوب إلى العنوان <literal dir="ltr">http://<replaceable>server</replaceable>/nagios3/</literal>؛ لاحظ أن Nagios يراقب وحده بعض المتغيرات للجهاز الذي يعمل عليه. لكن لا تعمل بعض المزايا التفاعلية مثل إضافة التعليقات إلى المستضيف. هذه المزايا معطلة في إعدادات Nagios الافتراضية، إذ أن هذه الإعدادات مقيدة جداً لأسباب أمنية.</para>

	<para>كما هو موثق في <filename dir="ltr">/usr/share/doc/nagios3/README.Debian</filename>، لتفعيل بعض المزايا يجب تعديل <filename dir="ltr">/etc/nagios3/nagios.cfg</filename> وتغيير  قيمة المتغير <literal>check_external_commands</literal> إلى ”1“. كما نحتاج ضبط صلاحيات الكتابة للمجلدات التي يستخدمها Nagios، بأوامر تشبه ما يلي:</para>

        <screen><computeroutput># </computeroutput><userinput>service nagios3 stop</userinput>
<computeroutput>[...]
# </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios3/rw
</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios3
</userinput><computeroutput># </computeroutput><userinput>service nagios3 start</userinput>
<computeroutput>[...]</computeroutput></screen>
      </section>
      <section>
        <title>الضبط</title>

	<para>واجهة الوب في Nagios جميلة نسبيًا، لكنها لا تسمح بتغيير الإعدادات، ولا يمكن استخدامها لإضافة أجهزة أو خدمات لمراقبتها. كل الإعداد تديره ملفات يشير إليها ملف الإعداد المركزي، وهو <filename dir="ltr">/etc/nagios3/nagios.cfg</filename>.</para>

	<para>قبل الغوص في هذه الملفات، يجب فهم بعض مفاهيم Nagios. يشمل الإعداد مجموعة من الأنواع المختلفة من الكائنات:</para>
        <itemizedlist>
          <listitem>
	    <para><emphasis>host</emphasis> (المستضيف) هو الجهاز الذي ستتم مراقبته؛</para>
          </listitem>
          <listitem>
	    <para><emphasis>hostgroup</emphasis> هي مجموعة من المستضيفين يجب تجميعهم معاً عند العرض، أو لتجميع بعض الإعدادات المشتركة؛</para>
          </listitem>
          <listitem>
	    <para><emphasis>service</emphasis> (الخدمة) هي عنصر قابل للقياس متعلق بمستضيف أو بمجموعة من المستضيفين. الغالب أنها فحص لخدمة شبكية ما، لكن يمكن أن تشمل اختبار متغيرات أخرى أيضًا والتحقق أن قيمها ضمن مجال مقبول (مثلاً، مساحة القرص الحرة أو حمل المعالج)؛</para>
          </listitem>
          <listitem>
	    <para><emphasis>servicegroup</emphasis> هي مجموعة من الخدمات التي يجب تجميعها معاً عند العرض؛</para>
          </listitem>
          <listitem>
	    <para><emphasis>contact</emphasis> هو شخص يتلقى التنبيهات؛</para>
          </listitem>
          <listitem>
	    <para><emphasis>contactgroup</emphasis> مجموعة من الأشخاص الذين يتلقون التنبيهات؛</para>
          </listitem>
          <listitem>
	    <para><emphasis>timeperiod</emphasis> الفاصل الزمني بين كل عملية تحقق من بعض الخدمات؛</para>
          </listitem>
          <listitem>
	    <para><emphasis>command</emphasis> هو سطر من الأوامر يستدعى للتحقق من خدمة معينة.</para>
          </listitem>
        </itemizedlist>

	<para>لكل كائن عدد من الخصائص (تختلف حسب نوعه) التي يمكن تعديلها. لا يمكن أن نضع قائمة كاملة بها لكثرتها، لكن أهم الخصائص هي العلاقات بين الكائنات.</para>

	<para>تستخدم الخدمة (<emphasis>service</emphasis>) أمراً (<emphasis>command</emphasis>) للتحقق من حالة ميزة على مستضيف (<emphasis>host</emphasis>) معين (أو مجموعة <emphasis>hostgroup</emphasis>) خلال فاصل زمني (<emphasis>timeperiod</emphasis>). في حال حدوث مشكلة، يرسل Nagios تنبيهاً لجميع أعضاء <emphasis>contactgroup</emphasis> المرتبطة بتلك الخدمة. يرسل التنبيه لكل عضو وفقاً لقناة الاتصال المحددة في كائن <emphasis>contact</emphasis> المقابل له.</para>

	<para>يسمح نظام الوراثة بتشارك مجموعة من الخصائص بين العديد من الكائنات دون تكرار المعلومات. كما يتضمن الإعداد الأولي عدد من الكائنات القياسية؛ إن تعريف مستضيف جديد أو خدمة أو جهة اتصال في معظم الأحيان هو مجرد اشتقاق للكائنات العامة المعرفة مسبقًا. الملفات في <filename>/etc/nagios3/conf.d/</filename> هي مصدر جيد لتعلم طريقة عمل هذه الكائنات.</para>

	<para>يستخدم مديرو النظم في شركة فلكوت الإعداد التالي:</para>

        <example>
          <title>الملف <filename dir="ltr">/etc/nagios3/conf.d/falcot.cfg</filename></title>

          <programlisting>define contact{
    name                            generic-contact
    service_notification_period     24x7
    host_notification_period        24x7
    service_notification_options    w,u,c,r
    host_notification_options       d,u,r
    service_notification_commands   notify-service-by-email
    host_notification_commands      notify-host-by-email
    register                        0 ; Template only
}
define contact{
    use             generic-contact
    contact_name    rhertzog
    alias           Raphael Hertzog
    email           hertzog@debian.org
}
define contact{
    use             generic-contact
    contact_name    rmas
    alias           Roland Mas
    email           lolando@debian.org
}

define contactgroup{
    contactgroup_name     falcot-admins
    alias                 Falcot Administrators
    members               rhertzog,rmas
}

define host{
    use                   generic-host ; Name of host template to use
    host_name             www-host
    alias                 www.falcot.com
    address               192.168.0.5
    contact_groups        falcot-admins
    hostgroups            debian-servers,ssh-servers
}
define host{
    use                   generic-host ; Name of host template to use
    host_name             ftp-host
    alias                 ftp.falcot.com
    address               192.168.0.6
    contact_groups        falcot-admins
    hostgroups            debian-servers,ssh-servers
}

# 'check_ftp' command with custom parameters
define command{
    command_name          check_ftp2
    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35
}

# Generic Falcot service
define service{
    name                  falcot-service
    use                   generic-service
    contact_groups        falcot-admins
    register              0
}

# Services to check on www-host
define service{
    use                   falcot-service
    host_name             www-host
    service_description   HTTP
    check_command         check_http
}
define service{
    use                   falcot-service
    host_name             www-host
    service_description   HTTPS
    check_command         check_https
}
define service{
    use                   falcot-service
    host_name             www-host
    service_description   SMTP
    check_command         check_smtp
}

# Services to check on ftp-host
define service{
    use                   falcot-service
    host_name             ftp-host
    service_description   FTP
    check_command         check_ftp2
}
</programlisting>
        </example>

	<para>يُعرِّف ملف الإعداد هذا مستضيفين لمراقبتهما. الأول مخدم وب، وتجرى عليه فحوصات على منفذ HTTP‏ (80) ومنفذ HTTPS‏ (443). يختبر Nagios أيضاً مخدم SMTP يعمل على المنفذ 25. المستضيف الثاني هو مخدم FTP، ويتضمن الاختبار التحقق أن الرد يتم خلال 20 ثانية. بعد هذا التأخير يولد <emphasis>warning</emphasis>؛ أما بعد 30 ثانية، فيصدر إنذار حرج. تظهر واجهة الوب الخاصة بـNagios أن خدمة SSH مراقبة أيضاً: هذه المراقبة ناتجة عن انضمام المستضيفون لمجموعة <literal>ssh-servers</literal>. الخدمة القياسية المقابلة معرفة في <filename dir="ltr">/etc/nagios3/conf.d/services_nagios2.cfg</filename>.</para>

	<para>لاحظ استخدام الوراثة: يرث الكائن من كائن آخر باستخدام ”use <replaceable>parent-name</replaceable>“. يجب أن يكون الكائن الأب قابلاً للتعرف، عبر إسناد اسم له في خاصية ”name <replaceable>identifier</replaceable>“. إذا كان الهدف من الكائن الأب أن يستعمل في الوراثة فقط دون أن يكون كائناً حقيقياً، عندها يعطى الخاصية ”register 0“ حتى لا يأخذه Nagios بعين الاعتبار، وبالتالي يتجاهل نقصان بعض المتغيرات المطلوبة في الحالة الطبيعية.</para>

        <sidebar>
          <title><emphasis>توثيق</emphasis> قائمة بخصائص الكائنات</title>

	  <para>يمكن التعمق في فهم الطرق المختلفة لإعداد Nagios من الوثائق المتاحة في حزمة <emphasis role="pkg">nagios3-doc</emphasis>. يمكن الوصول لهذه الوثائق مباشرة عبر واجهة الوب، من خلال وصلة ”Documentation“ في الزاوية اليسرى العليا. يتضمن التوثيق قائمة بأنواع الكائنات، مع جميع الخصائص التي يمكن أن تملكها. كما يشرح كيفية إنشاء ملحقات جديدة.</para>
        </sidebar>

        <sidebar>
          <title><emphasis>التعمق أكثر</emphasis> الاختبار عن بعد باستخدام NRPE</title>

	  <para>العديد من ملحقات Nagios تسمح باختبار بعض المتغيرات المحلية على المستضيف؛ إذا كانت هناك حاجة لإجراء مثل هذه الاختبارات مع تجميع نتائجها في مكان واحد، فيجب نشر الملحق NPRE‏ (<emphasis>Nagios Remote Plugin Executor</emphasis>). يجب تثبيت الحزمة <emphasis role="pkg">nagios-nrpe-plugin</emphasis> على مخدم Nagios، والحزمة <emphasis role="pkg">nagios-nrpe-server</emphasis> على الأجهزة التي يراد إجراء الاختبارات عليها. تأخذ الأخيرة إعداداتها من <filename dir="ltr">/etc/nagios/nrpe.cfg</filename>. يجب أن يحدد هذا الملف الاختبارات التي يمكن تنفيذها عن بعد، وعناوين IP للأجهزة التي يسمح لها بطلب هذه الاختبارات. تفعيل هذه الاختبارات البعيدة على طرف Nagios يتم ببساطة بإضافة الخدمات المقابلة لها باستخدام الأمر الجديد <emphasis>check_nrpe</emphasis>.</para>
        </sidebar>
      </section>
    </section>
  </section>
</chapter>
