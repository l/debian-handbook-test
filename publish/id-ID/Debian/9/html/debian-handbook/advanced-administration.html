<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html
    xmlns="http://www.w3.org/1999/xhtml"><head><meta
        http-equiv="Content-Type"
        content="text/html; charset=UTF-8" /><title
        xmlns:d="http://docbook.org/ns/docbook">Bab 12. Administrasi Tingkat Lanjut</title><link
        rel="stylesheet"
        type="text/css"
        href="Common_Content/css/default.css" /><link
        rel="stylesheet"
        media="print"
        href="Common_Content/css/print.css"
        type="text/css" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="generator"
        content="publican v4.3.2" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="package"
        content="Debian-debian-handbook-9-id-ID-1.0-1" /><meta
        name="keywords"
        content="RAID, LVM, FAI, Preseeding, Pemantauan, Virtualisasi, Xen, LXC" /><link
        rel="home"
        href="index.html"
        title="Buku Panduan Administrator Debian" /><link
        rel="up"
        href="index.html"
        title="Buku Panduan Administrator Debian" /><link
        rel="prev"
        href="sect.rtc-services.html"
        title="11.8. Layanan Komunikasi Real-Time" /><link
        rel="next"
        href="sect.virtualization.html"
        title="12.2. Virtualisasi" /><meta
        xmlns=""
        name="flattr:id"
        content="4pz9jq" /><link
        xmlns=""
        rel="canonical"
        href="https://debian-handbook.info/browse/id-ID/stable/advanced-administration.html" /></head><body><div
        id="banner"><a
          href="http://debian-handbook.info/get/"><span
            class="text">Download the ebook</span></a></div><p
        id="title"><a
          class="left"
          href="http://www.debian.org"><img
            alt="Product Site"
            src="Common_Content/images//image_left.png" /></a><a
          class="right"
          href="index.html"><img
            alt="Documentation Site"
            src="Common_Content/images//image_right.png" /></a></p><ul
        class="docnav top"><li
          class="previous"><a
            accesskey="p"
            href="sect.rtc-services.html"><strong>Sebelumnya</strong></a></li><li
          class="home">Buku Panduan Administrator Debian</li><li
          class="next"><a
            accesskey="n"
            href="sect.virtualization.html"><strong>Lanjut</strong></a></li></ul><div
        xml:lang="id-ID"
        class="chapter"
        lang="id-ID"><div
          class="titlepage"><div><div><h1
                class="title"><a
                  xmlns=""
                  id="advanced-administration"></a>Bab 12. Administrasi Tingkat Lanjut</h1></div></div></div><div
          class="toc"><dl
            class="toc"><dt><span
                class="section"><a
                  href="advanced-administration.html#sect.raid-and-lvm">12.1. RAID dan LVM</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.raid-soft">12.1.1. RAID Perangkat Lunak</a></span></dt><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.lvm">12.1.2. LVM</a></span></dt><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.raid-or-lvm">12.1.3. RAID atau LVM?</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.virtualization.html">12.2. Virtualisasi</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.virtualization.html#sect.xen">12.2.1. Xen</a></span></dt><dt><span
                    class="section"><a
                      href="sect.virtualization.html#sect.lxc">12.2.2. LXC</a></span></dt><dt><span
                    class="section"><a
                      href="sect.virtualization.html#id-1.15.5.14">12.2.3. Virtualisasi dengan KVM</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.automated-installation.html">12.3. Pemasangan Otomatis</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.fai">12.3.1. Fully Automatic Installer (FAI, Pemasang Otomatis Sepenuhnya)</a></span></dt><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.d-i-preseeding">12.3.2. Memprabibit Debian-Installer</a></span></dt><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.simple-cdd">12.3.3. Simple-CDD: Solusi Semua-Jadi-Satu</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.monitoring.html">12.4. Pemantauan</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.monitoring.html#sect.munin">12.4.1. Menyiapkan Munin</a></span></dt><dt><span
                    class="section"><a
                      href="sect.monitoring.html#sect.nagios">12.4.2. Menyiapkan Nagios</a></span></dt></dl></dd></dl></div><div
          class="highlights"><div
            class="para">
		Bab ini meninjau kembali beberapa aspek yang telah kami uraikan, dengan perspektif yang berbeda: alih-alih memasang pada sebuah komputer, kita akan mempelajari sistem deployment masal; alih-alih membuat volume RAID atau LVM pada saat instalasi, kita akan belajar melakukannya secara manual sehingga nanti kita dapat merevisi pilihan awal kita. Akhirnya, kita akan mendiskusikan perkakas pemantauan dan teknik virtualisasi. Sebagai konsekuensinya, bab ini secara lebih khusus menarget para administrator profesional, and sedikit kurang brfokus pada para individu yang bertanggungjawab atas jaringan rumahan mereka.
	</div></div><div
          class="section"><div
            class="titlepage"><div><div><h2
                  class="title"><a
                    xmlns=""
                    id="sect.raid-and-lvm"></a>12.1. RAID dan LVM</h2></div></div></div><div
            class="para">
			<a
              class="xref"
              href="installation.html">Bab 4, <em>Instalasi</em></a> mempresentasikan teknologi ini dari sudut pandang pemasang, dan bagaimana itu mengintegrasikan mereka untuk membuat deployment mereka mudah dari awal. Setelah instalasi awal, seorang administrator mesti bisa menangani keperluan ruang penyimpanan yang berkembang tanpa mesti mengandalkan instalasi ulang yang mahal. Maka mereka mesti paham peralatan yang diperlukan untuk memanipulasi volume RAID dan LVM.
		</div><div
            class="para">
			RAID dan LVM adalah teknik untuk mengabstrakkan volume yang dikait dari pasangan fisik mereka (yaitu hard disk atau partisi); yang pertama mengamankan data dari kegagalan perangkat keras dengan memperkenalkan redundansi, yang belakangan membuat manajemen volume lebih luwes dan tak bergantung kepada ukuran sebenarnya dari disk yang mendasarinya. Dalam kedua kasus, sistem pada akhirnya mendapat perangkat blok baru, yang dapat dipakai untuk membuat sistem berkas atau ruang swap, tanpa perlu mereka dipetakan ke satu disk fisik. RAID dan LVM datang dari latar belakang yang cukup berbeda, tapi fungsionalitas mereka sebagian dapat bertumpang tindih, sehingga mereka sering disinggung bersama-sama.
		</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>PERSPEKTIF</em></span> Btrfs menggabung LVM dan RAID</strong></p></div></div></div><div
              class="para">
			Walaupun LVM dan RAID adalah dua subsistem kernel yang berbeda, yang hadir di antara perangkat blok disk dan sistem berkas mereka, <span
                class="emphasis"><em>btrfs</em></span> adalah suatu sistem berkas baru, yang pada awalnya dikembangkan di Oracle, yang bertujuan menggabung set fitur dari LVM dan RAID serta lebih banyak lagi. Sebagian besar sudah berfungsi, dan walaupun masih di-tag "eksperimental" karena pengembangannya belum lengkap (beberapa fitur belum diimplementasi), itu telah terpakai dalam lingkungan produksi. <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://btrfs.wiki.kernel.org/">http://btrfs.wiki.kernel.org/</a></div>
		</div><div
              class="para">
			Diantara fitur yang menarik adalah kemampuan membuat snapshot dari suatu pohon sistem berkas pada sebarang waktu. Snapshot ini pada awalnya tak memakai sebarang ruang disk, data hanya diduplikasi ketika satu dari salinan-salinan dimodifikasi. Sistem berkas juga menangani kompresi transparan dari berkas, dan checksum memastikan integritas dari semua data yang disimpan.
		</div></div><div
            class="para">
			Pada kedua kasus RAID dan LVM, kernel menyediakan suatu berkas perangkat blok, mirip dengan yang berkaitan dengan suatu hard disk atau suatu partisi. Ketika suatu aplikasi, atau bagian lain dari kernel, meminta akses ke suatu blok dari perangkat seperti itu, subsistem yang sesuai mengarahkan blok ke lapisan fisik yang relevan. Bergantung kepada konfigurasi, blok ini dapat disimpan pada satu atau beberapa disk fisik, dan lokasi fisiknya mungkin tak berkorelasi langsung ke lokasi blok dalam perangkat lojik.
		</div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      xmlns=""
                      id="sect.raid-soft"></a>12.1.1. RAID Perangkat Lunak</h3></div></div></div><a
              id="id-1.15.4.6.2"
              class="indexterm"></a><div
              class="para">
				RAID adalah <span
                class="emphasis"><em>Redundant Array of Independent Disks</em></span> (larik redundan dari disk-disk independen). Tujuan dari sistem ini adalah untuk mencegah kehilangan data dalam kasus kegagalan hard disk. Prinsip umumnya cukup sederhana: data disimpan pada beberapa disk fisik alih-alih hanya satu, dengan tingkat redundansi yang dapat dikonfigurasi. Bergantung kepada banyaknya redundansi ini, dan bahkan dalam kejadian kegagalan disk yang tak terduga, data dapat direkonstruksi tanpa adanya kehilangan dari disk sisanya.
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>KULTUR</em></span> <span
                          class="foreignphrase"><em
                            class="foreignphrase">Independen</em></span> atau <span
                          class="foreignphrase"><em
                            class="foreignphrase">tidak mahal</em></span>?</strong></p></div></div></div><div
                class="para">
				I dalam RAID pada awalnya merupakan singkatan dari <span
                  class="emphasis"><em>inexpensive (tidak mahal)</em></span>, karena RAID memungkinkan kenaikan drastis keselamatan data tanpa memerlukan investasi disk canggih yang mahal. Namun mungkin karena masalah citra, kini lebih umum dianggap singkatan dari <span
                  class="emphasis"><em>independen</em></span>, yang tak membawa kesan murahan yang tak menarik.
			</div></div><div
              class="para">
				RAID dapat diwujudkan baik oleh perangkat keras khusus (modul RAID yang terintegrasi ke dalam kartu pengendali SCSI atau SATA) atau oleh abstraksi perangkat lunak (kernel). Apakah perangkat keras atau perangkat lunak, sistem RAID dengan redundansi yang cukup bisa secara transparan tetap operasional ketika sebuah disk gagal; lapisan atas tumpukan (aplikasi) bahkan dapat tetap mengakses data terlepas dari kegagalan. Tentu saja, "mode terdegradasi" ini dapat memiliki dampak pada kinerja, dan redundansi berkurang, sehingga kegagalan disk lebih lanjut dapat mengakibatkan kehilangan data. Dalam prakteknya, oleh karena itu, kita akan berusaha untuk hanya berada dalam mode terdegradasi ini selama diperlukannya untuk menggantikan disk yang gagal. Sekali disk baru terpasang sistem RAID dapat merekonstruksi data yang dibutuhkan untuk kembali ke mode aman. Aplikasi tidak akan melihat apa-apa, selain kecepatan akses berpotensi berkurang, sementara larik ada dalam mode terdegradasi atau selama fase rekonstruksi.
			</div><div
              class="para">
				Ketika RAID diimplementasikan oleh perangkat keras, konfigurasinya umumnya terjadi dalam alat konfigurasi BIOS, dan kernel akan menganggap sebuah array RAID sebagai satu disk, yang akan bekerja sebagai disk fisik standar, meskipun nama perangkat mungkin berbeda (tergantung pada driver).
			</div><div
              class="para">
				Kami hanya berfokus pada RAID perangkat lunak dalam buku ini.
			</div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.raid-levels"></a>12.1.1.1. Tingkat-tingkat RAID</h4></div></div></div><div
                class="para">
					RAID sebenarnya buka satu sistem, tapi berbagai sistem yang diidentifikasi oleh tingkat mereka; tingkat-tingkat itu dibedakan oleh tata letak dan banyaknya redundansi yang mereka sediakan. Semakin banyak redundan, semakin kebal kegagalan, karena sistem akan dapat terus bekerja dengan lebih banyak disk yang gagal. Kekurangannya adalah bahwa ruang yang dapat digunakan menyusut untuk satu set disk tertentu; dilihat dengan cara lain, akan diperlukan lebih banyak disk untuk menyimpan sejumlah data yang diberikan.
				</div><div
                class="variablelist"><dl
                  class="variablelist"><dt><span
                      class="term">RAID Linier</span></dt><dd><div
                      class="para">
								Meskipun subsistem RAID kernel memungkinkan menciptakan "linear RAID", ini bukan RAID yang benar, karena konfigurasi ini tidak melibatkan redundansi apapun. Kernel hanya mengumpulkan beberapa disk end-to-end dan menyediakan volume agregat yang dihasilkan sebagai satu disk virtual (satu perangkat blok). Itu adalah satu-satunya fungsinya. Konfigurasi ini jarang digunakan sendirian (lihat nanti untuk pengecualian), terutama karena kurangnya redundansi berarti bahwa salah satu disk gagal membuat seluruh agregat, dan karena itu semua data, tidak tersedia.
							</div></dd><dt><span
                      class="term">RAID-0</span></dt><dd><div
                      class="para">
								Tingkat ini tidak menyediakan redundansi apapun, tapi disk-disk tidak hanya sekadar dilekatkan satu di akhir yang lain: mereka dibagi dalam <span
                        class="emphasis"><em>stripe</em></span>, dan blok-blok di perangkat virtual disimpan dalam stripe di disk-disk fisik yang berbeda-beda. Dalam setup RAID-0 dua-disk, misalnya, blok bernomor genap dari perangkat virtual akan disimpan pada disk fisik pertama, sementara blok bernomor ganjil akan berakhir pada disk fisik kedua.
							</div><div
                      class="para">
								Sistem ini tidak bertujuan meningkatkan keandalan, karena (seperti dalam kasus linier) ketersediaan semua data hancur begitu satu disk gagal, tetapi meningkatkan kinerja: selama akses berurutan ke sejumlah besar data yang berdekatan, kernel akan mampu membaca dari kedua disk (atau menulis ke mereka) secara paralel, yang akan meningkatkan laju transfer data. Namun, penggunaan RAID-0 menyusut, ceruk ini diisi oleh LVM (lihat nanti).
							</div></dd><dt><span
                      class="term">RAID-1</span></dt><dd><div
                      class="para">
								Tingkat ini, juga dikenal sebagai "RAID mirroring", adalah yang paling sederhana dan setup yang paling banyak digunakan. Dalam bentuk standar, menggunakan dua disk fisik berukuran sama, dan memberikan volume logis berukuran yang sama lagi. Data disimpan identik pada disk kedua, maka dijuluki "mirror (cermin)". Ketika satu disk gagal, data ini masih tersedia di yang lain. Untuk data yang benar-benar penting, RAID-1 dapat tentu saja diatur pada disk yang lebih dari dua, dengan dampak langsung pada rasio biaya perangkat keras versus ruang muatan yang tersedia.
							</div><div
                      class="sidebar"><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>CATATAN</em></span> Ukuran klaster dan disk</strong></p></div></div></div><div
                        class="para">
								Jika dua disk dengan ukuran yang berbeda diatur dalam cermin, yang lebih besar tidak akan sepenuhnya digunakan, karena itu akan berisi data yang sama seperti yang terkecil dan tidak lebih. Ruang tersedia yang berguna yang disediakan oleh volume RAID-1 karena itu cocok dengan ukuran disk terkecil dalam array. Ini masih berlaku untuk volume RAID dengan tingkat yang lebih tinggi, meskipun redundansi disimpan dengan cara berbeda.
							</div><div
                        class="para">
								Karena itu penting, ketika menyiapkan array RAID (kecuali RAID-0 dan "linear RAID"), untuk menyusun hanya disk-disk berukuran identik atau sangat dekat, untuk menghindari membuang-buang sumber daya.
							</div></div><div
                      class="sidebar"><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>CATATAN</em></span> Disk cadangan</strong></p></div></div></div><div
                        class="para">
								Tingkat RAID yang mencakup redundansi memungkinkan menugaskan disk lebih banyak dari yang dibutuhkan untuk array. Tambahan disk digunakan sebagai suku cadang ketika salah satu disk utama gagal. Sebagai contoh, cermin dua disk ditambah cadangan satu, jika salah satu disk dari dua pertama gagal, kernel akan otomatis (dan segera) merekonstruksi cermin menggunakan disk cadangan, sehingga redundansi tetap terjamin setelah masa rekonstruksi. Ini dapat digunakan sebagai jenis lain dari perlindungan bagi data penting.
							</div><div
                        class="para">
								Kita akan dimaafkan untuk bertanya-tanya bagaimana hal ini lebih baik daripada sekadar mencerminkan pada tiga disk di awal. Keuntungan dari konfigurasi "disk cadangan" adalah bahwa disk cadangan dapat dipakai bersama pada beberapa volume RAID. Sebagai contoh, kita dapat memiliki tiga volume tercermin, dengan redundansi yang dijamin bahkan jika salah satu kegagalan disk, hanya dengan tujuh disk (tiga pasang, ditambah satu cadangan bersama), bukan sembilan disk yang akan dibutuhkan oleh tiga buah kembar tiga.
							</div></div><div
                      class="para">
								Tingkat RAID ini, walaupun mahal (karena hanya separuh dari ruang penyimpanan fisik, dalam keadaan terbaik, berguna), secara luas digunakan dalam praktek. Hal ini mudah untuk dipahami, dan memungkinkan cadangan yang sangat sederhana: karena disk kedua memiliki isi yang identik, salah satu dari mereka dapat sementara diekstraksi dengan tidak berdampak pada sistem yang bekerja. Kinerja baca sering meningkat karena kernel bisa membaca setengah dari data pada setiap disk secara paralel, sementara kinerja tulis tidak terlalu turun parah. Dalam sebuah array RAID-1 N disk, data tetap tersedia bahkan dengan N-1 disk gagal.
							</div></dd><dt><span
                      class="term">RAID-4</span></dt><dd><div
                      class="para">
								Tingkat RAID ini, tidak banyak dipakai, menggunakan N disk untuk menyimpan data yang berguna, dan disk tambahan untuk menyimpan informasi redundansi. Jika disk itu gagal, sistem dapat merekonstruksi isinya dari N yang lain. Jika salah satu dari N disk data gagal, N-1 sisa yang dikombinasikan dengan disk "paritas" berisi cukup informasi untuk merekonstruksi data yang dibutuhkan.
							</div><div
                      class="para">
								RAID-4 tidak terlalu mahal karena itu hanya melibatkan peningkatan biaya satu-dari-N dan tidak memiliki dampak yang terlihat pada kinerja baca, tapi penulisan melambat. Selanjutnya, karena menulis ke salah satu dari N disk juga melibatkan menulis ke disk paritas, yang terakhir melihat menulis lebih banyak daripada yang pertama, dan usia pakainya dapat memendek secara dramatis sebagai akibatnya. Data pada array RAID-4 aman hanya sampai dengan satu disk gagal (dari N+1).
							</div></dd><dt><span
                      class="term">RAID-5</span></dt><dd><div
                      class="para">
								RAID-5 menjawab masalah asimetri dari RAID-4: blok paritas disebar ke seluruh N+1 disk, tanpa ada satu disk yang memiliki peran tertentu.
							</div><div
                      class="para">
								Kinerja baca dan tulis identik dengan RAID-4. Di sini, sistem tetap berfungsi bila satu disk (dari N+1) gagal, tapi tak boleh lebih.
							</div></dd><dt><span
                      class="term">RAID-6</span></dt><dd><div
                      class="para">
								RAID-6 dapat dianggap perluasan dari RAID-5, dimana setiap seri N blok melibatkan dua blok redundansi, dan setiap seri N+2 blok disebar ke N+2 disk.
							</div><div
                      class="para">
								Tingkat RAID ini sedikit lebih mahal daripada dua sebelumnya, tapi itu membawa beberapa keamanan tambahan karena sampai dengan dua drive (dari N+2) bisa gagal tanpa mengorbankan ketersediaan data. Kekurangannya adalah bahwa sekarang operasi tulis melibatkan menulis satu blok data dan dua blok redundansi, yang membuat mereka lebih lambat lagi.
							</div></dd><dt><span
                      class="term">RAID-1+0</span></dt><dd><div
                      class="para">
								This isn't strictly speaking, a RAID level, but a stacking of two RAID groupings. Starting from 2×N disks, one first sets them up by pairs into N RAID-1 volumes; these N volumes are then aggregated into one, either by “linear RAID” or (increasingly) by LVM. This last case goes farther than pure RAID, but there's no problem with that.
							</div><div
                      class="para">
								RAID-1+0 dapat bertahan dari beberapa disk gagal: sampai N dalam array 2×N yang dijelaskan di atas, asal bahwa setidaknya satu disk tetap bekerja di setiap pasangan RAID-1.
							</div><div
                      class="sidebar"><a
                        xmlns=""
                        id="sidebar.raid-10"></a><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>LEBIH JAUH</em></span> RAID-10</strong></p></div></div></div><div
                        class="para">
								RAID-10 umumnya dianggap sebagai sinonim dari RAID-1+0, namun kekhususan Linux membuat itu sebenarnya generalisasi. Konfigurasi ini memungkinkan sistem dimana setiap blok disimpan pada dua disk berbeda, bahkan dengan cacah disk ganjil, salinan disebar ke model yang dapat dikonfigurasi.
							</div><div
                        class="para">
								Kinerja akan bervariasi tergantung pada model repartisi dan tingkat redundansi yang dipilih, dan beban kerja dari volume logis.
							</div></div></dd></dl></div><div
                class="para">
					Jelas, tingkat RAID akan dipilih sesuai dengan kendala dan persyaratan setiap aplikasi. Perhatikan bahwa satu komputer dapat memiliki beberapa array RAID yang berbeda dengan konfigurasi yang berbeda.
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.raid-setup"></a>12.1.1.2. Menyiapkan RAID</h4></div></div></div><a
                id="id-1.15.4.6.9.2"
                class="indexterm"></a><div
                class="para">
					Menyiapkan volume RAID memerlukan paket <span
                  class="pkg pkg">mdadm</span>; ini menyediakan perintah <code
                  class="command">mdadm</code>, yang memungkinkan membuat dan memanipulasi array RAID, maupun skrip dan alat-alat yang mengintegrasikan ke seluruh sistem, termasuk sistem pemantauan.
				</div><div
                class="para">
					Contoh kita akan menjadi server dengan sejumlah disk, beberapa di antaranya sudah digunakan, sisanya tersedia untuk menyiapkan RAID. Kita awalnya memiliki disk dan partisi sebagai berikut:
				</div><div
                xmlns:d="http://docbook.org/ns/docbook"
                class="itemizedlist"><ul><li
                    class="listitem"><div
                      class="para">
							disk <code
                        class="filename">sdb</code>, 4 GB, sepenuhnya tersedia;
						</div></li><li
                    class="listitem"><div
                      class="para">
							disk <code
                        class="filename">sdc</code>, 4 GB, ini juga sepenuhnya tersedia;
						</div></li><li
                    class="listitem"><div
                      class="para">
							pada disk <code
                        class="filename">sdd</code>, hanya partisi <code
                        class="filename">sdd2</code> (sekitar 4 GB) tersedia;
						</div></li><li
                    class="listitem"><div
                      class="para">
							akhirnya, disk <code
                        class="filename">sde</code>, masih 4 GB, sepenuhnya tersedia.
						</div></li></ul></div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>CATATAN</em></span> Mengidentifikasi volume RAID yang ada</strong></p></div></div></div><div
                  class="para">
					Berkas <code
                    class="filename">/proc/mdstat</code> memuat daftar volume yang ada dan keadaan mereka. Ketika membuat volume RAID baru, mesti hati-hati untuk tidak memberi nama yang sama dengan volume yang sudah ada.
				</div></div><div
                class="para">
					Kita akan menggunakan unsur-unsur fisik ini untuk membangun dua volume, satu RAID-0 dan satu cermin (RAID-1). Mari kita mulai dengan volume RAID-0:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</code></strong>
<code
                  class="computeroutput">mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
# </code><strong
                  class="userinput"><code>mdadm --query /dev/md0</code></strong>
<code
                  class="computeroutput">/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md0</code></strong>
<code
                  class="computeroutput">/dev/md0:
        Version : 1.2
  Creation Time : Wed May  6 09:24:34 2015
     Raid Level : raid0
     Array Size : 8387584 (8.00 GiB 8.59 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:24:34 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

     Chunk Size : 512K

           Name : mirwiz:0  (local to host mirwiz)
           UUID : bb085b35:28e821bd:20d697c9:650152bb
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       1       8       32        1      active sync   /dev/sdc
# </code><strong
                  class="userinput"><code>mkfs.ext4 /dev/md0</code></strong>
<code
                  class="computeroutput">mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 2095104 4k blocks and 524288 inodes
Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </code><strong
                  class="userinput"><code>mkdir /srv/raid-0</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mount /dev/md0 /srv/raid-0</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/raid-0</code></strong>
<code
                  class="computeroutput">Filesystem      Size  Used Avail Use% Mounted on
/dev/md0        7.9G   18M  7.4G   1% /srv/raid-0
</code></pre><div
                class="para">
					Perintah <code
                  class="command">mdadm --create</code> memerlukan beberapa parameter: nama volume yang akan dibuat (<code
                  class="filename">/dev/md*</code>, dengan MD singkatan dari <span
                  class="foreignphrase"><em
                    class="foreignphrase">Multiple Devices</em></span>), tingkat RAID, cacah disk (yang wajib meskipun sebagian besar bermakna hanya dengan RAID-1 dan di atasnya), dan drive fisik yang akan digunakan. Setelah perangkat dibuat, kita dapat menggunakannya seperti kita akan menggunakan sebuah partisi normal, membuat sebuah sistem berkas di atasnya, mengait sistem berkas itu, dan sebagainya. Perhatikan bahwa penciptaan kita atas suatu volume RAID-0 pada <code
                  class="filename">md0</code> hanya kebetulan, dan penomoran array tidak perlu berkorelasi dengan pilihan banyaknya redundansi. Hal ini juga memungkinkan untuk membuat array RAID bernama, dengan memberikan parameter <code
                  class="command">mdadm</code> seperti misalnya <code
                  class="filename">/dev/md/linear</code> bukan <code
                  class="filename">/dev/md0</code>.
				</div><div
                class="para">
					Penciptaan RAID-1 mengikuti cara yang sama, perbedaannya hanya menjadi terlihat setelah penciptaan:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%
Continue creating array? </code><strong
                  class="userinput"><code>y</code></strong>
<code
                  class="computeroutput">mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md1 started.
# </code><strong
                  class="userinput"><code>mdadm --query /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
        Version : 1.2
  Creation Time : Wed May  6 09:30:19 2015
     Raid Level : raid1
     Array Size : 4192192 (4.00 GiB 4.29 GB)
  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:30:40 2015
          State : clean, resyncing (PENDING) 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       1       8       64        1      active sync   /dev/sde
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
          State : clean
[...]
</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>TIPS</em></span> RAID, disk, dan partisi</strong></p></div></div></div><div
                  class="para">
					Seperti digambarkan oleh contoh kita, peranti RAID dapat dibangun dari partisi disk, dan tidak memerlukan disk penuh.
				</div></div><div
                class="para">
					Beberapa komentar perlu disinggung. Pertama, <code
                  class="command">mdadm</code> tahu bahwa elemen-elemen fisik memiliki ukuran yang berbeda; karena hal ini menyiratkan bahwa sebagian ruang akan hilang pada elemen yang lebih besar, konfirmasi diperlukan.
				</div><div
                class="para">
					Lebih penting lagi, perhatikan keadaan cermin. Keadaan normal dari suatu cermin RAID adalah bahwa kedua disk memiliki isi yang tepat sama. Namun, tidak ada yang menjamin ini ketika volume pertama kali dibuat. Subsistem RAID karena itu akan memberikan jaminan itu sendiri, dan akan ada tahap sinkronisasi segera setelah perangkat RAID dibuat. Setelah beberapa waktu (lama persisnya akan tergantung pada ukuran sebenarnya dari disk...), array RAID berpindah ke keadaan "aktif" atau "bersih". Perhatikan bahwa selama fase rekonstruksi ini, cermin ada dalam mode terdegradasi, dan redundansi tidak dijamin. Sebuah disk yang gagal selama jendela risiko itu bisa mengakibatkan kehilangan semua data. Namun, sejumlah besar data penting, jarang disimpan pada sebuah array RAID yang baru dibuat sebelum sinkronisasi awalnya. Catat bahwa bahkan dalam mode terdegradasi, <code
                  class="filename">/dev/md1</code> dapat digunakan, dan sebuah sistem berkas dapat dibuat di atasnya, maupun data dapat disalin ke sana.
				</div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>TIPS</em></span> Memulai mirror dalam mode terdegradasi</strong></p></div></div></div><div
                  class="para">
					Kadang-kadang dua disk tidak tersedia seketika saat seseorang ingin memulai suatu cermin RAID-1, misalnya karena salah satu disk yang rencananya akan disertakan sudah digunakan untuk menyimpan data yang ingin dipindah ke array. Dalam keadaan seperti itu, dimungkinkan untuk sengaja menciptakan array RAID-1 yang terdegradasi dengan memberikan <code
                    class="filename">missing</code>, bukan berkas perangkat sebagai salah satu argumen untuk <code
                    class="command">mdadm</code>. Setelah data telah disalin ke "cermin", disk lama dapat ditambahkan ke array. Sinkronisasi kemudian akan terjadi, memberikan kita redundansi yang diinginkan di awal.
				</div></div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>TIPS</em></span> Menyiapkan cermin tanpa sinkronisasi</strong></p></div></div></div><div
                  class="para">
					Volume RAID-1 sering dibuat untuk digunakan sebagai disk baru, sering dianggap kosong. Isi awal sebenarnya dari disk ini karena itu tidak sangat relevan, karena hanya perlu diketahui bahwa data setelah penciptaan volume, khususnya sistem berkas, dapat diakses setelahnya.
				</div><div
                  class="para">
					Karena itu orang mungkin bertanya-tanya tentang titik sinkronisasi kedua disk pada waktu penciptaan. Mengapa peduli apakah isi identik pada zona volume yang kita ketahui hanya dapat dibaca setelah kita telah menulis?
				</div><div
                  class="para">
					Untungnya, tahap sinkronisasi ini dapat dihindari dengan memberikan opsi <code
                    class="literal">--assume-clean</code> untuk <code
                    class="command">mdadm</code>. Namun, pilihan ini dapat menyebabkan kejutan dalam kasus-kasus di mana data awal akan dibaca (misalnya jika sistem berkas tersebut sudah hadir pada disk fisik), itulah sebabnya itu tidak diaktifkan secara default.
				</div></div><div
                class="para">
					Sekarang mari kita lihat apa yang terjadi ketika salah satu elemen array RAID-1 gagal. <code
                  class="command">mdadm</code>, khususnya opsi <code
                  class="literal">--fail</code>, memungkinkan simulasi suatu kegagalan disk:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --fail /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: set /dev/sde faulty in /dev/md1
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Update Time : Wed May  6 09:39:39 2015
          State : clean, degraded 
 Active Devices : 1
Working Devices : 1
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 19

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       0        0        2      removed

       1       8       64        -      faulty   /dev/sde</code></pre><div
                class="para">
					Isi dari volume masih dapat diakses (dan, jika dipasang, aplikasi tidak menyadari apapun), tapi keselamatan data tidak dijamin lagi: seandainya <code
                  class="filename">sdd</code> disk gagal bergantian, data akan hilang. Kami ingin menghindari risiko, jadi kami akan mengganti disk yang gagal dengan yang baru, <code
                  class="filename">sdf</code>:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --add /dev/sdf</code></strong>
<code
                  class="computeroutput">mdadm: added /dev/sdf
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
   Raid Devices : 2
  Total Devices : 3
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:48:49 2015
          State : clean, degraded, recovering 
 Active Devices : 1
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 1

 Rebuild Status : 28% complete

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 26

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      spare rebuilding   /dev/sdf

       1       8       64        -      faulty   /dev/sde
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Update Time : Wed May  6 09:49:08 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 41

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf

       1       8       64        -      faulty   /dev/sde</code></pre><div
                class="para">
					Di sini lagi, kernel secara otomatis memicu tahap rekonstruksi yang ketika berlangsung, meskipun volume masih dapat diakses, berada dalam mode terdegradasi. Setelah rekonstruksi berakhir, array RAID kembali ke keadaan normal. Kita kemudian dapat memberitahu ke sistem bahwa disk <code
                  class="filename">sde</code> akan dihapus dari array, sehingga berakhir dengan RAID mirror klasik pada dua disk:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --remove /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: hot removed /dev/sde from /dev/md1
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf</code></pre><div
                class="para">
					Selanjutnya drive dapat secara fisik dicabut saat server berikutnya dimatikan, atau bahkan dicabut saat menyala ketika konfigurasi hardware mengizinkan hot-swap. Konfigurasi tersebut termasuk beberapa pengendali SCSI, kebanyakan disk SATA, dan drive eksternal yang beroperasi pada USB atau Firewire.
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.backup-raid-config"></a>12.1.1.3. Mem-back up Konfigurasi</h4></div></div></div><div
                class="para">
					Kebanyakan meta-data tentang volume RAID disimpan secara langsung pada disk yang menusun array ini, sehingga kernel dapat mendeteksi array dan komponen mereka dan merakit mereka secara otomatis saat sistem mulai berjalan. Namun, membuat cadangan konfigurasi ini disarankan, karena deteksi ini tidak kebal kesalahan, dan hanya diharapkan bahwa itu akan gagal tepat dalam keadaan yang sensitif. Dalam contoh kita, jika kegagalan disk <code
                  class="filename">sde</code> telah nyata (bukan simulasi) dan sistem sudah direstart tanpa menghapus disk <code
                  class="filename">sde</code> ini, disk ini bisa mulai bekerja lagi karena telah dijajaki selama reboot. Kernel kemudian akan memiliki tiga elemen fisik, masing-masing mengklaim mengandung setengah dari volume RAID yang sama. Sumber kebingungan lain dapat datang ketika volume RAID dari dua server dikonsolidasi hanya ke satu server. Jika array ini sedang berjalan biasanya sebelum disk dipindahkan, kernel akan mampu mendeteksi dan merakit kembali pasangan dengan benar; tetapi jika disk yang dipindah telah diagregasi ke dalam <code
                  class="filename">md1</code> pada server lama, dan server baru telah memiliki <code
                  class="filename">md1</code>, salah satu cermin akan diubah nama.
				</div><div
                class="para">
					Karena itu cadangan konfigurasi penting, walaupun hanya untuk referensi. Cara standar untuk melakukannya adalah dengan menyunting berkas <code
                  class="filename">/etc/mdadm/mdadm.conf</code>, contohnya tercantum di sini:
				</div><div
                class="example"><a
                  xmlns=""
                  id="example.mdadm-conf"></a><p
                  class="title"><strong>Contoh 12.1. berkas konfigurasi <code
                      class="command">mdadm</code></strong></p><div
                  class="example-contents"><pre
                    class="programlisting"># mdadm.conf
#
# Please refer to mdadm.conf(5) for information about this file.
#

# by default (built-in), scan all partitions (/proc/partitions) and all
# containers for MD superblocks. alternatively, specify devices to scan, using
# wildcards if desired.
DEVICE /dev/sd*

# auto-create devices with Debian standard permissions
CREATE owner=root group=disk mode=0660 auto=yes

# automatically tag new arrays as belonging to the local system
HOMEHOST &lt;system&gt;

# instruct the monitoring daemon where to send mail alerts
MAILADDR root

# definitions of existing MD arrays
ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464

# This configuration was auto-generated on Thu, 17 Jan 2013 16:21:01 +0100
# by mkconf 3.2.5-3</pre></div></div><div
                class="para">
					Salah satu rincian paling berguna adalah opsi <code
                  class="literal">DEVICE</code>, yang berisi daftar perangkat tempat sistem akan secara otomatis mencari komponen volume RAID saat start-up. Dalam contoh kita, kita menggantikan nilai default, <code
                  class="literal">partitions containers</code>, dengan daftar eksplisit berkas perangkat, karena kita memilih untuk menggunakan seluruh disk dan tidak hanya partisi, untuk beberapa volume.
				</div><div
                class="para">
					Dua baris terakhir dalam contoh kita adalah yang memungkinkan kernel untuk secara aman memilih nomor volume yang ditetapkan ke array mana. Metadata yang tersimpan pada disk itu sendiri cukup untuk membangun kembali volume, tetapi tidak untuk menentukan nomor volume (dan nama perangkat <code
                  class="filename">/dev/md*</code> yang cocok).
				</div><div
                class="para">
					Untungnya, baris-baris ini dapat dihasilkan secara otomatis:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --misc --detail --brief /dev/md?</code></strong>
<code
                  class="computeroutput">ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</code></pre><div
                class="para">
					Isi dari dua baris terakhir ini tidak tergantung pada daftar disk yang disertakan dalam volume. Maka tidak diperlukan untuk meregenerasi baris-baris ini ketika menggantikan disk gagal dengan yang baru. Di sisi lain, perawatan harus diambil untuk memperbarui berkas ketika membuat atau menghapus sebuah array RAID.
				</div></div></div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      xmlns=""
                      id="sect.lvm"></a>12.1.2. LVM</h3></div></div></div><a
              id="id-1.15.4.7.2"
              class="indexterm"></a><a
              id="id-1.15.4.7.3"
              class="indexterm"></a><div
              class="para">
				LVM, <span
                class="emphasis"><em>Logical Volume Manager</em></span>, adalah pendekatan lain untuk mengabstrakkan volume logis dari dukungan fisik mereka, yang berfokus pada peningkatan fleksibilitas daripada meningkatkan kehandalan. LVM dapat mengubah volume logis secara transparan bagi aplikasi; sebagai contoh, sangat mungkin untuk menambahkan disk baru, memigrasi data ke mereka, dan menghapus disk lama, tanpa melepas kait volume.
			</div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.lvm-concepts"></a>12.1.2.1. Konsep LVM</h4></div></div></div><div
                class="para">
					Fleksibilitas ini dicapai dengan tingkat abstraksi yang melibatkan tiga konsep.
				</div><div
                class="para">
					Pertama, PV (<span
                  class="emphasis"><em>Physical Volume</em></span>) adalah entitas terdekat dengan perangkat keras: itu bisa berupa partisi pada disk atau seluruh disk, atau bahkan perangkat blok lain (termasuk, sebagai contoh, sebuah array RAID). Perhatikan bahwa ketika sebuah elemen fisik diatur hingga menjadi PV untuk LVM, itu mesti hanya diakses melalui LVM, jika tidak sistem akan bingung.
				</div><div
                class="para">
					Sejumlah PV dapat dikumpulkan dalam VG (<span
                  class="emphasis"><em>Volume Group</em></span>), yang dapat dibandingkan dengan disk virtual dan extensible. VG abstrak, dan tidak muncul dalam perangkat berkas di hirarki <code
                  class="filename">/dev</code>, sehingga tidak ada risiko menggunakan mereka secara langsung.
				</div><div
                class="para">
					Jenis ke tiga objek adalah LV (<span
                  class="emphasis"><em>Logical Volume</em></span>), yang berupa potongan dari suatu VG; jika kita memakai analogi VG-sebagai-disk, LV setara dengan partisi. LV muncul sebagai perangkat blok dengan entri di <code
                  class="filename">/dev</code>, dan dapat digunakan seperti setiap partisi fisik lainnya dapat (paling sering, mewadahi sebuah sistem berkas atau ruang swap).
				</div><div
                class="para">
					Yang penting adalah bahwa pemisahan VG ke LV sepenuhnya independen dari komponen fisiknya (PV). VG dengan hanya satu komponen fisik (disk misalnya) dapat dipisah menjadi selusin volume logis; demikian pula, sebuah VG dapat menggunakan beberapa disk fisik dan muncul sebagai satu volume logis yang besar. Satu-satunya kendala, jelas, adalah bahwa ukuran total yang dialokasikan untuk LV tidak bisa lebih dari total kapasitas dari PV dalam kelompok volume.
				</div><div
                class="para">
					Namun sering masuk akal untuk memiliki semacam keseragaman antara komponen fisik VG, dan untuk membagi VG menjadi volume logis yang akan memiliki pola penggunaan serupa. Misalnya, jika perangkat keras yang tersedia termasuk disk cepat dan disk lambat, yang cepat dapat dikelompokkan ke satu VG dan yang lambat ke lain; potongan pertama dapat kemudian ditugaskan untuk aplikasi yang membutuhkan akses data yang cepat, sementara yang kedua akan disimpan untuk tugas-tugas yang kurang menuntut.
				</div><div
                class="para">
					Dalam kasus apapun, perlu diingat bahwa LV tidak perlu melekat ke PV manapun. Dimungkinkan untuk mempengaruhi mana data dari LV secara fisik disimpan, tapi kemungkinan ini tidak diperlukan untuk penggunaan sehari-hari. Sebaliknya: ketika set komponen fisik VG berkembang, lokasi penyimpanan fisik yang sesuai dengan LV tertentu dapat bermigrasi di seluruh disk (dan tentu saja tetap di dalam PVs yang ditugaskan untuk VG).
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.lvm-setup"></a>12.1.2.2. Menyiapkan LVM</h4></div></div></div><div
                class="para">
					Mari kita sekarang ikuti, langkah demi langkah, proses pengaturan LVM untuk kasus penggunaan yang khas: kami ingin menyederhanakan situasi kompleks penyimpanan. Situasi seperti ini biasanya terjadi setelah beberapa sejarah yang panjang dan berbelit dari akumulasi langkah-langkah sementara. Untuk tujuan ilustrasi, kami akan mempertimbangkan server yang kebutuhan penyimpanannya telah berubah dari waktu ke waktu, berakhir dalam labirin dari partisi-partisi yang terpecah ke beberapa disk yang terpakai sebagian. Secara lebih konkret, partisi berikut tersedia:
				</div><div
                xmlns:d="http://docbook.org/ns/docbook"
                class="itemizedlist"><ul><li
                    class="listitem"><div
                      class="para">
							pada disk <code
                        class="filename">sdb</code>, sebuah partisi <code
                        class="filename">sdb2</code>, 4 GB;
						</div></li><li
                    class="listitem"><div
                      class="para">
							pada disk <code
                        class="filename">sdc</code>, sebuah partisi <code
                        class="filename">sdc3</code>, 3 GB;
						</div></li><li
                    class="listitem"><div
                      class="para">
							disk <code
                        class="filename">sdd</code>, 4 GB, sepenuhnya tersedia;
						</div></li><li
                    class="listitem"><div
                      class="para">
							pada disk <code
                        class="filename">sdf</code>, partisi <code
                        class="filename">sdf1</code>, 4 GB; dan partisi <code
                        class="filename">sdf2</code>, 5 GB.
						</div></li></ul></div><div
                class="para">
					Selain itu, mari kita asumsikan bahwa disk <code
                  class="filename">sdb</code> dan <code
                  class="filename">sdf</code> adalah lebih cepat daripada dua lainnya.
				</div><div
                class="para">
					Tujuan kami adalah untuk mengatur tiga volume logis untuk tiga aplikasi yang berbeda: server berkas memerlukan ruang penyimpanan 5 GB, sebuah basis data (1 GB) dan ruang untuk back-up (12 GB). Dua yang pertama perlu kinerja yang baik, tapi back-up kurang kritis dalam hal kecepatan akses. Semua kendala ini mencegah penggunaan partisi sendirian; menggunakan LVM dapat mengabstraksi ukuran fisik dari perangkat, sehingga satu-satunya batas adalah jumlah ruang yang tersedia.
				</div><div
                class="para">
					Alat-alat yang diperlukan ada dalam paket <span
                  class="pkg pkg">lvm2</span> dan dependensinya. Ketika mereka sedang diinstal, pengaturan LVM mengambil tiga langkah, cocok dengan konsep tiga tingkat.
				</div><div
                class="para">
					Pertama, kami siapkan volume fisik menggunakan <code
                  class="command">pvcreate</code>:
				</div><a
                xmlns=""
                id="screen.pvcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvdisplay</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvcreate /dev/sdb2</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdb2" successfully created
# </code><strong
                  class="userinput"><code>pvdisplay</code></strong>
<code
                  class="computeroutput">  "/dev/sdb2" is a new physical volume of "4.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/sdb2
  VG Name               
  PV Size               4.00 GiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I

# </code><strong
                  class="userinput"><code>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdc3" successfully created
  Physical volume "/dev/sdd" successfully created
  Physical volume "/dev/sdf1" successfully created
  Physical volume "/dev/sdf2" successfully created
# </code><strong
                  class="userinput"><code>pvdisplay -C</code></strong>
<code
                  class="computeroutput">  PV         VG   Fmt  Attr PSize PFree
  /dev/sdb2       lvm2 ---  4.00g 4.00g
  /dev/sdc3       lvm2 ---  3.09g 3.09g
  /dev/sdd        lvm2 ---  4.00g 4.00g
  /dev/sdf1       lvm2 ---  4.10g 4.10g
  /dev/sdf2       lvm2 ---  5.22g 5.22g
</code></pre><div
                class="para">
					Sejauh ini, masih baik; perhatikan bahwa PV dapat disiapkan pada seluruh disk maupun pada partisi individunya. Seperti yang ditunjukkan di atas, perintah <code
                  class="command">pvdisplay</code> menampilkan daftar PVs yang ada, dengan dua format keluaran mungkin.
				</div><div
                class="para">
					Sekarang mari kita merakit elemen-elemen fisik ini menjadi VG menggunakan <code
                  class="command">vgcreate</code>. Kita akan mengumpulkan hanya PV-PV dari disk cepat ke VG <code
                  class="filename">vg_critical</code>; VG lain, <code
                  class="filename">vg_normal</code>, juga akan memuat elemen-elemen yang lebih lambat.
				</div><a
                xmlns=""
                id="screen.vgcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>vgdisplay</code></strong>
<code
                  class="computeroutput">  No volume groups found
# </code><strong
                  class="userinput"><code>vgcreate vg_critical /dev/sdb2 /dev/sdf1</code></strong>
<code
                  class="computeroutput">  Volume group "vg_critical" successfully created
# </code><strong
                  class="userinput"><code>vgdisplay</code></strong>
<code
                  class="computeroutput">  --- Volume group ---
  VG Name               vg_critical
  System ID             
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               8.09 GiB
  PE Size               4.00 MiB
  Total PE              2071
  Alloc PE / Size       0 / 0   
  Free  PE / Size       2071 / 8.09 GiB
  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp

# </code><strong
                  class="userinput"><code>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</code></strong>
<code
                  class="computeroutput">  Volume group "vg_normal" successfully created
# </code><strong
                  class="userinput"><code>vgdisplay -C</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize  VFree 
  vg_critical   2   0   0 wz--n-  8.09g  8.09g
  vg_normal     3   0   0 wz--n- 12.30g 12.30g
</code></pre><div
                class="para">
					Di sini lagi, perintahnya agak sederhana (dan <code
                  class="command">vgdisplay</code> mengusulkan dua format output). Perhatikan bahwa sangat mungkin untuk menggunakan dua partisi dari disk fisik yang sama ke dua VG yang berbeda. Perhatikan juga bahwa kita menggunakan awalan <code
                  class="filename">vg_</code> untuk nama VG kita, tapi itu tidak lebih dari sebuah konvensi.
				</div><div
                class="para">
					Kita sekarang memiliki dua "disk virtual", masing-masing berukuran sekitar 8 GB dan 12 GB. Mari kita sekarang mengukir mereka ke dalam "partisi virtual" (LV). Ini melibatkan perintah <code
                  class="command">lvcreate</code>, dan sintaks yang agak lebih kompleks:
				</div><a
                xmlns=""
                id="screen.lvcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>lvdisplay</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>lvcreate -n lv_files -L 5G vg_critical</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_files" created
# </code><strong
                  class="userinput"><code>lvdisplay</code></strong>
<code
                  class="computeroutput">  --- Logical volume ---
  LV Path                /dev/vg_critical/lv_files
  LV Name                lv_files
  VG Name                vg_critical
  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT
  LV Write Access        read/write
  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400
  LV Status              available
  # open                 0
  LV Size                5.00 GiB
  Current LE             1280
  Segments               2
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0

# </code><strong
                  class="userinput"><code>lvcreate -n lv_base -L 1G vg_critical</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_base" created
# </code><strong
                  class="userinput"><code>lvcreate -n lv_backups -L 12G vg_normal</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_backups" created
# </code><strong
                  class="userinput"><code>lvdisplay -C</code></strong>
<code
                  class="computeroutput">  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_base    vg_critical -wi-a---  1.00g                                           
  lv_files   vg_critical -wi-a---  5.00g                                           
  lv_backups vg_normal   -wi-a--- 12.00g</code></pre><div
                class="para">
					Dua parameter diperlukan ketika membuat volume logis; mereka harus diberikan ke <code
                  class="command">lvcreate</code> sebagai opsi. Nama LV yang akan dibuat ditetapkan dengan opsi <code
                  class="literal">-n</code>, dan ukurannya biasanya diberikan menggunakan opsi <code
                  class="literal">-L</code>. Tentu saja kita juga perlu memberitahu ke perintah, VG mana yang dikenai operasi, maka diberikanlah parameter terakhir pada baris perintah.
				</div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>LEBIH JAUH</em></span> opsi-opsi <code
                            class="command">lvcreate</code></strong></p></div></div></div><div
                  class="para">
					The <code
                    class="command">lvcreate</code> command has several options to allow tweaking how the LV is created.
				</div><div
                  class="para">
					Mari kita pertama menjelaskan opsi <code
                    class="literal">-l</code>, dengannya ukuran LV dapat diberikan sebagai cacah blok (sebagai lawan dari unit "manusia" yang kita digunakan di atas). Blok-blok ini (disebut PE, <span
                    class="emphasis"><em>physical extents</em></span> dalam istilah LVM) adalah unit-unit ruang penyimpanan yang bersebelahan di PV, dan mereka tidak dapat dipecah di LV. Ketika seseorang ingin menentukan ruang penyimpanan untuk LV secara cukup presisi, misalnya menggunakan seluruh ruang yang tersedia, opsi <code
                    class="literal">-l</code> mungkin akan lebih disukai daripada <code
                    class="literal">-L</code>.
				</div><div
                  class="para">
					Memungkinkan juga untuk menunjuk pada lokasi fisik LV, sehingga extent disimpan pada PV tertentu (tentu saja masih tetap ada di dalam yang ditugaskan untuk VG). Karena kita tahu bahwa <code
                    class="filename">sdb</code> lebih cepat daripada <code
                    class="filename">sdf</code>, kita mungkin ingin menyimpan <code
                    class="filename">lv_base</code> di sana jika kita ingin memberikan keuntungan kepada server basis data dibandingkan dengan server berkas. Baris perintah menjadi: <code
                    class="command">lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</code>. Perhatikan bahwa perintah ini bisa gagal jika PV tidak memiliki cukup extent bebas. Dalam contoh kita, kita mungkin harus membuat <code
                    class="filename">lv_base</code> sebelum <code
                    class="filename">lv_files</code> untuk menghindari situasi ini - atau membebaskan sebagian ruang di <code
                    class="filename">sdb2</code> dengan perintah <code
                    class="command">pvmove</code>.
				</div></div><div
                class="para">
					Volume logis, sekali dibuat, akan menjadi berkas perangkat blok dalam <code
                  class="filename">/dev/mapper/</code>:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>ls -l /dev/mapper</code></strong>
<code
                  class="computeroutput">total 0
crw------- 1 root root 10, 236 Jun 10 16:52 control
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2
# </code><strong
                  class="userinput"><code>ls -l /dev/dm-*</code></strong>
<code
                  class="computeroutput">brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0
brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1
brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2
</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>CATATAN</em></span> Mendeteksi otomatis volume LVM</strong></p></div></div></div><div
                  class="para">
					Ketika komputer boot, unit layanan systemd <code
                    class="filename">lvm2-activation</code> mengeksekusi <code
                    class="command">vgchange -aay</code> untuk "mengaktifkan" kelompok volume: memindai perangkat yang tersedia; yang telah diinisialisasi sebagai fisik untuk LVM didaftarkan ke subsistem LVM, yang berasal dari kelompok-kelompok volume dirakit, dan volume logis yang relevan dimulai dan dibuat tersedia. Karena itu tidak perlu menyunting berkas konfigurasi ketika membuat atau memodifikasi volume-volume LVM.
				</div><div
                  class="para">
					Namun, perlu diketahui bahwa tata letak elemen LVM (volume fisik dan logis, dan kelompok-kelompok volume) direkam cadang dalam <code
                    class="filename">/etc/lvm/backup</code>, yang dapat berguna dalam hal ada masalah (atau hanya untuk sekedar mengintip di balik layar).
				</div></div><div
                class="para">
					Untuk membuat semua lebih mudah, taut simbolik juga dibuat dalam direktori-direktori yang cocok dengan VG:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>ls -l /dev/vg_critical</code></strong>
<code
                  class="computeroutput">total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0
# </code><strong
                  class="userinput"><code>ls -l /dev/vg_normal</code></strong>
<code
                  class="computeroutput">total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</code></pre><div
                class="para">
					LV kemudian dapat digunakan persis seperti partisi standar:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mkfs.ext4 /dev/vg_normal/lv_backups</code></strong>
<code
                  class="computeroutput">mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 3145728 4k blocks and 786432 inodes
Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d
[...]
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </code><strong
                  class="userinput"><code>mkdir /srv/backups</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mount /dev/vg_normal/lv_backups /srv/backups</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/backups</code></strong>
<code
                  class="computeroutput">Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_normal-lv_backups   12G   30M   12G   1% /srv/backups
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>cat /etc/fstab</code></strong>
<code
                  class="computeroutput">[...]
/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2
/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2
/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</code></pre><div
                class="para">
					Dari sudut pandang aplikasi, berbagai partisi kecil sekarang telah diabstrakkan ke dalam satu volume 12 GB besar, dengan nama yang lebih mudah.
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.lvm-over-time"></a>12.1.2.3. LVM Dari Waktu Ke Waktu</h4></div></div></div><div
                class="para">
					Meskipun kemampuan untuk mengagregasi partisi atau disk fisik itu nyaman, ini bukanlah keuntungan utama yang dibawa oleh LVM. Fleksibilitas yang dibawanya terutama teramati seiring berjalannya waktu, ketika kebutuhan berevolusi. Dalam contoh kita, mari kita asumsikan bahwa berkas besar baru harus disimpan, dan bahwa LV yang didedikasikan untuk server berkas terlalu kecil untuk menampung mereka. Karena kita belum menggunakan seluruh ruang yang tersedia di <code
                  class="filename">vg_critical</code>, kita bisa perbesar <code
                  class="filename">lv_files</code>. Untuk tujuan tersebut, kita akan menggunakan perintah <code
                  class="command">lvresize</code>, lalu <code
                  class="command">resize2fs</code> untuk mengadaptasi sistem berkas:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/files/</code></strong>
<code
                  class="computeroutput">Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files
# </code><strong
                  class="userinput"><code>lvdisplay -C vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 5.00g
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   2   2   0 wz--n- 8.09g 2.09g
# </code><strong
                  class="userinput"><code>lvresize -L 7G vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).
  Logical volume lv_files successfully resized
# </code><strong
                  class="userinput"><code>lvdisplay -C vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 7.00g
# </code><strong
                  class="userinput"><code>resize2fs /dev/vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">resize2fs 1.42.12 (29-Aug-2014)
Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.

# </code><strong
                  class="userinput"><code>df -h /srv/files/</code></strong>
<code
                  class="computeroutput">Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>HATI-HATI</em></span> Mengubah ukuran sistem berkas</strong></p></div></div></div><div
                  class="para">
					Tidak semua sistem berkas dapat diubah ukurannya secara daring; mengubah ukuran volume oleh karena itu mungkin pertama memerlukan melepas kait sistem berkas dan mengait ulang setelah itu. Tentu saja, jika seseorang ingin mengecilkan ruang yang dialokasikan untuk LV, sistem berkas harus diperkecil dulu; urutan dibalik ketika perubahan ukuran untuk arah lain: volume logis harus diperbesar sebelum sistem berkas di atasnya. Hal ini cukup sederhana, karena kapanpun ukuran sistem tidak boleh lebih besar dari perangkat blok tempat dia berada (apakah perangkat berupa partisi fisik atau volume logis).
				</div><div
                  class="para">
					Sistem berkas ext3, ext4, dan xfs dapat diperbesar secara daring, tanpa melepas kain; menyusutkan memerlukan melepas kait. Sistem berkas reiserfs memungkinkan perubahan ukuran secara daring di kedua arah. ext2 tidak memungkinkan keduanya, dan selalu membutuhkan melepas kait.
				</div></div><div
                class="para">
					Kita bisa melanjutkan dengan cara yang sama untuk memperbesar volume yang mewadai basis data, tapi kita telah mencapai batas ruang VG yang tersedia:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/base/</code></strong>
<code
                  class="computeroutput">Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_base 1008M  854M  104M  90% /srv/base
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree 
  vg_critical   2   2   0 wz--n- 8.09g 92.00m</code></pre><div
                class="para">
					Tidak masalah, karena LVM memungkinkan menambahkan volume fisik ke grup volume yang ada. Misalnya, mungkin kita telah memperhatikan bahwa partisi <code
                  class="filename">sdb1</code>, yang sejauh ini digunakan di luar LVM, hanya berisi arsip yang dapat dipindahkan ke <code
                  class="filename">lv_backups</code>. Kita sekarang dapat mendaur ulang itu dan mengintegrasikannya ke grup volume, dan dengan demikian memperoleh kembali ruang bebas. Ini adalah tujuan dari perintah <code
                  class="command">vgextend</code>. Tentu saja, partisi harus disiapkan sebagai sebuah volume fisik terlebih dahulu. Setelah VG telah diperbesar, kita dapat menggunakan perintah sejenis seperti yang sebelumnya untuk menumbuhkan volume logis kemudian sistem berkasnya:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvcreate /dev/sdb1</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdb1" successfully created
# </code><strong
                  class="userinput"><code>vgextend vg_critical /dev/sdb1</code></strong>
<code
                  class="computeroutput">  Volume group "vg_critical" successfully extended
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   3   2   0 wz--n- 9.09g 1.09g
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>df -h /srv/base/</code></strong>
<code
                  class="computeroutput">Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>LEBIH JAUH</em></span> LVM tingkat lanjut</strong></p></div></div></div><div
                  class="para">
					LVM also caters for more advanced uses, where many details can be specified by hand. For instance, an administrator can tweak the size of the blocks that make up physical and logical volumes, as well as their physical layout. It is also possible to move blocks across PVs, for instance to fine-tune performance or, in a more mundane way, to free a PV when one needs to extract the corresponding physical disk from the VG (whether to affect it to another VG or to remove it from LVM altogether). The manual pages describing the commands are generally clear and detailed. A good entry point is the <span
                    class="citerefentry"><span
                      class="refentrytitle">lvm</span>(8)</span> manual page.
				</div></div></div></div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      xmlns=""
                      id="sect.raid-or-lvm"></a>12.1.3. RAID atau LVM?</h3></div></div></div><div
              class="para">
				RAID dan LVM keduanya membawa keuntungan tak terbantahkan bila kita abaikan kasus sederhana komputer desktop dengan satu hard disk dengan pola penggunaan tidak berubah dari waktu ke waktu. Namun, RAID dan LVM mengambil arah yang berbeda, dengan tujuan divergen, dan sah-sah saja bertanya-tanya mana yang harus diambil. Jawaban paling tepat akan tentu saja tergantung pada kebutuhan saat ini dan masa mendatang.
			</div><div
              class="para">
				Ada beberapa kasus sederhana dimana pertanyaan tidak benar-benar muncul. Jika kebutuhan adalah untuk mengamankan data terhadap kegagalan perangkat keras, maka jelas RAID akan disiapkan pada array disk, karena LVM tidak benar-benar menjawab masalah ini. Si sisi lain, jika kebutuhan adalah untuk skema penyimpanan yang fleksibel dimana volume dibuat independen terhadap tata letak fisik dari disk, RAID tidak banyak membantu dan LVM akan menjadi pilihan yang tepat.
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>CATATAN</em></span> Jika kinerja penting…</strong></p></div></div></div><div
                class="para">
				Jika kecepatan masukan/keluar adalah esensinya, terutama dalam hal waktu akses, menggunakan LVM dan/atau RAID di salah satu dari banyak kombinasi mungkin memiliki dampak pada kinerja, dan ini mungkin mempengaruhi keputusan untuk memilih yang mana. Namun, perbedaan-perbedaan dalam kinerja benar-benar kecil, dan hanya terukur dalam beberapa kasus penggunaan. Jika kinerja penting, keuntungan terbaik yang diperoleh adalah menggunakan media penyimpanan bukan rotasi (<a
                  id="id-1.15.4.8.4.2.1"
                  class="indexterm"></a> <span
                  class="emphasis"><em>solid-state drive</em></span>); biaya per megabyte mereka lebih tinggi daripada hard disk drive standar, dan kapasitas mereka biasanya lebih kecil, tapi mereka memberikan kinerja yang sangat baik untuk akses acak. Jika pola penggunaan mencakup banyak operasi keluaran/masukan yang terpencar di seluruh sistem berkas, misalnya untuk basis data tempat query-query yang kompleks rutin dijalankan, maka keuntungan dari menjalankan mereka pada SSD jauh lebih besar daripada apa pun yang bisa diperoleh dengan memilih LVM atas RAID atau sebaliknya. Dalam situasi ini, pilihan harus ditentukan oleh pertimbangan selain murni kecepatan, karena aspek kinerja paling mudah ditangani dengan menggunakan SSD.
			</div></div><div
              class="para">
				Use case ketiga yang menarik adalah ketika seseorang hanya ingin mengagregat dua disk ke dalam satu volume, baik untuk alasan kinerja atau memiliki sistem berkas tunggal yang lebih besar daripada salah satu disk yang tersedia. Hal ini dapat dijawab oleh RAID 0 (atau bahkan linear-RAID) maupun dengan volume LVM. Dalam situasi ini, dan tanpa batasan tambahan kendala (misalnya, menjaga sejalan dengan sisa komputer jika mereka hanya menggunakan RAID), konfigurasi pilihan akan seringkali adalah LVM. Penyiapan awal hampir tidak lebih kompleks, dan bahwa sedikit peningkatan kompleksitas terbayar oleh fleksibilitas tambahan yang dibawah oleh LVM jika persyaratan berubah atau jika disk baru perlu ditambahkan.
			</div><div
              class="para">
				Kemudian tentu saja, ada kasus penggunaan yang benar-benar menarik, dimana sistem penyimpanan perlu dibuat tahan terhadap kegagalan perangkat keras dan fleksibel tentang alokasi volume. RAID maupun LVM masing-masing dapat menjawab kedua persyaratan; ini adalah di mana kita menggunakan keduanya pada saat yang sama -- atau lebih tepatnya, satu di atas yang lain. Skema yang memiliki semua tapi belum menjadi standar karena RAID dan LVM telah mencapai kedewasaan untuk memastikan redundansi data pertama dengan pengelompokan disk dalam sejumlah kecil larik RAID besar, dan menggunakan larik RAID ini sebagai volume fisik LVM; partisi logis kemudian dapat ditoreh dari LV-LV ini untuk sistem berkas. Nilai jual konfigurasi ini adalah bahwa ketika sebuah disk gagal, hanya sejumlah kecil larik RAID yang perlu dibangun kembali, sehingga membatasi waktu yang dihabiskan oleh administrator untuk pemulihan.
			</div><div
              class="para">
				Mari kita ambil contoh konkret: departemen hubungan masyarakat di Falcot Corp memerlukan sebuah workstation untuk penyuntingan video, tapi anggaran departemen tidak mengizinkan berinvestasi di perangkat keras kelas tinggi secara lengkap. Keputusan dibuat untuk mendukung perangkat keras yang khusus untuk sifat grafis pekerjaan (monitor dan kartu video), dan tetap dengan perangkat keras generik untuk penyimpanan. Namun, seperti sudah dikenal luas, video digital memiliki beberapa persyaratan khusus untuk penyimpanan: banyaknya data yang akan disimpan besar, dan kecepatan pembacaan dan penulisan data ini penting untuk keseluruhan kinerja sistem (lebih daripada waktu akses rata-rata, misalnya). Batasan-batasn ini perlu dipenuhi dengan perangkat keras generik, dalam kasus ini dua hard disk drive SATA 300 GB; data sistem juga harus dibuat tahan terhadap kegagalan perangkat keras, termasuk sebagian data pengguna. Klip video yang diedit memang harus aman, tetapi tidak perlu bergegas menyunting video yang tertunda, karena mereka masih berada pada kaset.
			</div><div
              class="para">
				RAID-1 dan LVM digabungkan untuk memenuhi batasan-batasan ini. Disk dilekatkan pada dua pengendali SATA yang berbeda untuk mengoptimalkan akses paralel dan mengurangi risiko kegagalan simultan, dan karena itu mereka muncul sebagai <code
                class="filename">sda</code> dan <code
                class="filename">sdc</code>. Mereka dipartisi secara identik mengikut skema sebagai berikut:
			</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>fdisk -l /dev/sda</code></strong>
<code
                class="computeroutput">
Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x00039a9f

Device    Boot     Start       End   Sectors Size Id Type
/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect
/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris
/dev/sda3        4000185 586099395 582099210 298G 5  Extended
/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect
/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect
/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</code></pre><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
						Partisi pertama dari kedua disk (sekitar 1 GB) dirakit menjadi volume RAID-1, <code
                      class="filename">md0</code>. Cermin ini langsung digunakan untuk menyimpan sistem berkas root.
					</div></li><li
                  class="listitem"><div
                    class="para">
						Partisi <code
                      class="filename">sda2</code> dan <code
                      class="filename">sdc2</code> digunakan sebagai partisi swap, menyediakan total 2 GB ruang swap. Dengan RAM 1 GB, workstation memiliki sejumlah memori tersedia yang nyaman.
					</div></li><li
                  class="listitem"><div
                    class="para">
						<code
                      class="filename">sda5</code> dan partisi <code
                      class="filename">sdc5</code>, serta <code
                      class="filename">sda6</code> dan <code
                      class="filename">sdc6</code>, dirakit menjadi dua volume RAID-1 baru masing-masing sekitar 100 GB, <code
                      class="filename">md1</code> dan <code
                      class="filename">md2 </code>. Kedua cermin diinisialisasi sebagai volume fisik untuk LVM, dan ditugaskan ke grup volume <code
                      class="filename">vg_raid</code>. Maka VG ini berisi ruang sekitar 200 GB yang aman.
					</div></li><li
                  class="listitem"><div
                    class="para">
						Sisa partisi, <code
                      class="filename">sda7</code> dan <code
                      class="filename">sdc7</code>, langsung digunakan sebagai volume fisik, dan ditugaskan untuk VG lain yang disebut <code
                      class="filename">vg_bulk</code>, yang karena itu menjadi ruang sekitar 200 GB.
					</div></li></ul></div><div
              class="para">
				Setelah VGs dibuat, mereka dapat dipartisi dengan cara yang sangat fleksibel. Harus tetap diingat bahwa LV yang dibuat dalam <code
                class="filename">vg_raid</code> akan dipertahankan bahkan jika salah satu disk gagal, yang tidak akan terjadi untuk LV dibuat di <code
                class="filename">vg_bulk</code>; di sisi lain, yang kedua akan dialokasikan secara paralel pada kedua disk, yang memungkinkan kecepatan baca atau tulis yang lebih tinggi untuk berkas-berkas besar.
			</div><div
              class="para">
				Karena itu kita akan membuat LV <code
                class="filename">lv_usr</code>, <code
                class="filename">lv_var</code>, dan <code
                class="filename">lv_home</code> pada <code
                class="filename">vg_raid</code>, untuk mewadahi sistem berkas yang cocok; LV besar lain, <code
                class="filename">lv_movies</code>, akan digunakan untuk mewadahi film-film versi definitif setelah penyuntingan. VG lain akan dibagi menjadi <code
                class="filename">lv_rushes</code>yang besar, untuk data langsung dari kamera video digital, dan <code
                class="filename">lv_tmp</code> untuk berkas-berkas sementara. Lokasi area kerja adalah pilihan yang kurang mudah untuk dibuat: sementara kinerja yang baik diperlukan untuk volume itu, apakah layak risiko kehilangan pekerjaan jika disk gagal selama sesi menyunting? Tergantung pada jawaban untuk pertanyaan itu, LV yang relevan akan dibuat pada satu VG atau yang lain.
			</div><div
              class="para">
				Kita sekarang memiliki sebagian redundansi untuk data penting dan banyak fleksibilitas dalam bagaimana ruang yang tersedia dibagi atas berbagai aplikasi. Bila perangkat lunak baru diinstal kemudian (untuk mengedit klip audio, misalnya), LV yang mewadahi <code
                class="filename">/usr/</code> dapat diperbesar dengan mudah.
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>CATATAN</em></span> Mengapa tiga volume RAID-1?</strong></p></div></div></div><div
                class="para">
				Kita bisa saja menyiapkan hanya satu volume RAID-1, untuk melayani sebagai volume fisik bagi <code
                  class="filename">vg_raid</code>. Lalu mengapa membuat tiga?
			</div><div
                class="para">
				Alasan untuk pemecahan pertama (<code
                  class="filename">md0</code> vs yang lain) adalah tentang keamanan data: data yang ditulis ke kedua elemen dari cermin RAID 1 persis sama, dan karena itu mungkin untuk melewati lapisan RAID dan mengait salah satu disk secara langsung. Dalam kasus bug kernel, misalnya, atau jika metadata LVM menjadi rusak, itu masih mungkin untuk mem-boot sistem minimal untuk mengakses data penting seperti tata letak disk dalam RAID dan volume LVM; metadata dapat kemudian direkonstruksi dan berkas dapat diakses lagi, sehingga sistem dapat dibawa kembali ke keadaan nominal.
			</div><div
                class="para">
				The rationale for the second split (<code
                  class="filename">md1</code> vs. <code
                  class="filename">md2</code>) is less clear-cut, and more related to acknowledging that the future is uncertain. When the workstation is first assembled, the exact storage requirements are not necessarily known with perfect precision; they can also evolve over time. In our case, we can't know in advance the actual storage space requirements for video rushes and complete video clips. If one particular clip needs a very large amount of rushes, and the VG dedicated to redundant data is less than halfway full, we can re-use some of its unneeded space. We can remove one of the physical volumes, say <code
                  class="filename">md2</code>, from <code
                  class="filename">vg_raid</code> and either assign it to <code
                  class="filename">vg_bulk</code> directly (if the expected duration of the operation is short enough that we can live with the temporary drop in performance), or undo the RAID setup on <code
                  class="filename">md2</code> and integrate its components <code
                  class="filename">sda6</code> and <code
                  class="filename">sdc6</code> into the bulk VG (which grows by 200 GB instead of 100 GB); the <code
                  class="filename">lv_rushes</code> logical volume can then be grown according to requirements.
			</div></div></div></div></div><ul
        class="docnav"><li
          class="previous"><a
            accesskey="p"
            href="sect.rtc-services.html"><strong>Sebelumnya</strong>11.8. Layanan Komunikasi Real-Time</a></li><li
          class="up"><a
            accesskey="u"
            href="#"><strong>Induk</strong></a></li><li
          class="home"><a
            accesskey="h"
            href="index.html"><strong>Depan</strong></a></li><li
          class="next"><a
            accesskey="n"
            href="sect.virtualization.html"><strong>Lanjut</strong>12.2. Virtualisasi</a></li></ul></body></html>
