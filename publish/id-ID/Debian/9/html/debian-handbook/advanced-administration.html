<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html
    xmlns="http://www.w3.org/1999/xhtml"><head><meta
        http-equiv="Content-Type"
        content="text/html; charset=UTF-8" /><title
        xmlns:d="http://docbook.org/ns/docbook">Bab 12. Administrasi Tingkat Lanjut</title><link
        rel="stylesheet"
        type="text/css"
        href="Common_Content/css/default.css" /><link
        rel="stylesheet"
        media="print"
        href="Common_Content/css/print.css"
        type="text/css" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="generator"
        content="publican v4.3.2" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="package"
        content="Debian-debian-handbook-9-id-ID-1.0-1" /><meta
        name="keywords"
        content="RAID, LVM, FAI, Preseeding, Pemantauan, Virtualisasi, Xen, LXC" /><link
        rel="home"
        href="index.html"
        title="Buku Panduan Administrator Debian" /><link
        rel="up"
        href="index.html"
        title="Buku Panduan Administrator Debian" /><link
        rel="prev"
        href="sect.rtc-services.html"
        title="11.8. Layanan Komunikasi Real-Time" /><link
        rel="next"
        href="sect.virtualization.html"
        title="12.2. Virtualisasi" /><meta
        xmlns=""
        name="flattr:id"
        content="4pz9jq" /><link
        xmlns=""
        rel="canonical"
        href="https://debian-handbook.info/browse/id-ID/stable/advanced-administration.html" /></head><body><div
        id="banner"><a
          href="http://debian-handbook.info/get/"><span
            class="text">Download the ebook</span></a></div><p
        id="title"><a
          class="left"
          href="http://www.debian.org"><img
            alt="Product Site"
            src="Common_Content/images//image_left.png" /></a><a
          class="right"
          href="index.html"><img
            alt="Documentation Site"
            src="Common_Content/images//image_right.png" /></a></p><ul
        class="docnav top"><li
          class="previous"><a
            accesskey="p"
            href="sect.rtc-services.html"><strong>Sebelumnya</strong></a></li><li
          class="home">Buku Panduan Administrator Debian</li><li
          class="next"><a
            accesskey="n"
            href="sect.virtualization.html"><strong>Lanjut</strong></a></li></ul><div
        xml:lang="id-ID"
        class="chapter"
        lang="id-ID"><div
          class="titlepage"><div><div><h1
                class="title"><a
                  xmlns=""
                  id="advanced-administration"></a>Bab 12. Administrasi Tingkat Lanjut</h1></div></div></div><div
          class="toc"><dl
            class="toc"><dt><span
                class="section"><a
                  href="advanced-administration.html#sect.raid-and-lvm">12.1. RAID dan LVM</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.raid-soft">12.1.1. RAID Perangkat Lunak</a></span></dt><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.lvm">12.1.2. LVM</a></span></dt><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.raid-or-lvm">12.1.3. RAID atau LVM?</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.virtualization.html">12.2. Virtualisasi</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.virtualization.html#sect.xen">12.2.1. Xen</a></span></dt><dt><span
                    class="section"><a
                      href="sect.virtualization.html#sect.lxc">12.2.2. LXC</a></span></dt><dt><span
                    class="section"><a
                      href="sect.virtualization.html#id-1.15.5.14">12.2.3. Virtualisasi dengan KVM</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.automated-installation.html">12.3. Pemasangan Otomatis</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.fai">12.3.1. Fully Automatic Installer (FAI, Pemasang Otomatis Sepenuhnya)</a></span></dt><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.d-i-preseeding">12.3.2. Preseeding Debian-Installer</a></span></dt><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.simple-cdd">12.3.3. Simple-CDD: Solusi Semua-Jadi-Satu</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.monitoring.html">12.4. Pemantauan</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.monitoring.html#sect.munin">12.4.1. Menyiapkan Munin</a></span></dt><dt><span
                    class="section"><a
                      href="sect.monitoring.html#sect.nagios">12.4.2. Menyiapkan Nagios</a></span></dt></dl></dd></dl></div><div
          class="highlights"><div
            class="para">
		Bab ini meninjau kembali beberapa aspek yang telah kami uraikan, dengan perspektif yang berbeda: alih-alih memasang pada sebuah komputer, kita akan mempelajari sistem deployment masal; alih-alih membuat volume RAID atau LVM pada saat instalasi, kita akan belajar melakukannya secara manual sehingga nanti kita dapat merevisi pilihan awal kita. Akhirnya, kita akan mendiskusikan perkakas pemantauan dan teknik virtualisasi. Sebagai konsekuensinya, bab ini secara lebih khusus menarget para administrator profesional, and sedikit kurang brfokus pada para individu yang bertanggungjawab atas jaringan rumahan mereka.
	</div></div><div
          class="section"><div
            class="titlepage"><div><div><h2
                  class="title"><a
                    xmlns=""
                    id="sect.raid-and-lvm"></a>12.1. RAID dan LVM</h2></div></div></div><div
            class="para">
			<a
              class="xref"
              href="installation.html">Bab 4, <em>Instalasi</em></a> mempresentasikan teknologi ini dari sudut pandang pemasang, dan bagaimana itu mengintegrasikan mereka untuk membuat deployment mereka mudah dari awal. Setelah instalasi awal, seorang administrator mesti bisa menangani keperluan ruang penyimpanan yang berkembang tanpa mesti mengandalkan instalasi ulang yang mahal. Maka mereka mesti paham peralatan yang diperlukan untuk memanipulasi volume RAID dan LVM.
		</div><div
            class="para">
			RAID dan LVM adalah teknik untuk mengabstrakkan volume yang dikait dari pasangan fisik mereka (yaitu hard disk atau partisi); yang pertama mengamankan data dari kegagalan perangkat keras dengan memperkenalkan redundansi, yang belakangan membuat manajemen volume lebih luwes dan tak bergantung kepada ukuran sebenarnya dari disk yang mendasarinya. Dalam kedua kasus, sistem pada akhirnya mendapat perangkat blok baru, yang dapat dipakai untuk membuat sistem berkas atau ruang swap, tanpa perlu mereka dipetakan ke satu disk fisik. RAID dan LVM datang dari latar belakang yang cukup berbeda, tapi fungsionalitas mereka sebagian dapat bertumpang tindih, sehingga mereka sering disinggung bersama-sama.
		</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>PERSPEKTIF</em></span> Btrfs menggabung LVM dan RAID</strong></p></div></div></div><div
              class="para">
			Walaupun LVM dan RAID adalah dua subsistem kernel yang berbeda, yang hadir di antara perangkat blok disk dan sistem berkas mereka, <span
                class="emphasis"><em>btrfs</em></span> adalah suatu sistem berkas baru, yang pada awalnya dikembangkan di Oracle, yang bertujuan menggabung set fitur dari LVM dan RAID serta lebih banyak lagi. Sebagian besar sudah berfungsi, dan walaupun masih di-tag "eksperimental" karena pengembangannya belum lengkap (beberapa fitur belum diimplementasi), itu telah terpakai dalam lingkungan produksi. <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://btrfs.wiki.kernel.org/">http://btrfs.wiki.kernel.org/</a></div>
		</div><div
              class="para">
			Diantara fitur yang menarik adalah kemampuan membuat snapshot dari suatu pohon sistem berkas pada sebarang waktu. Snapshot ini pada awalnya tak memakai sebarang ruang disk, data hanya diduplikasi ketika satu dari salinan-salinan dimodifikasi. Sistem berkas juga menangani kompresi transparan dari berkas, dan checksum memastikan integritas dari semua data yang disimpan.
		</div></div><div
            class="para">
			Pada kedua kasus RAID dan LVM, kernel menyediakan suatu berkas perangkat blok, mirip dengan yang berkaitan dengan suatu hard disk atau suatu partisi. Ketika suatu aplikasi, atau bagian lain dari kernel, meminta akses ke suatu blok dari perangkat seperti itu, subsistem yang sesuai mengarahkan blok ke lapisan fisik yang relevan. Bergantung kepada konfigurasi, blok ini dapat disimpan pada satu atau beberapa disk fisik, dan lokasi fisiknya mungkin tak berkorelasi langsung ke lokasi blok dalam perangkat lojik.
		</div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      xmlns=""
                      id="sect.raid-soft"></a>12.1.1. RAID Perangkat Lunak</h3></div></div></div><a
              id="id-1.15.4.6.2"
              class="indexterm"></a><div
              class="para">
				RAID adalah <span
                class="emphasis"><em>Redundant Array of Independent Disks</em></span> (larik redundan dari disk-disk independen). Tujuan dari sistem ini adalah untuk mencegah kehilangan data dalam kasus kegagalan hard disk. Prinsip umumnya cukup sederhana: data disimpan pada beberapa disk fisik alih-alih hanya satu, dengan tingkat redundansi yang dapat dikonfigurasi. Bergantung kepada banyaknya redundansi ini, dan bahkan dalam kejadian kegagalan disk yang tak terduga, data dapat direkonstruksi tanpa adanya kehilangan dari disk sisanya.
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>KULTUR</em></span> <span
                          class="foreignphrase"><em
                            class="foreignphrase">Independen</em></span> atau <span
                          class="foreignphrase"><em
                            class="foreignphrase">tidak mahal</em></span>?</strong></p></div></div></div><div
                class="para">
				I dalam RAID pada awalnya merupakan singkatan dari <span
                  class="emphasis"><em>inexpensive (tidak mahal)</em></span>, karena RAID memungkinkan kenaikan drastis keselamatan data tanpa memerlukan investasi disk canggih yang mahal. Namun mungkin karena masalah citra, kini lebih umum dianggap singkatan dari <span
                  class="emphasis"><em>independen</em></span>, yang tak membawa kesan murahan yang tak menarik.
			</div></div><div
              class="para">
				RAID dapat diwujudkan baik oleh perangkat keras khusus (modul RAID yang terintegrasi ke dalam kartu pengendali SCSI atau SATA) atau oleh abstraksi perangkat lunak (kernel). Apakah perangkat keras atau perangkat lunak, sistem RAID dengan redundansi yang cukup bisa secara transparan tetap operasional ketika sebuah disk gagal; lapisan atas tumpukan (aplikasi) bahkan dapat tetap mengakses data terlepas dari kegagalan. Tentu saja, "mode terdegradasi" ini dapat memiliki dampak pada kinerja, dan redundansi berkurang, sehingga kegagalan disk lebih lanjut dapat mengakibatkan kehilangan data. Dalam prakteknya, oleh karena itu, kita akan berusaha untuk hanya berada dalam mode terdegradasi ini selama diperlukannya untuk menggantikan disk yang gagal. Sekali disk baru terpasang sistem RAID dapat merekonstruksi data yang dibutuhkan untuk kembali ke mode aman. Aplikasi tidak akan melihat apa-apa, selain kecepatan akses berpotensi berkurang, sementara larik ada dalam mode terdegradasi atau selama fase rekonstruksi.
			</div><div
              class="para">
				Ketika RAID diimplementasikan oleh perangkat keras, konfigurasinya umumnya terjadi dalam alat konfigurasi BIOS, dan kernel akan menganggap sebuah array RAID sebagai satu disk, yang akan bekerja sebagai disk fisik standar, meskipun nama perangkat mungkin berbeda (tergantung pada driver).
			</div><div
              class="para">
				Kami hanya berfokus pada RAID perangkat lunak dalam buku ini.
			</div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.raid-levels"></a>12.1.1.1. Tingkat-tingkat RAID</h4></div></div></div><div
                class="para">
					RAID sebenarnya buka satu sistem, tapi berbagai sistem yang diidentifikasi oleh tingkat mereka; tingkat-tingkat itu dibedakan oleh tata letak dan banyaknya redundansi yang mereka sediakan. Semakin banyak redundan, semakin kebal kegagalan, karena sistem akan dapat terus bekerja dengan lebih banyak disk yang gagal. Kekurangannya adalah bahwa ruang yang dapat digunakan menyusut untuk satu set disk tertentu; dilihat dengan cara lain, akan diperlukan lebih banyak disk untuk menyimpan sejumlah data yang diberikan.
				</div><div
                class="variablelist"><dl
                  class="variablelist"><dt><span
                      class="term">RAID Linier</span></dt><dd><div
                      class="para">
								Meskipun subsistem RAID kernel memungkinkan menciptakan "linear RAID", ini bukan RAID yang benar, karena konfigurasi ini tidak melibatkan redundansi apapun. Kernel hanya mengumpulkan beberapa disk end-to-end dan menyediakan volume agregat yang dihasilkan sebagai satu disk virtual (satu perangkat blok). Itu adalah satu-satunya fungsinya. Konfigurasi ini jarang digunakan sendirian (lihat nanti untuk pengecualian), terutama karena kurangnya redundansi berarti bahwa salah satu disk gagal membuat seluruh agregat, dan karena itu semua data, tidak tersedia.
							</div></dd><dt><span
                      class="term">RAID-0</span></dt><dd><div
                      class="para">
								Tingkat ini tidak menyediakan redundansi apapun, tapi disk-disk tidak hanya sekadar dilekatkan satu di akhir yang lain: mereka dibagi dalam <span
                        class="emphasis"><em>stripe</em></span>, dan blok-blok di perangkat virtual disimpan dalam stripe di disk-disk fisik yang berbeda-beda. Dalam setup RAID-0 dua-disk, misalnya, blok bernomor genap dari perangkat virtual akan disimpan pada disk fisik pertama, sementara blok bernomor ganjil akan berakhir pada disk fisik kedua.
							</div><div
                      class="para">
								Sistem ini tidak bertujuan meningkatkan keandalan, karena (seperti dalam kasus linier) ketersediaan semua data hancur begitu satu disk gagal, tetapi meningkatkan kinerja: selama akses berurutan ke sejumlah besar data yang berdekatan, kernel akan mampu membaca dari kedua disk (atau menulis ke mereka) secara paralel, yang akan meningkatkan laju transfer data. Namun, penggunaan RAID-0 menyusut, ceruk ini diisi oleh LVM (lihat nanti).
							</div></dd><dt><span
                      class="term">RAID-1</span></dt><dd><div
                      class="para">
								Tingkat ini, juga dikenal sebagai "RAID mirroring", adalah yang paling sederhana dan setup yang paling banyak digunakan. Dalam bentuk standar, menggunakan dua disk fisik berukuran sama, dan memberikan volume logis berukuran yang sama lagi. Data disimpan identik pada disk kedua, maka dijuluki "mirror (cermin)". Ketika satu disk gagal, data ini masih tersedia di yang lain. Untuk data yang benar-benar penting, RAID-1 dapat tentu saja diatur pada disk yang lebih dari dua, dengan dampak langsung pada rasio biaya perangkat keras versus ruang muatan yang tersedia.
							</div><div
                      class="sidebar"><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>CATATAN</em></span> Ukuran klaster dan disk</strong></p></div></div></div><div
                        class="para">
								If two disks of different sizes are set up in a mirror, the bigger one will not be fully used, since it will contain the same data as the smallest one and nothing more. The useful available space provided by a RAID-1 volume therefore matches the size of the smallest disk in the array. This still holds for RAID volumes with a higher RAID level, even though redundancy is stored differently.
							</div><div
                        class="para">
								It is therefore important, when setting up RAID arrays (except for RAID-0 and “linear RAID”), to only assemble disks of identical, or very close, sizes, to avoid wasting resources.
							</div></div><div
                      class="sidebar"><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>CATATAN</em></span> Disk cadangan</strong></p></div></div></div><div
                        class="para">
								RAID levels that include redundancy allow assigning more disks than required to an array. The extra disks are used as spares when one of the main disks fails. For instance, in a mirror of two disks plus one spare, if one of the first two disks fails, the kernel will automatically (and immediately) reconstruct the mirror using the spare disk, so that redundancy stays assured after the reconstruction time. This can be used as another kind of safeguard for critical data.
							</div><div
                        class="para">
								One would be forgiven for wondering how this is better than simply mirroring on three disks to start with. The advantage of the “spare disk” configuration is that the spare disk can be shared across several RAID volumes. For instance, one can have three mirrored volumes, with redundancy assured even in the event of one disk failure, with only seven disks (three pairs, plus one shared spare), instead of the nine disks that would be required by three triplets.
							</div></div><div
                      class="para">
								This RAID level, although expensive (since only half of the physical storage space, at best, is useful), is widely used in practice. It is simple to understand, and it allows very simple backups: since both disks have identical contents, one of them can be temporarily extracted with no impact on the working system. Read performance is often increased since the kernel can read half of the data on each disk in parallel, while write performance isn't too severely degraded. In case of a RAID-1 array of N disks, the data stays available even with N-1 disk failures.
							</div></dd><dt><span
                      class="term">RAID-4</span></dt><dd><div
                      class="para">
								This RAID level, not widely used, uses N disks to store useful data, and an extra disk to store redundancy information. If that disk fails, the system can reconstruct its contents from the other N. If one of the N data disks fails, the remaining N-1 combined with the “parity” disk contain enough information to reconstruct the required data.
							</div><div
                      class="para">
								RAID-4 isn't too expensive since it only involves a one-in-N increase in costs and has no noticeable impact on read performance, but writes are slowed down. Furthermore, since a write to any of the N disks also involves a write to the parity disk, the latter sees many more writes than the former, and its lifespan can shorten dramatically as a consequence. Data on a RAID-4 array is safe only up to one failed disk (of the N+1).
							</div></dd><dt><span
                      class="term">RAID-5</span></dt><dd><div
                      class="para">
								RAID-5 menjawab masalah asimetri dari RAID-4: blok paritas disebar ke seluruh N+1 disk, tanpa ada satu disk yang memiliki peran tertentu.
							</div><div
                      class="para">
								Kinerja baca dan tulis identik dengan RAID-4. Di sini, sistem tetap berfungsi bila satu disk (dari N+1) gagal, tapi tak boleh lebih.
							</div></dd><dt><span
                      class="term">RAID-6</span></dt><dd><div
                      class="para">
								RAID-6 dapat dianggap perluasan dari RAID-5, dimana setiap seri N blok melibatkan dua blok redundansi, dan setiap seri N+2 blok disebar ke N+2 disk.
							</div><div
                      class="para">
								This RAID level is slightly more expensive than the previous two, but it brings some extra safety since up to two drives (of the N+2) can fail without compromising data availability. The counterpart is that write operations now involve writing one data block and two redundancy blocks, which makes them even slower.
							</div></dd><dt><span
                      class="term">RAID-1+0</span></dt><dd><div
                      class="para">
								This isn't strictly speaking, a RAID level, but a stacking of two RAID groupings. Starting from 2×N disks, one first sets them up by pairs into N RAID-1 volumes; these N volumes are then aggregated into one, either by “linear RAID” or (increasingly) by LVM. This last case goes farther than pure RAID, but there's no problem with that.
							</div><div
                      class="para">
								RAID-1+0 can survive multiple disk failures: up to N in the 2×N array described above, provided that at least one disk keeps working in each of the RAID-1 pairs.
							</div><div
                      class="sidebar"><a
                        xmlns=""
                        id="sidebar.raid-10"></a><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>LEBIH JAUH</em></span> RAID-10</strong></p></div></div></div><div
                        class="para">
								RAID-10 is generally considered a synonym of RAID-1+0, but a Linux specificity makes it actually a generalization. This setup allows a system where each block is stored on two different disks, even with an odd number of disks, the copies being spread out along a configurable model.
							</div><div
                        class="para">
								Performances will vary depending on the chosen repartition model and redundancy level, and of the workload of the logical volume.
							</div></div></dd></dl></div><div
                class="para">
					Obviously, the RAID level will be chosen according to the constraints and requirements of each application. Note that a single computer can have several distinct RAID arrays with different configurations.
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.raid-setup"></a>12.1.1.2. Menyiapkan RAID</h4></div></div></div><a
                id="id-1.15.4.6.9.2"
                class="indexterm"></a><div
                class="para">
					Setting up RAID volumes requires the <span
                  class="pkg pkg">mdadm</span> package; it provides the <code
                  class="command">mdadm</code> command, which allows creating and manipulating RAID arrays, as well as scripts and tools integrating it to the rest of the system, including the monitoring system.
				</div><div
                class="para">
					Our example will be a server with a number of disks, some of which are already used, the rest being available to setup RAID. We initially have the following disks and partitions:
				</div><div
                xmlns:d="http://docbook.org/ns/docbook"
                class="itemizedlist"><ul><li
                    class="listitem"><div
                      class="para">
							the <code
                        class="filename">sdb</code> disk, 4 GB, is entirely available;
						</div></li><li
                    class="listitem"><div
                      class="para">
							the <code
                        class="filename">sdc</code> disk, 4 GB, is also entirely available;
						</div></li><li
                    class="listitem"><div
                      class="para">
							on the <code
                        class="filename">sdd</code> disk, only partition <code
                        class="filename">sdd2</code> (about 4 GB) is available;
						</div></li><li
                    class="listitem"><div
                      class="para">
							finally, a <code
                        class="filename">sde</code> disk, still 4 GB, entirely available.
						</div></li></ul></div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>NOTE</em></span> Identifying existing RAID volumes</strong></p></div></div></div><div
                  class="para">
					The <code
                    class="filename">/proc/mdstat</code> file lists existing volumes and their states. When creating a new RAID volume, care should be taken not to name it the same as an existing volume.
				</div></div><div
                class="para">
					We're going to use these physical elements to build two volumes, one RAID-0 and one mirror (RAID-1). Let's start with the RAID-0 volume:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</code></strong>
<code
                  class="computeroutput">mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
# </code><strong
                  class="userinput"><code>mdadm --query /dev/md0</code></strong>
<code
                  class="computeroutput">/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md0</code></strong>
<code
                  class="computeroutput">/dev/md0:
        Version : 1.2
  Creation Time : Wed May  6 09:24:34 2015
     Raid Level : raid0
     Array Size : 8387584 (8.00 GiB 8.59 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:24:34 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

     Chunk Size : 512K

           Name : mirwiz:0  (local to host mirwiz)
           UUID : bb085b35:28e821bd:20d697c9:650152bb
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       1       8       32        1      active sync   /dev/sdc
# </code><strong
                  class="userinput"><code>mkfs.ext4 /dev/md0</code></strong>
<code
                  class="computeroutput">mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 2095104 4k blocks and 524288 inodes
Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </code><strong
                  class="userinput"><code>mkdir /srv/raid-0</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mount /dev/md0 /srv/raid-0</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/raid-0</code></strong>
<code
                  class="computeroutput">Filesystem      Size  Used Avail Use% Mounted on
/dev/md0        7.9G   18M  7.4G   1% /srv/raid-0
</code></pre><div
                class="para">
					The <code
                  class="command">mdadm --create</code> command requires several parameters: the name of the volume to create (<code
                  class="filename">/dev/md*</code>, with MD standing for <span
                  class="foreignphrase"><em
                    class="foreignphrase">Multiple Device</em></span>), the RAID level, the number of disks (which is compulsory despite being mostly meaningful only with RAID-1 and above), and the physical drives to use. Once the device is created, we can use it like we'd use a normal partition, create a filesystem on it, mount that filesystem, and so on. Note that our creation of a RAID-0 volume on <code
                  class="filename">md0</code> is nothing but coincidence, and the numbering of the array doesn't need to be correlated to the chosen amount of redundancy. It's also possible to create named RAID arrays, by giving <code
                  class="command">mdadm</code> parameters such as <code
                  class="filename">/dev/md/linear</code> instead of <code
                  class="filename">/dev/md0</code>.
				</div><div
                class="para">
					Creation of a RAID-1 follows a similar fashion, the differences only being noticeable after the creation:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%
Continue creating array? </code><strong
                  class="userinput"><code>y</code></strong>
<code
                  class="computeroutput">mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md1 started.
# </code><strong
                  class="userinput"><code>mdadm --query /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
        Version : 1.2
  Creation Time : Wed May  6 09:30:19 2015
     Raid Level : raid1
     Array Size : 4192192 (4.00 GiB 4.29 GB)
  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:30:40 2015
          State : clean, resyncing (PENDING) 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       1       8       64        1      active sync   /dev/sde
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
          State : clean
[...]
</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>TIP</em></span> RAID, disks and partitions</strong></p></div></div></div><div
                  class="para">
					As illustrated by our example, RAID devices can be constructed out of disk partitions, and do not require full disks.
				</div></div><div
                class="para">
					A few remarks are in order. First, <code
                  class="command">mdadm</code> notices that the physical elements have different sizes; since this implies that some space will be lost on the bigger element, a confirmation is required.
				</div><div
                class="para">
					More importantly, note the state of the mirror. The normal state of a RAID mirror is that both disks have exactly the same contents. However, nothing guarantees this is the case when the volume is first created. The RAID subsystem will therefore provide that guarantee itself, and there will be a synchronization phase as soon as the RAID device is created. After some time (the exact amount will depend on the actual size of the disks…), the RAID array switches to the “active” or “clean” state. Note that during this reconstruction phase, the mirror is in a degraded mode, and redundancy isn't assured. A disk failing during that risk window could lead to losing all the data. Large amounts of critical data, however, are rarely stored on a freshly created RAID array before its initial synchronization. Note that even in degraded mode, the <code
                  class="filename">/dev/md1</code> is usable, and a filesystem can be created on it, as well as some data copied on it.
				</div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>TIPS</em></span> Memulai mirror dalam mode terdegradasi</strong></p></div></div></div><div
                  class="para">
					Sometimes two disks are not immediately available when one wants to start a RAID-1 mirror, for instance because one of the disks one plans to include is already used to store the data one wants to move to the array. In such circumstances, it is possible to deliberately create a degraded RAID-1 array by passing <code
                    class="filename">missing</code> instead of a device file as one of the arguments to <code
                    class="command">mdadm</code>. Once the data have been copied to the “mirror”, the old disk can be added to the array. A synchronization will then take place, giving us the redundancy that was wanted in the first place.
				</div></div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>TIPS</em></span> Menyiapkan cermin tanpa sinkronisasi</strong></p></div></div></div><div
                  class="para">
					Volume RAID-1 sering dibuat untuk digunakan sebagai disk baru, sering dianggap kosong. Isi awal sebenarnya dari disk ini karena itu tidak sangat relevan, karena hanya perlu diketahui bahwa data setelah penciptaan volume, khususnya sistem berkas, dapat diakses setelahnya.
				</div><div
                  class="para">
					Karena itu orang mungkin bertanya-tanya tentang titik sinkronisasi kedua disk pada waktu penciptaan. Mengapa peduli apakah isi identik pada zona volume yang kita ketahui hanya dapat dibaca setelah kita telah menulis?
				</div><div
                  class="para">
					Untungnya, tahap sinkronisasi ini dapat dihindari dengan memberikan opsi <code
                    class="literal">--assume-clean</code> untuk <code
                    class="command">mdadm</code>. Namun, pilihan ini dapat menyebabkan kejutan dalam kasus-kasus di mana data awal akan dibaca (misalnya jika sistem berkas tersebut sudah hadir pada disk fisik), itulah sebabnya itu tidak diaktifkan secara default.
				</div></div><div
                class="para">
					Sekarang mari kita lihat apa yang terjadi ketika salah satu elemen array RAID-1 gagal. <code
                  class="command">mdadm</code>, khususnya opsi <code
                  class="literal">--fail</code>, memungkinkan simulasi suatu kegagalan disk:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --fail /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: set /dev/sde faulty in /dev/md1
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Update Time : Wed May  6 09:39:39 2015
          State : clean, degraded 
 Active Devices : 1
Working Devices : 1
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 19

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       0        0        2      removed

       1       8       64        -      faulty   /dev/sde</code></pre><div
                class="para">
					Isi dari volume masih dapat diakses (dan, jika dipasang, aplikasi tidak menyadari apapun), tapi keselamatan data tidak dijamin lagi: seandainya <code
                  class="filename">sdd</code> disk gagal bergantian, data akan hilang. Kami ingin menghindari risiko, jadi kami akan mengganti disk yang gagal dengan yang baru, <code
                  class="filename">sdf</code>:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --add /dev/sdf</code></strong>
<code
                  class="computeroutput">mdadm: added /dev/sdf
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
   Raid Devices : 2
  Total Devices : 3
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:48:49 2015
          State : clean, degraded, recovering 
 Active Devices : 1
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 1

 Rebuild Status : 28% complete

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 26

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      spare rebuilding   /dev/sdf

       1       8       64        -      faulty   /dev/sde
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Update Time : Wed May  6 09:49:08 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 41

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf

       1       8       64        -      faulty   /dev/sde</code></pre><div
                class="para">
					Di sini lagi, kernel secara otomatis memicu tahap rekonstruksi yang ketika berlangsung, meskipun volume masih dapat diakses, berada dalam mode terdegradasi. Setelah rekonstruksi berakhir, array RAID kembali ke keadaan normal. Kita kemudian dapat memberitahu ke sistem bahwa disk <code
                  class="filename">sde</code> akan dihapus dari array, sehingga berakhir dengan RAID mirror klasik pada dua disk:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --remove /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: hot removed /dev/sde from /dev/md1
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf</code></pre><div
                class="para">
					Selanjutnya drive dapat secara fisik dicabut saat server berikutnya dimatikan, atau bahkan dicabut saat menyala ketika konfigurasi hardware mengizinkan hot-swap. Konfigurasi tersebut termasuk beberapa pengendali SCSI, kebanyakan disk SATA, dan drive eksternal yang beroperasi pada USB atau Firewire.
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.backup-raid-config"></a>12.1.1.3. Mem-back up Konfigurasi</h4></div></div></div><div
                class="para">
					Most of the meta-data concerning RAID volumes are saved directly on the disks that make up these arrays, so that the kernel can detect the arrays and their components and assemble them automatically when the system starts up. However, backing up this configuration is encouraged, because this detection isn't fail-proof, and it is only expected that it will fail precisely in sensitive circumstances. In our example, if the <code
                  class="filename">sde</code> disk failure had been real (instead of simulated) and the system had been restarted without removing this <code
                  class="filename">sde</code> disk, this disk could start working again due to having been probed during the reboot. The kernel would then have three physical elements, each claiming to contain half of the same RAID volume. Another source of confusion can come when RAID volumes from two servers are consolidated onto one server only. If these arrays were running normally before the disks were moved, the kernel would be able to detect and reassemble the pairs properly; but if the moved disks had been aggregated into an <code
                  class="filename">md1</code> on the old server, and the new server already has an <code
                  class="filename">md1</code>, one of the mirrors would be renamed.
				</div><div
                class="para">
					Backing up the configuration is therefore important, if only for reference. The standard way to do it is by editing the <code
                  class="filename">/etc/mdadm/mdadm.conf</code> file, an example of which is listed here:
				</div><div
                class="example"><a
                  xmlns=""
                  id="example.mdadm-conf"></a><p
                  class="title"><strong>Contoh 12.1. berkas konfigurasi <code
                      class="command">mdadm</code></strong></p><div
                  class="example-contents"><pre
                    class="programlisting"># mdadm.conf
#
# Please refer to mdadm.conf(5) for information about this file.
#

# by default (built-in), scan all partitions (/proc/partitions) and all
# containers for MD superblocks. alternatively, specify devices to scan, using
# wildcards if desired.
DEVICE /dev/sd*

# auto-create devices with Debian standard permissions
CREATE owner=root group=disk mode=0660 auto=yes

# automatically tag new arrays as belonging to the local system
HOMEHOST &lt;system&gt;

# instruct the monitoring daemon where to send mail alerts
MAILADDR root

# definitions of existing MD arrays
ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464

# This configuration was auto-generated on Thu, 17 Jan 2013 16:21:01 +0100
# by mkconf 3.2.5-3</pre></div></div><div
                class="para">
					One of the most useful details is the <code
                  class="literal">DEVICE</code> option, which lists the devices where the system will automatically look for components of RAID volumes at start-up time. In our example, we replaced the default value, <code
                  class="literal">partitions containers</code>, with an explicit list of device files, since we chose to use entire disks and not only partitions, for some volumes.
				</div><div
                class="para">
					Dua baris terakhir dalam contoh kita adalah yang memungkinkan kernel untuk secara aman memilih nomor volume yang ditetapkan ke array mana. Metadata yang tersimpan pada disk itu sendiri cukup untuk membangun kembali volume, tetapi tidak untuk menentukan nomor volume (dan nama perangkat <code
                  class="filename">/dev/md*</code> yang cocok).
				</div><div
                class="para">
					Untungnya, baris-baris ini dapat dihasilkan secara otomatis:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --misc --detail --brief /dev/md?</code></strong>
<code
                  class="computeroutput">ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</code></pre><div
                class="para">
					Isi dari dua baris terakhir ini tidak tergantung pada daftar disk yang disertakan dalam volume. Maka tidak diperlukan untuk meregenerasi baris-baris ini ketika menggantikan disk gagal dengan yang baru. Di sisi lain, perawatan harus diambil untuk memperbarui berkas ketika membuat atau menghapus sebuah array RAID.
				</div></div></div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      xmlns=""
                      id="sect.lvm"></a>12.1.2. LVM</h3></div></div></div><a
              id="id-1.15.4.7.2"
              class="indexterm"></a><a
              id="id-1.15.4.7.3"
              class="indexterm"></a><div
              class="para">
				LVM, <span
                class="emphasis"><em>Logical Volume Manager</em></span>, adalah pendekatan lain untuk mengabstrakkan volume logis dari dukungan fisik mereka, yang berfokus pada peningkatan fleksibilitas daripada meningkatkan kehandalan. LVM dapat mengubah volume logis secara transparan bagi aplikasi; sebagai contoh, sangat mungkin untuk menambahkan disk baru, memigrasi data ke mereka, dan menghapus disk lama, tanpa melepas kait volume.
			</div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.lvm-concepts"></a>12.1.2.1. Konsep LVM</h4></div></div></div><div
                class="para">
					Fleksibilitas ini dicapai dengan tingkat abstraksi yang melibatkan tiga konsep.
				</div><div
                class="para">
					Pertama, PV (<span
                  class="emphasis"><em>Physical Volume</em></span>) adalah entitas terdekat dengan perangkat keras: itu bisa berupa partisi pada disk atau seluruh disk, atau bahkan perangkat blok lain (termasuk, sebagai contoh, sebuah array RAID). Perhatikan bahwa ketika sebuah elemen fisik diatur hingga menjadi PV untuk LVM, itu mesti hanya diakses melalui LVM, jika tidak sistem akan bingung.
				</div><div
                class="para">
					Sejumlah PV dapat dikumpulkan dalam VG (<span
                  class="emphasis"><em>Volume Group</em></span>), yang dapat dibandingkan dengan disk virtual dan extensible. VG abstrak, dan tidak muncul dalam perangkat berkas di hirarki <code
                  class="filename">/dev</code>, sehingga tidak ada risiko menggunakan mereka secara langsung.
				</div><div
                class="para">
					Jenis ke tiga objek adalah LV (<span
                  class="emphasis"><em>Logical Volume</em></span>), yang berupa potongan dari suatu VG; jika kita memakai analogi VG-sebagai-disk, LV setara dengan partisi. LV muncul sebagai perangkat blok dengan entri di <code
                  class="filename">/dev</code>, dan dapat digunakan seperti setiap partisi fisik lainnya dapat (paling sering, mewadahi sebuah sistem berkas atau ruang swap).
				</div><div
                class="para">
					The important thing is that the splitting of a VG into LVs is entirely independent of its physical components (the PVs). A VG with only a single physical component (a disk for instance) can be split into a dozen logical volumes; similarly, a VG can use several physical disks and appear as a single large logical volume. The only constraint, obviously, is that the total size allocated to LVs can't be bigger than the total capacity of the PVs in the volume group.
				</div><div
                class="para">
					Namun sering masuk akal untuk memiliki semacam keseragaman antara komponen fisik VG, dan untuk membagi VG menjadi volume logis yang akan memiliki pola penggunaan serupa. Misalnya, jika perangkat keras yang tersedia termasuk disk cepat dan disk lambat, yang cepat dapat dikelompokkan ke satu VG dan yang lambat ke lain; potongan pertama dapat kemudian ditugaskan untuk aplikasi yang membutuhkan akses data yang cepat, sementara yang kedua akan disimpan untuk tugas-tugas yang kurang menuntut.
				</div><div
                class="para">
					Dalam kasus apapun, perlu diingat bahwa LV tidak perlu melekat ke PV manapun. Dimungkinkan untuk mempengaruhi mana data dari LV secara fisik disimpan, tapi kemungkinan ini tidak diperlukan untuk penggunaan sehari-hari. Sebaliknya: ketika set komponen fisik VG berkembang, lokasi penyimpanan fisik yang sesuai dengan LV tertentu dapat bermigrasi di seluruh disk (dan tentu saja tetap di dalam PVs yang ditugaskan untuk VG).
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.lvm-setup"></a>12.1.2.2. Menyiapkan LVM</h4></div></div></div><div
                class="para">
					Mari kita sekarang ikuti, langkah demi langkah, proses pengaturan LVM untuk kasus penggunaan yang khas: kami ingin menyederhanakan situasi kompleks penyimpanan. Situasi seperti ini biasanya terjadi setelah beberapa sejarah yang panjang dan berbelit dari akumulasi langkah-langkah sementara. Untuk tujuan ilustrasi, kami akan mempertimbangkan server yang kebutuhan penyimpanannya telah berubah dari waktu ke waktu, berakhir dalam labirin dari partisi-partisi yang terpecah ke beberapa disk yang terpakai sebagian. Secara lebih konkret, partisi berikut tersedia:
				</div><div
                xmlns:d="http://docbook.org/ns/docbook"
                class="itemizedlist"><ul><li
                    class="listitem"><div
                      class="para">
							pada disk <code
                        class="filename">sdb</code>, sebuah partisi <code
                        class="filename">sdb2</code>, 4 GB;
						</div></li><li
                    class="listitem"><div
                      class="para">
							pada disk <code
                        class="filename">sdc</code>, sebuah partisi <code
                        class="filename">sdc3</code>, 3 GB;
						</div></li><li
                    class="listitem"><div
                      class="para">
							disk <code
                        class="filename">sdd</code>, 4 GB, sepenuhnya tersedia;
						</div></li><li
                    class="listitem"><div
                      class="para">
							pada disk <code
                        class="filename">sdf</code>, partisi <code
                        class="filename">sdf1</code>, 4 GB; dan partisi <code
                        class="filename">sdf2</code>, 5 GB.
						</div></li></ul></div><div
                class="para">
					Selain itu, mari kita asumsikan bahwa disk <code
                  class="filename">sdb</code> dan <code
                  class="filename">sdf</code> adalah lebih cepat daripada dua lainnya.
				</div><div
                class="para">
					Tujuan kami adalah untuk mengatur tiga volume logis untuk tiga aplikasi yang berbeda: server berkas memerlukan ruang penyimpanan 5 GB, sebuah basis data (1 GB) dan ruang untuk back-up (12 GB). Dua yang pertama perlu kinerja yang baik, tapi back-up kurang kritis dalam hal kecepatan akses. Semua kendala ini mencegah penggunaan partisi sendirian; menggunakan LVM dapat mengabstraksi ukuran fisik dari perangkat, sehingga satu-satunya batas adalah jumlah ruang yang tersedia.
				</div><div
                class="para">
					Alat-alat yang diperlukan ada dalam paket <span
                  class="pkg pkg">lvm2</span> dan dependensinya. Ketika mereka sedang diinstal, pengaturan LVM mengambil tiga langkah, cocok dengan konsep tiga tingkat.
				</div><div
                class="para">
					Pertama, kami siapkan volume fisik menggunakan <code
                  class="command">pvcreate</code>:
				</div><a
                xmlns=""
                id="screen.pvcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvdisplay</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvcreate /dev/sdb2</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdb2" successfully created
# </code><strong
                  class="userinput"><code>pvdisplay</code></strong>
<code
                  class="computeroutput">  "/dev/sdb2" is a new physical volume of "4.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/sdb2
  VG Name               
  PV Size               4.00 GiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I

# </code><strong
                  class="userinput"><code>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdc3" successfully created
  Physical volume "/dev/sdd" successfully created
  Physical volume "/dev/sdf1" successfully created
  Physical volume "/dev/sdf2" successfully created
# </code><strong
                  class="userinput"><code>pvdisplay -C</code></strong>
<code
                  class="computeroutput">  PV         VG   Fmt  Attr PSize PFree
  /dev/sdb2       lvm2 ---  4.00g 4.00g
  /dev/sdc3       lvm2 ---  3.09g 3.09g
  /dev/sdd        lvm2 ---  4.00g 4.00g
  /dev/sdf1       lvm2 ---  4.10g 4.10g
  /dev/sdf2       lvm2 ---  5.22g 5.22g
</code></pre><div
                class="para">
					Sejauh ini, masih baik; perhatikan bahwa PV dapat disiapkan pada seluruh disk maupun pada partisi individunya. Seperti yang ditunjukkan di atas, perintah <code
                  class="command">pvdisplay</code> menampilkan daftar PVs yang ada, dengan dua format keluaran mungkin.
				</div><div
                class="para">
					Sekarang mari kita merakit elemen-elemen fisik ini menjadi VG menggunakan <code
                  class="command">vgcreate</code>. Kita akan mengumpulkan hanya PV-PV dari disk cepat ke VG <code
                  class="filename">vg_critical</code>; VG lain, <code
                  class="filename">vg_normal</code>, juga akan memuat elemen-elemen yang lebih lambat.
				</div><a
                xmlns=""
                id="screen.vgcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>vgdisplay</code></strong>
<code
                  class="computeroutput">  No volume groups found
# </code><strong
                  class="userinput"><code>vgcreate vg_critical /dev/sdb2 /dev/sdf1</code></strong>
<code
                  class="computeroutput">  Volume group "vg_critical" successfully created
# </code><strong
                  class="userinput"><code>vgdisplay</code></strong>
<code
                  class="computeroutput">  --- Volume group ---
  VG Name               vg_critical
  System ID             
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               8.09 GiB
  PE Size               4.00 MiB
  Total PE              2071
  Alloc PE / Size       0 / 0   
  Free  PE / Size       2071 / 8.09 GiB
  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp

# </code><strong
                  class="userinput"><code>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</code></strong>
<code
                  class="computeroutput">  Volume group "vg_normal" successfully created
# </code><strong
                  class="userinput"><code>vgdisplay -C</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize  VFree 
  vg_critical   2   0   0 wz--n-  8.09g  8.09g
  vg_normal     3   0   0 wz--n- 12.30g 12.30g
</code></pre><div
                class="para">
					Here again, commands are rather straightforward (and <code
                  class="command">vgdisplay</code> proposes two output formats). Note that it is quite possible to use two partitions of the same physical disk into two different VGs. Note also that we used a <code
                  class="filename">vg_</code> prefix to name our VGs, but it is nothing more than a convention.
				</div><div
                class="para">
					Kita sekarang memiliki dua "disk virtual", masing-masing berukuran sekitar 8 GB dan 12 GB. Mari kita sekarang mengukir mereka ke dalam "partisi virtual" (LV). Ini melibatkan perintah <code
                  class="command">lvcreate</code>, dan sintaks yang agak lebih kompleks:
				</div><a
                xmlns=""
                id="screen.lvcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>lvdisplay</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>lvcreate -n lv_files -L 5G vg_critical</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_files" created
# </code><strong
                  class="userinput"><code>lvdisplay</code></strong>
<code
                  class="computeroutput">  --- Logical volume ---
  LV Path                /dev/vg_critical/lv_files
  LV Name                lv_files
  VG Name                vg_critical
  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT
  LV Write Access        read/write
  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400
  LV Status              available
  # open                 0
  LV Size                5.00 GiB
  Current LE             1280
  Segments               2
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0

# </code><strong
                  class="userinput"><code>lvcreate -n lv_base -L 1G vg_critical</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_base" created
# </code><strong
                  class="userinput"><code>lvcreate -n lv_backups -L 12G vg_normal</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_backups" created
# </code><strong
                  class="userinput"><code>lvdisplay -C</code></strong>
<code
                  class="computeroutput">  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_base    vg_critical -wi-a---  1.00g                                           
  lv_files   vg_critical -wi-a---  5.00g                                           
  lv_backups vg_normal   -wi-a--- 12.00g</code></pre><div
                class="para">
					Dua parameter diperlukan ketika membuat volume logis; mereka harus diberikan ke <code
                  class="command">lvcreate</code> sebagai opsi. Nama LV yang akan dibuat ditetapkan dengan opsi <code
                  class="literal">-n</code>, dan ukurannya biasanya diberikan menggunakan opsi <code
                  class="literal">-L</code>. Tentu saja kita juga perlu memberitahu ke perintah, VG mana yang dikenai operasi, maka diberikanlah parameter terakhir pada baris perintah.
				</div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>LEBIH JAUH</em></span> opsi-opsi <code
                            class="command">lvcreate</code></strong></p></div></div></div><div
                  class="para">
					The <code
                    class="command">lvcreate</code> command has several options to allow tweaking how the LV is created.
				</div><div
                  class="para">
					Mari kita pertama menjelaskan opsi <code
                    class="literal">-l</code>, dengannya ukuran LV dapat diberikan sebagai cacah blok (sebagai lawan dari unit "manusia" yang kita digunakan di atas). Blok-blok ini (disebut PE, <span
                    class="emphasis"><em>physical extents</em></span> dalam istilah LVM) adalah unit-unit ruang penyimpanan yang bersebelahan di PV, dan mereka tidak dapat dipecah di LV. Ketika seseorang ingin menentukan ruang penyimpanan untuk LV secara cukup presisi, misalnya menggunakan seluruh ruang yang tersedia, opsi <code
                    class="literal">-l</code> mungkin akan lebih disukai daripada <code
                    class="literal">-L</code>.
				</div><div
                  class="para">
					Memungkinkan juga untuk menunjuk pada lokasi fisik LV, sehingga extent disimpan pada PV tertentu (tentu saja masih tetap ada di dalam yang ditugaskan untuk VG). Karena kita tahu bahwa <code
                    class="filename">sdb</code> lebih cepat daripada <code
                    class="filename">sdf</code>, kita mungkin ingin menyimpan <code
                    class="filename">lv_base</code> di sana jika kita ingin memberikan keuntungan kepada server basis data dibandingkan dengan server berkas. Baris perintah menjadi: <code
                    class="command">lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</code>. Perhatikan bahwa perintah ini bisa gagal jika PV tidak memiliki cukup extent bebas. Dalam contoh kita, kita mungkin harus membuat <code
                    class="filename">lv_base</code> sebelum <code
                    class="filename">lv_files</code> untuk menghindari situasi ini - atau membebaskan sebagian ruang di <code
                    class="filename">sdb2</code> dengan perintah <code
                    class="command">pvmove</code>.
				</div></div><div
                class="para">
					Volume logis, sekali dibuat, akan menjadi berkas perangkat blok dalam <code
                  class="filename">/dev/mapper/</code>:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>ls -l /dev/mapper</code></strong>
<code
                  class="computeroutput">total 0
crw------- 1 root root 10, 236 Jun 10 16:52 control
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2
# </code><strong
                  class="userinput"><code>ls -l /dev/dm-*</code></strong>
<code
                  class="computeroutput">brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0
brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1
brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2
</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>CATATAN</em></span> Mendeteksi otomatis volume LVM</strong></p></div></div></div><div
                  class="para">
					Ketika komputer boot, unit layanan systemd <code
                    class="filename">lvm2-activation</code> mengeksekusi <code
                    class="command">vgchange -aay</code> untuk "mengaktifkan" kelompok volume: memindai perangkat yang tersedia; yang telah diinisialisasi sebagai fisik untuk LVM didaftarkan ke subsistem LVM, yang berasal dari kelompok-kelompok volume dirakit, dan volume logis yang relevan dimulai dan dibuat tersedia. Karena itu tidak perlu menyunting berkas konfigurasi ketika membuat atau memodifikasi volume-volume LVM.
				</div><div
                  class="para">
					Namun, perlu diketahui bahwa tata letak elemen LVM (volume fisik dan logis, dan kelompok-kelompok volume) direkam cadang dalam <code
                    class="filename">/etc/lvm/backup</code>, yang dapat berguna dalam hal ada masalah (atau hanya untuk sekedar mengintip di balik layar).
				</div></div><div
                class="para">
					Untuk membuat semua lebih mudah, taut simbolik juga dibuat dalam direktori-direktori yang cocok dengan VG:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>ls -l /dev/vg_critical</code></strong>
<code
                  class="computeroutput">total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0
# </code><strong
                  class="userinput"><code>ls -l /dev/vg_normal</code></strong>
<code
                  class="computeroutput">total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</code></pre><div
                class="para">
					LV kemudian dapat digunakan persis seperti partisi standar:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mkfs.ext4 /dev/vg_normal/lv_backups</code></strong>
<code
                  class="computeroutput">mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 3145728 4k blocks and 786432 inodes
Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d
[...]
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </code><strong
                  class="userinput"><code>mkdir /srv/backups</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mount /dev/vg_normal/lv_backups /srv/backups</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/backups</code></strong>
<code
                  class="computeroutput">Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_normal-lv_backups   12G   30M   12G   1% /srv/backups
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>cat /etc/fstab</code></strong>
<code
                  class="computeroutput">[...]
/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2
/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2
/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</code></pre><div
                class="para">
					Dari sudut pandang aplikasi, berbagai partisi kecil sekarang telah diabstrakkan ke dalam satu volume 12 GB besar, dengan nama yang lebih mudah.
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.lvm-over-time"></a>12.1.2.3. LVM Dari Waktu Ke Waktu</h4></div></div></div><div
                class="para">
					Meskipun kemampuan untuk mengagregasi partisi atau disk fisik itu nyaman, ini bukanlah keuntungan utama yang dibawa oleh LVM. Fleksibilitas yang dibawanya terutama teramati seiring berjalannya waktu, ketika kebutuhan berevolusi. Dalam contoh kita, mari kita asumsikan bahwa berkas besar baru harus disimpan, dan bahwa LV yang didedikasikan untuk server berkas terlalu kecil untuk menampung mereka. Karena kita belum menggunakan seluruh ruang yang tersedia di <code
                  class="filename">vg_critical</code>, kita bisa perbesar <code
                  class="filename">lv_files</code>. Untuk tujuan tersebut, kita akan menggunakan perintah <code
                  class="command">lvresize</code>, lalu <code
                  class="command">resize2fs</code> untuk mengadaptasi sistem berkas:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/files/</code></strong>
<code
                  class="computeroutput">Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files
# </code><strong
                  class="userinput"><code>lvdisplay -C vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 5.00g
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   2   2   0 wz--n- 8.09g 2.09g
# </code><strong
                  class="userinput"><code>lvresize -L 7G vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).
  Logical volume lv_files successfully resized
# </code><strong
                  class="userinput"><code>lvdisplay -C vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 7.00g
# </code><strong
                  class="userinput"><code>resize2fs /dev/vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">resize2fs 1.42.12 (29-Aug-2014)
Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.

# </code><strong
                  class="userinput"><code>df -h /srv/files/</code></strong>
<code
                  class="computeroutput">Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>HATI-HATI</em></span> Mengubah ukuran sistem berkas</strong></p></div></div></div><div
                  class="para">
					Tidak semua sistem berkas dapat diubah ukurannya secara daring; mengubah ukuran volume oleh karena itu mungkin pertama memerlukan melepas kait sistem berkas dan mengait ulang setelah itu. Tentu saja, jika seseorang ingin mengecilkan ruang yang dialokasikan untuk LV, sistem berkas harus diperkecil dulu; urutan dibalik ketika perubahan ukuran untuk arah lain: volume logis harus diperbesar sebelum sistem berkas di atasnya. Hal ini cukup sederhana, karena kapanpun ukuran sistem tidak boleh lebih besar dari perangkat blok tempat dia berada (apakah perangkat berupa partisi fisik atau volume logis).
				</div><div
                  class="para">
					Sistem berkas ext3, ext4, dan xfs dapat diperbesar secara daring, tanpa melepas kain; menyusutkan memerlukan melepas kait. Sistem berkas reiserfs memungkinkan perubahan ukuran secara daring di kedua arah. ext2 tidak memungkinkan keduanya, dan selalu membutuhkan melepas kait.
				</div></div><div
                class="para">
					Kita bisa melanjutkan dengan cara yang sama untuk memperbesar volume yang mewadai basis data, tapi kita telah mencapai batas ruang VG yang tersedia:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/base/</code></strong>
<code
                  class="computeroutput">Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_base 1008M  854M  104M  90% /srv/base
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree 
  vg_critical   2   2   0 wz--n- 8.09g 92.00m</code></pre><div
                class="para">
					Tidak masalah, karena LVM memungkinkan menambahkan volume fisik ke grup volume yang ada. Misalnya, mungkin kita telah memperhatikan bahwa partisi <code
                  class="filename">sdb1</code>, yang sejauh ini digunakan di luar LVM, hanya berisi arsip yang dapat dipindahkan ke <code
                  class="filename">lv_backups</code>. Kita sekarang dapat mendaur ulang itu dan mengintegrasikannya ke grup volume, dan dengan demikian memperoleh kembali ruang bebas. Ini adalah tujuan dari perintah <code
                  class="command">vgextend</code>. Tentu saja, partisi harus disiapkan sebagai sebuah volume fisik terlebih dahulu. Setelah VG telah diperbesar, kita dapat menggunakan perintah sejenis seperti yang sebelumnya untuk menumbuhkan volume logis kemudian sistem berkasnya:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvcreate /dev/sdb1</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdb1" successfully created
# </code><strong
                  class="userinput"><code>vgextend vg_critical /dev/sdb1</code></strong>
<code
                  class="computeroutput">  Volume group "vg_critical" successfully extended
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   3   2   0 wz--n- 9.09g 1.09g
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>df -h /srv/base/</code></strong>
<code
                  class="computeroutput">Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>LEBIH JAUH</em></span> LVM tingkat lanjut</strong></p></div></div></div><div
                  class="para">
					LVM also caters for more advanced uses, where many details can be specified by hand. For instance, an administrator can tweak the size of the blocks that make up physical and logical volumes, as well as their physical layout. It is also possible to move blocks across PVs, for instance to fine-tune performance or, in a more mundane way, to free a PV when one needs to extract the corresponding physical disk from the VG (whether to affect it to another VG or to remove it from LVM altogether). The manual pages describing the commands are generally clear and detailed. A good entry point is the <span
                    class="citerefentry"><span
                      class="refentrytitle">lvm</span>(8)</span> manual page.
				</div></div></div></div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      xmlns=""
                      id="sect.raid-or-lvm"></a>12.1.3. RAID atau LVM?</h3></div></div></div><div
              class="para">
				RAID dan LVM keduanya membawa keuntungan tak terbantahkan bila kita abaikan kasus sederhana komputer desktop dengan satu hard disk dengan pola penggunaan tidak berubah dari waktu ke waktu. Namun, RAID dan LVM mengambil arah yang berbeda, dengan tujuan divergen, dan sah-sah saja bertanya-tanya mana yang harus diambil. Jawaban paling tepat akan tentu saja tergantung pada kebutuhan saat ini dan masa mendatang.
			</div><div
              class="para">
				Ada beberapa kasus sederhana dimana pertanyaan tidak benar-benar muncul. Jika kebutuhan adalah untuk mengamankan data terhadap kegagalan perangkat keras, maka jelas RAID akan disiapkan pada array disk, karena LVM tidak benar-benar menjawab masalah ini. Si sisi lain, jika kebutuhan adalah untuk skema penyimpanan yang fleksibel dimana volume dibuat independen terhadap tata letak fisik dari disk, RAID tidak banyak membantu dan LVM akan menjadi pilihan yang tepat.
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>CATATAN</em></span> Jika kinerja penting…</strong></p></div></div></div><div
                class="para">
				Jika kecepatan masukan/keluar adalah esensinya, terutama dalam hal waktu akses, menggunakan LVM dan/atau RAID di salah satu dari banyak kombinasi mungkin memiliki dampak pada kinerja, dan ini mungkin mempengaruhi keputusan untuk memilih yang mana. Namun, perbedaan-perbedaan dalam kinerja benar-benar kecil, dan hanya terukur dalam beberapa kasus penggunaan. Jika kinerja penting, keuntungan terbaik yang diperoleh adalah menggunakan media penyimpanan bukan rotasi (<a
                  id="id-1.15.4.8.4.2.1"
                  class="indexterm"></a> <span
                  class="emphasis"><em>solid-state drive</em></span>); biaya per megabyte mereka lebih tinggi daripada hard disk drive standar, dan kapasitas mereka biasanya lebih kecil, tapi mereka memberikan kinerja yang sangat baik untuk akses acak. Jika pola penggunaan mencakup banyak operasi keluaran/masukan yang terpencar di seluruh sistem berkas, misalnya untuk basis data tempat query-query yang kompleks rutin dijalankan, maka keuntungan dari menjalankan mereka pada SSD jauh lebih besar daripada apa pun yang bisa diperoleh dengan memilih LVM atas RAID atau sebaliknya. Dalam situasi ini, pilihan harus ditentukan oleh pertimbangan selain murni kecepatan, karena aspek kinerja paling mudah ditangani dengan menggunakan SSD.
			</div></div><div
              class="para">
				The third notable use case is when one just wants to aggregate two disks into one volume, either for performance reasons or to have a single filesystem that is larger than any of the available disks. This case can be addressed both by a RAID-0 (or even linear-RAID) and by an LVM volume. When in this situation, and barring extra constraints (for instance, keeping in line with the rest of the computers if they only use RAID), the configuration of choice will often be LVM. The initial set up is barely more complex, and that slight increase in complexity more than makes up for the extra flexibility that LVM brings if the requirements change or if new disks need to be added.
			</div><div
              class="para">
				Kemudian tentu saja, ada kasus penggunaan yang benar-benar menarik, dimana sistem penyimpanan perlu dibuat tahan terhadap kegagalan perangkat keras dan fleksibel tentang alokasi volume. RAID maupun LVM masing-masing dapat menjawab kedua persyaratan; ini adalah di mana kita menggunakan keduanya pada saat yang sama -- atau lebih tepatnya, satu di atas yang lain. Skema yang memiliki semua tapi belum menjadi standar karena RAID dan LVM telah mencapai kedewasaan untuk memastikan redundansi data pertama dengan pengelompokan disk dalam sejumlah kecil larik RAID besar, dan menggunakan larik RAID ini sebagai volume fisik LVM; partisi logis kemudian dapat ditoreh dari LV-LV ini untuk sistem berkas. Nilai jual konfigurasi ini adalah bahwa ketika sebuah disk gagal, hanya sejumlah kecil larik RAID yang perlu dibangun kembali, sehingga membatasi waktu yang dihabiskan oleh administrator untuk pemulihan.
			</div><div
              class="para">
				Mari kita ambil contoh konkret: departemen hubungan masyarakat di Falcot Corp memerlukan sebuah workstation untuk penyuntingan video, tapi anggaran departemen tidak mengizinkan berinvestasi di perangkat keras kelas tinggi secara lengkap. Keputusan dibuat untuk mendukung perangkat keras yang khusus untuk sifat grafis pekerjaan (monitor dan kartu video), dan tetap dengan perangkat keras generik untuk penyimpanan. Namun, seperti sudah dikenal luas, video digital memiliki beberapa persyaratan khusus untuk penyimpanan: banyaknya data yang akan disimpan besar, dan kecepatan pembacaan dan penulisan data ini penting untuk keseluruhan kinerja sistem (lebih daripada waktu akses rata-rata, misalnya). Batasan-batasn ini perlu dipenuhi dengan perangkat keras generik, dalam kasus ini dua hard disk drive SATA 300 GB; data sistem juga harus dibuat tahan terhadap kegagalan perangkat keras, termasuk sebagian data pengguna. Klip video yang diedit memang harus aman, tetapi tidak perlu bergegas menyunting video yang tertunda, karena mereka masih berada pada kaset.
			</div><div
              class="para">
				RAID-1 and LVM are combined to satisfy these constraints. The disks are attached to two different SATA controllers to optimize parallel access and reduce the risk of a simultaneous failure, and they therefore appear as <code
                class="filename">sda</code> and <code
                class="filename">sdc</code>. They are partitioned identically along the following scheme:
			</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>fdisk -l /dev/sda</code></strong>
<code
                class="computeroutput">
Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x00039a9f

Device    Boot     Start       End   Sectors Size Id Type
/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect
/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris
/dev/sda3        4000185 586099395 582099210 298G 5  Extended
/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect
/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect
/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</code></pre><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
						The first partitions of both disks (about 1 GB) are assembled into a RAID-1 volume, <code
                      class="filename">md0</code>. This mirror is directly used to store the root filesystem.
					</div></li><li
                  class="listitem"><div
                    class="para">
						The <code
                      class="filename">sda2</code> and <code
                      class="filename">sdc2</code> partitions are used as swap partitions, providing a total 2 GB of swap space. With 1 GB of RAM, the workstation has a comfortable amount of available memory.
					</div></li><li
                  class="listitem"><div
                    class="para">
						The <code
                      class="filename">sda5</code> and <code
                      class="filename">sdc5</code> partitions, as well as <code
                      class="filename">sda6</code> and <code
                      class="filename">sdc6</code>, are assembled into two new RAID-1 volumes of about 100 GB each, <code
                      class="filename">md1</code> and <code
                      class="filename">md2</code>. Both these mirrors are initialized as physical volumes for LVM, and assigned to the <code
                      class="filename">vg_raid</code> volume group. This VG thus contains about 200 GB of safe space.
					</div></li><li
                  class="listitem"><div
                    class="para">
						The remaining partitions, <code
                      class="filename">sda7</code> and <code
                      class="filename">sdc7</code>, are directly used as physical volumes, and assigned to another VG called <code
                      class="filename">vg_bulk</code>, which therefore ends up with roughly 200 GB of space.
					</div></li></ul></div><div
              class="para">
				Once the VGs are created, they can be partitioned in a very flexible way. One must keep in mind that LVs created in <code
                class="filename">vg_raid</code> will be preserved even if one of the disks fails, which will not be the case for LVs created in <code
                class="filename">vg_bulk</code>; on the other hand, the latter will be allocated in parallel on both disks, which allows higher read or write speeds for large files.
			</div><div
              class="para">
				We will therefore create the <code
                class="filename">lv_usr</code>, <code
                class="filename">lv_var</code> and <code
                class="filename">lv_home</code> LVs on <code
                class="filename">vg_raid</code>, to host the matching filesystems; another large LV, <code
                class="filename">lv_movies</code>, will be used to host the definitive versions of movies after editing. The other VG will be split into a large <code
                class="filename">lv_rushes</code>, for data straight out of the digital video cameras, and a <code
                class="filename">lv_tmp</code> for temporary files. The location of the work area is a less straightforward choice to make: while good performance is needed for that volume, is it worth risking losing work if a disk fails during an editing session? Depending on the answer to that question, the relevant LV will be created on one VG or the other.
			</div><div
              class="para">
				We now have both some redundancy for important data and much flexibility in how the available space is split across the applications. Should new software be installed later on (for editing audio clips, for instance), the LV hosting <code
                class="filename">/usr/</code> can be grown painlessly.
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>CATATAN</em></span> Mengapa tiga volume RAID-1?</strong></p></div></div></div><div
                class="para">
				We could have set up one RAID-1 volume only, to serve as a physical volume for <code
                  class="filename">vg_raid</code>. Why create three of them, then?
			</div><div
                class="para">
				The rationale for the first split (<code
                  class="filename">md0</code> vs. the others) is about data safety: data written to both elements of a RAID-1 mirror are exactly the same, and it is therefore possible to bypass the RAID layer and mount one of the disks directly. In case of a kernel bug, for instance, or if the LVM metadata become corrupted, it is still possible to boot a minimal system to access critical data such as the layout of disks in the RAID and LVM volumes; the metadata can then be reconstructed and the files can be accessed again, so that the system can be brought back to its nominal state.
			</div><div
                class="para">
				The rationale for the second split (<code
                  class="filename">md1</code> vs. <code
                  class="filename">md2</code>) is less clear-cut, and more related to acknowledging that the future is uncertain. When the workstation is first assembled, the exact storage requirements are not necessarily known with perfect precision; they can also evolve over time. In our case, we can't know in advance the actual storage space requirements for video rushes and complete video clips. If one particular clip needs a very large amount of rushes, and the VG dedicated to redundant data is less than halfway full, we can re-use some of its unneeded space. We can remove one of the physical volumes, say <code
                  class="filename">md2</code>, from <code
                  class="filename">vg_raid</code> and either assign it to <code
                  class="filename">vg_bulk</code> directly (if the expected duration of the operation is short enough that we can live with the temporary drop in performance), or undo the RAID setup on <code
                  class="filename">md2</code> and integrate its components <code
                  class="filename">sda6</code> and <code
                  class="filename">sdc6</code> into the bulk VG (which grows by 200 GB instead of 100 GB); the <code
                  class="filename">lv_rushes</code> logical volume can then be grown according to requirements.
			</div></div></div></div></div><ul
        class="docnav"><li
          class="previous"><a
            accesskey="p"
            href="sect.rtc-services.html"><strong>Sebelumnya</strong>11.8. Layanan Komunikasi Real-Time</a></li><li
          class="up"><a
            accesskey="u"
            href="#"><strong>Induk</strong></a></li><li
          class="home"><a
            accesskey="h"
            href="index.html"><strong>Depan</strong></a></li><li
          class="next"><a
            accesskey="n"
            href="sect.virtualization.html"><strong>Lanjut</strong>12.2. Virtualisasi</a></li></ul></body></html>
