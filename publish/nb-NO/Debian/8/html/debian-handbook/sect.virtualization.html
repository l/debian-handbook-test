<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html
    xmlns="http://www.w3.org/1999/xhtml"><head><meta
        http-equiv="Content-Type"
        content="text/html; charset=UTF-8" /><title
        xmlns:d="http://docbook.org/ns/docbook">12.2. Visualisering</title><link
        rel="stylesheet"
        type="text/css"
        href="Common_Content/css/default.css" /><link
        rel="stylesheet"
        media="print"
        href="Common_Content/css/print.css"
        type="text/css" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="generator"
        content="publican v4.3.2" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="package"
        content="Debian-debian-handbook-8-nb-NO-1.0-1" /><meta
        name="keywords"
        content="RAID, LVM, FAI, Å komme foran, Overvåking, Visualisering, Xen, LXC" /><link
        rel="home"
        href="index.html"
        title="Håndbok for Debian-administratoren" /><link
        rel="up"
        href="advanced-administration.html"
        title="Kapittel 12. Avansert administrasjon" /><link
        rel="prev"
        href="advanced-administration.html"
        title="Kapittel 12. Avansert administrasjon" /><link
        rel="next"
        href="sect.automated-installation.html"
        title="12.3. Automated Installation" /><link
        xmlns=""
        rel="canonical"
        href="https://debian-handbook.info/browse/nb-NO/stable/sect.virtualization.html" /></head><body><div
        id="banner"><a
          href="http://debian-handbook.info/get/"><span
            class="text">Download the ebook</span></a></div><p
        id="title"><a
          class="left"
          href="http://www.debian.org"><img
            alt="Product Site"
            src="Common_Content/images//image_left.png" /></a><a
          class="right"
          href="index.html"><img
            alt="Documentation Site"
            src="Common_Content/images//image_right.png" /></a></p><ul
        class="docnav top"><li
          class="previous"><a
            accesskey="p"
            href="advanced-administration.html"><strong>Forrige</strong></a></li><li
          class="home">Håndbok for Debian-administratoren</li><li
          class="next"><a
            accesskey="n"
            href="sect.automated-installation.html"><strong>Neste</strong></a></li></ul><div
        class="section"><div
          class="titlepage"><div><div><h2
                class="title"><a
                  xmlns=""
                  id="sect.virtualization"></a>12.2. Visualisering</h2></div></div></div><a
          id="id-1.15.5.2"
          class="indexterm"></a><div
          class="para">
			Virtualisering er en av de viktigste fremskritt i de seneste årenes datautvikling. Begrepet omfatter ulike abstraksjoner og teknikker som simulerer virtuelle datamaskiner med varierende grad av uavhengighet på selve maskinvaren. En fysisk tjener kan så være vert for flere systemer som arbeider samtidig og i isolasjon. Bruksområdene er mange, og ofte utledes fra denne isolasjon: For eksempel testmiljøer med varierende konfigurasjoner, eller separasjon av vertsbaserte tjenester mellom ulike virtuelle maskiner for sikkerheten.
		</div><div
          class="para">
			Det er flere virtualiseringsløsninger, hver med sine egne fordeler og ulemper. Denne boken vil fokusere på Xen, LXC, og KVM, mens andre viktige implementeringer omfatter de følgende:
		</div><a
          id="id-1.15.5.5"
          class="indexterm"></a><a
          id="id-1.15.5.6"
          class="indexterm"></a><a
          id="id-1.15.5.7"
          class="indexterm"></a><a
          id="id-1.15.5.8"
          class="indexterm"></a><a
          id="id-1.15.5.9"
          class="indexterm"></a><a
          id="id-1.15.5.10"
          class="indexterm"></a><div
          xmlns:d="http://docbook.org/ns/docbook"
          class="itemizedlist"><ul><li
              class="listitem"><div
                class="para">
					QEMU er en programvare-emulator for en fullverdig datamaskin. Prestasjonen er langt fra den hastigheten man kunne oppnå ved å kjøre den opprinnelige, men den tillater å kjøre umodifiserte eller eksperimentelle operativsystemer på den emulerte maskinvaren. Den tillater også å emulere en annen maskinvare-arkitektur: For eksempel, kan et <span
                  class="emphasis"><em>amd64</em></span>-system emulere en <span
                  class="emphasis"><em>arm</em></span>-datamaskin. QEMU er fri programvare. <div
                  xmlns=""
                  class="url">→ <a
                    xmlns="http://www.w3.org/1999/xhtml"
                    href="http://www.qemu.org/">http://www.qemu.org/</a></div>
				</div></li><li
              class="listitem"><div
                class="para">
					Bochs er en annen gratis virtuell maskin, men den emulerer bare x86-arkitekturene (i386 eller amd64).
				</div></li><li
              class="listitem"><div
                class="para">
					VMWare er en proprietær virtuell maskin; som er en av de eldste der ute, er det også en av de mest kjente. Det fungerer på prinsipper som ligner på QEMU. VMWare foreslår avanserte funksjoner som å ta øyeblikksbilder av en kjørende virtuell maskin. <div
                  xmlns=""
                  class="url">→ <a
                    xmlns="http://www.w3.org/1999/xhtml"
                    href="http://www.vmware.com/">http://www.vmware.com/</a></div>
				</div></li><li
              class="listitem"><div
                class="para">
					VirtualBox er en virtuell maskin som stort sett er fri programvare (noen ekstra komponenter er tilgjengelige under en proprietær lisens). Dessverre er det i Debians "contrib"-del fordi den inneholder noen ferdigbygde filer som ikke kan bygges opp igjen uten en proprietær kompilator. Mens VirtualBox er yngre enn VMWare og begrenset til i386 eller amd64 arkitekturer, inneholder den fortsatt muligheten til å ta noen øyeblikksbilder og andre interessante funksjoner. <div
                  xmlns=""
                  class="url">→ <a
                    xmlns="http://www.w3.org/1999/xhtml"
                    href="http://www.virtualbox.org/">http://www.virtualbox.org/</a></div>
				</div></li></ul></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    xmlns=""
                    id="sect.xen"></a>12.2.1. Xen</h3></div></div></div><div
            class="para">
				Xen <a
              id="id-1.15.5.12.2.1"
              class="indexterm"></a> er en "paravirtualiserings"-løsning. Den introduserer et tynt abstraksjonslag, kalt en "hypervisor", mellom maskinvaren og de øvre systemer; Dette fungerer som en dommer som kontrollerer tilgangen til maskinvaren fra de virtuelle maskinene. Men den håndterer bare noen av instruksjonene, resten kjøres direkte av maskinvaren på vegne av systemene. Den største fordelen er at prestasjonen blir ikke dårligere, og systemer kjører med nær sin opprinnelige hastighet; ulempen er at operativsystem-kjernene man ønsker å bruke på en Xen hypervisor, trenger tilpasning for å kjøre på Xen.
			</div><div
            class="para">
				La oss bruke litt tid på vilkår. Hypervisoren er det nederste laget, som kjører går er direkte på maskinvaren, selv under kjernen. Dette hypervisor kan dele resten av programvaren over flere <span
              class="emphasis"><em>domains</em></span>, som kan sees på så mange virtuelle maskiner. En av disse domenene (den første som blir startet) er kjent som <span
              class="emphasis"><em>dom0</em></span>, og har en spesiell rolle, siden bare dette domenet kan kontrollere hypervisor og kjøring av andre domener. Disse andre domener er kjent som<span
              class="emphasis"><em>domU</em></span>. Med andre ord, og fra et brukersynspunkt <span
              class="emphasis"><em>dom0</em></span>-et samsvarer med "verten" i andre visualiseringssystmer, mens en <span
              class="emphasis"><em>domU</em></span> kan bli sett på som en "gjest".
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>CULTURE</em></span> Xen og de ulike versjonene av Linux</strong></p></div></div></div><div
              class="para">
				Xen ble opprinnelig utviklet som et sett av oppdateringer ut fra det offisielle treet, men uten å bli integrert i Linux-kjernen. Samtidig krevde flere kommende virtualiseringssystemer (inkludert KVM) noen generiske virtualisering-relaterte funksjoner for å lette integrering sin, og Linux-kjernen fikk dette settet av funksjoner (kjent som <span
                class="emphasis"><em>paravirt_ops</em></span> eller <span
                class="emphasis"><em>pv_ops</em></span>-grensesnittet). Ettersom Xen dupliserte noen av funksjonaliteten til dette grensesnittet, kunde de ikke bli akseptert offentlig .
			</div><div
              class="para">
				XenSource, selskapet bak Xen, måttr derfor legge til Xen i dette nye rammeverket, slik at Xens rettelser kunne flettes inn i den offisielle Linux-kjernen. Det betydde mye omskriving av kode, og selv om XenSource snart hadde en fungerende versjon basert på paravirt_ops-grensesnittet, ble rettelsene bare gradvis fusjonert inn den offisielle kjernen. Flettingen ble ferdigstilt i Linux 3.0. <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://wiki.xenproject.org/wiki/XenParavirtOps">http://wiki.xenproject.org/wiki/XenParavirtOps</a></div>
			</div><div
              class="para">
				Ettersom <span
                class="distribution distribution">Jessie</span> er basert på Linux-kjernes versjon 3.16, inkluderer standardpakkene <span
                class="pkg pkg">linux-image-686-pae</span> og <span
                class="pkg pkg">linux-image-amd64</span> den nødvendige koden, og distribusjons-spesifikke rettelser som trengs til <span
                class="distribution distribution">Squeeze</span> og tidligere versjoner av Debian er ikke nødvendig lenger. <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix">http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix</a></div>
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>NOTE</em></span> Arkitekturer som er kompatible med Xen</strong></p></div></div></div><div
              class="para">
				Xen er foreløpig kun tilgjengelig for i386-, amd64-, arm64- og armh-arkitekturer.
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>CULTURE</em></span> Xen og ikke-Linux kjerner</strong></p></div></div></div><div
              class="para">
				Xen krever endringer i alle operativsystemer man ønsker å kjøre den på. Her har ikke alle kjerner har samme nivå av modenhet. Mange er fullt funksjonelle, både som dom0 og DOMU: Linux 3.0 og senere, NetBSD 4.0 og senere, og OpenSolaris. Andre fungere bare som en domU. Du kan sjekke status for hvert operativsystem i Xen wikien: <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen">http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen</a></div> <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://wiki.xenproject.org/wiki/DomU_Support_for_Xen">http://wiki.xenproject.org/wiki/DomU_Support_for_Xen</a></div>
			</div><div
              class="para">
				Men hvis Xen kan stole på maskinvarefunksjonene øremerket til virtualisering (som bare er til stede i nyere prosessorer), kan til og med ikke-modifiserte operativsystemer kjøres som domU (inkludert Windows).
			</div></div><div
            class="para">
				Å bruke Xen under Debians krever tre komponenter:
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						Hypervisoren selv. Etter tilgjengelig maskinvare, vil den aktuelle pakken være enten <span
                    class="pkg pkg">xen-hypervisor-4.4-amd64</span>, <span
                    class="pkg pkg">xen-hypervisor-4.4-armhf</span>, eller <span
                    class="pkg pkg">xen-hypervisor-4.4-arm64</span>.
					</div></li><li
                class="listitem"><div
                  class="para">
						En kjerne som kjører på den aktuelle hypervisoren. Enhver kjerne nyere enn 3.0 vil gjøre det, inkludert 3.16 versjon i <span
                    class="distribution distribution">Jessie</span>.
					</div></li><li
                class="listitem"><div
                  class="para">
						i386 arkitekturen krever også et standard bibliotek med de riktige oppdateringer som drar nytte av Xen; Dette er i <span
                    class="pkg pkg">libc6-xen</span>-pakken.
					</div></li></ul></div><div
            class="para">
				For å unngå å måtte velge disse komponentene for hånd, er noen hjelpepakker tilgjengelige (for eksempel <span
              class="pkg pkg">xen-linux-system-amd64</span>). De trekker alle inn en kjent, god kombinasjon med de aktuelle hypervisor- og kjerne-pakkene. Hypervisoren har også med <span
              class="pkg pkg">xen-utils-4.4</span>, som inneholder verktøy for å kontrollere hypervisoren fra dom0. Dette bringer i sin tur det aktuelle standard biblioteket. Under installasjonen av alt dette, lager også konfigurasjonsskriptene en ny oppføring i Grub oppstart-menyen, slik som å starte den valgt kjernen i en Xen dom0. Merk imidlertid at denne inngangen vanligvis er satt som den første på listen, og vil derfor bli valgt som standard. Hvis det ikke er ønsket, vil følgende kommandoer endre det:
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen
</code></strong><code
              class="computeroutput"># </code><strong
              class="userinput"><code>update-grub
</code></strong></pre><div
            class="para">
				Når disse nødvendigheter er installert, er neste skritt å teste hvordan l dom0 selv virker. Dette innebærer omstart for hypervisoren og Xen-kjernen. Systemet skal starte på vanlig måte, med noen ekstra meldinger på konsollen under de tidlige initialiseringstrinnene.
			</div><div
            class="para">
				Nå er det faktisk på tide å installere nyttige systemer på domU-systemene med verktøy fra <span
              class="pkg pkg">xen-tools</span>. Denne pakken leverer <code
              class="command">xen-create-image</code>-kommandoen, som i stor grad automatiserer oppgaven. Den eneste nødvendige parameteren er <code
              class="literal">--hostname</code>, som gir navn til domU-en. Andre valg er viktige, men de kan lagres i <code
              class="filename">/etc/xen-tools/xen-tools.conf</code>-konfigurasjonsfilen, og fraværet deres fra kommandolinjen utløser ikke en feil. Det er derfor viktig å enten sjekke innholdet i denne filen før du oppretter bilder, eller å bruke ekstra parametre i bruken av <code
              class="command">xen-create-image</code>. Viktige parametre omfatter de følgende:
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--memory</code>, for å spesifisere hvor mye RAM som er øremerket til det systemet som nettopp er laget;
					</div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--size</code> og <code
                    class="literal">--swap</code>, for å definere størrelsen på den “virtual disks” som er tilgjengelig for domU-en;
					</div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--debootstrap</code>, for å få det nye systemet skal bli installert med <code
                    class="command">debootstrap</code>; i det tilfellet vil også <code
                    class="literal">--dist</code>-valget oftest bli brukt (med et distribusjonsnavn som <span
                    class="distribution distribution">jessie</span>).
					</div><div
                  class="sidebar"><div
                    class="titlepage"><div><div><p
                          class="title"><strong><span
                              class="emphasis"><em>GOING FURTHER</em></span> Å instalere et ikke-Debian system i domU</strong></p></div></div></div><div
                    class="para">
						Med et ikke-Linux-system, må en, ved hjelp av <code
                      class="literal">--kernel</code>-valget, passe på å definere kjernen domU må bruke.
					</div></div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--dhcp</code> sier at domUs nettverkskonfigurasjon skal skaffes av DHCP, mens <code
                    class="literal">--ip</code> tillater å definere en statisk IP-adresse.
					</div></li><li
                class="listitem"><div
                  class="para">
						Til slutt må lagringsmetode velges for bildet som skal opprettes (de som vil bli sett på som harddisker fra domuU). Den enkleste metoden, tilsvarende <code
                    class="literal">--dir</code>-valget, er å opprette en fil på dom0 for hver enhet der domU skal være. For systemer som bruker LVM, er alternativet å bruke <code
                    class="literal">--lvm</code>-valget, fulgt av navnet på en volumgruppe; <code
                    class="command">xen-create-image</code> vil deretter opprette et nytt logisk volum inne i den gruppen, og dette logiske volumet vil bli tilgjengelig for domU-et som en harddisk.
					</div><div
                  class="sidebar"><div
                    class="titlepage"><div><div><p
                          class="title"><strong><span
                              class="emphasis"><em>NOTE</em></span> Lagring i domU</strong></p></div></div></div><div
                    class="para">
						Hele harddisker kan også bli eksportert til domU, samt partisjoner, RAID-matriser eller eksisterende logiske data fra tidligere. Disse operasjonene blir imidlertid ikke automatisert av <code
                      class="command">xen-create-image</code>, så å redigere Xen-bildets oppsettsfil er greit etter det første oppsettet med <code
                      class="command">xen-create-image</code>.
					</div></div></li></ul></div><div
            class="para">
				Så snart disse valgene er gjort, kan vi lage bildet til vår fremtidige Xen domU:
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</code></strong>
<code
              class="computeroutput">
[...]
General Information
--------------------
Hostname       :  testxen
Distribution   :  jessie
Mirror         :  http://ftp.debian.org/debian/
Partitions     :  swap            128Mb (swap)
                  /               2G    (ext3)
Image type     :  sparse
Memory size    :  128Mb
Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64
Initrd path    :  /boot/initrd.img-3.16.0-4-amd64
[...]
Logfile produced at:
         /var/log/xen-tools/testxen.log

Installation Summary
---------------------
Hostname        :  testxen
Distribution    :  jessie
MAC Address     :  00:16:3E:8E:67:5C
IP-Address(es)  :  dynamic
RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b
Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ
</code></pre><div
            class="para">
				Vi har nå en virtuell maskin, mål det er for øyeblikket ikke kjører (og bruker derfor bare plass på harddisken til dom0). Selvfølgelig kan vi skape flere bilder, kanskje med ulike parametere.
			</div><div
            class="para">
				Før du slår disse virtuelle maskinene på, må vi definere tilgangen deres. De kan selvfølgelig sees som isolerte maskiner, som bare nås gjennom sine systemkonsoller. Men dette samsvarer sjelden med bruksmønsteret. Mesteparten av tiden blir en domU betraktes som en ekstern tjener, og kun tilgjengelig gjennom et nettverk. Det vil være ganske upraktisk å legge til et nettverkskort for hver domU; som er grunnen til at Xen tillater å lage virtuelle grensesnitt, som hvert domene kan se og bruke som standard. Merk at disse kortene, selv om de er virtuelle, bare vil være nyttige så snart de er koblet til et nettverk, selv et virtuelt et. Xen har flere nettverksmodeller for det:
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						Den enkleste er <span
                    class="emphasis"><em>bridge</em></span>-modellen. Alle eth0-nettverkskort (både i dom0- og domU-systemer) oppfører seg som om de var direkte koblet til en Ethernet-svitsj.
					</div></li><li
                class="listitem"><div
                  class="para">
						Så følger <span
                    class="emphasis"><em>routing</em></span>-modellen, hvor dom0 oppfører seg som en ruter som står mellom domU-systemer og det (fysiske) eksterne nettverket.
					</div></li><li
                class="listitem"><div
                  class="para">
						Til slutt, i <span
                    class="emphasis"><em>NAT</em></span>-modellen, der dom0 igjen er mellom domU-systemene og resten av nettverket, men domU systemene er ikke direkte tilgjengelig utenfra, og trafikken går gjennom noen nettverksadresse-oversettelser på dom0-et.
					</div></li></ul></div><div
            class="para">
				Disse tre nettverksnodene innebefatter en rekke grensesnitt med uvanlige navn, for eksempel <code
              class="filename">vif*</code>, <code
              class="filename">veth*</code>, <code
              class="filename">peth*</code> og <code
              class="filename">xenbr0</code>. Xen-hypervisoren setter dem opp, uansett med hvilken layout de er definert i, under kontroll av verktøyet for brukerrom. Siden NAT- og ruting-modellene bare er tilpasset det enkelte tilfelle, vil vi bare omhandle brobyggingsmodellen.
			</div><div
            class="para">
				Standardkonfigurasjon av Xen-pakkene endrer ikke hele systemets nettverksoppsett. Men <code
              class="command">xend</code>-nissen er konfigurert for å integrere inn virtuelle nettverksgrensesnitt i alle tilstedeværende nettverksbroer (der <code
              class="filename">xenbr0</code> tar forrang dersom flere slike broer finnes). Vi må derfor settes opp en bro i <code
              class="filename">/etc/network/interfaces</code> (som krever installasjon av <span
              class="pkg pkg">bridge-utils</span>-pakken, som er grunnen til at <span
              class="pkg pkg">xen-utils-4.4</span>-pakken anbefaler den) for å erstatte den eksisterende eth0-inngangen:
			</div><pre
            class="programlisting">auto xenbr0
iface xenbr0 inet dhcp
    bridge_ports eth0
    bridge_maxwait 0
</pre><div
            class="para">
				Etter omstart, for å sørge for at brua blir opprettet automatisk, kan vi nå starte domU med Xen kontrollverktøyet, spesielt <code
              class="command">xl</code>-kommandoen. Denne kommandoen tillater ulike håndteringer av domenene, inkludert å føre dem opp og starte/stoppe dem.
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xl list</code></strong>
<code
              class="computeroutput">Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   463     1     r-----      9.8
# </code><strong
              class="userinput"><code>xl create /etc/xen/testxen.cfg</code></strong>
<code
              class="computeroutput">Parsing config from /etc/xen/testxen.cfg
# </code><strong
              class="userinput"><code>xl list</code></strong>
<code
              class="computeroutput">Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   366     1     r-----     11.4
testxen                                      1   128     1     -b----      1.1</code></pre><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>TOOL</em></span> Valg av verktøysamling for å håndtere Xen VM</strong></p></div></div></div><a
              id="id-1.15.5.12.24.2"
              class="indexterm"></a><a
              id="id-1.15.5.12.24.3"
              class="indexterm"></a><div
              class="para">
				I Debian 7 og eldre versjoner var kommandolinjeverktøyet <code
                class="command">xm</code> referansen når en skulle administrere Xen virtuelle maskiner. Nå er det erstattet av <code
                class="command">xl</code> som er mest bakoverkompatibelt. Men de er ikke det eneste tilgjengelige verktøyet: <code
                class="command">virsh</code> i libvirt og <code
                class="command">xe</code> til XenServer's XAPI (kommersielt tilbud for Xen), er alternative verktøy.
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>CAUTION</em></span> Bare en domU per bilde!</strong></p></div></div></div><div
              class="para">
				Mens det er selvfølgelig mulig å ha flere domU-systemer som kjører parallelt, har alle behov for å bruke sitt eget bilde, siden hver domU er laget for å tro det kjører på sin egen maskinvare (bortsett fra den lille biten av kjernen som snakker til hypervisor). Spesielt er det ikke mulig for to domU-systemer, som kjører samtidig, å dele lagringsplass. Hvis domU-systemene ikke kjører samtidig, er det imidlertid fullt mulig å gjenbruke en enkel vekselminne-partisjonen eller partisjonen som er vert for filsystemet <code
                class="filename">/home</code>.
			</div></div><div
            class="para">
				Merk at <code
              class="filename">testxen</code>-domU bruker virkelig minne tatt fra RAM som ellers ville være tilgjengelig for dom0, og ikke simulert minne. Når du bygger en tjener som skal være vert for Xen-bruk, pass på å sette av tilstrekkelig fysisk RAM.
			</div><div
            class="para">
				Se der!! Vår virtuelle maskinen starter opp. Vi får tilgang til den i en av to modi. Den vanlige måten er å koble seg til "eksternt" gjennom nettverket, slik som vi ville koble seg til en ekte maskin; Det vil som regel enten kreve oppsett av en DHCP-tjener, eller en DNS-konfigurasjon. Den andre måten, som kan være den eneste måten hvis nettverks-konfigurasjonen var feil, er å bruke <code
              class="filename">hvc0</code>konsollet, med <code
              class="command">xl console</code>-kommandoen:
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xl console testxen</code></strong>
<code
              class="computeroutput">[...]

Debian GNU/Linux 8 testxen hvc0

testxen login: </code></pre><div
            class="para">
				Man kan så åpne en sesjon, akkurat som man ville gjøre hvis du sitter med den virtuelle maskinens tastatur. Frakobling fra denne konsollen oppnås med <span
              class="keycap"><strong>Control</strong></span>+<span
              class="keycap"><strong>]</strong></span>-tastekombinasjon.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>TIP</em></span> Å få konsollen umiddelbart</strong></p></div></div></div><div
              class="para">
				Noen ganger ønsker man å starte et domU-system og med en gang få adgang til konsollen dens; Dette er grunnen til at <code
                class="command">xl create</code>-kommandoen velger en <code
                class="literal">-c</code>-bryter. Å starte en domU med denne bryteren vil vise alle meldingene når systemet starter.
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>TOOL</em></span> OpenXenManager</strong></p></div></div></div><div
              class="para">
				OpenXenManager (i <span
                class="pkg pkg">openxenmanager</span>-pakken) er et grafisk grensesnitt som tillater fjernadministrasjon av Xen-domener via Xen API. Den kan dermed eksternt styre Xen-domener, og har med de fleste av funksjonene i <code
                class="command">xl</code>-kommandoen.
			</div></div><div
            class="para">
				Når domU kjører, kan den brukes akkurat som en hvilken som helst annen tjener (siden den er et GNU/Linux-system tross alt). Imidlertid tillater den virtuelle maskinstatusen noen ekstra funksjoner. For eksempel kan en domU midlertidig stoppes, og så begynne igjen, med <code
              class="command">xl pause</code> og <code
              class="command">xl unpause</code>-kommandoer. Merk at selv om domU i pause ikke bruker noen prosessorkraft, er det tildelte minne fortsatt i bruk. Det kan være interessant å vurdere <code
              class="command">xl save</code> and <code
              class="command">xl restore</code>kommandoene: Å spare en domU frigjør ressursene den tidligere brukte, inkludert RAM. Når gjenopptatt (eller avpauset, for den saks skyld), legger ikke domU en gang merke til noe utover tiden som går. Hvis en domU var i gang når dom0 er stengt ned, lagrer skriptpakken automatisk domU-en, og gjenopprette den ved neste oppstart. Dette vil selvfølgelig medføre at de standard ubekvemmelighetene inntrufne påløper når en bærbar datamaskin legges i dvale. For eksempel, spesielt hvis domU er suspendert for lenge, kan nettverkstilkoblinger utløpe. Merk også at Xen så langt er uforenlig med en stor del av ACPI strømstyring, noe som utelukker suspensjon av vert-(dom0)systemet.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>DOCUMENTATION</em></span> <code
                        class="command">xl</code> valg</strong></p></div></div></div><div
              class="para">
				De fleste av <code
                class="command">xl</code>-underkommandoer forventer ett eller flere argumenter, ofte et domU nave. Disse argumentene er godt beskrevet på denne manualsiden <span
                class="citerefentry"><span
                  class="refentrytitle">xl</span>(1)</span>.
			</div></div><div
            class="para">
				Stanse eller restarte en domU kan gjøres enten fra domU-en (med <code
              class="command">shutdown</code> command) eller fra dom0, med <code
              class="command">xl shutdown</code> eller <code
              class="command">xl reboot</code>.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>GOING FURTHER</em></span> Advanced Xen</strong></p></div></div></div><div
              class="para">
				Xen har mange flere funksjoner enn vi kan beskrive i et par avsnitt. Spesielt er systemet meget dynamisk, og mange parametere for en domene (for eksempel mengden av avsatt hukommelse, de synlige harddisker, oppførselen til oppgave planleggeren, og så videre) kan justeres selv når domenet er i gang. En domU kan også overføres på tvers av tjenere uten å bli stengt ned, og uten å miste sine nettverkstilkoblinger! For alle disse avanserte mulighetene er primærkilden til informasjon den offisielle Xen-dokumentasjonen. <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://www.xen.org/support/documentation.html">http://www.xen.org/support/documentation.html</a></div>
			</div></div></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    xmlns=""
                    id="sect.lxc"></a>12.2.2. LXC</h3></div></div></div><a
            id="id-1.15.5.13.2"
            class="indexterm"></a><div
            class="para">
				Selv om den brukes til å bygge "virtuelle maskiner", er LXC ikke strengt tatt et virtualiseringssystem, men et system for å isolere grupper av prosesser fra hverandre, selv om de alle kjører på den samme verten. Den trekker veksler på et sett av nyere utviklinger i Linux-kjernen, velkjent som <span
              class="emphasis"><em>control groups</em></span>, der forskjellige sett med prosesser som kalles "grupper" har forskjellige visninger av forskjellige aspekter ved det totale systemet. Mest kjent blant disse aspektene er prosess-identifikatorene, nettverks-konfigurasjonene og monteringspunktene. En slik gruppe av isolerte prosesser vil ikke ha noen adgang til de andre prosesser i systemet, og gruppens adgang til filsystemet kan være begrenset til en spesifikk undergruppe. Den kan også ha sitt eget nettverksgrensesnitt og rutingstabell, og den kan være konfigurert til å bare se et delsett av de tilgjengelige verktøy som finnes i systemet.
			</div><div
            class="para">
				Disse funksjonene kan kombineres for å isolere en hel prosessfamilie som starter fra <code
              class="command">init</code>-prossessen, og det resulterende settet ser mye ut som en virtuell maskin. Det offisielle navnet på et slikt oppsett er en "beholder" (derav LXC-forkortelsen:<span
              class="emphasis"><em>LinuX Containers</em></span>), men en ganske viktig forskjell til "ekte" virtuelle maskiner, som leveres av Xen eller KVM, er at det ikke er noen andrekjerne; beholderen bruker den samme kjernen som vertsystemet. Dette har både fordeler og ulemper: Fordelene inkluderer utmerket ytelse grunnet total mangel på ekstrabelastning, og det faktum at kjernen har full oversikt over alle prosesser som kjører på systemet, slik at planleggingen kan være mer effektiv enn hvis to uavhengige kjerner skulle planlegge ulike oppgavesett. Den største blant ulempene er at det er umulig å kjøre en annen kjerne i en beholder (enten en annen Linux-versjon eller et annet operativsystem i det hele tatt).
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>NOTE</em></span> LXC isolasjonsgrenser</strong></p></div></div></div><div
              class="para">
				LXC beholdere gir ikke det isolasjonsnivået som oppnås med tyngre emulatorer eller virutaliserere. Spesielt:
			</div><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
						Ettersom kjernen er delt mellom vertsystemet og beholderne, kan prosesser avgrenset til beholdere fortsatt få tilgang til kjernemeldinger, noe som kan føre til informasjonslekkasje hvis meldingene er sendt ut fra en beholder;
					</div></li><li
                  class="listitem"><div
                    class="para">
						av lignende grunner, hvis en beholder er kompromittert og et sikkerhetsproblem i kjernen utnyttes, kan de øvrige beholdere også bli påvirket;
					</div></li><li
                  class="listitem"><div
                    class="para">
						på filsystemet, kjernen sjekker tillatelser etter de numeriske identifisere for brukere og grupper. Disse identifikatorene kan utpeke ulike brukere og grupper avhengig av beholderen, noe en bør huske på om skrivbare deler av filsystemet er delt mellom beholdere.
					</div></li></ul></div></div><div
            class="para">
				Siden vi har å gjøre med isolasjon og ikke vanlig virtualisering, er å sette opp LXC beholdere mer komplisert enn bare å kjøre en Debian-installer på en virtuell maskin. Vi vil beskrive noen forutsetninger, og deretter gå videre til nettverkskonfigurasjonen. Da vil vi faktisk være i stand til å lage systemet som skal kjøres i beholderen.
			</div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.13.7"></a>12.2.2.1. Innledende skritt</h4></div></div></div><div
              class="para">
					<span
                class="pkg pkg">lxc</span>-pakken inneholder de verktøyene som kreves for å kjøre LXC, og må derfor være installert.
				</div><div
              class="para">
					LXC krever også oppsettsystemet <span
                class="emphasis"><em>control groups</em></span> som er et virtuelt filsystem til å monteres på <code
                class="filename">/sys/fs/cgroup</code>. Ettersom Debian 8 byttet til systemd, som også er avhengig av kontrollgrupper gjøres dette nå automatisk ved oppstart uten ytterligere konfigurasjon.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="sect.lxc.network"></a>12.2.2.2. Nettverksoppsett</h4></div></div></div><div
              class="para">
					Målet med å installere LXC er å sette opp virtuelle maskiner; mens vi selvfølgelig kan holde dem isolert fra nettverket, og bare kommunisere med dem via filsystemet, innebærer de fleste brukstilfeller å minst gi minimal nettverkstilgang til beholderne. I det typiske tilfellet vil hver beholder får et virtuelt nettverksgrensesnitt koblet til det virkelige nettverket via en bro. Dette virtuelle grensesnittet kan kobles enten direkte på vertens fysiske nettverksgrensesnitt (der beholderen er direkte på nettverket), eller på et annet virtuelt grensesnitt som er definert hos verten (og verten kan da filtrere eller rute trafikk). I begge tilfelle kreves <span
                class="pkg pkg">bridge-utils</span> pakken.
				</div><div
              class="para">
					Det enkle tilfellet gjelder bare redigering <code
                class="filename">/etc/network/interfaces</code>, å flytte oppsettet for det fysiske grensesnittet (for eksempel <code
                class="literal">eth0</code>) til et brogrensesnitt (vanligvis <code
                class="literal">br0</code>), og konfigurere koblingen mellom dem. For eksempel, hvis nettverkskonfigurasjonsfilen i utgangspunktet inneholder oppføringer som for eksempel de følgende:
				</div><pre
              class="programlisting">auto eth0
iface eth0 inet dhcp</pre><div
              class="para">
					Bør de deaktiveres og erstattes med følgende:
				</div><pre
              class="programlisting">#auto eth0
#iface eth0 inet dhcp

auto br0
iface br0 inet dhcp
  bridge-ports eth0</pre><div
              class="para">
					Effekten av denne konfigurasjonen vil ligne på hva som ville blitt oppnådd dersom beholderne var maskiner koblet til det samme fysiske nettverket som vert. Bro-konfigurasjon håndterer transitt av Ethernet-rammer mellom alle bro-grensesnitt som inkluderer fysisk <code
                class="literal">eth0</code> samt grensesnittet definert for beholderne.
				</div><div
              class="para">
					I tilfeller der denne konfigurasjonen ikke kan brukes (for eksempel hvis ingen offentlige IP-adresser kan tildeles beholderne), blir et virtuelt <span
                class="emphasis"><em>tap</em></span> grensesnitt opprettet og koblet til broen. Den tilsvarende nettverksammenhengen blir da som en vert med en et andre nettverkskort koblet til en egen bryter, med også beholderne koblet til denne bryteren. Verten fungerer da som en inngangsport for beholdere hvis de er ment å kommunisere med omverdenen.
				</div><div
              class="para">
					I tillegg til <span
                class="pkg pkg">bridge-utils</span>, krever denne "rike" konfigurasjonen <span
                class="pkg pkg">vde2</span>-pakken; <code
                class="filename">/etc/network/interfaces</code>-filen blir da:
				</div><pre
              class="programlisting"># Interface eth0 is unchanged
auto eth0
iface eth0 inet dhcp

# Virtual interface 
auto tap0
iface tap0 inet manual
  vde2-switch -t tap0

# Bridge for containers
auto br0
iface br0 inet static
  bridge-ports tap0
  address 10.0.0.1
  netmask 255.255.255.0
</pre><div
              class="para">
					Nettverket kan så bli satt opp enten statisk i beholderne, eller dynamisk med en DHCP-tjener som kjører hos verten. En slik DHCP-tjener må konfigureres til å svare på spørsmål om <code
                class="literal">br0</code>-grensesnittet.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.13.9"></a>12.2.2.3. Å sette opp systemet</h4></div></div></div><div
              class="para">
					La oss nå satt opp filsystemet som skal brukes av beholderen. Siden denne "virtuelle maskinen" vil ikke kjøres direkte på maskinvare , er noen finjusteringer nødvendige sammenlignet med et standard filsystem, spesielt så langt som kjernen, enheter og konsollene angår. Heldigvis inkluderer <span
                class="pkg pkg">lxc</span> skript som stort sett automatiserer denne konfigurasjonen. For eksempel vil følgende kommandoer (som krever <span
                class="pkg pkg">debootstrap</span> og <span
                class="pkg pkg">rsync</span> packages) installere en Debian beholder:
				</div><pre
              class="screen"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-create -n testlxc -t debian
</code></strong><code
                class="computeroutput">debootstrap is /usr/sbin/debootstrap
Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... 
Downloading debian minimal ...
I: Retrieving Release 
I: Retrieving Release.gpg 
[...]
Download complete.
Copying rootfs to /var/lib/lxc/testlxc/rootfs...
[...]
Root password is 'sSiKhMzI', please change !
root@mirwiz:~# </code>
</pre><div
              class="para">
					Merk at filsystemet opprinnelig er opprettet i<code
                class="filename">/var/cache/lxc</code>, og deretter flyttet til den katalogen filsystemet skal til. Dette gjør det mulig å lage identiske beholdere mye raskere, ettersom det da bare kreves kopiering.
				</div><div
              class="para">
					Merk at Debian-skriptet for å opprette maler godtar et <code
                class="option">--arch</code>-valg for å spesifisere arkitekturen til systemet som skal installeres, og et <code
                class="option">--release</code>-valg hvis du ønsker å installere noe annet enn den nåværende stabile utgaven av Debian. Du kan også sette omgivelsesvariabelen <code
                class="literal">MIRROR</code> til å peke på et lokalt Debian speil.
				</div><div
              class="para">
					Nå inneholder det nyopprettede filsystemet et minimalt Debian-system, og som standard har ikke beholderen nettverksgrensesnitt (utover filmonteringen). Siden dette ikke er virkelig ønsket, vil vi endre beholderens konfigurasjonsfil (<code
                class="filename">/var/lib/lxc/testlxc/config</code>) og legge til noen få <code
                class="literal">lxc.network.*</code>-innganger:
				</div><pre
              class="programlisting">lxc.network.type = veth
lxc.network.flags = up
lxc.network.link = br0
lxc.network.hwaddr = 4a:49:43:49:79:20
</pre><div
              class="para">
					Disse oppføringene betyr, henholdsvis, at et virtuelt grensesnitt vil bli opprettet i beholderen; at det automatisk vil vist når det blir meldt at beholderen er startet; at det automatisk vil bli koblet til <code
                class="literal">br0</code>-broen hos verten; og at MAC-adressen vil være som spesifisert. Skulle denne siste posten mangle eller være deaktivert, vil det genereres en tilfeldig MAC-adresse.
				</div><div
              class="para">
					En annen nyttig inngang i den filen er innstillingen for vertsnavnet:
				</div><pre
              class="programlisting">lxc.utsname = testlxc
</pre></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.13.10"></a>12.2.2.4. Å starte beholderen</h4></div></div></div><div
              class="para">
					Nå som vårt virtuelle maskinbilde er klart, la oss starte beholderen:
				</div><pre
              class="screen scale"
              width="94"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-start --daemon --name=testlxc
</code></strong><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-console -n testlxc
</code></strong><code
                class="computeroutput">Debian GNU/Linux 8 testlxc tty1

testlxc login: </code><strong
                class="userinput"><code>root</code></strong><code
                class="computeroutput">
Password: 
Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
root@testlxc:~# </code><strong
                class="userinput"><code>ps auxwf</code></strong>
<code
                class="computeroutput">USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init
root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald
root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D
root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux
root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux
root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux
root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     
root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \_ -bash
root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \_ ps auxfw
root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102
root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e
root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux
root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux
root@testlxc:~# </code></pre><div
              class="para">
					Nå er vi i beholderen; vår tilgang til prosessene er begrenset til bare de som er startet fra beholderen selv, og vår tilgang til filsystemet er tilsvarende begrenset til den øremerkede undergruppen i hele filsystemet (<code
                class="filename">/var/lib/lxc/testlxc/rootfs</code>). Vi kan gå ut av konsollet med <span
                class="keycap"><strong>Control</strong></span>+<span
                class="keycap"><strong>a</strong></span> <span
                class="keycap"><strong>q</strong></span>.
				</div><div
              class="para">
					Legg merke til at vi kjørte beholderen som en bakgrunnsprosess, takket være <code
                class="option">--daemon</code>-valget til <code
                class="command">lxc-start</code>. Vi i kan avbryte beholderen med en kommando slik som <code
                class="command">lxc-stop --name=testlxc</code>.
				</div><div
              class="para">
					<span
                class="pkg pkg">lxc</span>-pakken inneholder et initialiseringsskript som automatisk kan starte en eller flere beholdere når verten starter opp (det er avhengig av <code
                class="command">lxc-autostart</code> som starter beholdere der <code
                class="literal">lxc.start.auto</code>-valget er satt til 1). Mer finkornet kontroll over oppstartsrekkefølgen er mulig med <code
                class="literal">lxc.start.order</code> og <code
                class="literal">lxc.group</code>. Som standard, starter klargjøringsskriptet først beholdere som er en del av <code
                class="literal">onboot</code>-gruppen og deretter beholdere som ikke er en del av en gruppe. I begge tilfeller er rekkefølgen innenfor en gruppe er definert av <code
                class="literal">lxc.start.order</code>-valget.
				</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>GOING FURTHER</em></span> Synliggjøring av masse</strong></p></div></div></div><div
                class="para">
					Siden LXC er et meget lett isolasjonssystem, kan det spesielt tilpasses til å være et massivt vertskap for virtuelle servere. Nettverkskonfigurasjonen vil trolig være litt mer avansert enn hva vi beskrev ovenfor, men den "rike" konfigurasjon som bruker <code
                  class="literal">tap</code> og <code
                  class="literal">veth</code>-grensesnitt skulle i mange tilfelle være nok.
				</div><div
                class="para">
					Det kan også være fornuftig å ha en del av filsystemet felles, slik som <code
                  class="filename">/usr</code> og <code
                  class="filename">/lib</code>-undertrærne, slik at man unngår å duplisere programvaren som kanskje må være felles for flere containere. Dette vil vanligvis oppnås med <code
                  class="literal">lxc.mount.entry</code>-innganger i beholdernes konfigurasjonsfil. En interessant bieffekt er at prosessene da vil bruke mindre fysisk minne, siden kjernen er i stand til å oppdage felles programmer. Den marginale belastningen for en ekstra beholder kan da reduseres til diskplassen øremerket til dens spesifikke data, og noen ekstra prosesser som kjernen må planlegge og administrere.
				</div><div
                class="para">
					Vi har selvfølgelig ikke beskrevet alle de tilgjengelige alternativene, selvfølgelig. Mer omfattende informasjon kan fås fra <span
                  class="citerefentry"><span
                    class="refentrytitle">lxc</span>(7)</span> og <span
                  class="citerefentry"><span
                    class="refentrytitle">lxc.container.conf</span>(5)</span>-manualsider og sidene de refererer til.
				</div></div></div></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    xmlns=""
                    id="id-1.15.5.14"></a>12.2.3. Virtualisering med KVM</h3></div></div></div><a
            id="id-1.15.5.14.2"
            class="indexterm"></a><div
            class="para">
				KVM, som står for <span
              class="emphasis"><em>Kernel-based Virtual Machine</em></span>, er først og fremst en kjernemodul som gir det meste av infrastrukturen som kan brukes av en visualiserer, men er ikke selv en visualiserer. Faktisk kontroll av visualiseringen håndteres av en QEMU-basert applikasjon. Ikke være bekymret om denne seksjonen nevner <code
              class="command">qemu-*</code>-kommandoer, den er fremdeles om KVM.
			</div><div
            class="para">
				I motsetning til andre visualiseringssystemer, ble KVM fusjonert inn i Linux-kjernen helt fra starten. Utviklerne valgte å dra nytte av prosessorens instruksjonssett øremerket til visualisering (Intel-VT og AMD-V), som holder KVM lett, elegant og ikke ressurskrevende. Motstykket, selvfølgelig, er at KVM ikke fungerer på alle datamaskiner, men bare på dem med riktige prosessorer. For x86-datamaskiner, kan du bekrefte at du 'har en slik prosessor ved å se etter "VMX" eller "svm" i CPU flagg oppført i <code
              class="filename">/proc/cpuinfo</code>.
			</div><div
            class="para">
				Med Red Hats aktive støtte til utviklingen, har KVM mer eller mindre blitt referansen for Linux virtualisering.
			</div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.6"></a>12.2.3.1. Innledende skritt</h4></div></div></div><a
              id="id-1.15.5.14.6.2"
              class="indexterm"></a><div
              class="para">
					I motsetning til verktøy som VirtualBox, har KVM selv ikke noe brukergrensesnitt for å opprette og administrere virtuelle maskiner. <span
                class="pkg pkg">qemu-kvm</span>-pakken gir bare en kjørbar som kan starte en virtuell maskin, samt et initialiseringsscript som laster de aktuelle kjernemodulene.
				</div><a
              id="id-1.15.5.14.6.4"
              class="indexterm"></a><a
              id="id-1.15.5.14.6.5"
              class="indexterm"></a><div
              class="para">
					Heldigvis gir Red Hat også et annet sett med verktøy for å løse dette problemet ved utvikling av <span
                class="emphasis"><em>libvirt</em></span>-bibliotektet og de tilhørende <span
                class="emphasis"><em>virtual machine manager</em></span>-verktøyene. libvirt kan administrere virtuelle maskiner på en enhetlig måte, uavhengig av virtualiseringen bak i kulissene (det støtter for tiden QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare og UML). <code
                class="command">virtual-manager</code> er et grafisk grensesnitt som bruker libvirt til å opprette og administrere virtuelle maskiner.
				</div><a
              id="id-1.15.5.14.6.7"
              class="indexterm"></a><div
              class="para">
					Vi installerer først de nødvendige pakker med <code
                class="command">apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</code>. <span
                class="pkg pkg">libvirt-bin</span> gir <code
                class="command">libvirtd</code>-nissen, som tillater (potensielt ekstern) håndtering av virtuelle maskiner som kjører på verten, og starter de nødvendige VM-er når verten starter opp. I tillegg gir denne pakken <code
                class="command">virsh</code>-kommandolinjeverktøy som gjør det mulig å styre <code
                class="command">libvirtd</code>-håndterte maskiner.
				</div><div
              class="para">
					<span
                class="pkg pkg">virtinst</span> pakken leverer <code
                class="command">virt-install</code>, som tillater å lage virtuelle maskiner fra kommandolinjen. Avslutningsvis gir <span
                class="pkg pkg">virt-viewer</span> tilgang til et VMs grafiske konsoll.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.7"></a>12.2.3.2. Nettverksoppsett</h4></div></div></div><div
              class="para">
					Akkurat som i Xen og LXC, innebærer den hyppigste nettverkskonfigurasjon en bro som grupperer nettverksgrensesnittene og de virtuelle maskinene (se <a
                class="xref"
                href="sect.virtualization.html#sect.lxc.network">Seksjon 12.2.2.2, «Nettverksoppsett»</a>).
				</div><div
              class="para">
					Alternativt, og i standardkonfigurasjonen levert av KVM, er den virtuelle maskinen tildelt en privat adresse (i 192.168.122.0/24-området), og NAT er satt opp slik at VM kan få tilgang til nettverket utenfor.
				</div><div
              class="para">
					Resten av denne seksjonen forutsetter at verten har et <code
                class="literal">eth0</code> fysisk grensesnitt og en <code
                class="literal">br0</code>-bro, og den første er knyttet til den siste.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.8"></a>12.2.3.3. Installasjon med <code
                      class="command">virt-install</code></h4></div></div></div><a
              id="id-1.15.5.14.8.2"
              class="indexterm"></a><div
              class="para">
					Creating a virtual machine is very similar to installing a normal system, except that the virtual machine's characteristics are described in a seemingly endless command line.
				</div><div
              class="para">
					Practically speaking, this means we will use the Debian installer, by booting the virtual machine on a virtual DVD-ROM drive that maps to a Debian DVD image stored on the host system. The VM will export its graphical console over the VNC protocol (see <a
                class="xref"
                href="sect.remote-login.html#sect.remote-desktops">Seksjon 9.2.2, «Å bruke eksterne grafiske skrivebord»</a> for details), which will allow us to control the installation process.
				</div><div
              class="para">
					We first need to tell libvirtd where to store the disk images, unless the default location (<code
                class="filename">/var/lib/libvirt/images/</code>) is fine.
				</div><pre
              class="screen"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>mkdir /srv/kvm</code></strong>
<code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>virsh pool-create-as srv-kvm dir --target /srv/kvm</code></strong>
<code
                class="computeroutput">Pool srv-kvm created

root@mirwiz:~# </code></pre><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>TIP</em></span> Add your user to the libvirt group</strong></p></div></div></div><div
                class="para">
					All samples in this section assume that you are running commands as root. Effectively, if you want to control a local libvirt daemon, you need either to be root or to be a member of the <code
                  class="literal">libvirt</code> group (which is not the case by default). Thus if you want to avoid using root rights too often, you can add yoursel to the <code
                  class="literal">libvirt</code> group and run the various commands under your user identity.
				</div></div><div
              class="para">
					Let us now start the installation process for the virtual machine, and have a closer look at <code
                class="command">virt-install</code>'s most important options. This command registers the virtual machine and its parameters in libvirtd, then starts it so that its installation can proceed.
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virt-install --connect qemu:///system  <span
                    id="virtinst.connect"><img
                      class="callout"
                      src="Common_Content/images/1.png"
                      alt="1" /></span>
               --virt-type kvm           <span
                    id="virtinst.type"><img
                      class="callout"
                      src="Common_Content/images/2.png"
                      alt="2" /></span>
               --name testkvm            <span
                    id="virtinst.name"><img
                      class="callout"
                      src="Common_Content/images/3.png"
                      alt="3" /></span>
               --ram 1024                <span
                    id="virtinst.ram"><img
                      class="callout"
                      src="Common_Content/images/4.png"
                      alt="4" /></span>
               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <span
                    id="virtinst.disk"><img
                      class="callout"
                      src="Common_Content/images/5.png"
                      alt="5" /></span>
               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <span
                    id="virtinst.cdrom"><img
                      class="callout"
                      src="Common_Content/images/6.png"
                      alt="6" /></span>
               --network bridge=br0      <span
                    id="virtinst.network"><img
                      class="callout"
                      src="Common_Content/images/7.png"
                      alt="7" /></span>
               --vnc                     <span
                    id="virtinst.vnc"><img
                      class="callout"
                      src="Common_Content/images/8.png"
                      alt="8" /></span>
               --os-type linux           <span
                    id="virtinst.os"><img
                      class="callout"
                      src="Common_Content/images/9.png"
                      alt="9" /></span>
               --os-variant debianwheezy
</code></strong><code
                class="computeroutput">
Starting install...
Allocating 'testkvm.qcow'             |  10 GB     00:00
Creating domain...                    |    0 B     00:00
Guest installation complete... restarting guest.
</code></pre><div
              class="calloutlist"><table
                border="0"
                summary="Callout list"><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.connect"><img
                          class="callout"
                          src="Common_Content/images/1.png"
                          alt="1" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							The <code
                        class="literal">--connect</code> option specifies the “hypervisor” to use. Its form is that of an URL containing a virtualization system (<code
                        class="literal">xen://</code>, <code
                        class="literal">qemu://</code>, <code
                        class="literal">lxc://</code>, <code
                        class="literal">openvz://</code>, <code
                        class="literal">vbox://</code>, and so on) and the machine that should host the VM (this can be left empty in the case of the local host). In addition to that, and in the QEMU/KVM case, each user can manage virtual machines working with restricted permissions, and the URL path allows differentiating “system” machines (<code
                        class="literal">/system</code>) from others (<code
                        class="literal">/session</code>).
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.type"><img
                          class="callout"
                          src="Common_Content/images/2.png"
                          alt="2" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							Since KVM is managed the same way as QEMU, the <code
                        class="literal">--virt-type kvm</code> allows specifying the use of KVM even though the URL looks like QEMU.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.name"><img
                          class="callout"
                          src="Common_Content/images/3.png"
                          alt="3" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							The <code
                        class="literal">--name</code> option defines a (unique) name for the virtual machine.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.ram"><img
                          class="callout"
                          src="Common_Content/images/4.png"
                          alt="4" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							The <code
                        class="literal">--ram</code> option allows specifying the amount of RAM (in MB) to allocate for the virtual machine.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.disk"><img
                          class="callout"
                          src="Common_Content/images/5.png"
                          alt="5" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							The <code
                        class="literal">--disk</code> specifies the location of the image file that is to represent our virtual machine's hard disk; that file is created, unless present, with a size (in GB) specified by the <code
                        class="literal">size</code> parameter. The <code
                        class="literal">format</code> parameter allows choosing among several ways of storing the image file. The default format (<code
                        class="literal">raw</code>) is a single file exactly matching the disk's size and contents. We picked a more advanced format here, that is specific to QEMU and allows starting with a small file that only grows when the virtual machine starts actually using space.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.cdrom"><img
                          class="callout"
                          src="Common_Content/images/6.png"
                          alt="6" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							The <code
                        class="literal">--cdrom</code> option is used to indicate where to find the optical disk to use for installation. The path can be either a local path for an ISO file, an URL where the file can be obtained, or the device file of a physical CD-ROM drive (i.e. <code
                        class="literal">/dev/cdrom</code>).
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.network"><img
                          class="callout"
                          src="Common_Content/images/7.png"
                          alt="7" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							The <code
                        class="literal">--network</code> specifies how the virtual network card integrates in the host's network configuration. The default behavior (which we explicitly forced in our example) is to integrate it into any pre-existing network bridge. If no such bridge exists, the virtual machine will only reach the physical network through NAT, so it gets an address in a private subnet range (192.168.122.0/24).
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.vnc"><img
                          class="callout"
                          src="Common_Content/images/8.png"
                          alt="8" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--vnc</code> states that the graphical console should be made available using VNC. The default behavior for the associated VNC server is to only listen on the local interface; if the VNC client is to be run on a different host, establishing the connection will require setting up an SSH tunnel (see <a
                        class="xref"
                        href="sect.remote-login.html#sect.ssh-port-forwarding">Seksjon 9.2.1.3, «Å lage krypterte tunneler med portvideresending (Port Forwarding)»</a>). Alternatively, the <code
                        class="literal">--vnclisten=0.0.0.0</code> can be used so that the VNC server is accessible from all interfaces; note that if you do that, you really should design your firewall accordingly.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.os"><img
                          class="callout"
                          src="Common_Content/images/9.png"
                          alt="9" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							The <code
                        class="literal">--os-type</code> and <code
                        class="literal">--os-variant</code> options allow optimizing a few parameters of the virtual machine, based on some of the known features of the operating system mentioned there.
						</div></td></tr></table></div><div
              class="para">
					At this point, the virtual machine is running, and we need to connect to the graphical console to proceed with the installation process. If the previous operation was run from a graphical desktop environment, this connection should be automatically started. If not, or if we operate remotely, <code
                class="command">virt-viewer</code> can be run from any graphical environment to open the graphical console (note that the root password of the remote host is asked twice because the operation requires 2 SSH connections):
				</div><pre
              class="screen"><code
                class="computeroutput">$ </code><strong
                class="userinput"><code>virt-viewer --connect qemu+ssh://root@<em
                    class="replaceable">server</em>/system testkvm
</code></strong><code
                class="computeroutput">root@server's password: 
root@server's password: </code></pre><div
              class="para">
					When the installation process ends, the virtual machine is restarted, now ready for use.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.9"></a>12.2.3.4. Managing Machines with <code
                      class="command">virsh</code></h4></div></div></div><a
              id="id-1.15.5.14.9.2"
              class="indexterm"></a><div
              class="para">
					Now that the installation is done, let us see how to handle the available virtual machines. The first thing to try is to ask <code
                class="command">libvirtd</code> for the list of the virtual machines it manages:
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system list --all
 Id Name                 State
----------------------------------
  - testkvm              shut off
</code></strong></pre><div
              class="para">
					Let's start our test virtual machine:
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system start testkvm
</code></strong><code
                class="computeroutput">Domain testkvm started</code></pre><div
              class="para">
					We can now get the connection instructions for the graphical console (the returned VNC display can be given as parameter to <code
                class="command">vncviewer</code>):
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system vncdisplay testkvm
</code></strong><code
                class="computeroutput">:0</code></pre><div
              class="para">
					Other available <code
                class="command">virsh</code> subcommands include:
				</div><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">reboot</code> to restart a virtual machine;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">shutdown</code> to trigger a clean shutdown;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">destroy</code>, to stop it brutally;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">suspend</code> to pause it;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">resume</code> to unpause it;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">autostart</code> to enable (or disable, with the <code
                      class="literal">--disable</code> option) starting the virtual machine automatically when the host starts;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">undefine</code> to remove all traces of the virtual machine from <code
                      class="command">libvirtd</code>.
						</div></li></ul></div><div
              class="para">
					All these subcommands take a virtual machine identifier as a parameter.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      xmlns=""
                      id="id-1.15.5.14.10"></a>12.2.3.5. Installing an RPM based system in Debian with yum</h4></div></div></div><div
              class="para">
					If the virtual machine is meant to run a Debian (or one of its derivatives), the system can be initialized with <code
                class="command">debootstrap</code>, as described above. But if the virtual machine is to be installed with an RPM-based system (such as Fedora, CentOS or Scientific Linux), the setup will need to be done using the <code
                class="command">yum</code> utility (available in the package of the same name).
				</div><div
              class="para">
					The procedure requires using <code
                class="command">rpm</code> to extract an initial set of files, including notably <code
                class="command">yum</code> configuration files, and then calling <code
                class="command">yum</code> to extract the remaining set of packages. But since we call <code
                class="command">yum</code> from outside the chroot, we need to make some temporary changes. In the sample below, the target chroot is <code
                class="filename">/srv/centos</code>.
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>rootdir="/srv/centos"
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>mkdir -p "$rootdir" /etc/rpm
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>echo "%_dbpath /var/lib/rpm" &gt; /etc/rpm/macros.dbpath
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>rpm --nodeps --root "$rootdir" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</code></strong><code
                class="computeroutput">rpm: RPM should not be used directly install RPM packages, use Alien instead!
rpm: However assuming you know what you are doing...
warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY
# </code><strong
                class="userinput"><code>sed -i -e "s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g" $rootdir/etc/yum.repos.d/*.repo
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>yum --assumeyes --installroot $rootdir groupinstall core
</code></strong><code
                class="computeroutput">[...]
# </code><strong
                class="userinput"><code>sed -i -e "s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g" $rootdir/etc/yum.repos.d/*.repo
</code></strong></pre></div></div></div><ul
        class="docnav"><li
          class="previous"><a
            accesskey="p"
            href="advanced-administration.html"><strong>Forrige</strong>Kapittel 12. Avansert administrasjon</a></li><li
          class="up"><a
            accesskey="u"
            href="#"><strong>Opp</strong></a></li><li
          class="home"><a
            accesskey="h"
            href="index.html"><strong>Hjem</strong></a></li><li
          class="next"><a
            accesskey="n"
            href="sect.automated-installation.html"><strong>Neste</strong>12.3. Automated Installation</a></li></ul></body></html>
