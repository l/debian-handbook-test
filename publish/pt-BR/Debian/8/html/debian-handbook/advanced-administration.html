<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html
    xmlns="http://www.w3.org/1999/xhtml"><head><meta
        http-equiv="Content-Type"
        content="text/html; charset=UTF-8" /><title
        xmlns:d="http://docbook.org/ns/docbook">Capítulo 12. Administração Avançada</title><link
        rel="stylesheet"
        type="text/css"
        href="Common_Content/css/default.css" /><link
        rel="stylesheet"
        media="print"
        href="Common_Content/css/print.css"
        type="text/css" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="generator"
        content="publican v4.3.2" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="package"
        content="Debian-debian-handbook-8-pt-BR-1.0-1" /><meta
        name="keywords"
        content="RAID, LVM, FAI, Pré-configuração, Monitoramento, Virtualização, Xen, LXC" /><link
        rel="home"
        href="index.html"
        title="O Manual do Administrador Debian" /><link
        rel="up"
        href="index.html"
        title="O Manual do Administrador Debian" /><link
        rel="prev"
        href="sect.rtc-services.html"
        title="11.8. Serviços de Comunicação em Tempo Real" /><link
        rel="next"
        href="sect.virtualization.html"
        title="12.2. Virtualização" /><link
        xmlns=""
        rel="canonical"
        href="https://debian-handbook.info/browse/pt-BR/stable/advanced-administration.html" /></head><body><div
        id="banner"><a
          href="http://debian-handbook.info/get/"><span
            class="text">Download the ebook</span></a></div><p
        id="title"><a
          class="left"
          href="http://www.debian.org"><img
            alt="Product Site"
            src="Common_Content/images//image_left.png" /></a><a
          class="right"
          href="index.html"><img
            alt="Documentation Site"
            src="Common_Content/images//image_right.png" /></a></p><ul
        class="docnav top"><li
          class="previous"><a
            accesskey="p"
            href="sect.rtc-services.html"><strong>Anterior</strong></a></li><li
          class="home">O Manual do Administrador Debian</li><li
          class="next"><a
            accesskey="n"
            href="sect.virtualization.html"><strong>Próxima</strong></a></li></ul><div
        xml:lang="pt-BR"
        class="chapter"
        lang="pt-BR"><div
          class="titlepage"><div><div><h1
                class="title"><a
                  xmlns=""
                  id="advanced-administration"></a>Capítulo 12. Administração Avançada</h1></div></div></div><div
          class="toc"><dl
            class="toc"><dt><span
                class="section"><a
                  href="advanced-administration.html#sect.raid-and-lvm">12.1. RAID e LVM</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.raid-soft">12.1.1. RAID Por Software</a></span></dt><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.lvm">12.1.2. LVM</a></span></dt><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.raid-or-lvm">12.1.3. RAID ou LVM?</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.virtualization.html">12.2. Virtualização</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.virtualization.html#sect.xen">12.2.1. Xen</a></span></dt><dt><span
                    class="section"><a
                      href="sect.virtualization.html#sect.lxc">12.2.2. LXC</a></span></dt><dt><span
                    class="section"><a
                      href="sect.virtualization.html#id-1.15.5.14">12.2.3. Virtualização com KVM</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.automated-installation.html">12.3. Instalação Automatizada</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.fai">12.3.1. Instalador Completamente Automático (FAI)</a></span></dt><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.d-i-preseeding">12.3.2. Preseeding Debian-Installer</a></span></dt><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.simple-cdd">12.3.3. Simple-CDD: A Solução Tudo-Em-Um</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.monitoring.html">12.4. Monitoramento</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.monitoring.html#sect.munin">12.4.1. Configurando o Munin</a></span></dt><dt><span
                    class="section"><a
                      href="sect.monitoring.html#sect.nagios">12.4.2. Configurando o Nagios</a></span></dt></dl></dd></dl></div><div
          class="highlights"><div
            class="para">
		Este capítulo retoma alguns aspectos já descritos, com uma perspectiva diferente: em vez de instalar em um único computador, vamos estudar a implantação de sistemas em massa; em vez de criar volumes RAID ou LVM no momento da instalação, vamos aprender a fazer tudo na mão para que mais tarde possamos rever nossas escolhas iniciais. Finalmente, vamos discutir as ferramentas de monitoramento e técnicas de virtualização. Como consequência, este capítulo é mais particularmente alvo de administradores profissionais e centra-se um pouco menos nos indivíduos responsáveis pela sua rede doméstica.
	</div></div><div
          class="section"><div
            class="titlepage"><div><div><h2
                  class="title"><a
                    xmlns=""
                    id="sect.raid-and-lvm"></a>12.1. RAID e LVM</h2></div></div></div><div
            class="para">
			<a
              class="xref"
              href="installation.html">Capítulo 4, <em>Instalação</em></a> apresentou estas tecnologias do ponto de vista do instalador e como ele as integrou para fazer a sua implantação fácil desde o início. Após a instalação inicial, um administrador deve ser capaz de lidar com as necessidades de espaço de armazenamento em evolução, sem ter que recorrer a uma reinstalação cara. Devem, portanto, compreender as ferramentas necessárias para manipular volumes RAID e LVM.
		</div><div
            class="para">
			RAID e LVM são duas técnicas para abstrair os volumes montados a partir de seus equivalentes físicos (reais unidades de disco rígido ou partições do mesmo), o primeiro protege os dados de falhas no hardware através da introdução de redundância, o último torna o gerenciamento de volumes mais flexível e independente do tamanho real nos discos. Em ambos os casos, o sistema acaba com novos volumes (partições, blocos), que podem ser usados para criar sistemas de arquivos ou espaço de troca, sem necessariamente ter eles mapeados em um disco físico. LVM e RAID vêm de origens bem diferentes, mas sua funcionalidade pode sobrepor-se um pouco, é por isso que eles são muitas vezes mencionados juntos.
		</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>PERSPECTIVE</em></span> Btrfs combina LVM e RAID</strong></p></div></div></div><div
              class="para">
			Enquanto LVM e RAID são dois subsistemas do kernel distintos que estão entre os dispositivos de bloco do disco e seus sistemas de arquivos, <span
                class="emphasis"><em>Btrfs</em></span> é um novo sistema de arquivos, desenvolvido inicialmente pela Oracle, que pretende combinar os conjuntos de recursos de LVM e RAID e muito mais. É sobretudo funcional, embora ainda seja definido como "experimental", pois seu desenvolvimento é incompleto (alguns recursos ainda não estão implementados). <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://btrfs.wiki.kernel.org/">http://btrfs.wiki.kernel.org/</a></div>
		</div><div
              class="para">
			Entre as características marcantes estão a capacidade de tirar um instantâneo de uma árvore de diretórios em qualquer ponto no tempo. Este instantâneo inicialmente não utiliza nenhum espaço em disco, os dados só serão duplicados quando um dos arquivos copiados for modificado. O sistema de arquivos também lida com a compressão transparente de arquivos e somas de verificação (checksums) garantem a integridade de todos os dados armazenados.
		</div></div><div
            class="para">
			Em ambos os casos RAID e LVM, o kernel fornece um arquivo de dispositivo de bloco semelhantes aos que correspondem a uma unidade de disco rígido ou partição. Quando um pedido ou uma outra parte do núcleo, requer o acesso a um bloco de um tal dispositivo, as rotas de subsistemas apropriadas do bloco são usadas para a camada física relevante. Dependendo da configuração, este bloco pode ser armazenado em um ou vários discos físicos e sua localização física pode não ser directamente relacionada com a localização do bloco no dispositivo lógico.
		</div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      xmlns=""
                      id="sect.raid-soft"></a>12.1.1. RAID Por Software</h3></div></div></div><a
              id="id-1.15.4.6.2"
              class="indexterm"></a><div
              class="para">
				RAID significa <span
                class="emphasis"><em> Redundant Array of Independent Disks - conjunto redundante de discos independentes</em></span>. O objetivo deste sistema é evitar perda de dados em caso de falha do disco rígido. O princípio geral é bastante simples: os dados são armazenados em vários discos físicos em vez de apenas um, com um nível configurável de redundância. Dependendo desta quantidade de redundância, e mesmo no caso de uma falha de disco inesperado, dados podem ser reconstruídos sem perdas dos restantes discos.
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>CULTURA</em></span> <span
                          class="foreignphrase"><em
                            class="foreignphrase">Independent</em></span> or <span
                          class="foreignphrase"><em
                            class="foreignphrase">inexpensive</em></span>?</strong></p></div></div></div><div
                class="para">
				O I da sigla RAID inicialmente significava <span
                  class="emphasis"><em>inexpensive</em></span> (barato), por que o RAID permitia um aumento drástico na segurança de dados sem precisar investir em discos de alta qualidade. Provavelmente, devido a questões de melhoria da imagem, o I é agora normalmente chamado de <span
                  class="emphasis"><em>independent</em></span>, para não ficar com este aspecto de economia.
			</div></div><div
              class="para">
				O RAID pode ser implementado tanto por hardware dedicado (módulos RAID integrados em placas controladoras SCSI ou SATA) ou por abstração de software (o núcleo). Seja por hardware ou software, um sistema RAID com redundância suficiente pode, de forma transparente, continuar operacional quando um disco falha; as camadas superiores da pilha (aplicações) podem até manter o acesso aos dados apesar da falha. Claro que, esse “modo degradado” pode ter impacto na performance, e a redundância é reduzida, então uma falha profunda do disco pode levar a perda de dados. Na prática, entretanto, um irá se esforçar para apenas ficar nesse modo degradado o tempo que for necessário para que se possa substituir o disco falho. Uma vez que o novo disco seja colocado, o sistema RAID pode reconstruir os dados necessários e então retornar ao modo seguro. As aplicações não notarão nada, fora a potencial redução da velocidade de acesso, enquanto a array estiver no modo degradado ou durante a fase de reconstrução.
			</div><div
              class="para">
				Quando o RAID é implementado por hardware, sua configuração geralmente acontece dentro da ferramenta de configuração da BIOS, e o núcleo irá considerar uma array RAID como um único disco, que irá funcionar como um disco físico padrão, embora o nome do dispositivo possa ser diferente (dependendo do driver).
			</div><div
              class="para">
				Nós apenas focamos em RAID de software neste livro.
			</div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.raid-levels"></a>12.1.1.1. Diferentes Níveis de RAID</h4></div></div></div><div
                class="para">
					RAID não é na verdade um único sistema, mas vários sistemas identificados por seus níveis; os níveis diferem por sua disposição e quantidade de redundância que eles fornecem. Quanto mais redundante, mais à prova de falhas, uma vez que o sistema será capaz de continuar a trabalhar quando mais discos falharem. A contrapartida é que reduz o espaço utilizável para um dado conjunto de discos; visto de outra forma, mais discos serão necessários para armazenar a mesma quantidade de dados.
				</div><div
                class="variablelist"><dl
                  class="variablelist"><dt><span
                      class="term">RAID Linear</span></dt><dd><div
                      class="para">
								Mesmo o que o subsistema de RAID do núcleo permita a criação de um “RAID linear”, isso não é um RAID propriamente, já que essa configuração não envolve qualquer redundância. O núcleo apenas agrega vários discos fim-a-fim e provê o volume agregado resultante como um disco virtual (um dispositivo de bloco). Essa é sua única função. Essa configuração raramente é usada por ela própria (veja mais adiante sobre as exceções), especialmente porque a falta de redundância significa que a falha de um disco faz com que todo o agregado, e portanto todos os dados, fiquem indisponíveis.
							</div></dd><dt><span
                      class="term">RAID-0</span></dt><dd><div
                      class="para">
								Esse nível também não provê nenhuma redundância, mas os discos não são simplesmente ligados pelo final um após o outro: eles são divididos em <span
                        class="emphasis"><em>listras (stripes)</em></span>, e os blocos no dispositivo virtual são armazenados em listras (stripes) em discos físicos alternados. Em uma configuração de RAID-0 de dois discos, por exemplo, em blocos de número par do dispositivo virtual serão armazenados no primeiro disco físico, enquanto os blocos ímpares ficarão no segundo disco físico.
							</div><div
                      class="para">
								Esse sistema não tem por objetivo um aumento de credibilidade, já que (como em um caso linear) a disponibilidade de todos os dados é comprometida assim que um disco falhar, mas um aumento de desempenho: durante um acesso sequencial a grandes quantidades de dados contíguos, o núcleo será capaz de ler a partir dos dois discos (ou escrever neles) em paralelo, o que incrementa a taxa de transferência de dados. Contudo, o uso do RAID-0 está murchando, seu nicho está sendo preenchido pelo LVM (veja mais adiante).
							</div></dd><dt><span
                      class="term">RAID-1</span></dt><dd><div
                      class="para">
								Esse nível, também conhecido como “espelhamento RAID”, é tanto o mais simples quanto a mais amplamente usada configuração. Em sua forma padrão, ele usa dois discos físicos de mesmo tamanho e fornece um volume lógico de mesmo tamanho mais uma vez. Os dados são armazenados identicamente nos dois discos, por isso o apelido “espelho”. Quando um disco falha, os dados ainda estão disponíveis no outro. Para dados realmente críticos, RAID-1 pode, é claro, ser configurado para mais de dois discos, com impacto direto na relação de custo de hardware versus espaço de carga disponível.
							</div><div
                      class="sidebar"><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>NOTA</em></span> Discos e tamanhos de cluster</strong></p></div></div></div><div
                        class="para">
								Se dois discos de tamanhos diferentes são criados em um espelho, o maior não será totalmente usado, pois ele irá conter os mesmos dados como o menor e nada mais. O espaço útil disponível fornecido por um volume RAID-1, portanto, corresponde ao tamanho do disco menor na matriz. Isso ainda vale para volumes RAID com um maior nível RAID, apesar de redundância é armazenada de forma diferente.
							</div><div
                        class="para">
								Por isso é importante, ao configurar arrays RAID (exceto RAID-0 "RAID linear"), só montar discos de tamanhos idênticos, ou muito perto, para evitar o desperdício de recursos.
							</div></div><div
                      class="sidebar"><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>NOTA</em></span> Discos de reposição</strong></p></div></div></div><div
                        class="para">
								Níveis RAID que incluem redundância permitem atribuir mais discos do que o necessário para uma array. Os discos extras são usados como reservas quando um dos principais discos falha. Por exemplo, em um espelho de dois discos e mais um reserva, se um dos dois primeiros discos falhar, o núcleo irá automaticamente (e imediatamente) reconstruir o espelho usando o disco reserva, para que a redundância continue garantida após o momento de reconstrução. Isso pode ser usado como outro tipo de salva guarda para dados críticos.
							</div><div
                        class="para">
								Alguém poderia ser perdoado por questionar como isso pode ser melhor do que simplesmente espelhar três discos como início. A vantagem da configuração de um “disco reserva” é que o disco reserva pode ser compartilhado entre vários volumes RAID. Por exemplo, é possível ter três volumes espelhados, com garantia de redundância mesmo no caso de falha de um disco, com apenas sete discos (três pares, mais um reserva compartilhado), ao invés dos nove discos que seriam necessários para três trigêmeos.
							</div></div><div
                      class="para">
								Esse nível de RAID, embora caro (já que apenas metade do espaço físico de armazenagem, na melhor das hipóteses, é útil), é amplamente usado na prática. Ele é simples de entender, e ele permite cópias de segurança (backups) bem simples: como os dois discos tem conteúdos idênticos, um deles pode ser temporariamente extraído, sem impacto no sistema em funcionamento. O desempenho de leitura geralmente é incrementado , já que o núcleo pode ler metade dos dados em cada disco, em paralelo, enquanto o desempenho de escrita não é muito severamente degradado. No caso de uma array RAID-1 de N discos, os dados continuam disponíveis mesmo com a falha do disco N-1.
							</div></dd><dt><span
                      class="term">RAID-4</span></dt><dd><div
                      class="para">
								Esse nível de RAID, não amplamente usado, usa N discos para armazenar dados úteis, e um disco extra para armazenar informação redundante. Se esse disco falhar, o sistema pode reconstruir seu conteúdo a partir do outro N. Se um dos N discos de dados falhar, O N-1 remanescente combinado com o disco “paridade” contém informação suficiente para reconstruir os dados requeridos.
							</div><div
                      class="para">
								RAID-4 não é muito caro já que ele apenas envolve um incremento de um-em-N nos custos e não se tem impacto perceptível no desempenho de leitura, mas a escrita é mais devagar. Além disso, como uma escrita em qualquer um dos N discos também envolve a escrita no disco de paridade, esse último tem muito mais escritas que o anterior, e sua vida útil pode ser dramaticamente diminuída como consequência. Os dados na array RAID-4 só está segura até um disco falhar (dos N+1).
							</div></dd><dt><span
                      class="term">RAID-5</span></dt><dd><div
                      class="para">
								RAID-5 resolve o problema de assimetria do RAID-4: a paridade de blocos é distribuída por todos os N+1 discos, sendo que nenhum tem um papel particular.
							</div><div
                      class="para">
								A performance de leitura e escrita são idênticas ao RAID-4. Aqui novamente, o sistema continua funcional mesmo com a falha de um disco (do N+1), mas não mais.
							</div></dd><dt><span
                      class="term">RAID-6</span></dt><dd><div
                      class="para">
								RAID-6 pode ser considerado uma extensão do RAID-5, onde cada série de N blocos envolvem dois blocos redundantes, e cada série de N+2 blocos e distribuída sobre N+2 discos.
							</div><div
                      class="para">
								Esse nível de RAID é levemente mais caro que os dois anteriores, mas ele traz alguma segurança extra já que até dois drives (dos N+2) podem falhar sem comprometer a disponibilidade dos dados. A contraparte é que as operações de escrita agora envolvem escrever um bloco de dados e dois blocos de redundância, o que os torna ainda mais lento.
							</div></dd><dt><span
                      class="term">RAID-1+0</span></dt><dd><div
                      class="para">
								Isso não é, estritamente falando, um nível RAID, mas um empilhamento de dois agrupamentos RAID. A partir de 2×N discos, primeiro se configura eles por pares em volumes N RAID-1; esses volumes N são então agregados em um só, seja por “linear RAID” ou (cada vez mais) por LVM. Esse último caso vai além do puro RAID, mas não existe problema quanto a isso.
							</div><div
                      class="para">
								RAID-1+0 pode sobreviver com múltiplas falhas nos discos: até N na 2xN série descrita acima, provendo ao menos um disco funcional em cada par de RAID-1.
							</div><div
                      class="sidebar"><a
                        xmlns=""
                        id="sidebar.raid-10"></a><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>Aprofundando</em></span> RAID-10</strong></p></div></div></div><div
                        class="para">
								RAID-10 é ,geralmente , considerado um sinônimo de RAID-1+0, mas uma especifidade Linux faz com que ele seja realmente uma generalização. Essa configuração permite um sistema aonde cada bloco seja armazenado em dois diferentes discos, mesmo com um número impar de discos, sendo as cópias espalhadas ao longo de um modelo configurável.
							</div><div
                        class="para">
								A performance variará dependendo da escolha do modelo de repartição e do nível de redundância, e da carga de trabalho do volume lógico.
							</div></div></dd></dl></div><div
                class="para">
					Obviamente, o nível de RAID será escolhido de acordo com as restrições e requerimentos de cada aplicação. Note que um computador sozinho pode ter diversos tipos de RAIDs distintos com diversas configurações.
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.raid-setup"></a>12.1.1.2. Configurando um RAID</h4></div></div></div><a
                id="id-1.15.4.6.9.2"
                class="indexterm"></a><div
                class="para">
					Configurar volumes RAID requer o pacote <span
                  class="pkg pkg">mdadm</span>; ele provê o comando <code
                  class="command">mdadm</code>, que permite a criação e maipulação de arrays RAID, assim como scripts e ferramentas para integração com o resto do sistema, incluindo o sistema de monitoração.
				</div><div
                class="para">
					Nossos exemplo será um servidor com um número de discos, sendo que alguns já estão em uso, e o resto está disponível para a configuração do RAID. Nós inicialmente temos os seguintes discos e partições:
				</div><div
                xmlns:d="http://docbook.org/ns/docbook"
                class="itemizedlist"><ul><li
                    class="listitem"><div
                      class="para">
							o disco <code
                        class="filename">sdb</code>, 4 GB, está completamente disponível;
						</div></li><li
                    class="listitem"><div
                      class="para">
							o disco <code
                        class="filename">sdc</code>, 4 GB, também está completamente disponível;
						</div></li><li
                    class="listitem"><div
                      class="para">
							no disco <code
                        class="filename">sdd</code>, somente a partição <code
                        class="filename">sdd2</code> (cerca de 4 GB) está disponível;
						</div></li><li
                    class="listitem"><div
                      class="para">
							finalmente, um disco <code
                        class="filename">sde</code>, ainda com 4 GB, disponível.
						</div></li></ul></div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>NOTA</em></span> Identificando os volumes RAID existentes</strong></p></div></div></div><div
                  class="para">
					O arquivo <code
                    class="filename">/proc/mdstat</code> lista os volumes existentes e seus estados. Quando criando um novo volume RAID, devemos tomar cuidado para não nomeá-lo da mesma maneira que um volume existente.
				</div></div><div
                class="para">
					Iremos usar estes elementos físicos para criar dois volumes, um RAID-0 e um espelho (RAID-1). Comecemos com o volume RAID-0:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</code></strong>
<code
                  class="computeroutput">mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
# </code><strong
                  class="userinput"><code>mdadm --query /dev/md0</code></strong>
<code
                  class="computeroutput">/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md0</code></strong>
<code
                  class="computeroutput">/dev/md0:
        Version : 1.2
  Creation Time : Wed May  6 09:24:34 2015
     Raid Level : raid0
     Array Size : 8387584 (8.00 GiB 8.59 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:24:34 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

     Chunk Size : 512K

           Name : mirwiz:0  (local to host mirwiz)
           UUID : bb085b35:28e821bd:20d697c9:650152bb
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       1       8       32        1      active sync   /dev/sdc
# </code><strong
                  class="userinput"><code>mkfs.ext4 /dev/md0</code></strong>
<code
                  class="computeroutput">mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 2095104 4k blocks and 524288 inodes
Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </code><strong
                  class="userinput"><code>mkdir /srv/raid-0</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mount /dev/md0 /srv/raid-0</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/raid-0</code></strong>
<code
                  class="computeroutput">Filesystem      Size  Used Avail Use% Mounted on
/dev/md0        7.9G   18M  7.4G   1% /srv/raid-0
</code></pre><div
                class="para">
					O comando <code
                  class="command">mdadm --create</code> requer vários parâmetros: o nome do volume a ser criado (<code
                  class="filename">/dev/md*</code>, com MD significando <span
                  class="foreignphrase"><em
                    class="foreignphrase">Multiple Device</em></span>), o nível RAID, o número de discos (que é obrigatório, apesar de ser significante apenas com RAID-1 e acima), e os drives físicos a usar. Uma vez que o dispositivo seja criado, nós podemos usá-lo como usamos uma partição normal, criando um sistema de arquivos nela, montando esse sistema de arquivos, e assim por diante. Note que nossa criação de um volume RAID-0 em <code
                  class="filename">md0</code> não passa de coincidência, e a numeração da array não precisa ser correlacionada com a quantidade escolhida de redundância. Também é possível criar arrays RAID nomeadas, dando ao <code
                  class="command">mdadm</code> parâmetros como <code
                  class="filename">/dev/md/linear</code> ao invés de <code
                  class="filename">/dev/md0</code>.
				</div><div
                class="para">
					A criação do RAID-1 segue estilo similar, as diferenças somente serão notadas após a criação:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%
Continue creating array? </code><strong
                  class="userinput"><code>y</code></strong>
<code
                  class="computeroutput">mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md1 started.
# </code><strong
                  class="userinput"><code>mdadm --query /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
        Version : 1.2
  Creation Time : Wed May  6 09:30:19 2015
     Raid Level : raid1
     Array Size : 4192192 (4.00 GiB 4.29 GB)
  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:30:40 2015
          State : clean, resyncing (PENDING) 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       1       8       64        1      active sync   /dev/sde
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
          State : clean
[...]
</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>DICA</em></span> RAID, discos e partições</strong></p></div></div></div><div
                  class="para">
					Como ilustrado pelo nosso exemplo, dispositivos RAID podem ser construídos à partir de partições de disco, e não necessitam discos inteiros.
				</div></div><div
                class="para">
					Algumas observações em ordem. Primeiro, <code
                  class="command">mdadm</code> nota que os elementos físicos possuem tamanhos diferentes; já que isso implica que algum espaço será perdido no maior elemento, uma confirmação é necessária.
				</div><div
                class="para">
					Ainda mais importante, note o estado do espelhamento.O estado normal de um espelho RAID é que os dois discos tenham exatamente o mesmo conteúdo. Contudo, nada garante que esse é o caso quando o volume é criado pela primeira vez. O subsistema RAID irá, por conseguinte, prover essa garantia por si mesmo, e acontecerá uma fase de sincronização assim que o dispositivo RAID for criado. Após algum tempo (a quantidade exata irá depender do real tamanho dos discos…), a array RAID alternará para o estado “ativo” ou "limpo". Note que durante essa fase de reconstrução, o espelho está em modo degradado, e a redundância não é garantida. Um disco falhando durante essa janela de risco poderia levar a perda de todos os dados. Grandes quantidades de dados críticos, contudo, raramente são armazenados em uma array RAID recentemente criada, antes de sua sincronização inicial. Note que mesmo em modo degradado, o <code
                  class="filename">/dev/md1</code> é usável, e um sistema de arquivos pode ser criado nele, assim como alguns dados podem ser copiados para ele.
				</div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>DICA</em></span> Começando um espelho em modo reduzido</strong></p></div></div></div><div
                  class="para">
					Às vezes, dois discos não estão imediatamente disponíveis quando se quer iniciar um espelho RAID-1, por exemplo, porque se pretende incluir um dos discos já usado para armazenar os dados que se quer passar para a array. Em tais circunstâncias, é possível criar deliberadamente uma degradada array RAID-1, passando <code
                    class="filename">missing</code> em vez de um arquivo de dispositivo como um dos argumentos para <code
                    class="command">mdadm</code>. Uma vez que os dados tenham sido copiados para o "espelho", o disco antigo pode ser adicionado à array. A sincronização irá então acontecer, dando-nos a redundância que foi desejada, em primeiro lugar.
				</div></div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>DICA</em></span> Configurando um espelho sem sincronização</strong></p></div></div></div><div
                  class="para">
					Volumes RAID-1 são geralmente criados para serem usados como disco novo, geralmente considerados vazios. O conteúdo inicial real do disco portanto não é muito relevante, já que alguém apenas precisa saber que os dados escritos após a criação do volume, em particular o sistema de arquivos, podem ser acessados mais tarde.
				</div><div
                  class="para">
					Pode-se portanto querer saber sobre o ponto de sincronização de ambos os discos no momento da criação. Por que se importar se os conteúdos são idênticos em zonas do volume que nós sabemos que apenas serão lidas após nós termos escrito nelas?
				</div><div
                  class="para">
					Felizmente, essa fase de sincronização pode ser evitada passando a opção <code
                    class="literal">--assume-clean</code> para <code
                    class="command">mdadm</code>. Contudo, essa opção pode levar a surpresas no caso de os dados iniciais serem lidos (por exemplo se um sistema de arquivos já esteja presente nos discos físicos), que é o por que dela não ser habilitada por padrão.
				</div></div><div
                class="para">
					Agora vamos ver o que acontece quando um dos elementos da array RAID-1 falha. O <code
                  class="command">mdadm</code>, em particular sua opção <code
                  class="literal">--fail</code>, permite simular uma falha de disco desse tipo:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --fail /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: set /dev/sde faulty in /dev/md1
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Update Time : Wed May  6 09:39:39 2015
          State : clean, degraded 
 Active Devices : 1
Working Devices : 1
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 19

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       0        0        2      removed

       1       8       64        -      faulty   /dev/sde</code></pre><div
                class="para">
					O conteúdo do volume ainda está acessível (e, se montado, as aplicações não notarão nada), mas a segurança dos dados não é mais garantida: se, por sua vez, o disco <code
                  class="filename">sdd</code> falhar, os dados serão perdidos. Nós queremos evitar esse risco, então nós vamos substituir o disco falho por um novo, <code
                  class="filename">sdf</code>:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --add /dev/sdf</code></strong>
<code
                  class="computeroutput">mdadm: added /dev/sdf
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
   Raid Devices : 2
  Total Devices : 3
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:48:49 2015
          State : clean, degraded, recovering 
 Active Devices : 1
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 1

 Rebuild Status : 28% complete

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 26

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      spare rebuilding   /dev/sdf

       1       8       64        -      faulty   /dev/sde
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Update Time : Wed May  6 09:49:08 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 41

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf

       1       8       64        -      faulty   /dev/sde</code></pre><div
                class="para">
					Aqui, mais uma vez, o núcleo automaticamente dispara uma fase de reconstrução, durante a qual o volume, embora ainda acessível, está em um modo degradado. Uma vez que a reconstrução esteja terminada, a array RAID está de volta ao estado normal. Pode-se então dizer ao sistema que o disco <code
                  class="filename">sde</code> está para ser removido da array, para que se possa terminar com um espelhamento RAID clássico nos dois discos:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --remove /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: hot removed /dev/sde from /dev/md1
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf</code></pre><div
                class="para">
					A partir de então, a unidade pode ser fisicamente removida quando o servidor está para ser desligado, ou até mesmo removida com o sistema ligado (hot-removed) quando a configuração de hardware permite tal operação (hot-swap). Tais configurações incluem alguns controladores SCSI, a maioria dos discos SATA e unidades externas que operam com USB ou Firewire.
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.backup-raid-config"></a>12.1.1.3. Fazendo Backup da Configuração</h4></div></div></div><div
                class="para">
					A maioria dos meta-dados referentes a volumes RAID são salvos diretamente nos discos que compoem essas arrays, para que o núcleo possa detectar as arrays e seus componentes e montá-los automaticamente quando o sistema for iniciado. Contudo, é encorajado o uso de cópia de segurança dessa configuração, porque essa detecção não é à prova de falhas, e só se tem a expectativa de falha dela precisamente em circunstâncias sensíveis. Em nosso exemplo, se a falha do disco <code
                  class="filename">sde</code> tivesse sido real (ao invés de simulada) e o sistema tivesse sido reiniciado sem a remoção desse disco <code
                  class="filename">sde</code>, esse disco poderia começar a trabalhar novamente por ter sido verificado durante a reinicialização. O núcleo iria ter então três elementos físicos, cada um clamando por conter metade do mesmo volume RAID. Outra fonte de confusão pode vir quando volumes RAID de dois servidores são consolidados em apenas um servidor apenas. Se essas arrays estavam rodando normalmente antes dos discos serem removidos, o núcleo seria capaz de detectar e remontar os pares de maneira apropriada; mas se os discos movidos tiverem sido agregados em um <code
                  class="filename">md1</code> no servidor antigo, e o novo servidor já tiver um <code
                  class="filename">md1</code>, um dos espelhos seria renomeado.
				</div><div
                class="para">
					Fazer uma cópia de segurança da configuração é portanto importante, mesmo que apenas para referência. A maneira padrão de fazer isso é editando o arquivo <code
                  class="filename">/etc/mdadm/mdadm.conf</code>, um exemplo do que é listado aqui:
				</div><div
                class="example"><a
                  xmlns=""
                  id="example.mdadm-conf"></a><p
                  class="title"><strong>Exemplo 12.1. <code
                      class="command">mdadm</code> arquivo de configuração</strong></p><div
                  class="example-contents"><pre
                    class="programlisting"># mdadm.conf
#
# Please refer to mdadm.conf(5) for information about this file.
#

# by default (built-in), scan all partitions (/proc/partitions) and all
# containers for MD superblocks. alternatively, specify devices to scan, using
# wildcards if desired.
DEVICE /dev/sd*

# auto-create devices with Debian standard permissions
CREATE owner=root group=disk mode=0660 auto=yes

# automatically tag new arrays as belonging to the local system
HOMEHOST &lt;system&gt;

# instruct the monitoring daemon where to send mail alerts
MAILADDR root

# definitions of existing MD arrays
ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464

# This configuration was auto-generated on Thu, 17 Jan 2013 16:21:01 +0100
# by mkconf 3.2.5-3</pre></div></div><div
                class="para">
					Um dos detalhes mais úteis é a opção <code
                  class="literal">DEVICE</code>, que lista os dispositivos aonde o sistema irá automaticamente procurar por componentes dos volumes RAID no momento da inicialização. No nosso exemplo, nós substituímos o valor padrão, <code
                  class="literal">partitions containers</code>, por uma explícita lista de arquivos de dispositivos, já que nós escolhemos usar discos inteiros e não apenas partições, para alguns volumes.
				</div><div
                class="para">
					As duas últimas linhas em nosso exemplo são aquelas que permitem ao núcleo escolher, com segurança, qual número de volume atribuir a qual array. O metadado armazenado nos próprios discos são suficientes para remontar (re-assemble) os volumes, mas não para determinar o número do volume (e o nome de dispositivo que coincide com <code
                  class="filename">/dev/md*</code>).
				</div><div
                class="para">
					Felizmente, estas linhas podem ser geradas automaticamente:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --misc --detail --brief /dev/md?</code></strong>
<code
                  class="computeroutput">ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</code></pre><div
                class="para">
					O conteúdo dessas duas últimas linhas não depende da lista de discos incluídos no volume. Logo, não é necessário regenerar essas linhas quando se for substituir um disco falho por um novo. Por outro lado, tem que se tomar o cuidado de atualizar o arquivo ao se criar ou remover uma array RAID.
				</div></div></div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      xmlns=""
                      id="sect.lvm"></a>12.1.2. LVM</h3></div></div></div><a
              id="id-1.15.4.7.2"
              class="indexterm"></a><a
              id="id-1.15.4.7.3"
              class="indexterm"></a><div
              class="para">
				LVM, o <span
                class="emphasis"><em>Logical Volume Manager</em></span>, é uma outra abordagem para abstrair volumes lógicos a partir de seus suportes físicos, que se concentra em aumentar a flexibilidade em vez de aumentar a confiabilidade. O LVM permite mudar um volume lógico de forma transparente, até aonde os aplicativos tem interesse ; por exemplo, é possível adicionar novos discos, migrar os dados para eles, e remover os discos velhos, sem desmontar o volume.
			</div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.lvm-concepts"></a>12.1.2.1. Conceitos sobre LVM</h4></div></div></div><div
                class="para">
					Esta flexibilidade é atingida graças ao nível de abstração envolvendo três conceitos.
				</div><div
                class="para">
					Primeiro, o PV (<span
                  class="emphasis"><em>Physical Volume</em></span>) é a entidade mais próxima ao hardware: ele pode ser partições em um disco, ou um disco inteiro, ou até mesmo qualquer outro dispositivo de bloco (incluindo, por exemplo, uma array RAID). Note que quando um elemento é configurado para ser um PV para o LVM, ele deveria ser acessado via LVM apenas, de outra forma o sistema irá ficar confuso.
				</div><div
                class="para">
					Vários PVs podem ser agrupados em um VG (<span
                  class="emphasis"><em>Volume Group</em></span>), que pode ser comparado com discos tanto virtual quanto extensível. VGs são abstratos, e não aparecem em um arquivo de dispositivo na hierarquia <code
                  class="filename">/dev</code>, então não a risco em usá-los diretamente.
				</div><div
                class="para">
					O terceiro tipo de objeto é o LV (<span
                  class="emphasis"><em>Logical Volume</em></span>), que é um pedaço de um VG; se nós mantermos a analogia VG-como-disco, o LV se compara a uma partição. O LV aparece como um dispositivo de bloco com uma entrada em <code
                  class="filename">/dev</code>, e ele pode ser usado como qualquer outra partição física pode ser (mais comumente, para hospedar um sistema de arquivos ou espaço swap).
				</div><div
                class="para">
					A coisa importante é que a divisão de um VG em LVs é inteiramente independente de seus componentes físicos (os PVs). Um VG com apenas um componente físico (um disco por exemplo) pode ser dividido em uma dúzia de volumes lógicos; similarmente, um VG pode usar vários discos físicos e parecer como um único e grande volume lógico. A única restrição, obviamente, é que o tamanho total alocado aos LVs não podem ser maiores que a capacidade total dos PVs no grupo de volume.
				</div><div
                class="para">
					Geralmente faz sentido, contudo, ter algum tipo de homogeneidade entre os componentes físicos de um VG, e dividir o VG em volumes lógicos que irão ter padrões de uso similares. Por exemplo, se o hardware disponível inclui discos rápidos e discos lentos, os rápidos poderiam ser agrupados em um VG e os lentos em outro; pedaços do primeiro podem então se designados para aplicações que requerem rápido acesso a dados, enquanto o segundo seria mantido para tarefas de menor demanda.
				</div><div
                class="para">
					Em todo caso, tenha em mente que um LV não está particularmente anexado a nenhum PV. É possível influenciar aonde os dados de um LV são fisicamente armazenados, mas essa possibilidade não é necessária para o uso do dia a dia. Pelo contrário: quando o conjunto de componentes físicos de um VG evolui, as localizações de armazenagem física correspondentes a um LV em particular podem ser migradas entre discos (enquanto se mantém dentro de PVs atribuídos ao VG, é claro).
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.lvm-setup"></a>12.1.2.2. Configurando um LVM</h4></div></div></div><div
                class="para">
					Vamos agora seguir, passo a passo, o processo de configurar um LVM para um caso de uso típico: nós queremos simplificar uma situação complexa de armazenagem. Uma situação dessas geralmente acontece após alguma longa e complicada história de medidas temporárias acumuladas. Para propósitos de ilustração, nós vamos considerar um servidor aonde a armazenagem precisa ter alterações com o passar do tempo, terminando em um labirinto de partições disponíveis, divididas em vários discos parcialmente usados. Em termos mais concretos, as seguintes partições estão disponíveis:
				</div><div
                xmlns:d="http://docbook.org/ns/docbook"
                class="itemizedlist"><ul><li
                    class="listitem"><div
                      class="para">
							no disco <code
                        class="filename">sdb</code>, uma partição <code
                        class="filename">sdb2</code>, 4 GB;
						</div></li><li
                    class="listitem"><div
                      class="para">
							no disco <code
                        class="filename">sdc</code>, uma partição <code
                        class="filename">sdc3</code>, 3 GB;
						</div></li><li
                    class="listitem"><div
                      class="para">
							o disco <code
                        class="filename">sdd</code>, 4 GB, está completamente disponível;
						</div></li><li
                    class="listitem"><div
                      class="para">
							no disco <code
                        class="filename">sdf</code>, uma partição <code
                        class="filename">sdf1</code>, 4 GB; e uma partição <code
                        class="filename">sdf2</code>, 5 GB.
						</div></li></ul></div><div
                class="para">
					Complementando, vamos assumir que os discos <code
                  class="filename">sdb</code> e <code
                  class="filename">sdf</code> são mais rápidos do que os outros dois.
				</div><div
                class="para">
					Nosso objetivo é configurar três volumes lógicos para três diferentes aplicações: um servidor de arquivos necessitando 5 GB de espaço de armazenagem, um banco de dados (1 GB) e algum espaço para cópias de segurança (12 GB). Os dois primeiros precisam de bom desempenho, mas cópias de segurança são menos críticas em termos de velocidade de acesso. Todos essas limitações impedem o uso de partições propriamente; usando LVM pode-se abstrair o tamanho físico dos dispositivos, então o único limite é o total de espaço disponível.
				</div><div
                class="para">
					As ferramentas necessárias estão no pacote <span
                  class="pkg pkg">lvm2</span> e suas dependência. Quando os mesmos estiverem instalados, configurarar o LVM terá três etapas, cobrindo três níveis de conceitos.
				</div><div
                class="para">
					Primeiro, nós preparamos o volumes físicos utilizando <code
                  class="command">pvcreate</code>:
				</div><a
                xmlns=""
                id="screen.pvcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvdisplay</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvcreate /dev/sdb2</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdb2" successfully created
# </code><strong
                  class="userinput"><code>pvdisplay</code></strong>
<code
                  class="computeroutput">  "/dev/sdb2" is a new physical volume of "4.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/sdb2
  VG Name               
  PV Size               4.00 GiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I

# </code><strong
                  class="userinput"><code>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdc3" successfully created
  Physical volume "/dev/sdd" successfully created
  Physical volume "/dev/sdf1" successfully created
  Physical volume "/dev/sdf2" successfully created
# </code><strong
                  class="userinput"><code>pvdisplay -C</code></strong>
<code
                  class="computeroutput">  PV         VG   Fmt  Attr PSize PFree
  /dev/sdb2       lvm2 ---  4.00g 4.00g
  /dev/sdc3       lvm2 ---  3.09g 3.09g
  /dev/sdd        lvm2 ---  4.00g 4.00g
  /dev/sdf1       lvm2 ---  4.10g 4.10g
  /dev/sdf2       lvm2 ---  5.22g 5.22g
</code></pre><div
                class="para">
					Até agora tudo bem; note que o PV (volume físico) pode ser configurado em um disco inteiro assim como em partições individuais do mesmo. Como demonstrado acima, o comando <code
                  class="command">pvdisplay</code> lista os PVs existentes, com dois possíveis formatos de saída.
				</div><div
                class="para">
					Agora vamos montar esses elementos físicos em VGs usando <code
                  class="command">vgcreate</code>. Nós vamos reunir apenas PVs dos discos rápidos em um VG <code
                  class="filename">vg_critical</code>; o outro VG, <code
                  class="filename">vg_normal</code>, irá incluir também elementos mais lentos.
				</div><a
                xmlns=""
                id="screen.vgcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>vgdisplay</code></strong>
<code
                  class="computeroutput">  No volume groups found
# </code><strong
                  class="userinput"><code>vgcreate vg_critical /dev/sdb2 /dev/sdf1</code></strong>
<code
                  class="computeroutput">  Volume group "vg_critical" successfully created
# </code><strong
                  class="userinput"><code>vgdisplay</code></strong>
<code
                  class="computeroutput">  --- Volume group ---
  VG Name               vg_critical
  System ID             
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               8.09 GiB
  PE Size               4.00 MiB
  Total PE              2071
  Alloc PE / Size       0 / 0   
  Free  PE / Size       2071 / 8.09 GiB
  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp

# </code><strong
                  class="userinput"><code>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</code></strong>
<code
                  class="computeroutput">  Volume group "vg_normal" successfully created
# </code><strong
                  class="userinput"><code>vgdisplay -C</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize  VFree 
  vg_critical   2   0   0 wz--n-  8.09g  8.09g
  vg_normal     3   0   0 wz--n- 12.30g 12.30g
</code></pre><div
                class="para">
					Aqui novamente, os comandos são bem simples (e <code
                  class="command">vgdisplay</code> propõem dois formatos de saída). Note que é perfeitamente possível usar duas partições de um mesmo disco físico em dois VGs diferentes. Note também que nós usamos um prefixo <code
                  class="filename">vg_</code> para nomear nossos VGs, mas isso não é nada mais que uma convenção.
				</div><div
                class="para">
					Nós agora temos dois "discos virtuais", com o tamanho de 8 GB e 12 GB, respectivamente.Vamos transformá-los em "partições virtuais" (LVs). Isto envolve o comando <code
                  class="command">lvcreate</code>, e uma sintaxe um pouco mais complexa:
				</div><a
                xmlns=""
                id="screen.lvcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>lvdisplay</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>lvcreate -n lv_files -L 5G vg_critical</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_files" created
# </code><strong
                  class="userinput"><code>lvdisplay</code></strong>
<code
                  class="computeroutput">  --- Logical volume ---
  LV Path                /dev/vg_critical/lv_files
  LV Name                lv_files
  VG Name                vg_critical
  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT
  LV Write Access        read/write
  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400
  LV Status              available
  # open                 0
  LV Size                5.00 GiB
  Current LE             1280
  Segments               2
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0

# </code><strong
                  class="userinput"><code>lvcreate -n lv_base -L 1G vg_critical</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_base" created
# </code><strong
                  class="userinput"><code>lvcreate -n lv_backups -L 12G vg_normal</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_backups" created
# </code><strong
                  class="userinput"><code>lvdisplay -C</code></strong>
<code
                  class="computeroutput">  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_base    vg_critical -wi-a---  1.00g                                           
  lv_files   vg_critical -wi-a---  5.00g                                           
  lv_backups vg_normal   -wi-a--- 12.00g</code></pre><div
                class="para">
					São necessários dois parâmetros para a criação de volumes lógicos; eles tem que ser passados ao <code
                  class="command">lvcreate</code> como opções. O nome do LV a ser criado é especificado com a opção <code
                  class="literal">-n</code>, e seu tamanho geralmente é dado usando a opção <code
                  class="literal">-L</code>. Nós também precisamos dizer ao comando qual o VG a ser operado, é claro, sendo portanto o último parâmetro da linha de comando.
				</div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>Aprofundamento</em></span> <code
                            class="command">lvcreate</code> opções</strong></p></div></div></div><div
                  class="para">
					O comando <code
                    class="command">lvcreate</code> possui diversas opções que permitem manipular como o LV é criado.
				</div><div
                  class="para">
					Vamos primeiro descrever a opção <code
                    class="literal">-l</code>, com a qual o tamanho do LV pode ser dados como um número de blocos (como o oposto das unidades “humanas” que nós usamos acima). Esses blocos (chamados de PEs, <span
                    class="emphasis"><em>physical extents</em></span>, nos termos LVM) são unidades contíguas de espaço de armazenamento em PVs, e elas não podem ser divididas entre LVs. Se alguém quiser definir espaço de armazenamento para um LV com alguma precisão, por exemplo para usar todo o espaço disponível, a opção <code
                    class="literal">-l</code> provavelmente será preferida, ao invés da <code
                    class="literal">-L</code>.
				</div><div
                  class="para">
					Também é possível sugerir uma localização física de um LV, para que suas extensões sejam armazenadas em um PV em particular (enquanto se mantém dentro dos atribuídos ao VG, é claro). Como nós sabemos que o <code
                    class="filename">sdb</code> é mais rápido que o <code
                    class="filename">sdf</code>, nós talvez queiramos armazenar o <code
                    class="filename">lv_base</code> lá, caso nós queiramos dar uma vantagem ao servidor de banco de dados, em comparação com o servidor de arquivos. A linha de comando se torna: <code
                    class="command">lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</code>. Note que esse comando pode falhar se o PV não tiver extensões livres suficientes. Em nosso exemplo, nós provavelmente teríamos que criar <code
                    class="filename">lv_base</code> antes de <code
                    class="filename">lv_files</code> para evitar essa situação – ou liberar algum espaço em <code
                    class="filename">sdb2</code> com o comando <code
                    class="command">pvmove</code>.
				</div></div><div
                class="para">
					Volumes lógicos, quando criados, são representados como dispositivos de blocos no <code
                  class="filename">/dev/mapper/</code>:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>ls -l /dev/mapper</code></strong>
<code
                  class="computeroutput">total 0
crw------- 1 root root 10, 236 Jun 10 16:52 control
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2
# </code><strong
                  class="userinput"><code>ls -l /dev/dm-*</code></strong>
<code
                  class="computeroutput">brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0
brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1
brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2
</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>NOTA</em></span> Auto-detectando volumes LVM</strong></p></div></div></div><div
                  class="para">
					Quando o computador é inicializado, a unidade de serviço do systemd <code
                    class="filename">lvm2-activation</code> executa o <code
                    class="command">vgchange -aay</code> para "ativar" os grupos de volume; ele faz uma busca nos dispositivos disponíveis; aqueles que tiverem sido inicializados como volumes físicos para o LVM são registrados em um subsistema LVM, aqueles que pertencem aos grupos de volume são montados, e os volumes lógicos relevantes são iniciados e tornados disponíveis. Não existe, portanto, necessidade de editar arquivos de configuração quando se cria ou modifica volumes LVM.
				</div><div
                  class="para">
					Note, contudo, que o layout dos elementos LVM (volumes físicos e lógicos, e grupos de volume) tem cópia de segurança em <code
                    class="filename">/etc/lvm/backup</code>, que pode ser útil em caso de um problema (ou apenas para da uma espiada embaixo do capô).
				</div></div><div
                class="para">
					Para simplificar, links simbólicos são convenientemente criados em diretórios que coincidem com os VGs:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>ls -l /dev/vg_critical</code></strong>
<code
                  class="computeroutput">total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0
# </code><strong
                  class="userinput"><code>ls -l /dev/vg_normal</code></strong>
<code
                  class="computeroutput">total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</code></pre><div
                class="para">
					Os LVs então podem ser utilizados exatamente como partições padrão:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mkfs.ext4 /dev/vg_normal/lv_backups</code></strong>
<code
                  class="computeroutput">mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 3145728 4k blocks and 786432 inodes
Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d
[...]
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </code><strong
                  class="userinput"><code>mkdir /srv/backups</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mount /dev/vg_normal/lv_backups /srv/backups</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/backups</code></strong>
<code
                  class="computeroutput">Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_normal-lv_backups   12G   30M   12G   1% /srv/backups
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>cat /etc/fstab</code></strong>
<code
                  class="computeroutput">[...]
/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2
/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2
/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</code></pre><div
                class="para">
					Do ponto de vista das aplicações, a miríade de pequenas partições foi abstraída em um grande volume de 12 GB, com um nome amigável.
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.lvm-over-time"></a>12.1.2.3. LVM ao longo do tempo</h4></div></div></div><div
                class="para">
					Mesmo que a habilidade de agregar partições ou discos físicos seja conveniente, essa não é a principal vantagem trazida pelo LVM. A flexibilidade que ele trás é especialmente notada com o passar do tempo, quando as necessidades se desenvolvem. Em nosso exemplo, vamos assumir que novos e grandes arquivos tem que ser armazenados, e que o LV dedicado ao servidor de arquivos é muito pequeno para acomodá-los. Como nós não usamos todo o espaço disponível em <code
                  class="filename">vg_critical</code>, nós podemos crescer o <code
                  class="filename">lv_files</code>. Para esse propósito, nós iremos usar o comando <code
                  class="command">lvresize</code>, e então o <code
                  class="command">resize2fs</code> para adaptar o sistema de arquivos em conformidadde:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/files/</code></strong>
<code
                  class="computeroutput">Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files
# </code><strong
                  class="userinput"><code>lvdisplay -C vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 5.00g
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   2   2   0 wz--n- 8.09g 2.09g
# </code><strong
                  class="userinput"><code>lvresize -L 7G vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).
  Logical volume lv_files successfully resized
# </code><strong
                  class="userinput"><code>lvdisplay -C vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 7.00g
# </code><strong
                  class="userinput"><code>resize2fs /dev/vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">resize2fs 1.42.12 (29-Aug-2014)
Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.

# </code><strong
                  class="userinput"><code>df -h /srv/files/</code></strong>
<code
                  class="computeroutput">Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>ATENÇÃO</em></span> Redimensionando sistemas de arquivos</strong></p></div></div></div><div
                  class="para">
					Nem todos os sistemas de arquivos podem ser redimensionados em tempo real; redimensionar um volume podem portanto requere primeiro desmontar o sistema de arquivos e remontá-lo depois. Claro que, se alguém quiser diminuir o espaço alocado para um LV, o sistema de arquivos tem que ser diminuído primeiro; a ordem é revertida quando o redimensionamento segue na outra direção: o volume lógico tem que ser cultivado antes do sistema de arquivos existente nele. Isso é bastante simples, já que em nenhum momento o tamanho do sistema de arquivos tem que ser maior que o dispositivo de bloco aonde ele reside (seja esse dispositivo uma partição física ou um volume lógico).
				</div><div
                  class="para">
					Os sistemas de arquivo ext3, ext4 and xfs podem ser cultivados em tempo real, sem serem desmontados; encolhé-los requer uma desmontagem. O sistema de arquivos reiserfs permite o redimensionamento em tempo real nas duas direções. O venerável ext2 não permite, e sempre requer o desmonte.
				</div></div><div
                class="para">
					Nós poderíamos proceder de maneira similar para estender o volume que hospeda o banco de dados, apenas se nós tivermos alcançado o limite de espaço disponível do VG:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/base/</code></strong>
<code
                  class="computeroutput">Sist. Arq.           Tam. Usado Disp. Uso% Montado em
/dev/mapper/vg_critical-lv_base 1008M  854M  104M  90% /srv/base
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree 
  vg_critical   2   2   0 wz--n- 8.09g 92.00m</code></pre><div
                class="para">
					Não importa, já que o LVM permite a adição de volumes físico em grupos de volumes existentes. Por exemplo, talvez nós percebamos que a partição <code
                  class="filename">sdb1</code>, que era até agora usada fora do LVM, apenas contém arquivos que poderiam ser movidos para <code
                  class="filename">lv_backups</code>. Nós podemos agora reciclá-la e integrá-la ao grupo de volume, e assim recuperar algum espaço disponível. Esse é o propósito do comando <code
                  class="command">vgextend</code>. Claro que, a partição tem que ser preparada antes como um volume físico. Uma vez que o VG tenha sido estendido, nós podemos usar comandos similares aos anteriores para cultivar o volume lógico e então o sistema de arquivos:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvcreate /dev/sdb1</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdb1" successfully created
# </code><strong
                  class="userinput"><code>vgextend vg_critical /dev/sdb1</code></strong>
<code
                  class="computeroutput">  Volume group "vg_critical" successfully extended
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   3   2   0 wz--n- 9.09g 1.09g
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>df -h /srv/base/</code></strong>
<code
                  class="computeroutput">Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>Aprofundamento</em></span> LVM avançado</strong></p></div></div></div><div
                  class="para">
					O LVM também serve para usos mais avançados, aonde muitos detalhes podem ser especificados a mão. Por exemplo, um administrador pode ajustar o tamanho dos blocos que compõem os volumes físicos e lógicos, assim como seus layouts físicos. Também é possível mover blocos entre PVs, por exemplo para um ajuste fino no desempenho ou, de maneira mais mundana, liberar um PV quando alguém precisa extrair o disco físico correspondente de um VG (seja para designá-lo para outro VG ou removê-lo do LVM totalmente). As páginas de manual que descrevem os comandos geralmente são claras e detalhadas. Um bom ponto de partida é a página de manual <span
                    class="citerefentry"><span
                      class="refentrytitle">lvm</span>(8)</span>.
				</div></div></div></div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      xmlns=""
                      id="sect.raid-or-lvm"></a>12.1.3. RAID ou LVM?</h3></div></div></div><div
              class="para">
				Tanto o RAID quanto o LVM irão trazer vantagens indiscutíveis assim que se deixar o simples caso de um computador de mesa com um único disco rígido, aonde o padrão de uso não muda com o tempo. Contudo, RAID e LVM vão em direções diferentes, com objetivos divergentes, e é legítimo questionar qual deles deve ser adotado. A resposta mais apropriada irá, é claro, depender das necessidades atuais e previstas.
			</div><div
              class="para">
				Existem alguns casos simples aonde a questão realmente não surge. Se a necessidade é salvaguardar dados contra falhas de hardware, então, obviamente, o RAID será configurado em uma array redundante de discos, já que o LVM realmente não é designado para esse problema. Se, por outro lado, a necessidade é por um esquema de armazenamento flexível aonde os volumes são feitos independente do layout físico dos disco, o RAID não ajuda muito e o LVM será a escolha natural.
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>NOTA</em></span> Se o desempenho importa…</strong></p></div></div></div><div
                class="para">
				Se a velocidade de entrada/saída é essencial, especialmente em termos de tempo de acesso, o uso do LVM e/ou RAID em uma das muitas combinações pode ter algum impacto no desempenho, e isso pode influenciar nas decisões sobre qual escolher. Contudo, essas diferenças de desempenho são realmente pequenas, e só serão mensuráveis em alguns casos de uso. Se desempenho importa, o melhor ganho a ser obtido seria no uso de mídia não rotativa (<a
                  id="id-1.15.4.8.4.2.1"
                  class="indexterm"></a><span
                  class="emphasis"><em>solid-state drives</em></span> ou SSDs); seu custo por megabyte é maior que os discos rígidos padrão, e sua capacidade geralmente é menor, mas ele fornece desempenho excelente para acessos aleatórios. Se o padrão de uso inclui muitas operações de entrada/saída espalhadas por todo o sistema de arquivos, por exemplo, para bancos de dados aonde consultas complexas são executadas rotineiramente, então a vantagem de executá-las em um SSD de longe superam tudo o que pode ser ganho escolhendo LVM sobre RAID ou o contrário. Nessas situações, a escolha deveria ser determinada por outras considerações ao invés de velocidade pura, já que o aspecto desempenho é mais facilmente lidado usando SSDs.
			</div></div><div
              class="para">
				O terceiro caso de uso notável é quando alguém apenas quer agregar dois discos em um volume, seja por questões de desempenho, ou para ter um único sistema de arquivos que seja maior que qualquer dos discos disponíveis. Esse caso pode ser resolvido pelo RAID-0 (ou mesmo um linear-RAID) e por um volume LVM. Quando nesta situação, e salvo restrições extras (por exemplo, manter em sintonia com o resto dos computadores se eles usam apenas RAID), a configuração de escolha irá geralmente ser LVM. A configuração inicial é um pouco mais complexa, mas esse incremento na complexidade mais do que compensado pela flexibilidade extra que o LVM trás caso as necessidades mudem ou se novos discos precisem ser adicionados.
			</div><div
              class="para">
				Então, é claro, temos uma caso de uso realmente interessante, aonde o sistema de armazenamento precisa ser feito tanto para resistência de falha de hardware quanto flexível quando se trata de alocação de volume. Nem RAID nem LVM podem atender esses dois requisitos por conta própria; não tem problema, é aqui que nós usamos os dois ao mesmo tempo — ou melhor, um em cima do outro. O esquema que tem tudo, mas só se tornou um padrão quando o RAID e o LVM alcançaram a maturidade, é para garantir a redundância de dados, primeiro pelo agrupamento de discos em um pequeno número de grandes arrays RAID, e para usar essas arrays RAID como volumes físicos LVM; partições lógicas serão então esculpidas a partir desses LVs para sistemas de arquivos. O ponto forte dessa configuração é que, quando um disco falha, apenas um pequeno número das arrays RAID precisará ser reconstru[ida, limitando assim o tempo gasto pelo administrador para recuperação.
			</div><div
              class="para">
				Vamos ver um exemplo concreto: o departamento de relações públicas da Falcot Corp precisa de uma estação de trabalho para edição de vídeo, mas o orçamento do departamento não permite investir em hardware de ponta para a finalidade. Uma decisão é tomada para favorecer o hardware que é específico para a natureza gráfica do trabalho (monitor e placa de vídeo), e ficar com o hardware genérico para armazenamento. No entanto, como é amplamente conhecido, o vídeo digital tem sim alguns requisitos especiais para seu armazenamento: a quantidade de dados a ser armazenado é grande, e a taxa de transferência para leitura e escrita desses dados é importante para o desempenho geral do sistema (mais que o tempo de acesso típico, por exemplo). Essas restrições precisam ser preenchidas com hardware genérico, neste caso com dois discos rígidos SATA de 300 GB; os dados do sistema também tem que ser resistentes a falha de hardware, assim como alguns dos dados do usuário. Videoclipes editados tem que realmente estar seguros, mas vídeo com edição pendente é menos critico, já que eles ainda estão nas fitas de vídeo.
			</div><div
              class="para">
				O RAID-1 e o LVM são combinados para satisfazer essas restrições. Os discos são anexados a duas controladoras SATA diferentes, para otimizar acesso paralelo e reduzir o risco de falhas simultâneas, e eles portanto aparecem como <code
                class="filename">sda</code> e <code
                class="filename">sdc</code>. Eles são particionados de forma idêntica, seguindo o seguinte esquema:
			</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>fdisk -l /dev/sda</code></strong>
<code
                class="computeroutput">
Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x00039a9f

Device    Boot     Start       End   Sectors Size Id Type
/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect
/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris
/dev/sda3        4000185 586099395 582099210 298G 5  Extended
/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect
/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect
/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</code></pre><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
						As primeiras partições em ambos os discos (por volta de 1 GB) são montadas em um volume RAID-1, <code
                      class="filename">md0</code>. Este espelho é diretamente usado para armazenar o sistema de arquivos raiz.
					</div></li><li
                  class="listitem"><div
                    class="para">
						As partições <code
                      class="filename">sda2</code> e <code
                      class="filename">sdc2</code> são usadas como partição swap, provendo um total de 2 GB de espaço swap. Com 1 GB de RAM, a estação de trabalho encontra uma quantidade confortável de memoria disponível.
					</div></li><li
                  class="listitem"><div
                    class="para">
						As partições <code
                      class="filename">sda5</code> e <code
                      class="filename">sdc5</code>, assim como a <code
                      class="filename">sda6</code> e <code
                      class="filename">sdc6</code>, são montadas em dois novos volumes RAID-1 de 100 GB cada, <code
                      class="filename">md1</code> e <code
                      class="filename">md2</code>. Os dois espelhos são iniciados como volumes físicos para o LVM, e atribuídos para o grupo de volume <code
                      class="filename">vg_raid</code>. Esse VG , portanto, contém 200 GB de espaço seguro.
					</div></li><li
                  class="listitem"><div
                    class="para">
						As partições que sobraram, <code
                      class="filename">sda7</code> e <code
                      class="filename">sdc7</code>,são diretamente usadas como volumes físicos, e associadas a outro VG chamado <code
                      class="filename">vg_bulk</code>, o qual portanto terminará com aproximadamente 200 GB de espaço.
					</div></li></ul></div><div
              class="para">
				Uma vez que os VGs sejam criados, eles podem ser particionados de maneira bem flexível. É preciso ter em mente que os LVs criados em <code
                class="filename">vg_raid</code> serão preservados mesmo que um dos discos falhe, o que não será o caso para LVs criados em <code
                class="filename">vg_bulk</code>; por outro lado, o último será alocado em paralelo nos dois discos, o que permite altas velocidades de leitura ou escrita para arquivos grandes.
			</div><div
              class="para">
				Portanto nós iremos criar os LVs <code
                class="filename">lv_usr</code>, <code
                class="filename">lv_var</code> e <code
                class="filename">lv_home</code> no <code
                class="filename">vg_raid</code>, para hospedar os sistemas de arquivos correspondentes; outro grande LV, <code
                class="filename">lv_movies</code>, será usado para hospedar as versões definitivas dos filmes após a edição. O outro VG será dividido em um grande <code
                class="filename">lv_rushes</code>, para dados tirados de câmeras digitais de vídeo, e um <code
                class="filename">lv_tmp</code> para arquivos temporários. A localização da área de trabalho é uma escolha menos simples de fazer: enquanto bom desempenho é necessário para esse volume, vale o risco de perda de trabalho se uma falha de disco ocorrer durante uma sessão de edição? Dependendo da resposta essa pergunta, o LV relevante será criado em um VG ou em outro.
			</div><div
              class="para">
				Nós agora temos tanto alguma redundância para dados importantes quanto muita flexibilidade no modo como o espaço disponível é dividido entre as aplicações. Para novo software que for instalado mais tarde (para edição de clips de áudio, por exemplo), o LV que hospeda <code
                class="filename">/usr/</code> pode ser aumentado sem dor de cabeça.
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>NOTA</em></span> Por que três volumes RAID-1?</strong></p></div></div></div><div
                class="para">
				Nós poderíamos ter configurado somente um volume RAID-1, para servir como volume físico para <code
                  class="filename">vg_raid</code>. Por que criar três deles, então?
			</div><div
                class="para">
				A razão para a primeira divisão (<code
                  class="filename">md0</code> vs. os outros) é para segurança de dados: dados escritos nos dois elementos de um espelho RAID-1 são exatamente os mesmos, e é portanto possível ignorar a camada RAID e montar um dos discos diretamente. No caso de um bug no núcleo, por exemplo, ou se o metadado do LVM se for corrompido, ainda é possível inicializar um sistema mínimo para acessar dados críticos como o layout de discos nos volumes RAID e LVM; o metadado pode então ser reconstruído e os arquivos podem ser acessados novamente, de modo que o sistema pode ser trazido de volta ao seu estado nominal.
			</div><div
                class="para">
				A razão para a segunda divisão (<code
                  class="filename">md1</code> vs. <code
                  class="filename">md2</code>) não é tão bem definida, e mais relacionada ao reconhecimento que o futuro é incerto. Quando a estação de trabalho é montada na primeira vez, os requisitos exatos de armazenamento não são necessariamente conhecidos com uma precisão perfeita; eles podem também evoluir ao longo do tempo. Em nosso caso, nós não podemos saber com antecedência os requisitos de espaço de armazenamento real para cenas de vídeo e video clips completos. Se um clip em particular precisa de uma grande quantidade de cenas, e o VG dedicado a dados redundantes está cheio menos que a metade, nós podemos reusar algum de seu espaço desnecessário. Nós podemos remover um dos volumes físicos, digamos o <code
                  class="filename">md2</code>, de <code
                  class="filename">vg_raid</code> e então ou atribuí-lo ao <code
                  class="filename">vg_bulk</code> diretamente (se a duração esperada da operação é curta o suficiente para que nós possamos viver com a temporária queda no desempenho), ou desfazer a configuração RAID em <code
                  class="filename">md2</code> e integrar seus componentes <code
                  class="filename">sda6</code> e <code
                  class="filename">sdc6</code> no VG maior (o que aumenta para 200 GB ao invés de 100 GB); o volume lógico <code
                  class="filename">lv_rushes</code> pode então ser aumentado de acordo com os requisitos.
			</div></div></div></div></div><ul
        class="docnav"><li
          class="previous"><a
            accesskey="p"
            href="sect.rtc-services.html"><strong>Anterior</strong>11.8. Serviços de Comunicação em Tempo Real</a></li><li
          class="up"><a
            accesskey="u"
            href="#"><strong>Acima</strong></a></li><li
          class="home"><a
            accesskey="h"
            href="index.html"><strong>Principal</strong></a></li><li
          class="next"><a
            accesskey="n"
            href="sect.virtualization.html"><strong>Próxima</strong>12.2. Virtualização</a></li></ul></body></html>
