<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html
    xmlns="http://www.w3.org/1999/xhtml"><head><meta
        http-equiv="Content-Type"
        content="text/html; charset=UTF-8" /><title
        xmlns:d="http://docbook.org/ns/docbook">فصل 12. مدیریت پیشرفته</title><link
        rel="stylesheet"
        type="text/css"
        href="Common_Content/css/default.css" /><link
        rel="stylesheet"
        media="print"
        href="Common_Content/css/print.css"
        type="text/css" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="generator"
        content="publican v4.3.2" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="package"
        content="Debian-debian-handbook-9-fa-IR-1.0-1" /><meta
        name="keywords"
        content="RAID, LVM, FAI, Preseeding, مانیتورینگ, مجازی‌سازی, Xen, LXC" /><link
        rel="home"
        href="index.html"
        title="راهنمای جامع دبیان" /><link
        rel="up"
        href="index.html"
        title="راهنمای جامع دبیان" /><link
        rel="prev"
        href="sect.rtc-services.html"
        title="11.8. سرویس‌های ارتباطی بلادرنگ" /><link
        rel="next"
        href="sect.virtualization.html"
        title="12.2. مجازی‌سازی" /><meta
        xmlns=""
        name="flattr:id"
        content="4pz9jq" /><link
        xmlns=""
        rel="canonical"
        href="https://debian-handbook.info/browse/fa-IR/stable/advanced-administration.html" /></head><body
      dir="rtl"><div
        id="banner"><a
          href="http://debian-handbook.info/get/"><span
            class="text">Download the ebook</span></a></div><p
        id="title"><a
          class="left"
          href="http://www.debian.org"><img
            alt="Product Site"
            src="Common_Content/images//image_left.png" /></a><a
          class="right"
          href="index.html"><img
            alt="Documentation Site"
            src="Common_Content/images//image_right.png" /></a></p><ul
        class="docnav top"><li
          class="previous"><a
            accesskey="p"
            href="sect.rtc-services.html"><strong>قبلی</strong></a></li><li
          class="home">راهنمای جامع دبیان</li><li
          class="next"><a
            accesskey="n"
            href="sect.virtualization.html"><strong>بعدی</strong></a></li></ul><div
        xml:lang="fa-IR"
        class="chapter"
        lang="fa-IR"><div
          class="titlepage"><div><div><h1
                class="title"><a
                  xmlns=""
                  id="advanced-administration"></a>
      فصل 12. مدیریت پیشرفته</h1></div></div></div><div
          class="toc"><dl
            class="toc"><dt><span
                class="section"><a
                  href="advanced-administration.html#sect.raid-and-lvm">12.1. RAID و LVM</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.raid-soft">12.1.1. RAID نرم‌افزاری</a></span></dt><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.lvm">12.1.2. LVM</a></span></dt><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.raid-or-lvm">12.1.3. RAID یا LVM؟</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.virtualization.html">12.2. مجازی‌سازی</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.virtualization.html#sect.xen">12.2.1. Xen</a></span></dt><dt><span
                    class="section"><a
                      href="sect.virtualization.html#sect.lxc">12.2.2. LXC</a></span></dt><dt><span
                    class="section"><a
                      href="sect.virtualization.html#id-1.15.5.14">12.2.3. مجازی‌سازی با KVM</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.automated-installation.html">12.3. نصب خودکار</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.fai">12.3.1. نصب‌کننده تمام خودکار (FAI)</a></span></dt><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.d-i-preseeding">12.3.2. گردآوری debian-installer</a></span></dt><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.simple-cdd">12.3.3. Simple-CDD: یک راهکار جامع</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.monitoring.html">12.4. مانیتورینگ</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.monitoring.html#sect.munin">12.4.1. راه‌اندازی Munin</a></span></dt><dt><span
                    class="section"><a
                      href="sect.monitoring.html#sect.nagios">12.4.2. راه‌اندازی Nagios</a></span></dt></dl></dd></dl></div><div
          class="highlights"><div
            class="para">
		این فصل به مرور مفاهیمی که تاکنون به آن‌ها پرداخته‌ایم می‌پردازد، اما با رویکردی متفاوت: بجای نصب یک رایانه تکی، به مطالعه نصب-انبوه روی سیستم‌ها می‌پردازیم؛ بجای نصب RAID یا LVM در زمان نصب، اینکار را به صورت دستی در زمان دیگری که نیاز داشته باشیم انجام می‌دهیم. در نهایت، درباره ابزارهای مانیتورینگ و تکنیک‌های مجازی‌سازی صحبت خواهیم کرد. به همین دلیل، مخاطب این فصل روی مدیرسیستم‌های حرفه‌ای تمرکز دارد و به جنبه‌های شخصی و انفرادی این کار کمتر توجه می‌کند.
	</div></div><div
          class="section"><div
            class="titlepage"><div><div><h2
                  class="title"><a
                    xmlns=""
                    id="sect.raid-and-lvm"></a>12.1. RAID و LVM</h2></div></div></div><div
            class="para">
			قسمت <a
              class="xref"
              href="installation.html">
      فصل 4, <em>نصب</em></a> از نقطه نظر فرآیند نصب به بررسی این فناوری‌ها و اینکه چگونه می‌توان از ابتدا آن‌ها را به سادگی مدیریت کرد، پرداخت. پس از نصب اولیه، یک مدیرسیستم باید بتواند نیازهای رو به افزایش فضای دیسک را بدون راه‌اندازی فرآیند نصب برطرف کند. برای این منظور آن‌ها باید ابزارهای مورد نیاز برای تغییرات RAID و LVM را درک کنند.
		</div><div
            class="para">
			RAID و LVM فناوری‌هایی هستند که دستگاه‌های متصل را به شیوه‌ای انتزاعی از همتای فیزیکی خود جدا می‌سازند (درایوهای هارد-دیسک یا پارتیشن‌ها)؛ اولی با استفاده از تکنیک تکرار برای ایمن‌سازی داده در صورت بروز مشکل سخت‌افزاری و دومی با استفاده از منعطف‌ساختن مدیریت دستگاه‌ها جدا از اندازه واقعی آن‌ها کار می‌کنند. در هر دو مورد، سیستم با دستگاه‌های بلاک-محور جدید روبه‌رو می‌شود که می‌تواند برای ایجاد فایل‌سیستم‌ها یا فضای swap مورد استفاده قرار گیرد، بدون اینکه ضرورتا به یک دیسک فیزیکی نگاشت شوند. RAID و LVM از پیش‌زمینه‌های متفاوتی می‌آیند، اما عملکرد آن‌ها می‌تواند با یکدیگر تداخل داشته باشد که به همین دلیل همراه با هم نام برده می‌شوند.
		</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>چشم‌انداز</em></span> ترکیب RAID و LVM در Btrfs</strong></p></div></div></div><div
              class="para">
			از آنجا که RAID و LVM دو زیرسیستم کاملا جدا از یکدیگر در کرنل هستند که در مورد دستگاه‌های بلاک-محور و فایل‌سیستم‌های آنان بکار می‌روند، <span
                class="emphasis"><em>btrfs</em></span> فایل‌سیستم جدیدی بحساب می‌آید، که در ابتدا توسط اوراکل توسعه یافت و قصد ترکیب ویژگی‌های RAID و LVM را دارد. با اینکه هنوز برچسب “آزمایشی” روی آن وجود دارد و برخی ویژگی‌هایش به صورت کامل پیاده‌سازی نشده است، در محیط‌های واقعی کاربردهای گوناگونی دارد. <div
                xmlns=""
                class="url">→ <a
                  xmlns="http://www.w3.org/1999/xhtml"
                  href="http://btrfs.wiki.kernel.org/">http://btrfs.wiki.kernel.org/</a></div>
		</div><div
              class="para">
			از میان ویژگی‌های آن، توانایی ایجاد snapshot از فایل‌سیستم در هر لحظه از زمان وجود دارد. این رونوشت از snapshot به صورت اولیه هیچ فضایی را اشغال نمی‌کند، داده زمانی تکرار می‌شود که یکی از این رونوشت‌ها تغییر کند. فایل‌سیستم همچنین از فشرده‌سازی شفاف فایل‌ها پشتیبانی و از checksum برای اطمینان از جامعیت داده‌های ذخیره شده استفاده می‌کند.
		</div></div><div
            class="para">
			در هر دو مورد RAID و LVM، کرنل یک دستگاه بلاک-محور فراهم می‌کند، مشابه آن‌هایی که برای درایو هارد دیسک یا یک پارتیشن بکار می‌رود. زمانی که یک برنامه، یا قسمتی از کرنل، درخواست دسترسی به چنین دستگاهی را داشته باشد، زیرسیستم مرتبط با آن فرآیند مسیریابی بلاک به لایه فیزیکی مرتبط را برقرار می‌کند. با توجه به پیکربندی، این بلاک می‌تواند در یک یا چند دیسک فیزیکی ذخیره شده باشد و مکان فیزیکی آن ممکن است به صورت مستقیم مرتبط با مکان بلاک در دستگاه مجازی نباشد.
		</div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      xmlns=""
                      id="sect.raid-soft"></a>12.1.1. RAID نرم‌افزاری</h3></div></div></div><a
              id="id-1.15.4.6.2"
              class="indexterm"></a><div
              class="para">
				RAID مخفف عبارت <span
                class="emphasis"><em>Redundant Array of Independent Disks</em></span> به معنی آرایه‌ای افزونه از دیسک‌های مستقل است. هدف این سیستم پیشگیری از بین رفتن داده در زمان نقص سخت‌افزاری هارد دیسک است. ایده اصلی آن بسیار ساده است: داده بجای اینکه در یک دیسک ذخیره گردد در چند دیسک فیزیکی انباشت می‌شود، همراه با یک سطح قابل پیکربندی از افزونگی. با توجه به این میزان از افزونگی، در زمان بروز یک رویداد سخت‌افزاری ناخواسته روی دیسک، داده می‌تواند از سایر دیسک‌های باقیمانده بازسازی شود.
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>فرهنگ</em></span> <span
                          class="foreignphrase"><em
                            class="foreignphrase">مستقل</em></span> یا <span
                          class="foreignphrase"><em
                            class="foreignphrase">ارزان</em></span>؟</strong></p></div></div></div><div
                class="para">
				حرف I در RAID سابق بر این به معنای <span
                  class="emphasis"><em>inexpensive</em></span> بود، چرا که RAID امنیت داده را به شدت و بدون نیاز به استفاده از دیسک‌های گران-قیمت بالا می‌برد. اما امروزه با توجه به نگرانی‌های فضای ذخیره‌سازی، از این حرفه بیشتر به معنای <span
                  class="emphasis"><em>independent</em></span> یاد می‌شود، که الزاما معنای ارزان بودن آن را به یاد نمی‌آورد.
			</div></div><div
              class="para">
				RAID می‌تواند هم به صورت سخت‌‌افزاری (افزونه‌های RAID منطبق با کارت‌های کنترل SCSI یا SATA) هم به صورت نرم‌افزاری (توسط کرنل) پیاده‌سازی شود. جدا از شیوه پیاده‌سازی آن، یک سیستم RAID در صورت وجود افزونگی کافی می‌تواند در زمان بروز نقص دیسک به کار خود ادامه دهد؛ لایه‌های بالایی آن (برنامه‌ها) حتی می‌توانند در صورت بروز مشکل به داده دسترسی داشته باشند. البته که این “حالت ناامن” می‌تواند عملکرد منفی روی سیستم بگذارد و منجر به کاهش سطح افزونگی گردد، بنابراین یک نقص دیسک دیگر، منجر به از دست دادن داده می‌شود. در عمل، تنها یک دیسک تلاش می‌کند که در این حالت تخریبی تا زمان برطرف شدن مشکل قرار بگیرد. زمانی که دیسک جدید جایگزین شود، سیستم RAID می‌تواند با بازسازی داده به حالت امن خود بازگردد. زمانی که آرایه در حالت ناامن یا فاز بازسازی داده قرار می‌گیرد، عملا وقفه‌ای در برنامه‌ها ایجاد نمی‌گردد بجز کاهش سرعت دسترسی به داده.
			</div><div
              class="para">
				زمانی که RAID به صورت سخت‌افزاری پیاده‌سازی شود، پیکربندی آن درون ابزار برپایی BIOS قرار می‌گیرد و کرنل یک آرایه RAID را به عنوان یک دیسک مجزا در نظر می‌گیرد، که مانند یک دیسک استاندارد کار خواهد کرد، با این حال نام دستگاه می‌تواند متفاوت باشد (بر اساس درایو بکار رفته).
			</div><div
              class="para">
				ما تنها به RAID نرم‌افزاری در این کتاب اشاره می‌کنیم.
			</div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.raid-levels"></a>12.1.1.1. سطوح مختلف RAID</h4></div></div></div><div
                class="para">
					RAID در حقیقت یک سیستم مجزا نیست، بلکه بازه‌ای از سیستم‌ها در سطوح مختلف است؛ این سطوح با توجه به ساختار و میزان افزونگی داده با یکدیگر فرق دارند. هر چه افزونگی بیشتر باشد، توانایی مواجه به دیسک‌های خراب در زمان نقص سخت‌افزاری بالاتر می‌رود. نقطه مقابل آن زمانی است که فضای موجود برای مجموعه‌ای از دیسک‌ها کاهش یابد؛ که در این صورت به دیسک‌های بیشتری برای ذخیره‌سازی داده نیاز است.
				</div><div
                class="variablelist"><dl
                  class="variablelist"><dt><span
                      class="term">RAID خطی</span></dt><dd><div
                      class="para">
								با اینکه زیرسیستم RAID در کرنل از “RAID خطی” پشتیبانی می‌کند، این یک RAID کارآمد بحساب نمی‌آید چرا که شامل هیچ سطحی از افزونگی داده نیست. کرنل صرفا با قرار دادن دیسک‌های مختلف در کنار یکدیگر و ایجاد یک فضای بزرگ‌تر یک دیسک مجازی (یک دستگاه بلاک-محور) بوجود می‌آورد. این تنها عملکرد آن است. از این تنظیم به ندرت استفاده می‌شود (بجز موارد خاص که در ادامه خواهید دید)، بخصوص زمانی که در صورت نبود افزونگی داده، نقص در یک دیسک باعث غیرقابل دسترس شدن داده در تمامی دیسک‌ها و کل سیستم می‌گردد.
							</div></dd><dt><span
                      class="term">RAID-0</span></dt><dd><div
                      class="para">
								این سطح نیز هیچ افزونگی داده‌ای را فراهم نمی‌کند ولی برخلاف سطح قبل دیسک‌ها به صورت پیوسته پشت سر هم قرار نمی‌گیرند: بلکه به <span
                        class="emphasis"><em>stripes</em></span> تقسیم می‌شوند و بلاک‌های دستگاه مجازی روی این stripe از دیسک‌های فیزیکی قرار می‌گیرند. برای نمونه، در یک تنظیم RAID-0 با دو دیسک، بلاک‌های شماره زوج از دستگاه مجازی روی دیسک فیزیکی اول و بلاک‌های شماره فرد روی دیسک فیزیکی دوم ذخیره می‌شوند.
							</div><div
                      class="para">
								هدف این سیستم افزایش قابلیت اعتماد نیست، چرا که (مانند حالت خطی) موجودیت داده به محض بروز نقص در دیسک به خطر می‌افتد، هدف آن افزایش عملکرد دیسک است: طی دسترسی ترتیبی به بخش بزرگی از داده‌های به هم پیوسته، کرنل می‌تواند به صورت موازی عملیات خواندن و نوشتن را روی هر دو دیسک انجام دهد که اینکار منجر به افزایش نرخ انتقال داده می‌گردد. با این حال، استفاده از RAID-0 در حال کاهش است، به صورتی که کاربرد آن با LVM جایگزین شده است (در ادامه خواهید دید).
							</div></dd><dt><span
                      class="term">RAID-1</span></dt><dd><div
                      class="para">
								این سطح، که با نام “RAID Mirroring” نیز شناخته می‌شود، ساده‌ترین و پرکاربردترین تنظیم مورد استفاده است. در حالت استاندارد، از دو دیسک فیزیکی هم اندازه استفاده می‌کند تا یک فضای منطقی به همان اندازه را فراهم کند. داده به صورت کاملا یکسان روی هر دو دیسک ذخیره می‌شود، که همان عبارت “mirror” است. زمانی که یک دیسک خراب شود داده از دیسک دیگر قابل دسترس است. برای داده‌های بسیار حیاتی، RAID-1 می‌تواند با بیش از دو دیسک تنظیم شود، که تاثیر مستقیم روی نرخ هزینه سخت‌افزار در مقابل فضای موجود خواهد گذاشت.
							</div><div
                      class="sidebar"><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>یادداشت</em></span> دیسک‌ها و اندازه‌های خوشه</strong></p></div></div></div><div
                        class="para">
								اگر دو دیسک با اندازه مختلف در این سطح بکار گرفته شوند، از دیسک بزرگ‌تر به طور کامل استفاده نخواهد شد چرا که تنها شامل تمام داده‌های دیسک کوچک‌تر است. بنابراین فضای قابل استفاده در یک تنظیم RAID-1 برابر با کوچک‌ترین دیسک موجود در آرایه است. این الگو در رابطه با سایر سطوح RAID پیشرفته‌تر نیز صادق است، با اینکه ممکن است از فرآیند افزونگی دیگری استفاده کنند.
							</div><div
                        class="para">
								به همین دلیل، مهم است که در زمان برپایی آرایه‌های RAID (بجز RAID-0 و خطی) از دیسک‌های با اندازه مشابه یا بسیار نزدیک به هم استفاده شود تا هیچ فضای اضافی تلف نگردد.
							</div></div><div
                      class="sidebar"><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>یادداشت</em></span> دیسک‌های کمکی</strong></p></div></div></div><div
                        class="para">
								سطوح RAID که شامل افزونگی داده هستند اجازه استفاده از دیسک‌های بیشتر در آرایه دیسک‌ها را فراهم می‌کنند. از این دیسک‌ها به منظور پشتیبان در زمان بروز نقص برای یکی از دیسک‌های موجود استفاده می‌شود. برای نمونه، در یک تنظیم دو دیسکه همراه با دیسک کمکی، اگر یکی از دیسک‌های اولیه معیوب شود، کرنل به صورت خودکار (و بلافاصله) فضای موجود را با استفاده از دیسک کمکی بازسازی می‌کند، به منظور اینکه اطمینان از عملیات افزونگی داده پس از زمان بازسازی حاصل گردد. از این روش می‌توان به عنوان گام اضافه برای نگهداری از داده‌های بسیار حساس استفاده کرد.
							</div><div
                        class="para">
								ممکن است به نظر آید که چطور این پیکربندی در مقایسه با استفاده از سه دیسک برای mirroring ممکن است مفید باشد. مزیت استفاده از پیکربندی “دیسک کمکی” این است که می‌تواند بین چندین فضای ذخیره‌سازی RAID به اشتراک گذاشته شود. برای نمونه، می‌توان از سه فضای ذخیره‌سازی mirrored استفاده کرد، همراه با افزونگی داده که در صورت بروز نقص در یکی از دیسک‌ها بکار می‌آید، همراه با هفت دیسک (سه زوج به همراه یک دیسک کمکی) بجای نه دیسک که مورد نیاز سه خانواده سه زوجی است.
							</div></div><div
                      class="para">
								این سطح RAID، با وجود هزینه بالا (از آنجا که در بهترین حالت از نصف فضای ذخیره‌سازی استفاده می‌شود) در عمل بسیار مورد استفاده قرار می‌گیرد. درک آن بسیار ساده است و امکان ایجاد پشتیبان‌های ساده وجود دارد: از آنجا که هر دو دیسک شامل محتوای یکسانی هستند، یکی از آن‌ها بدون ایجاد کوچکترین تاثیر منفی روی سیستم می‌تواند تخلیه شود. عملیات خواندن نیز بسیار سریع‌تر خواهد بود چرا که در هر لحظه کرنل به صورت همزمان نصف داده را از هر دو دیسک می‌تواند بخواند، با اینکه عملیات نوشتن آنطور که به نظر می‌آید تاثیر منفی ندارد. در مورد آرایه RAID-1 از N دیسک، حتی در صورت معیوب شدن N-1 دیسک داده کماکان قابل دسترس خواهد بود.
							</div></dd><dt><span
                      class="term">RAID-4</span></dt><dd><div
                      class="para">
								این سطح RAID، که کاربرد زیادی ندارد، از N دیسک برای ذخیره‌سازی داده مفید و از یک دیسک اضافی برای ذخیره‌سازی اطلاعات افزونگی استفاده می‌کند. اگر این دیسک اضافی معیوب شود، سیستم می‌تواند با آن N دیسک دیگر محتوای خود را بازسازی کند، به صورتی که N-1 دیسک باقیمانده همراه با دیسک “parity” شامل اطلاعات کافی برای بازسازی تمام اطلاعات هستند.
							</div><div
                      class="para">
								RAID-4 هزینه زیادی ندارد چرا که تنها شامل یک هزینه افزایشی یک در N می‌باشد که تاثیر بسزایی روی عملیات خواندن ندارد ولی عملیات نوشتن را کند می‌کند. علاوه بر این، از آنجا که نوشتن روی هر یک از N دیسک به معنای نوشتن روی دیسک parity است، این دیسک اضافی شاهد نوشتن‌های بیشتری نسبت به سایر دیسک‌ها است و همین دلیل نیز باعث کاهش طول عمر مفید آن می‌گردد. داده روی آرایه RAID-4 تنها در صورت معیوب شدن یک دیسک از N+1 دیسک موجود ایمن خواهد بود.
							</div></dd><dt><span
                      class="term">RAID-5</span></dt><dd><div
                      class="para">
								RAID-5 مشکل عدم تقارن در RAID-4 را برطرف می‌کند: بلاک‌های parity بین تمام N+1 دیسک دیگر پخش می‌شوند به صورتی که تنها یک دیسک نقش منحصربفرد نداشته باشد.
							</div><div
                      class="para">
								عملیات خواندن و نوشتن درست مانند RAID-4 است. در اینجا نیز، سیستم تا زمانی بکار خود ادامه می‌دهد که تنها یک دیسک معیوب از N+1 دیسک موجود باشد ولی نه بیشتر.
							</div></dd><dt><span
                      class="term">RAID-6</span></dt><dd><div
                      class="para">
								RAID-6 می‌تواند به عنوان افزونه‌ای برای RAID-5 در نظر گرفته شود، به صورتی که هر سری از N بلاک شامل دو بلاک افزونگی داده است و هر یک از این N+2 بلاک میان N+2 دیسک تقسیم می‌شود.
							</div><div
                      class="para">
								این سطح RAID در مقایسه با دو سطح قبلی هزینه بیشتری در پی دارد، ولی امنیت بیشتری نیز به همراه می‌آورد چرا که تا دو درایو از N+2 دیسک موجود در صورت معیوب شدن می‌توانند بکار خود ادامه دهند. نقطه مقابل آن این است که عملیات نوشتن شامل یک بلاک داده و دو بلاک افزونگی دیگر است، که این آرایه را در مقایسه با سطوح دیگر کندتر نیز می‌کند.
							</div></dd><dt><span
                      class="term">RAID-1+0</span></dt><dd><div
                      class="para">
								این به خودی خود یک سطح RAID بحساب نمی‌آید، بلکه بیشتر یک گروه‌بندی از سطوح دیگر است. با استفاده از 2N دیسک، مجموعه اول به N دیسک RAID-1 تقسیم می‌شود؛ سپس این فضای ذخیره‌سازی به یک واحد مجزا تبدیل می‌شود خواه با “RAID خطی” یا استفاده از LVM. این گزینه آخر چیزی بیش از RAID خالص است، اما مشکلی در استفاده از آن وجود ندارد.
							</div><div
                      class="para">
								RAID-1+0 می‌تواند از چندین نقص دیسک فرار کند: تا N دیسک از 2N آرایه تعریف شده بالا که برای هر کدام از زوج‌های RAID-1 فراهم شده است، می‌تواند معیوب شود.
							</div><div
                      class="sidebar"><a
                        xmlns=""
                        id="sidebar.raid-10"></a><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>مطالعه بیشتر</em></span> RAID-10</strong></p></div></div></div><div
                        class="para">
								RAID-10 بیشتر به عنوان مترادفی برای RAID-1+0 استفاده می‌شود، اما در لینوکس به عنوان یک استاندارد بکار گرفته می‌شود. این تنظیم شامل سیستمی است که هر بلاک آن درون دو دیسک متفاوت ذخیره می‌شود، حتی با تعداد دیسک‌های فرد، عملیات رونوشت‌گیری با استفاده از مدل قابل پیکربندی به صورت بهینه انجام می‌شود.
							</div><div
                        class="para">
								عملکرد کلی سیستم با توجه به سطح افزونگی و مدل پارتیشن‌بندی متفاوت خواهد بود، همچنین فضای ذخیره‌سازی منطقی نیز روی آن تاثیرگذار است.
							</div></div></dd></dl></div><div
                class="para">
					به طور مشخص، سطح RAID با توجه به محدودیت‌ها و نیازمندی‌های سیستم موجود انتخاب می‌شود. نکته اینکه یک رایانه می‌تواند شامل چندین آرایه RAID منحصربفرد به همراه پیکربندی‌های متفاوت باشد.
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.raid-setup"></a>12.1.1.2. برپایی RAID</h4></div></div></div><a
                id="id-1.15.4.6.9.2"
                class="indexterm"></a><div
                class="para">
					برپایی آرایه‌های RAID نیازمند بسته <span
                  class="pkg pkg">mdadm</span> است؛ که شامل دستور <code
                  class="command">mdadm</code> برای ایجاد و ویرایش این آرایه‌ها می‌باشد همراه با ابزارهای جانبی و اسکریپت‌هایی که آن را با سایر قسمت‌های سیستم از جمله مانیتورینگ یکپارچه می‌سازد.
				</div><div
                class="para">
					مثال ما شامل سروری با چندین دیسک است که برخی از آن‌ها استفاده شده و برخی دیگر که آزاد هستند به عنوان RAID بکار گرفته می‌شوند. در حالت اولیه دیسک‌ها و پارتیشن‌های زیر را در اختیار داریم:
				</div><div
                xmlns:d="http://docbook.org/ns/docbook"
                class="itemizedlist"><ul><li
                    class="listitem"><div
                      class="para">
							دیسک ۴ گیگابایت <code
                        class="filename">sdb</code> کاملا موجود است؛
						</div></li><li
                    class="listitem"><div
                      class="para">
							دیسک ۴ گیگابایت <code
                        class="filename">sdc</code> کاملا موجود است؛
						</div></li><li
                    class="listitem"><div
                      class="para">
							در دیسک <code
                        class="filename">sdd</code> تنها پارتیشن ۴ گیگابایت <code
                        class="filename">sdd2</code> موجود است؛
						</div></li><li
                    class="listitem"><div
                      class="para">
							در نهایت، دیسک ۴ گیگابایت <code
                        class="filename">sde</code> که کاملا موجود است.
						</div></li></ul></div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>یادداشت</em></span> شناسایی آرایه‌های موجود RAID</strong></p></div></div></div><div
                  class="para">
					فایل <code
                    class="filename">/proc/mdstat</code> فهرستی از آرایه‌های موجود و شرایط آن‌ها را نگهداری می‌کند. در زمان ایجاد یک آرایه جدید RAID، باید دقت کرد که نام آن با نام آرایه‌ای موجود برابر نباشد.
				</div></div><div
                class="para">
					با استفاده از این دیسک‌ها می‌خواهیم دو فضای ذخیره‌سازی بوجود آوریم، یکی RAID-0 و دیگری RAID-1. بیایید با آرایه RAID-0 شروع کنیم:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</code></strong>
<code
                  class="computeroutput">mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
# </code><strong
                  class="userinput"><code>mdadm --query /dev/md0</code></strong>
<code
                  class="computeroutput">/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md0</code></strong>
<code
                  class="computeroutput">/dev/md0:
        Version : 1.2
  Creation Time : Wed May  6 09:24:34 2015
     Raid Level : raid0
     Array Size : 8387584 (8.00 GiB 8.59 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:24:34 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

     Chunk Size : 512K

           Name : mirwiz:0  (local to host mirwiz)
           UUID : bb085b35:28e821bd:20d697c9:650152bb
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       1       8       32        1      active sync   /dev/sdc
# </code><strong
                  class="userinput"><code>mkfs.ext4 /dev/md0</code></strong>
<code
                  class="computeroutput">mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 2095104 4k blocks and 524288 inodes
Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </code><strong
                  class="userinput"><code>mkdir /srv/raid-0</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mount /dev/md0 /srv/raid-0</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/raid-0</code></strong>
<code
                  class="computeroutput">Filesystem      Size  Used Avail Use% Mounted on
/dev/md0        7.9G   18M  7.4G   1% /srv/raid-0
</code></pre><div
                class="para">
					دستور <code
                  class="command">mdadm --create</code> نیازمند چند پارامتر است: نام آرایه‌ای که قصد ایجادش را داریم (<code
                  class="filename">/dev/md*</code> که MD به معنی <span
                  class="foreignphrase"><em
                    class="foreignphrase">Multiple Device</em></span> است)، سطح RAID، تعداد دیسک‌ها (که اجباری است و از RAID-1 به بالا معنا دارد) و درایوهای فیزیکی قابل استفاده. زمانی که دستگاه ایجاد گردد، مانند یک پارتیشن عادی می‌توانیم از آن استفاده کرده، فایل سیستم ایجاد کنیم و آن را متصل سازیم. نکته اینکه ایجاد آرایه RAID-0 روی <code
                  class="filename">md0</code> تصادفی است و این شماره‌گذاری هیچ ارتباطی به سطوح مختلف RAID ندارد. همچنین امکان ایجاد آرایه‌های نامگذاری شده RAID نیز با استفاده از پارامترهایی نظیر <code
                  class="filename">/dev/md/linear</code> بجای <code
                  class="filename">/dev/md0</code> در دستور <code
                  class="command">mdadm</code> وجود دارد.
				</div><div
                class="para">
					ایجاد یک آرایه RAID-1 مشابه قبل است که تفاوت آن تنها پس از فرآیند ایجاد مشخص می‌گردد:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%
Continue creating array? </code><strong
                  class="userinput"><code>y</code></strong>
<code
                  class="computeroutput">mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md1 started.
# </code><strong
                  class="userinput"><code>mdadm --query /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
        Version : 1.2
  Creation Time : Wed May  6 09:30:19 2015
     Raid Level : raid1
     Array Size : 4192192 (4.00 GiB 4.29 GB)
  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:30:40 2015
          State : clean, resyncing (PENDING) 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       1       8       64        1      active sync   /dev/sde
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
          State : clean
[...]
</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>نکته</em></span> RAID، دیسک‌ها و پارتیشن‌ها</strong></p></div></div></div><div
                  class="para">
					همانطور که در مثال نیز مشخص است، دستگاه‌های RAID بدون پارتیشن‌بندی‌های دیسک نیز می‌توانند ایجاد گردند و نیازمند دیسک‌های کامل نمی‌باشند.
				</div></div><div
                class="para">
					چند نکته باقی می‌ماند. اول، <code
                  class="command">mdadm</code> تشخیص می‌دهد که دستگاه‌های فیزیکی شامل اندازه‌های متفاوت هستند؛ از آنجا که این امر منجر به گم شدن فضا در دیسک بزرگتر می‌شود، تاییدیه آن مورد نیاز است.
				</div><div
                class="para">
					مهمتر از آن به حالت دیسک mirror توجه کنید. حالت عادی در آرایه RAID که به صورت mirror باشد مشابهت کامل محتوا در هر دو دیسک است. با این حال، ضمانتی برای بوجود آمدن این حالت در زمان ایجاد آرایه وجود ندارد. زیرمجموعه RAID خود این ضمانت را ایجاد می‌کند و به محض اینکه دستگاه RAID ساخته شود عملیات همگام‌سازی صورت می‌گیرد. بعد از گذشت زمان (که وابسته به اندازه‌ دیسک‌ها است) آرایه RAID به حالت “فعال” یا “تمیز” تغییر می‌یابد. نکته اینکه در زمان این فاز بازسازی، mirror در یک حالت ناپایدار قرار می‌گیرد که افزونگی داده در آن تضمین نمی‌شود . دیسکی که در این بازه زمانی دچار مشکل گردد ممکن است به حذف داده بینجامد. با توجه به این موضوع، قبل از فاز همگام‌سازی معمولا داده‌های بزرگ و حساس روی آرایه RAID قرار نمی‌گیرند. حتی در حالت ناپایدار نیز، <code
                  class="filename">/dev/md1</code> قابل استفاده است و یک فایل سیستم می‌تواند روی آن ایجاد گردد و داده‌های روی آن قرار گیرند.
				</div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>نکته</em></span> آغاز یک mirror در حالت ناپایدار</strong></p></div></div></div><div
                  class="para">
					بعضی وقت‌ها در زمان آغاز mirror از RAID-1 دو دیسک بلافاصله قابل دسترس نخواهند بود، برای نمونه به این دلیل که یکی از دیسک‌ها برای ذخیره داده‌هایی استفاده شده است که دیگری می‌خواهد از آن mirror بگیرد. در چنین شرایطی، امکان ارجاع یک آرایه RAID-1 ناپایدار با استفاده از پارامتر <code
                    class="filename">missing</code> بجای نام دستگاه در <code
                    class="command">mdadm</code> وجود دارد. زمانی که از داده در “mirror” رونوشت گرفته شود، دیسک قدیمی می‌تواند به آرایه اضافه گردد. سپس همگام‌سازی صورت می‌گیرد که امکان افزونگی داده با استفاده از دیسک قدیمی را فراهم می‌آورد.
				</div></div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>نکته</em></span> برپایی یک mirror بدون همگام‌سازی</strong></p></div></div></div><div
                  class="para">
					آرایه‌های RAID-1 اغلب به منظور ایجاد یک دیسک جدید و خالی استفاده می‌شوند. از این رو محتوای اولیه دیسک خیلی حائز اهمیت نیست، از این رو باید دانست داده‌ای که پس از ایجاد فایل سیستم در آرایه قرار می‌گیرد قابل دسترس خواهد بود.
				</div><div
                  class="para">
					شاید این سوال پیش بیاید که هدف از همگام‌سازی دو دیسک در زمان ایجاد آرایه چیست. چرا به یکسان بودن محتوایی که روی ناحیه‌های این آرایه قرار می‌گیرد اهمیت بدهیم وقتی می‌دانیم تنها پس از نوشتن روی آن است که می‌توان به آن‌ها دسترسی داشت؟
				</div><div
                  class="para">
					خوشبختانه، این فاز همگام‌سازی می‌تواند با استفاده از گزینه <code
                    class="literal">--assume-clean</code> در <code
                    class="command">mdadm</code> در نظر گرفته نشود. اگرچه، این گزینه ممکن است منجر به سردرگمی در مواردی شود که داده اولیه خوانده خواهد شد (برای نمونه، وقتی یک فایل سیستم هم اکنون روی دیسک موجود باشد)، به همین دلیل است که به صورت پیشفرض فعال نیست.
				</div></div><div
                class="para">
					اکنون بیایید ببینیم در زمان بروز مشکل برای یکی از آرایه‌های RAID-1 چه اتفاقی می‌افتد. <code
                  class="command">mdadm</code> و به طور خاص گزینه <code
                  class="literal">--fail</code> آن، امکان شبیه‌سازی این نقص دیسک را بوجود می‌آورد:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --fail /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: set /dev/sde faulty in /dev/md1
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Update Time : Wed May  6 09:39:39 2015
          State : clean, degraded 
 Active Devices : 1
Working Devices : 1
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 19

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       0        0        2      removed

       1       8       64        -      faulty   /dev/sde</code></pre><div
                class="para">
					محتوای آرایه هنوز قابل دسترس است (و در صورت اتصال به فایل سیستم، وقفه‌ای در برنامه‌ها ایجاد نمی‌شود) اما امنیت داده دیگر تضمین نمی‌شود: در صورت بروز نقص برای دیسک <code
                  class="filename">sdd</code> داده از بین می‌رود. به منظور پیشگیری از این خطر دیسک معیوب را با <code
                  class="filename">sdf</code> جایگزین می‌کنیم:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --add /dev/sdf</code></strong>
<code
                  class="computeroutput">mdadm: added /dev/sdf
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
   Raid Devices : 2
  Total Devices : 3
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:48:49 2015
          State : clean, degraded, recovering 
 Active Devices : 1
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 1

 Rebuild Status : 28% complete

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 26

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      spare rebuilding   /dev/sdf

       1       8       64        -      faulty   /dev/sde
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Update Time : Wed May  6 09:49:08 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 41

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf

       1       8       64        -      faulty   /dev/sde</code></pre><div
                class="para">
					در اینجا نیز، کرنل به صورت خودکار فاز بازسازی آرایه را آغاز می‌کند که طی آن با وجود قابل دسترس بودن، آرایه در یک حالت ناپایدار قرار دارد. زمانی که بازسازی تمام شود، آرایه RAID به حالت عادی خود باز می‌گردد. به منظور سازگاری با حالت کلاسیک RAID که از دو دیسک برای mirror استفاده می‌کند، می‌توان دیسک <code
                  class="filename">sde</code> را حذف کرد؛
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --remove /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: hot removed /dev/sde from /dev/md1
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf</code></pre><div
                class="para">
					از این لحظه، درایو می‌تواند در زمان خاموش شدن سرور یا در صورت پشتیبانی سخت‌افزاری از how-swap به صورت دستی جدا گردد. چنین پیکربندی شامل برخی کنترلرهای SCSI، اغلب دیسک‌های SATA و درایوهای خارجی که روی USB یا Firewire است.
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.backup-raid-config"></a>12.1.1.3. پشتیبان‌گیری از پیکربندی</h4></div></div></div><div
                class="para">
					اکثر اطلاعات جانبی درباره آرایه‌های RAID به صورت مستقیم روی همین دیسک‌ها ذخیره‌سازی می‌شود، تا کرنل در زمان راه‌اندازی اولیه سیستم بتواند به صورت خودکار اجزای آرایه را تشکیل داده و آن را تنظیم کند. با این حال، پشتیبان‌گیری از این پیکربندی توصیه می‌شود چرا که این فرآیند تشخیص اولیه خالی از خطا نیست و تنها انتظار می‌رود که در موارد بسیار معدود دچار نقص گردد. در مثال ما، اگر نقص دیسک <code
                  class="filename">sde</code> واقعی (در مقابل شبیه‌سازی شده) باشد و سیستم بدون حذف <code
                  class="filename">sde</code> راه‌اندازی مجدد گردد، این دیسک ممکن است به فعالیت خود پس از عملیات تشخیص اولیه ادامه دهد. کرنل شامل سه دیسک فیزیکی است که هر کدام ادعا می‌کنند نصف فضای RAID را در اختیار دارند. حالت مبهم دیگر ترکیب آرایه‌های RAID از دو سرور مختلف در قالب یک سرور است. اگر این آرایه‌ها قبل از انتقال دیسک‌ها درست کار کنند، کرنل قادر خواهد بود که اجزای آن را شناسایی و پیکربندی کند؛ اما اگر دیسک‌های انتقال یافته درون آرایه <code
                  class="filename">md1</code> از سرور قدیم قرار بگیرند، در صورتی که سرور جدید آرایه <code
                  class="filename">md1</code> داشته باشد، یکی از mirrorها نامگذاری مجدد خواهد شد.
				</div><div
                class="para">
					از این رو، پشتیبان‌گیری از فایل پیکربندی اهمیت می‌یابد. شیوه استاندارد اینکار ویرایش فایل <code
                  class="filename">/etc/mdadm/mdadm.conf</code> است که مثالی از آن را در ادامه مشاهده می‌کنید:
				</div><div
                class="example"><a
                  xmlns=""
                  id="example.mdadm-conf"></a><p
                  class="title"><strong>
      مثال 12.1. فایل پیکربندی<code
                      class="command">mdadm</code></strong></p><div
                  class="example-contents"><pre
                    class="programlisting"># mdadm.conf
#
# Please refer to mdadm.conf(5) for information about this file.
#

# by default (built-in), scan all partitions (/proc/partitions) and all
# containers for MD superblocks. alternatively, specify devices to scan, using
# wildcards if desired.
DEVICE /dev/sd*

# auto-create devices with Debian standard permissions
CREATE owner=root group=disk mode=0660 auto=yes

# automatically tag new arrays as belonging to the local system
HOMEHOST &lt;system&gt;

# instruct the monitoring daemon where to send mail alerts
MAILADDR root

# definitions of existing MD arrays
ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464

# This configuration was auto-generated on Thu, 17 Jan 2013 16:21:01 +0100
# by mkconf 3.2.5-3</pre></div></div><div
                class="para">
					یکی از جزئیات کاربری آن گزینه <code
                  class="literal">DEVICE</code> است، که دستگاه‌های مورد نیاز برای جستجوی خودکار اجزای آرایه‌های RAID در سیستم را مشخص می‌کند. در مثال ما، ما گزینه پیشفرض <code
                  class="literal">partitions containers</code> را با فهرستی از دستگاه‌ها جایگزین کردیم چرا که قصد استفاده از تمام دیسک و نه قسمت‌هایی از پارتیشن آن را برای برخی آرایه‌ها داشتیم.
				</div><div
                class="para">
					دو خط آخر در مثال ما به کرنل اجازه می‌دهند که با استفاده از شماره آرایه عملیات تشخیص و راه‌اندازی آن‌ها را انجام دهد. اطلاعات جانبی ذخیره شده روی دیسک برای جمع‌آوری آرایه‌ها کافی است، ولی نه برای تشخیص شماره آن‌ها (و نام دستگاه <code
                  class="filename">/dev/md*</code> منطبق با آن).
				</div><div
                class="para">
					خوشبختانه، این خطوط به صورت خودکار تولید می‌شوند:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --misc --detail --brief /dev/md?</code></strong>
<code
                  class="computeroutput">ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</code></pre><div
                class="para">
					محتوای این دو خط آخر وابسته به تعداد دیسک‌های استفاده شده در آرایه نیست. پس هنگام جایگزینی یک دیسک معیوب با جدید نیازی به تولید مجدد این خطوط نیست. از طرف دیگر، در زمان ایجاد یا حذف یک آرایه RAID، این فایل باید بروزرسانی گردد.
				</div></div></div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      xmlns=""
                      id="sect.lvm"></a>12.1.2. LVM</h3></div></div></div><a
              id="id-1.15.4.7.2"
              class="indexterm"></a><a
              id="id-1.15.4.7.3"
              class="indexterm"></a><div
              class="para">
				LVM که مخفف <span
                class="emphasis"><em>Logical Volume Manager</em></span> است، روشی دیگر برای انتزاع دستگاه‌های منطقی از نمونه‌های فیزیکی است که بجای قابلیت اعتماد روی افزایش انعطاف‌پذیری تمرکز دارد. LVM امکان تغییر یک دستگاه منطقی را به شیوه‌ای شفاف برای برنامه‌های کاربردی آن بوجود می‌آورد؛ برای نمونه، امکان افزودن دیسک‌های جدید، مهاجرت داده به آن‌ها و حذف دیسک‌های قدیمی بدون قطع اتصال دستگاه مجازی وجود دارد.
			</div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.lvm-concepts"></a>12.1.2.1. مفاهیم LVM</h4></div></div></div><div
                class="para">
					این انعطاف‌پذیری از طریق یک سطح انتزاعی همراه با سه مفهوم بدست می‌آید.
				</div><div
                class="para">
					اول، PV یا <span
                  class="emphasis"><em>Physical Volume</em></span> نزدیک‌ترین موجودیت به سخت‌افزار است: می‌تواند پارتیشن‌های روی یک دیسک، یک دیسک کامل یا حتی سایر دستگاه‌های بلاک-محور (از جمله یک آرایه RAID) باشد. به یاد داشته باشید زمانی که یک عنصر فیزیکی به عنوان PV برای LVM تنظیم می‌شود، تنها باید توسط LVM قابل دسترس باشد در غیر اینصورت سیستم دچار سردرگمی می‌گردد.
				</div><div
                class="para">
					تعدادی از PVها می‌توانند درون یک خوشه VG یا <span
                  class="emphasis"><em>Volume Group</em></span> قرار بگیرند که می‌تواند با دیسک‌های مجازی و توسعه‌یافته مقایسه گردد. VGها انتزاعی هستند و درون سلسله‌مراتب <code
                  class="filename">/dev</code> به عنوان یک فایل ظاهر نمی‌شوند، بنابراین خطری در استفاده مستقیم از آن‌ها وجود ندارد.
				</div><div
                class="para">
					سومین مفهوم نیز LV یا <span
                  class="emphasis"><em>Logical Volume</em></span> نام دارد، که تکه‌ای از یک VG به حساب می‌آید؛ اگر VG را با یک دیسک مقایسه کنیم، LV مانند یک پارتیشن خواهد بود. LV به عنوان یک دستگاه بلاک-محور همراه با مدخلی در <code
                  class="filename">/dev</code> ظاهر می‌شود، که می‌تواند به عنوان هر پارتیشن فیزیکی دیگر مورد استفاده قرار گیرد (بیشتر در مورد یک فایل سیستم میزبان یا فضای swap).
				</div><div
                class="para">
					نکته مهم در تقسیم یک VG به LV این است که کاملا مستقل از اجزای فیزیکی آن (PV) انجام می‌شود. یک VG تنها با یک قسمت فیزیکی (مانند یک دیسک) می‌تواند به چندین دستگاه منطقی تقسیم شود؛ به طور مشابه، یک VG با چندین دیسک فیزیکی می‌تواند به عنوان یک دستگاه منطقی بزرگ ظاهر شود. تنها محدودیت مشخص این است که اندازه کل اختصاص یافته به LVها نمی‌تواند بیشتر از مجموع اندازه PVها در گروه دستگاه‌ها باشد.
				</div><div
                class="para">
					اغلب منطقی است که به منظور داشتن همگنی بین اجزای فیزیکی یک VG، آن را به دستگاه‌های مجازی تقسیم کرد که الگوهای مشابهی در کارکرد داشته باشند. برای نمونه، اگر سخت‌افزار موجود شامل دیسک‌های سریع و کند باشد، دیسک‌های سریع می‌توانند درون یک VG و دیسک‌های کند درون دیگری قرار گیرند؛ تکه‌های اولی می‌توانند به برنامه‌هایی اختصاص یابند که نیازمند دسترسی سریع به دیسک هستند، در صورتی که از دومی برای سایر وظایف متداول دیسک استفاده می‌شود.
				</div><div
                class="para">
					در هر صورت، به خاطر بسپارید که یک LV به طور مشخص به هیچ PV متصل نیست. امکان تاثیرگذاری روی جایی که داده از یک LV به صورت فیزیکی می‌آید وجود دارد، اما این امکان برای کاربردهای روزانه الزامی نیست. از طرف دیگر، زمانی که مجموعه فیزیکی از اجزای یک VG گسترش می‌یابند، مکان ذخیره‌سازی فیزیکی منطبق با یک LV می‌توانند بین چند دیسک مهاجرت کنند (به صورتی که درون PVهای اختصاص‌یافته به VG قرار داشته باشند).
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.lvm-setup"></a>12.1.2.2. برپایی LVM</h4></div></div></div><div
                class="para">
					بیایید فرآیند گام به گام برپایی LVM برای یک کاربرد متداول را دنبال کنیم: می‌خواهیم یک موقعیت ذخیره‌سازی پیچیده را ساده کنیم. چنین موقعیتی معمولا با گذشت زمان و گره خوردن مقیاس‌های موقتی انباشتگی صورت می‌گیرد. برای این منظور، سروری را در نظر می‌گیریم که نیازهای ذخیره‌سازی آن طی زمان تغییر کرده است که پارتیشن‌های موجود آن بین چندین دیسک فیزیکی مختلف به مانند یک مسیر مارپیچ قرار گرفته‌اند. به عبارت دیگر، پارتیشن‌های زیر موجود هستند:
				</div><div
                xmlns:d="http://docbook.org/ns/docbook"
                class="itemizedlist"><ul><li
                    class="listitem"><div
                      class="para">
							درون دیسک <code
                        class="filename">sdb</code>،‌ یک پارتیشن ۴ گیگابایت به نام <code
                        class="filename">sdb2</code>؛
						</div></li><li
                    class="listitem"><div
                      class="para">
							درون دیسک <code
                        class="filename">sdc</code>،‌ یک پارتیشن ۳ گیگابایت به نام <code
                        class="filename">sdc3</code>؛
						</div></li><li
                    class="listitem"><div
                      class="para">
							دیسک <code
                        class="filename">sdd</code>،‌ با ظرفیت ۴ گیگابایت کاملا موجود؛
						</div></li><li
                    class="listitem"><div
                      class="para">
							درون دیسک <code
                        class="filename">sdf</code>،‌ یک پارتیشن ۴ گیگابایت به نام <code
                        class="filename">sdf1</code> و یک پارتیشن ۵ گیگابایت به نام <code
                        class="filename">sdf2</code>.
						</div></li></ul></div><div
                class="para">
					علاوه بر این، تصور می‌کنیم که دیسک‌های <code
                  class="filename">sdb</code> و <code
                  class="filename">sdf</code> سریع‌تر از دو دیسک دیگر هستند.
				</div><div
                class="para">
					هدف ما برپایی یه دستگاه منطقی برای سه برنامه مختلف است: یک سرور فایل که به ۵ گیگابایت فضای ذخیره‌سازی نیاز دارد، یک پایگاه‌داده (۱ گیگابایت) و فضایی برای پشتیبان‌گیری (۱۲ گیگابایت). دوتای اول به عملکرد بالا نیاز دارند اما پشتیبان‌گیری چنین حساسیتی در دسترسی سریع ندارد. تمام این محدودیت‌ها از استفاده پارتیشن‌ها به صورت مستقیم جلوگیری می‌کنند؛ استفاده از LVM می‌تواند اندازه فیزیکی از دستگاه‌ها را انتزاعی کند، که تنها محدودیت آن مجموع فضای ذخیره‌سازی است.
				</div><div
                class="para">
					ابزارهای مورد نیاز در بسته <span
                  class="pkg pkg">lvm2</span> و وابستگی‌های آن قرار دارند. زمانی که نصب شوند، برپایی LVM شامل سه گام می‌شود که با سه سطح از مفاهیم آن مرتبط است.
				</div><div
                class="para">
					ابتدا دستگاه‌های فیزیکی را با استفاده از <code
                  class="command">pvcreate</code> آماده‌سازی می‌کنیم:
				</div><a
                xmlns=""
                id="screen.pvcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvdisplay</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvcreate /dev/sdb2</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdb2" successfully created
# </code><strong
                  class="userinput"><code>pvdisplay</code></strong>
<code
                  class="computeroutput">  "/dev/sdb2" is a new physical volume of "4.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/sdb2
  VG Name               
  PV Size               4.00 GiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I

# </code><strong
                  class="userinput"><code>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdc3" successfully created
  Physical volume "/dev/sdd" successfully created
  Physical volume "/dev/sdf1" successfully created
  Physical volume "/dev/sdf2" successfully created
# </code><strong
                  class="userinput"><code>pvdisplay -C</code></strong>
<code
                  class="computeroutput">  PV         VG   Fmt  Attr PSize PFree
  /dev/sdb2       lvm2 ---  4.00g 4.00g
  /dev/sdc3       lvm2 ---  3.09g 3.09g
  /dev/sdd        lvm2 ---  4.00g 4.00g
  /dev/sdf1       lvm2 ---  4.10g 4.10g
  /dev/sdf2       lvm2 ---  5.22g 5.22g
</code></pre><div
                class="para">
					تا اینجا مشکلی نیست؛ به یاد داشته باشید که یک PV می‌تواند روی یک دیسک کامل یا پارتیشن‌های انفرادی ایجاد گردد. دستور <code
                  class="command">pvdisplay</code> فهرستی از PVها را با دو قالب خروجی ممکن نمایش می‌دهد.
				</div><div
                class="para">
					اکنون بیایید این عناصر فیزیکی را با استفاده از <code
                  class="command">vgcreate</code> درون VG قرار دهیم. PV دیسک‌های سریع را درون VG به نام <code
                  class="filename">vg_critical</code> و دیسک‌های کند را درون VG به نام <code
                  class="filename">vg_normal</code> قرار می‌دهیم.
				</div><a
                xmlns=""
                id="screen.vgcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>vgdisplay</code></strong>
<code
                  class="computeroutput">  No volume groups found
# </code><strong
                  class="userinput"><code>vgcreate vg_critical /dev/sdb2 /dev/sdf1</code></strong>
<code
                  class="computeroutput">  Volume group "vg_critical" successfully created
# </code><strong
                  class="userinput"><code>vgdisplay</code></strong>
<code
                  class="computeroutput">  --- Volume group ---
  VG Name               vg_critical
  System ID             
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               8.09 GiB
  PE Size               4.00 MiB
  Total PE              2071
  Alloc PE / Size       0 / 0   
  Free  PE / Size       2071 / 8.09 GiB
  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp

# </code><strong
                  class="userinput"><code>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</code></strong>
<code
                  class="computeroutput">  Volume group "vg_normal" successfully created
# </code><strong
                  class="userinput"><code>vgdisplay -C</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize  VFree 
  vg_critical   2   0   0 wz--n-  8.09g  8.09g
  vg_normal     3   0   0 wz--n- 12.30g 12.30g
</code></pre><div
                class="para">
					در اینجا نیز دستورات واضح هستند و <code
                  class="command">vgdisplay</code> شامل دو قالب خروجی است. به یاد داشته باشید که امکان استفاده از دو پارتیشن یک دیسک فیزیکی درون دو VG مختلف وجود دارد. ما از یک پیشوند <code
                  class="filename">vg_</code> برای نامگذاری VGها استفاده کردیم، اما چیزی بیشتر از رعایت یک استاندارد نیست.
				</div><div
                class="para">
					اکنون دو “دیسک مجازی” به اندازه‌های ۸ و ۱۲ گیگابایت داریم. حال بیایید آن‌ها را به “پارتیشن‌های مجازی” یا LV تقسیم کنیم. اینکار با استفاده از دستور <code
                  class="command">lvcreate</code> و شیوه نگارشی پیچیده‌تر از گام‌های قبلی صورت می‌گیرد:
				</div><a
                xmlns=""
                id="screen.lvcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>lvdisplay</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>lvcreate -n lv_files -L 5G vg_critical</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_files" created
# </code><strong
                  class="userinput"><code>lvdisplay</code></strong>
<code
                  class="computeroutput">  --- Logical volume ---
  LV Path                /dev/vg_critical/lv_files
  LV Name                lv_files
  VG Name                vg_critical
  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT
  LV Write Access        read/write
  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400
  LV Status              available
  # open                 0
  LV Size                5.00 GiB
  Current LE             1280
  Segments               2
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0

# </code><strong
                  class="userinput"><code>lvcreate -n lv_base -L 1G vg_critical</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_base" created
# </code><strong
                  class="userinput"><code>lvcreate -n lv_backups -L 12G vg_normal</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_backups" created
# </code><strong
                  class="userinput"><code>lvdisplay -C</code></strong>
<code
                  class="computeroutput">  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_base    vg_critical -wi-a---  1.00g                                           
  lv_files   vg_critical -wi-a---  5.00g                                           
  lv_backups vg_normal   -wi-a--- 12.00g</code></pre><div
                class="para">
					برای ایجاد دستگاه‌های منطقی دو پارامتر مورد نیاز است؛ که باید به صورت گزینه‌ها به <code
                  class="command">lvcreate</code> ارسال گردند. نام LV که قصد ایجاد آن را داریم با گزینه <code
                  class="literal">-n</code> و اندازه آن با گزینه <code
                  class="literal">-L</code> مشخص می‌شود. البته، به دستور باید اعلام کنیم که از کدام VG می‌خواهیم استفاده شود.
				</div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>مطالعه بیشتر</em></span> گزینه‌های <code
                            class="command">lvcreate</code></strong></p></div></div></div><div
                  class="para">
					دستور <code
                    class="command">lvcreate</code> شامل چندین گزینه است که چگونگی ایجاد LV را مشخص می‌کند.
				</div><div
                  class="para">
					ابتدا بیایید گزینه <code
                    class="literal">-l</code> را توضیح دهیم، که با استفاده از آن اندازه LV می‌تواند به عنوان تعداد بلاک‌ها در مقایسه با واحدهای “انسانی” که در بالا استفاده کردیم مشخص گردد. این بلاک‌ها که به نام PE یا <span
                    class="emphasis"><em>physical extents</em></span> در زبان LVM مطرح می‌شوند، واحدهای پیوسته از فضای ذخیره‌سازی در PVها می‌باشند که نمی‌توان آن‌ها را بین LVها تقسیم کرد. زمانی که می‌خواهیم یک فضای ذخیره‌سازی با دقت بالا را برای یک LV تعریف کنیم، برای نمونه استفاده از فضای کامل، استفاده از گزینه <code
                    class="literal">-l</code> بر <code
                    class="literal">-L</code> اولویت پیدا می‌کند.
				</div><div
                  class="para">
					همچنین امکان اشاره به مکان فیزیکی یک LV به صورتی که محدوده آن درون یک PV مشخص ذخیره‌سازی شود وجود دارد (البته، با استفاده از گزینه‌های اختصاص‌یافته به VG). از آنجا که می‌دانیم <code
                    class="filename">sdb</code> از <code
                    class="filename">sdf</code> سریع‌تر است، اگر بخواهیم برای سرور پایگاه‌داده در مقایسه با سرور فایل برتری قائل شویم می‌توانیم <code
                    class="filename">lv_base</code> را در‌ آن ذخیره کنیم. پس دستور آن می‌شود: <code
                    class="command">lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</code>. به یاد داشته باشید که در صورت نبود محدوده کافی در PV این دستور ناموفق خواهد بود. در مثال ما، برای پیشگیری از این وضعیت، شاید بخواهیم <code
                    class="filename">lv_base</code> را قبل از <code
                    class="filename">lv_files</code> ایجاد یا برخی فضای موجود در <code
                    class="filename">sdb2</code> را با دستور <code
                    class="command">pvmove</code> آزاد کنیم.
				</div></div><div
                class="para">
					گروه‌های مجازی، زمانی که ایجاد گردند، به عنوان فایل‌های دستگاه درون <code
                  class="filename">/dev/mapper/</code> قرار می‌گیرند:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>ls -l /dev/mapper</code></strong>
<code
                  class="computeroutput">total 0
crw------- 1 root root 10, 236 Jun 10 16:52 control
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2
# </code><strong
                  class="userinput"><code>ls -l /dev/dm-*</code></strong>
<code
                  class="computeroutput">brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0
brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1
brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2
</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>یادداشت</em></span> شناسایی خودکار گروه‌های LVM</strong></p></div></div></div><div
                  class="para">
					زمانی که رایانه راه‌اندازی می‌شود واحد سرویس systemd به نام <code
                    class="filename">lvm2-activation</code> به اجرای <code
                    class="command">vgchange -aay</code> می‌پردازد که اینکار گروه‌های مجازی را “فعال” می‌کند: ابتدا به پویش دستگاه‌های موجود می‌پردازد؛ آن‌هایی که توسط گروه‌های فیزیکی برای LVM آماده‌سازی شده‌اند درون زیرسیستم آن قرار می‌گیرند، آن‌هایی که متعلق به گروه‌های مجازی باشند گردآوری شده و گروه‌های مجازی آن أغاز و قابل استفاده می‌گردند. بنابراین هنگام ایجاد یا تغییر گروه‌های LVM نیازی به ویرایش فایل‌های پیکربندی نیست.
				</div><div
                  class="para">
					با این حال، به یاد داشته باشید که ساختار عناصر LVM (گروه‌های فیزیکی و منطقی همراه با گروه‌های دستگاه) در <code
                    class="filename">/etc/lvm/backup</code> پشتیبان‌گیری می‌شوند، که می‌تواند در زمان بروز مشکل (یا اطلاع از عملکرد آن) مورد استفاده قرار گیرد.
				</div></div><div
                class="para">
					برای ساده‌تر کردن کارها، پیوندهای نمادین متعارف نیز در دایرکتوری‌های شامل VGها ایجاد شده است:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>ls -l /dev/vg_critical</code></strong>
<code
                  class="computeroutput">total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0
# </code><strong
                  class="userinput"><code>ls -l /dev/vg_normal</code></strong>
<code
                  class="computeroutput">total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</code></pre><div
                class="para">
					سپس LVها می‌توانند مانند پارتیشن‌های استاندارد مورد استفاده قرار گیرند:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mkfs.ext4 /dev/vg_normal/lv_backups</code></strong>
<code
                  class="computeroutput">mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 3145728 4k blocks and 786432 inodes
Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d
[...]
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </code><strong
                  class="userinput"><code>mkdir /srv/backups</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mount /dev/vg_normal/lv_backups /srv/backups</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/backups</code></strong>
<code
                  class="computeroutput">Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_normal-lv_backups   12G   30M   12G   1% /srv/backups
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>cat /etc/fstab</code></strong>
<code
                  class="computeroutput">[...]
/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2
/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2
/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</code></pre><div
                class="para">
					از دید برنامه‌های کاربردی، تعداد بیشمار پارتیشن‌ها اکنون به یک دستگاه بزرگ ۱۲ گیگابایت تبدیل شده است که نام راحت‌تری نیز دارد.
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        xmlns=""
                        id="sect.lvm-over-time"></a>12.1.2.3. LVM در گذر زمان</h4></div></div></div><div
                class="para">
					با اینکه توانایی گردآوری پارتیشن‌‌ها یا دیسک‌های فیزیکی بسیار متداول است، این تنها مزیت استفاده از LVM نیست. انعطاف‌پذیری آن در گذر زمان و تغییر رویکرد ذخیره‌سازی، مشخص می‌شود. در مثال ما، تصور کنیم که فایل‌های بزرگ جدیدی قرار است درون سرور فایل قرار گیرند که LV اختصاص‌یافته به آن گنجایش کافی را ندارد. از آنجا که از تمام فضای <code
                  class="filename">vg_critical</code> استفاده نکرده‌ایم، می‌توانیم <code
                  class="filename">lv_files</code> را گسترش دهیم. برای این منظور، با استفاده از دستور <code
                  class="command">lvresize</code> و <code
                  class="command">resize2fs</code> برای سازگاری فایل سیستم اینکار صورت می‌گیرد:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/files/</code></strong>
<code
                  class="computeroutput">Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files
# </code><strong
                  class="userinput"><code>lvdisplay -C vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 5.00g
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   2   2   0 wz--n- 8.09g 2.09g
# </code><strong
                  class="userinput"><code>lvresize -L 7G vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).
  Logical volume lv_files successfully resized
# </code><strong
                  class="userinput"><code>lvdisplay -C vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 7.00g
# </code><strong
                  class="userinput"><code>resize2fs /dev/vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">resize2fs 1.42.12 (29-Aug-2014)
Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.

# </code><strong
                  class="userinput"><code>df -h /srv/files/</code></strong>
<code
                  class="computeroutput">Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>احتیاط</em></span> تغییر اندازه فایل سیستم‌ها</strong></p></div></div></div><div
                  class="para">
					تمام فایل سیستم‌ها نمی‌توانند به صورت آنلاین تغییر اندازه یابند؛ تغییر اندازه یک دستگاه ابتدا نیازمند قطع اتصال آن به فایل سیستم سپس اتصال مجدد آن می‌شود. البته، اگر کسی بخواهد فضای اختصاص‌یافته به یک LV را کاهش دهد، ابتدا فایل سیستم باید کاهش پیدا کند؛ این ترتیب در زمان افزایش اندازه برعکس می‌شود: گروه مجازی قبل از فایل سیستم موجود در آن باید افزایش پیدا کند. این فرآیند بسیار واضح است، چرا که در هر زمان اندازه فایل سیستم نباید از اندازه دستگاه بلاک-محور روی آن بیشتر باشد (خواه این دستگاه یک پارتیشن فیزیکی باشد یا یک گروه مجازی).
				</div><div
                  class="para">
					فایل سیستم‌های ext3، ext4 و zfs بدون نیاز به قطع اتصال می‌توانند افزایش یابند؛ کاهش اندازه نیازمند قطع اتصال است. فایل سیستم reiserfs امکان تغییر اندازه آنلاین را در دو جهت فراهم می‌سازد. ext2 مقدس، اما نیازمند قطع اتصال در دو جهت است.
				</div></div><div
                class="para">
					برای توسعه گروهی که از پایگاه‌داده میزبانی می‌کند نیز به همین ترتیب می‌توان اقدام کرد، با این تفاوت که به انتهای فضای ذخیره‌سازی موجود VG رسیدیم:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/base/</code></strong>
<code
                  class="computeroutput">Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_base 1008M  854M  104M  90% /srv/base
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree 
  vg_critical   2   2   0 wz--n- 8.09g 92.00m</code></pre><div
                class="para">
					ایرادی ندارد، چرا که LVM امکان افزودن گروه‌های فیزیکی را به گروه‌های دستگاه موجود فراهم می‌سازد. برای نمونه، شاید متوجه شده‌ایم پارتیشن <code
                  class="filename">sdb1</code>، که خارج از LVM استفاده می‌شود، تنها شامل بایگانی‌هایی است که می‌تواند به <code
                  class="filename">lv_backups</code> انتقال یابد. اکنون می‌توانیم آن را بازیابی کرده و درون گروه مجازی قرار دهیم، در نتیجه برخی فضای موجود را احیا می‌کنیم. اینکار با استفاده از دستور <code
                  class="command">vgextend</code> صورت می‌گیرد. البته که پارتیشن ابتدا باید به صورت یک گروه فیزیکی آماده شود. زمانی که VG گسترش یافت، از دستورات مشابه می‌توانیم برای افزایش اندازه گروه مجازی و فایل سیستم استفاده کنیم:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvcreate /dev/sdb1</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdb1" successfully created
# </code><strong
                  class="userinput"><code>vgextend vg_critical /dev/sdb1</code></strong>
<code
                  class="computeroutput">  Volume group "vg_critical" successfully extended
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   3   2   0 wz--n- 9.09g 1.09g
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>df -h /srv/base/</code></strong>
<code
                  class="computeroutput">Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>مطالعه بیشتر</em></span> LVM پیشرفته</strong></p></div></div></div><div
                  class="para">
					LVM همچنین کاربردهای پیشرفته‌تری نیز دارد که جزئیات آن می‌تواند به صورت دستی وارد شود. برای نمونه، یک مدیر سیستم می‌تواند اندازه بلاک‌های گروه‌های فیزیکی و منطقی را تغییر دهد، همین طور ساختار فیزیکی آن‌ها را. همچنین امکان انتقال بلاک‌ها بین PVها موجود است، برای نمونه به منظور بهبود عملکرد یا آزاد کردن یک PV زمانی که نیاز به استخراج یک دیسک فیزیکی منطبق با خود از VG باشد (خواه با تاثیر روی VG دیگر یا حذف از LVM به صورت کلی). صفحات راهنمای دستورات معمولا واضح بوده و جزئیات بیشتری را مطرح می‌کنند. یک نقطه شروع مناسب صفحه راهنمای <span
                    class="citerefentry"><span
                      class="refentrytitle">lvm</span>(8)</span> است.
				</div></div></div></div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      xmlns=""
                      id="sect.raid-or-lvm"></a>12.1.3. RAID یا LVM؟</h3></div></div></div><div
              class="para">
				RAID و LVM بدون تردید دارای مزایایی هستند که در صورت کنار گذاشتن فرضیه یک رایانه رومیزی همراه با یک هارد دیسک که کارکرد آن در طول زمان تغییر نمی‌کند، مشخص می‌شوند. اگرچه، RAID و LVM در دو جهت مختلف حرکت می‌کنند و اهداف متفاوتی نیز دارند، که گاهی تصمیم‌گیری درباره استفاده صحیح از آن‌ها را دشوار می‌سازد. مناسب‌ترین پاسخ بستگی به نیازمندی‌های فعلی و قابل پیشبینی سیستم در آینده دارد.
			</div><div
              class="para">
				موارد ساده‌ای وجود دارد که پرسشی درباره آن‌ها مطرح نمی‌شود. اگر نیازمندی این باشد که داده برابر نقص سخت‌افزاری محافظت گردد، به طور مشخص باید از RAID به صورت آرایه‌ای از دیسک‌ها استفاده کرد، چرا که LVM درباره این مشکل راهکاری ندارد. از طرف دیگر، اگر نیازمندی این باشد که یک طرح ذخیره‌سازی انعطاف‌پذیر از گروه‌های مستقل ساختار فیزیکی دیسک‌ها تشکیل گردد، به طور مشخص باید از LVM استفاده کرد چرا که RAID درباره این مشکل راهکاری ندارد.
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>یادداشت</em></span> اگر عملکرد مهم باشد...</strong></p></div></div></div><div
                class="para">
				اگر سرعت ورودی/خروجی مطرح باشد، به خصوص در مورد زمان دسترسی، استفاده از LVM و/یا RAID با یکی از ترکیبات موجود ممکن است روی عملکرد تاثیر منفی بگذارد، که اینکار روی تصمیم‌گیری درباره انتخاب هر کدام اثر می‌گذارد. اگرچه، این تفاوت‌ها در عملکرد بسیار ناچیز بوده و تنها در چند مورد خاص قابل اندازه‌گیری می‌باشند. اگر عملکرد مهم باشد، بهترین گزینه استفاده از رسانه ذخیره‌سازی غیرقابل-چرخش است (<a
                  id="id-1.15.4.8.4.2.1"
                  class="indexterm"></a><span
                  class="emphasis"><em>solid-state drives</em></span> یا SSD)؛ هزینه آن‌ها برای هر مگابایت از دیسک‌ها معمولی بالاتر و ظرفیت کمتری نسبت به آن‌ها دارند، اما عملکرد فوق‌العاده‌ای درباره دسترسی تصادفی به حافظه دارند. اگر الگوی کارکرد شامل عملیات ورودی/خروجی باشد که در میان فایل سیستم پراکنده شده است، برای نمونه پرس و جوهای پیچیده پایگاه‌داده، آنگاه مزیت اجرای آن‌ها روی SSD به مراتب بیشتر از استفاده LVM یا RAID می‌تواند باشد. در این مواقع، انتخاب باید توسط سایر نیازمندی‌ها بجز سرعت صورت بگیرد، چرا که جنبه عملکرد آن به راحتی توسط SSD مدیریت می‌شود.
			</div></div><div
              class="para">
				سومین کارکرد قابل ذکر زمانی است که بخواهیم دو دیسک را در یک گروه بزرگ‌تر قرار دهیم، خواه به دلایل عملکرد یا داشتن یک فایل سیستم بزرگ‌تر از فضای هر کدام از دیسک‌ها). اینکار با استفاده از یک RAID-0 (یا حتی RAID خطی) و گروه LVM انجام می‌شود. در این موقعیت، بجز محدودیت‌های خارجی (برای نمونه، در چارچوب سایر رایانه‌ها بودن اگر آن‌ها نیز تنها از RAID استفاده کنند)، پیکربندی مورد نظر معمولا LVM خواهد بود. راه‌اندازی اولیه به ندرت پیچیده‌تر خواهد بود و این افزایش پیچیدگی در صورت تغییر نیازمندی‌ها یا افزودن دیسک‌ها جدید قابلیت انعطاف‌پذیری بیشتری را فراهم می‌آورد.
			</div><div
              class="para">
				البته یک مورد بسیار جالب نیز وجود دارد، که سیستم ذخیره‌سازی هم باید برابر نقص سخت‌افزاری مقاوم هم انعطاف‌پذیری لازم درباره اختصاص گروه‌های مختلف را داشته باشد. هیچ یک از راهکارهای RAID یا LVM به تنهایی نمی‌توانند این نیازمندی را پوشش دهند؛ اینجا است که از هر دو به صورت همزمان استفاده می‌کنیم - یا یکی بر فراز دیگری. طرح کلی در این مورد و با توجه به اینکه این دو فناوری به بلوغ رسیده‌اند این است که ابتدا با گروه‌بندی دیسک‌ها در تعداد آرایه‌های کوچک RAID از افزونگی داده اطمینان حاصل کرد سپس از این آرایه‌ها برای گروه‌های فیزیکی LVM استفاده کنیم؛ پارتیشن‌های منطقی از این LVها برای فایل سیستم بوجود می‌آیند. نتیجه این تنظیم این است که هنگامی که یک دیسک دچار نقص می‌گردد، تنها تعداد کمی از آرایه‌های RAID نیازمند بازسازی هستند، که اینکار زمان سپری شده توسط مدیر سیستم برای بازیابی را کاهش می‌دهد.
			</div><div
              class="para">
				بیایید یک مثال واقعی را بررسی کنیم: دپارتمان روابط عمومی در شرکت فالکوت نیازمند یک سیستم برای ویرایش تصویر است، اما هزینه‌های دپارتمان اجازه سرمایه‌گذاری در سخت‌افزارهای گران قیمت را نمی‌دهد. تصمیم گرفته شد که از سخت‌افزار مربوط به کار گرافیکی (مانیتور و کارت گرافیک) و از سخت‌افزار متداول تنها برای ذخیره‌سازی اطلاعات استفاده شود. اگرچه، همانطور که مشخص است، ویدیو دیجیتال نیازمندی ذخیره‌سازی مربوط به خود را دارد: حجم داده قابل ذخیره‌سازی زیاد است، پس نرخ تبادل داده برای خواندن و نوشتن در عملکرد کلی سیستم تاثیرگذار خواهد بود (برای نمونه، بیش از زمان دسترسی متداول). این محدودیت‌ها باید با سخت‌افزار عمومی موجود برطرف گردند که در این مورد دو هارد دیسک ۳۰۰ گیگابایت از نوع SATA می‌باشند؛ داده سیستمی همراه با داده کاربری باید برابر نقص‌های سخت‌افزاری نیز مقاوم گردند. ویدیو کلیپ‌های ویرایش شده باید از امنیت واقعی برخوردار بوده، اما ویدیوهای اولیه که منتظر ویرایش هستند از اهمیت کمتری برخوردارند، چرا که نسخه اصلی آن‌ها روی نوارهای ویدیویی موجود است.
			</div><div
              class="para">
				از RAID-1 و LVM به صورت ترکیبی برای رفع این محدودیت‌ها استفاده شده است. دیسک‌ها به دو کنترلر مختلف SATA به منظور بهینه‌سازی دسترسی موازی و کاهش خطر نقص همزمان متصل شده‌اند که به عنوان <code
                class="filename">sda</code> و <code
                class="filename">sdc</code> ظاهر می‌شوند. آن‌ها با توجه به طرح زیر پارتیشن‌بندی شده‌اند:
			</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>fdisk -l /dev/sda</code></strong>
<code
                class="computeroutput">
Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x00039a9f

Device    Boot     Start       End   Sectors Size Id Type
/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect
/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris
/dev/sda3        4000185 586099395 582099210 298G 5  Extended
/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect
/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect
/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</code></pre><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
						اولین پارتیشن‌های هر دو دیسک (حدود ۱ گیگابایت) به صورت یک آرایه RAID-1 در آمده‌اند، <code
                      class="filename">md0</code>. از این mirror به طور مستقیم برای ذخیره‌سازی فایل سیستم root استفاده می‌شود.
					</div></li><li
                  class="listitem"><div
                    class="para">
						پارتیشن‌های <code
                      class="filename">sda2</code> و <code
                      class="filename">sdc2</code> به عنوان swap استفاده شده‌اند، که مجموع فضای ۲ گیگابایت برای swap را فراهم می‌کنند. با ۱ گیگابایت RAM، رایانه مقدار کافی حافظه موجود را خواهد داشت.
					</div></li><li
                  class="listitem"><div
                    class="para">
						پارتیشن‌های <code
                      class="filename">sda5</code> و <code
                      class="filename">sdc5</code> همراه با <code
                      class="filename">sda6</code> و <code
                      class="filename">sdc6</code> هر کدام به یک آرایه RAID-1 به اندازه ۱۰۰ گیگابایت تقسیم شده‌اند که به نام‌های <code
                      class="filename">md1</code> و <code
                      class="filename">md2</code> موجود هستند. هر یک از این mirrorها به عنوان گروه‌های فیزیکی برای LVM راه‌اندازی شده‌اند که به گروه آرایه <code
                      class="filename">vg_raid</code> اختصاص یافته‌اند. بنابراین این VG شامل ۲۰۰ گیگابایت فضای امن است.
					</div></li><li
                  class="listitem"><div
                    class="para">
						پارتیشن‌های باقیمانده <code
                      class="filename">sda7</code> و <code
                      class="filename">sdc7</code> به طور مستقیم به عنوان گروه‌های فیزیکی <code
                      class="filename">vg_bulk</code> نامگذاری شده‌اند، که در نهایت فضایی معادل ۲۰۰ گیگابایت را شامل می‌شوند.
					</div></li></ul></div><div
              class="para">
				زمانی که VGها ایجاد شوند، به روشی بسیار منعطف می‌توانند پارتیشن‌بندی گردند. به یاد داشته باشید که LVهای ایجاد شده در <code
                class="filename">vg_raid</code> در صورت نقص دیسک‌ها نیز نگهداری می‌شوند که این مورد درباره LVهای ایجاد شده در <code
                class="filename">vg_bulk</code> صادق نیست؛ از طرف دیگر، مورد دوم به صورت موازی در اختیار هر دو دیسک قرار می‌گیرد، که امکان خواندن یا نوشتن فایل‌های بزرگ را فراهم می‌آورد.
			</div><div
              class="para">
				بنابراین گروه‌های منطقی <code
                class="filename">lv_usr</code>، <code
                class="filename">lv_var</code> و <code
                class="filename">lv_home</code> را در <code
                class="filename">vg_raid</code> ایجاد می‌کنیم تا از فایل سیستم‌های متناسب پشتیبانی گردد؛ از یک گروه منطقی دیگر بنام <code
                class="filename">lv_movies</code> برای ذخیره‌سازی ویدیوهای ویرایش شده استفاده می‌شود. از VG دیگر به منظور تقسیم <code
                class="filename">lv_rushes</code> برای داده‌های ورودی از دوربین‌های دیجیتال و یک <code
                class="filename">lv_tmp</code> برای فایل‌های موقت استفاده خواهد شد. مکان ذخیره‌سازی ناحیه کاری خود انتخاب دیگری است: با اینکه عملکرد خوب برای این آرایه مورد نیاز است، آیا ارزش دارد که کار را در قبال یک نقص سخت‌افزاری حین ویرایش ویدیو از دست بدهیم؟ با توجه به پاسخ این پرسش، LV مرتبط با یک VG یا دیگری ایجاد خواهد شد.
			</div><div
              class="para">
				اکنون هم افزونگی داده برای داده‌های مهم فراهم شده هم انعطاف‌پذیری بهتری در مورد تقسیم فضای موجود بین برنامه‌ها وجود دارد. زمانی که نرم‌افزار مربوطه نصب شود (برای نمونه، ویرایش کلیپ‌های صوتی) LV که از <code
                class="filename">/usr/</code> میزبانی می‌کند می‌تواند به آرامی رشد کند.
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>یادداشت</em></span> چرا از سه آرایه RAID-1 استفاده شد؟</strong></p></div></div></div><div
                class="para">
				می‌توانستیم از یک آرایه RAID-1 به منظور گروه فیزیکی برای <code
                  class="filename">vg_raid</code> استفاده می‌کردیم. پس چرا سه تا ایجاد کردیم؟
			</div><div
                class="para">
				منطق تقسیم اول (<code
                  class="filename">md0</code> مقابل دیگران) درباره امنیت داده است: داده‌ای که روی هر دو عنصر یک mirror از RAID-1 نوشته می‌شود کاملا یکسان است و این امکان وجود دارد که با نادیده‌گرفتن ساختار RAID یکی از دیسک‌ها را به صورت مستقیم متصل کنیم. در صورتی که یک باگ کرنل موجود باشد، برای نمونه اگر اطلاعات جانبی LVM خراب شود، امکان راه‌اندازی حداقلی سیستم برای دسترسی به داده حیاتی مانند ساختار دیسک‌های موجود در آرایه‌های RAID و LVM موجود است؛ این اطلاعات جانبی می‌تواند بازسازی شده و فایل‌ها دوباره قابل دسترس شوند، تا سیستم به حالت عادی خود بازگردد.
			</div><div
                class="para">
				منطق تقسیم دوم (<code
                  class="filename">md1</code> مقابل <code
                  class="filename">md2</code>) از صراحت کمتری برخوردار است، که بیشتر درباره نامشخص بودن آینده دلالت دارد. زمانی که این سیستم جمع‌آوری شد، نیازمندی‌های دقیق ذخیره‌سازی از همان ابتدا مشخص نبود؛ چرا که می‌توانست در گذر زمان تغییر کند. در این مورد، از قبل نمی‌دانیم چه فضایی بابت ذخیره‌سازی ویدیوهای اولیه و کلیپ‌های ویرایش شده نهایی لازم است. اگر یک کلیپ مشخص نیازمند تصاویر اولیه بسیاری باشد و VG اختصاص‌یافته به داده تکراری کمتر از نصف فضای ذخیره‌سازی را داشته باشد، می‌توانیم از برخی فضای استفاده نشده آن بهره ببریم. می‌توانیم یکی از گروه‌های فیزیکی مانند <code
                  class="filename">md2</code> را از <code
                  class="filename">vg_raid</code> حذف کرده و به طور مستقیم به <code
                  class="filename">vg_bulk</code> اختصاص دهیم (اگر مدت زمان انتظار رفته از عملیات به اندازه‌ای کم باشد که این کاهش عملکرد سیستم را تحمل کنیم) یا تنظیم RAID را روی <code
                  class="filename">md2</code> لغو کرده و اجزای آن یعنی <code
                  class="filename">sda6</code> و <code
                  class="filename">sdc6</code> را درون VG دیگر قرار دهیم (که بجای ۱۰۰ می‌شود ۲۰۰ گیگابایت)؛ گروه منطقی <code
                  class="filename">lv_rushes</code> سپس می‌تواند متناسب با نیازمندی‌های افزایش یابد.
			</div></div></div></div></div><ul
        class="docnav"><li
          class="previous"><a
            accesskey="p"
            href="sect.rtc-services.html"><strong>قبلی</strong>11.8. سرویس‌های ارتباطی بلادرنگ</a></li><li
          class="up"><a
            accesskey="u"
            href="#"><strong>بالا</strong></a></li><li
          class="home"><a
            accesskey="h"
            href="index.html"><strong>خانه</strong></a></li><li
          class="next"><a
            accesskey="n"
            href="sect.virtualization.html"><strong>بعدی</strong>12.2. مجازی‌سازی</a></li></ul></body></html>
